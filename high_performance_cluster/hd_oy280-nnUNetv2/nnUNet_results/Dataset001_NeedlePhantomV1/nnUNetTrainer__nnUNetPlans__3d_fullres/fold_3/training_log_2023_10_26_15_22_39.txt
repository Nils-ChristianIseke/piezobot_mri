
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [24, 56, 40], 'median_image_size_in_voxels': [22.0, 56.0, 36.0], 'spacing': [2.419999837875366, 1.46875, 1.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [2, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_NeedlePhantomV1', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.419999837875366, 1.46875, 1.46875], 'original_median_shape_after_transp': [22, 56, 36], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1610.0, 'mean': 465.1097412109375, 'median': 478.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 1246.0, 'std': 241.4937744140625}}} 
 
2023-10-26 15:22:39.683564: unpacking dataset... 
2023-10-26 15:22:51.424764: unpacking done... 
2023-10-26 15:22:51.425568: do_dummy_2d_data_aug: False 
2023-10-26 15:22:51.426420: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-26 15:22:51.426971: The split file contains 5 splits. 
2023-10-26 15:22:51.427174: Desired fold for training: 3 
2023-10-26 15:22:51.427364: This split has 35 training and 9 validation cases. 
2023-10-26 15:23:01.840168: Unable to plot network architecture: 
2023-10-26 15:23:01.840738: 'torch._C.Node' object is not subscriptable 
2023-10-26 15:23:01.866702:  
2023-10-26 15:23:01.866952: Epoch 0 
2023-10-26 15:23:01.867234: Current learning rate: 0.01 
2023-10-26 15:23:11.209174: train_loss -0.1779 
2023-10-26 15:23:11.209616: val_loss -0.492 
2023-10-26 15:23:11.209907: Pseudo dice [0.7767, 0.822, 0.9511, 0.0, 0.8819] 
2023-10-26 15:23:11.210155: Epoch time: 9.34 s 
2023-10-26 15:23:11.210375: Yayy! New best EMA pseudo Dice: 0.6863 
2023-10-26 15:23:12.195837:  
2023-10-26 15:23:12.196129: Epoch 1 
2023-10-26 15:23:12.196380: Current learning rate: 0.00999 
2023-10-26 15:23:16.572320: train_loss -0.5273 
2023-10-26 15:23:16.572756: val_loss -0.545 
2023-10-26 15:23:16.573081: Pseudo dice [0.7901, 0.8758, 0.9539, 0.0, 0.8973] 
2023-10-26 15:23:16.573396: Epoch time: 4.38 s 
2023-10-26 15:23:16.573673: Yayy! New best EMA pseudo Dice: 0.6881 
2023-10-26 15:23:17.643734:  
2023-10-26 15:23:17.644062: Epoch 2 
2023-10-26 15:23:17.644318: Current learning rate: 0.00998 
2023-10-26 15:23:22.003528: train_loss -0.5803 
2023-10-26 15:23:22.003950: val_loss -0.6232 
2023-10-26 15:23:22.004222: Pseudo dice [0.838, 0.8755, 0.956, 0.0, 0.9095] 
2023-10-26 15:23:22.004464: Epoch time: 4.36 s 
2023-10-26 15:23:22.004699: Yayy! New best EMA pseudo Dice: 0.6908 
2023-10-26 15:23:23.276944:  
2023-10-26 15:23:23.277237: Epoch 3 
2023-10-26 15:23:23.277487: Current learning rate: 0.00997 
2023-10-26 15:23:27.611027: train_loss -0.6103 
2023-10-26 15:23:27.611441: val_loss -0.6262 
2023-10-26 15:23:27.611734: Pseudo dice [0.8479, 0.8931, 0.9601, 0.0, 0.9036] 
2023-10-26 15:23:27.612103: Epoch time: 4.33 s 
2023-10-26 15:23:27.612455: Yayy! New best EMA pseudo Dice: 0.6938 
2023-10-26 15:23:28.731640:  
2023-10-26 15:23:28.731941: Epoch 4 
2023-10-26 15:23:28.732206: Current learning rate: 0.00996 
2023-10-26 15:23:33.053817: train_loss -0.6238 
2023-10-26 15:23:33.054292: val_loss -0.6492 
2023-10-26 15:23:33.054595: Pseudo dice [0.8647, 0.9046, 0.9612, 0.0, 0.9165] 
2023-10-26 15:23:33.054866: Epoch time: 4.32 s 
2023-10-26 15:23:33.055215: Yayy! New best EMA pseudo Dice: 0.6974 
2023-10-26 15:23:34.165118:  
2023-10-26 15:23:34.165418: Epoch 5 
2023-10-26 15:23:34.165762: Current learning rate: 0.00995 
2023-10-26 15:23:38.399609: train_loss -0.6348 
2023-10-26 15:23:38.400026: val_loss -0.6774 
2023-10-26 15:23:38.400301: Pseudo dice [0.855, 0.8865, 0.959, 0.4251, 0.9061] 
2023-10-26 15:23:38.400545: Epoch time: 4.24 s 
2023-10-26 15:23:38.400910: Yayy! New best EMA pseudo Dice: 0.7083 
2023-10-26 15:23:39.482100:  
2023-10-26 15:23:39.482412: Epoch 6 
2023-10-26 15:23:39.482677: Current learning rate: 0.00995 
2023-10-26 15:23:43.832870: train_loss -0.6597 
2023-10-26 15:23:43.833607: val_loss -0.6757 
2023-10-26 15:23:43.833930: Pseudo dice [0.8418, 0.8908, 0.9587, 0.4017, 0.9111] 
2023-10-26 15:23:43.834194: Epoch time: 4.35 s 
2023-10-26 15:23:43.834440: Yayy! New best EMA pseudo Dice: 0.7175 
2023-10-26 15:23:44.960423:  
2023-10-26 15:23:44.960743: Epoch 7 
2023-10-26 15:23:44.961014: Current learning rate: 0.00994 
2023-10-26 15:23:49.328845: train_loss -0.6723 
2023-10-26 15:23:49.329540: val_loss -0.6852 
2023-10-26 15:23:49.329844: Pseudo dice [0.8596, 0.9011, 0.9577, 0.5248, 0.9149] 
2023-10-26 15:23:49.330106: Epoch time: 4.37 s 
2023-10-26 15:23:49.330339: Yayy! New best EMA pseudo Dice: 0.7289 
2023-10-26 15:23:50.509436:  
2023-10-26 15:23:50.509852: Epoch 8 
2023-10-26 15:23:50.510263: Current learning rate: 0.00993 
2023-10-26 15:23:54.948374: train_loss -0.6608 
2023-10-26 15:23:54.948908: val_loss -0.6762 
2023-10-26 15:23:54.949182: Pseudo dice [0.8513, 0.8836, 0.9565, 0.5661, 0.9078] 
2023-10-26 15:23:54.949417: Epoch time: 4.44 s 
2023-10-26 15:23:54.949638: Yayy! New best EMA pseudo Dice: 0.7394 
2023-10-26 15:23:56.252179:  
2023-10-26 15:23:56.252556: Epoch 9 
2023-10-26 15:23:56.252822: Current learning rate: 0.00992 
2023-10-26 15:24:00.425750: train_loss -0.6735 
2023-10-26 15:24:00.426223: val_loss -0.6783 
2023-10-26 15:24:00.426559: Pseudo dice [0.8524, 0.8903, 0.9592, 0.5208, 0.9095] 
2023-10-26 15:24:00.426846: Epoch time: 4.17 s 
2023-10-26 15:24:00.427123: Yayy! New best EMA pseudo Dice: 0.7481 
2023-10-26 15:24:01.522565:  
2023-10-26 15:24:01.522915: Epoch 10 
2023-10-26 15:24:01.523177: Current learning rate: 0.00991 
2023-10-26 15:24:05.598195: train_loss -0.6744 
2023-10-26 15:24:05.598576: val_loss -0.68 
2023-10-26 15:24:05.598842: Pseudo dice [0.8547, 0.8942, 0.9608, 0.4743, 0.9248] 
2023-10-26 15:24:05.599096: Epoch time: 4.08 s 
2023-10-26 15:24:05.599308: Yayy! New best EMA pseudo Dice: 0.7554 
2023-10-26 15:24:06.738721:  
2023-10-26 15:24:06.739041: Epoch 11 
2023-10-26 15:24:06.739306: Current learning rate: 0.0099 
2023-10-26 15:24:11.047724: train_loss -0.6885 
2023-10-26 15:24:11.048109: val_loss -0.6839 
2023-10-26 15:24:11.048384: Pseudo dice [0.8628, 0.8929, 0.9616, 0.5178, 0.922] 
2023-10-26 15:24:11.048635: Epoch time: 4.31 s 
2023-10-26 15:24:11.048866: Yayy! New best EMA pseudo Dice: 0.763 
2023-10-26 15:24:12.191294:  
2023-10-26 15:24:12.191626: Epoch 12 
2023-10-26 15:24:12.191886: Current learning rate: 0.00989 
2023-10-26 15:24:16.404419: train_loss -0.6902 
2023-10-26 15:24:16.404811: val_loss -0.7039 
2023-10-26 15:24:16.405085: Pseudo dice [0.8675, 0.9113, 0.9634, 0.6309, 0.9235] 
2023-10-26 15:24:16.405348: Epoch time: 4.21 s 
2023-10-26 15:24:16.405576: Yayy! New best EMA pseudo Dice: 0.7727 
2023-10-26 15:24:17.508855:  
2023-10-26 15:24:17.509242: Epoch 13 
2023-10-26 15:24:17.509486: Current learning rate: 0.00988 
2023-10-26 15:24:21.648779: train_loss -0.6894 
2023-10-26 15:24:21.649209: val_loss -0.6992 
2023-10-26 15:24:21.649495: Pseudo dice [0.8627, 0.9074, 0.9621, 0.444, 0.9225] 
2023-10-26 15:24:21.649754: Epoch time: 4.14 s 
2023-10-26 15:24:21.649992: Yayy! New best EMA pseudo Dice: 0.7774 
2023-10-26 15:24:22.784019:  
2023-10-26 15:24:22.784371: Epoch 14 
2023-10-26 15:24:22.784666: Current learning rate: 0.00987 
2023-10-26 15:24:26.996276: train_loss -0.6918 
2023-10-26 15:24:26.996735: val_loss -0.7011 
2023-10-26 15:24:26.997033: Pseudo dice [0.8583, 0.909, 0.9608, 0.5216, 0.9215] 
2023-10-26 15:24:26.997288: Epoch time: 4.21 s 
2023-10-26 15:24:26.997520: Yayy! New best EMA pseudo Dice: 0.7831 
2023-10-26 15:24:28.127361:  
2023-10-26 15:24:28.127717: Epoch 15 
2023-10-26 15:24:28.128059: Current learning rate: 0.00986 
2023-10-26 15:24:32.275067: train_loss -0.6749 
2023-10-26 15:24:32.275441: val_loss -0.6859 
2023-10-26 15:24:32.275711: Pseudo dice [0.857, 0.8988, 0.9619, 0.5058, 0.9262] 
2023-10-26 15:24:32.275950: Epoch time: 4.15 s 
2023-10-26 15:24:32.276175: Yayy! New best EMA pseudo Dice: 0.7877 
2023-10-26 15:24:33.615994:  
2023-10-26 15:24:33.616307: Epoch 16 
2023-10-26 15:24:33.616569: Current learning rate: 0.00986 
2023-10-26 15:24:37.750699: train_loss -0.6896 
2023-10-26 15:24:37.751118: val_loss -0.6764 
2023-10-26 15:24:37.751415: Pseudo dice [0.8393, 0.8813, 0.9568, 0.5289, 0.9162] 
2023-10-26 15:24:37.751656: Epoch time: 4.14 s 
2023-10-26 15:24:37.751886: Yayy! New best EMA pseudo Dice: 0.7914 
2023-10-26 15:24:38.923234:  
2023-10-26 15:24:38.923566: Epoch 17 
2023-10-26 15:24:38.923882: Current learning rate: 0.00985 
2023-10-26 15:24:42.955574: train_loss -0.6942 
2023-10-26 15:24:42.956001: val_loss -0.682 
2023-10-26 15:24:42.956266: Pseudo dice [0.8571, 0.8934, 0.9635, 0.4948, 0.9196] 
2023-10-26 15:24:42.956509: Epoch time: 4.03 s 
2023-10-26 15:24:42.956739: Yayy! New best EMA pseudo Dice: 0.7948 
2023-10-26 15:24:44.092895:  
2023-10-26 15:24:44.093199: Epoch 18 
2023-10-26 15:24:44.093454: Current learning rate: 0.00984 
2023-10-26 15:24:48.116262: train_loss -0.6915 
2023-10-26 15:24:48.116684: val_loss -0.6963 
2023-10-26 15:24:48.116962: Pseudo dice [0.8755, 0.9072, 0.9634, 0.6462, 0.9276] 
2023-10-26 15:24:48.117202: Epoch time: 4.02 s 
2023-10-26 15:24:48.117434: Yayy! New best EMA pseudo Dice: 0.8018 
2023-10-26 15:24:49.313179:  
2023-10-26 15:24:49.313500: Epoch 19 
2023-10-26 15:24:49.313766: Current learning rate: 0.00983 
2023-10-26 15:24:53.531220: train_loss -0.6923 
2023-10-26 15:24:53.531618: val_loss -0.6924 
2023-10-26 15:24:53.531898: Pseudo dice [0.8623, 0.9085, 0.9609, 0.6394, 0.9248] 
2023-10-26 15:24:53.532144: Epoch time: 4.22 s 
2023-10-26 15:24:53.532377: Yayy! New best EMA pseudo Dice: 0.8075 
2023-10-26 15:24:54.753277:  
2023-10-26 15:24:54.753584: Epoch 20 
2023-10-26 15:24:54.753876: Current learning rate: 0.00982 
2023-10-26 15:24:58.698481: train_loss -0.693 
2023-10-26 15:24:58.698890: val_loss -0.6993 
2023-10-26 15:24:58.699159: Pseudo dice [0.8589, 0.9101, 0.9645, 0.6044, 0.9138] 
2023-10-26 15:24:58.699398: Epoch time: 3.95 s 
2023-10-26 15:24:58.699612: Yayy! New best EMA pseudo Dice: 0.8118 
2023-10-26 15:25:00.004961:  
2023-10-26 15:25:00.005297: Epoch 21 
2023-10-26 15:25:00.005562: Current learning rate: 0.00981 
2023-10-26 15:25:03.895568: train_loss -0.6888 
2023-10-26 15:25:03.895960: val_loss -0.7021 
2023-10-26 15:25:03.896239: Pseudo dice [0.857, 0.9063, 0.9631, 0.332, 0.9144] 
2023-10-26 15:25:03.896484: Epoch time: 3.89 s 
2023-10-26 15:25:04.924706:  
2023-10-26 15:25:04.925017: Epoch 22 
2023-10-26 15:25:04.925264: Current learning rate: 0.0098 
2023-10-26 15:25:08.969573: train_loss -0.6867 
2023-10-26 15:25:08.969985: val_loss -0.7016 
2023-10-26 15:25:08.970308: Pseudo dice [0.8658, 0.9057, 0.963, 0.4396, 0.9222] 
2023-10-26 15:25:08.970572: Epoch time: 4.05 s 
2023-10-26 15:25:10.000133:  
2023-10-26 15:25:10.000507: Epoch 23 
2023-10-26 15:25:10.000790: Current learning rate: 0.00979 
2023-10-26 15:25:14.040632: train_loss -0.6928 
2023-10-26 15:25:14.040997: val_loss -0.6953 
2023-10-26 15:25:14.041267: Pseudo dice [0.8765, 0.905, 0.9623, 0.5394, 0.8914] 
2023-10-26 15:25:14.041510: Epoch time: 4.04 s 
2023-10-26 15:25:14.041734: Yayy! New best EMA pseudo Dice: 0.8134 
2023-10-26 15:25:15.111099:  
2023-10-26 15:25:15.111406: Epoch 24 
2023-10-26 15:25:15.111649: Current learning rate: 0.00978 
2023-10-26 15:25:19.123300: train_loss -0.6987 
2023-10-26 15:25:19.123713: val_loss -0.7047 
2023-10-26 15:25:19.123999: Pseudo dice [0.865, 0.9114, 0.9619, 0.6742, 0.9285] 
2023-10-26 15:25:19.124272: Epoch time: 4.01 s 
2023-10-26 15:25:19.124494: Yayy! New best EMA pseudo Dice: 0.8189 
2023-10-26 15:25:20.233371:  
2023-10-26 15:25:20.233676: Epoch 25 
2023-10-26 15:25:20.233933: Current learning rate: 0.00977 
2023-10-26 15:25:24.235979: train_loss -0.7026 
2023-10-26 15:25:24.236726: val_loss -0.7067 
2023-10-26 15:25:24.237026: Pseudo dice [0.8653, 0.9091, 0.9646, 0.5939, 0.9222] 
2023-10-26 15:25:24.237261: Epoch time: 4.0 s 
2023-10-26 15:25:24.237490: Yayy! New best EMA pseudo Dice: 0.8221 
2023-10-26 15:25:25.412069:  
2023-10-26 15:25:25.412468: Epoch 26 
2023-10-26 15:25:25.412756: Current learning rate: 0.00977 
2023-10-26 15:25:29.378375: train_loss -0.727 
2023-10-26 15:25:29.378793: val_loss -0.7901 
2023-10-26 15:25:29.379109: Pseudo dice [0.8612, 0.9021, 0.9627, 0.0099, 0.9241] 
2023-10-26 15:25:29.379355: Epoch time: 3.97 s 
2023-10-26 15:25:30.419622:  
2023-10-26 15:25:30.419936: Epoch 27 
2023-10-26 15:25:30.420182: Current learning rate: 0.00976 
2023-10-26 15:25:34.658185: train_loss -0.759 
2023-10-26 15:25:34.658544: val_loss -0.7537 
2023-10-26 15:25:34.658817: Pseudo dice [0.8651, 0.9006, 0.9611, 0.5192, 0.9093] 
2023-10-26 15:25:34.659058: Epoch time: 4.24 s 
2023-10-26 15:25:35.707983:  
2023-10-26 15:25:35.708295: Epoch 28 
2023-10-26 15:25:35.708548: Current learning rate: 0.00975 
2023-10-26 15:25:39.798666: train_loss -0.7322 
2023-10-26 15:25:39.799059: val_loss -0.7743 
2023-10-26 15:25:39.799328: Pseudo dice [0.8564, 0.8626, 0.9563, 0.5348, 0.9086] 
2023-10-26 15:25:39.799563: Epoch time: 4.09 s 
2023-10-26 15:25:40.887783:  
2023-10-26 15:25:40.888092: Epoch 29 
2023-10-26 15:25:40.888346: Current learning rate: 0.00974 
2023-10-26 15:25:44.961784: train_loss -0.7482 
2023-10-26 15:25:44.962157: val_loss -0.7916 
2023-10-26 15:25:44.962437: Pseudo dice [0.8635, 0.8951, 0.9595, 0.2019, 0.9138] 
2023-10-26 15:25:44.962678: Epoch time: 4.07 s 
2023-10-26 15:25:46.045449:  
2023-10-26 15:25:46.045760: Epoch 30 
2023-10-26 15:25:46.046038: Current learning rate: 0.00973 
2023-10-26 15:25:50.145347: train_loss -0.7724 
2023-10-26 15:25:50.145783: val_loss -0.802 
2023-10-26 15:25:50.146072: Pseudo dice [0.8648, 0.9064, 0.9633, 0.0, 0.9134] 
2023-10-26 15:25:50.146313: Epoch time: 4.1 s 
2023-10-26 15:25:51.204145:  
2023-10-26 15:25:51.204452: Epoch 31 
2023-10-26 15:25:51.204720: Current learning rate: 0.00972 
2023-10-26 15:25:55.406771: train_loss -0.7691 
2023-10-26 15:25:55.407121: val_loss -0.742 
2023-10-26 15:25:55.407376: Pseudo dice [0.854, 0.9006, 0.9626, 0.6813, 0.9207] 
2023-10-26 15:25:55.407614: Epoch time: 4.2 s 
2023-10-26 15:25:56.506099:  
2023-10-26 15:25:56.506410: Epoch 32 
2023-10-26 15:25:56.506665: Current learning rate: 0.00971 
2023-10-26 15:26:00.689667: train_loss -0.7756 
2023-10-26 15:26:00.690089: val_loss -0.7827 
2023-10-26 15:26:00.690355: Pseudo dice [0.8517, 0.899, 0.9607, 0.4804, 0.9243] 
2023-10-26 15:26:00.690602: Epoch time: 4.18 s 
2023-10-26 15:26:01.947608:  
2023-10-26 15:26:01.947920: Epoch 33 
2023-10-26 15:26:01.948169: Current learning rate: 0.0097 
2023-10-26 15:26:06.144914: train_loss -0.7516 
2023-10-26 15:26:06.145384: val_loss -0.8001 
2023-10-26 15:26:06.145788: Pseudo dice [0.87, 0.909, 0.9596, 0.6612, 0.9174] 
2023-10-26 15:26:06.146090: Epoch time: 4.2 s 
2023-10-26 15:26:07.220811:  
2023-10-26 15:26:07.221109: Epoch 34 
2023-10-26 15:26:07.221351: Current learning rate: 0.00969 
2023-10-26 15:26:11.399597: train_loss -0.7702 
2023-10-26 15:26:11.399997: val_loss -0.7771 
2023-10-26 15:26:11.400268: Pseudo dice [0.8677, 0.9, 0.9629, 0.5998, 0.9239] 
2023-10-26 15:26:11.400508: Epoch time: 4.18 s 
2023-10-26 15:26:12.484302:  
2023-10-26 15:26:12.484609: Epoch 35 
2023-10-26 15:26:12.484860: Current learning rate: 0.00968 
2023-10-26 15:26:16.541303: train_loss -0.7611 
2023-10-26 15:26:16.541788: val_loss -0.8058 
2023-10-26 15:26:16.542122: Pseudo dice [0.8641, 0.9043, 0.9607, 0.6912, 0.9286] 
2023-10-26 15:26:16.542565: Epoch time: 4.06 s 
2023-10-26 15:26:16.542843: Yayy! New best EMA pseudo Dice: 0.8242 
2023-10-26 15:26:17.800092:  
2023-10-26 15:26:17.800413: Epoch 36 
2023-10-26 15:26:17.800667: Current learning rate: 0.00968 
2023-10-26 15:26:21.904452: train_loss -0.7764 
2023-10-26 15:26:21.904892: val_loss -0.8177 
2023-10-26 15:26:21.905161: Pseudo dice [0.8661, 0.885, 0.9583, 0.6139, 0.9175] 
2023-10-26 15:26:21.905403: Epoch time: 4.1 s 
2023-10-26 15:26:21.905638: Yayy! New best EMA pseudo Dice: 0.8266 
2023-10-26 15:26:23.094233:  
2023-10-26 15:26:23.094528: Epoch 37 
2023-10-26 15:26:23.094778: Current learning rate: 0.00967 
2023-10-26 15:26:27.171828: train_loss -0.7661 
2023-10-26 15:26:27.172217: val_loss -0.789 
2023-10-26 15:26:27.172536: Pseudo dice [0.8539, 0.9011, 0.9639, 0.7664, 0.9247] 
2023-10-26 15:26:27.172801: Epoch time: 4.08 s 
2023-10-26 15:26:27.173028: Yayy! New best EMA pseudo Dice: 0.8321 
2023-10-26 15:26:28.366723:  
2023-10-26 15:26:28.367057: Epoch 38 
2023-10-26 15:26:28.367309: Current learning rate: 0.00966 
2023-10-26 15:26:32.425199: train_loss -0.7892 
2023-10-26 15:26:32.425598: val_loss -0.8091 
2023-10-26 15:26:32.425905: Pseudo dice [0.8673, 0.9083, 0.9603, 0.6529, 0.9076] 
2023-10-26 15:26:32.426149: Epoch time: 4.06 s 
2023-10-26 15:26:32.426381: Yayy! New best EMA pseudo Dice: 0.8348 
2023-10-26 15:26:33.730232:  
2023-10-26 15:26:33.730536: Epoch 39 
2023-10-26 15:26:33.730793: Current learning rate: 0.00965 
2023-10-26 15:26:37.688655: train_loss -0.7715 
2023-10-26 15:26:37.689034: val_loss -0.8132 
2023-10-26 15:26:37.689448: Pseudo dice [0.8519, 0.9086, 0.9605, 0.6573, 0.9189] 
2023-10-26 15:26:37.689764: Epoch time: 3.96 s 
2023-10-26 15:26:37.690197: Yayy! New best EMA pseudo Dice: 0.8373 
2023-10-26 15:26:38.840120:  
2023-10-26 15:26:38.840427: Epoch 40 
2023-10-26 15:26:38.840692: Current learning rate: 0.00964 
2023-10-26 15:26:42.645611: train_loss -0.7838 
2023-10-26 15:26:42.645987: val_loss -0.8281 
2023-10-26 15:26:42.646268: Pseudo dice [0.8611, 0.8987, 0.964, 0.1356, 0.9216] 
2023-10-26 15:26:42.646506: Epoch time: 3.81 s 
2023-10-26 15:26:43.784722:  
2023-10-26 15:26:43.785042: Epoch 41 
2023-10-26 15:26:43.785287: Current learning rate: 0.00963 
2023-10-26 15:26:47.838122: train_loss -0.7706 
2023-10-26 15:26:47.838565: val_loss -0.802 
2023-10-26 15:26:47.838933: Pseudo dice [0.8656, 0.9099, 0.9616, 0.6184, 0.927] 
2023-10-26 15:26:47.839189: Epoch time: 4.05 s 
2023-10-26 15:26:48.892023:  
2023-10-26 15:26:48.892336: Epoch 42 
2023-10-26 15:26:48.892587: Current learning rate: 0.00962 
2023-10-26 15:26:52.813156: train_loss -0.7893 
2023-10-26 15:26:52.813528: val_loss -0.8022 
2023-10-26 15:26:52.813798: Pseudo dice [0.8693, 0.9095, 0.9635, 0.7533, 0.9253] 
2023-10-26 15:26:52.814080: Epoch time: 3.92 s 
2023-10-26 15:26:53.848507:  
2023-10-26 15:26:53.848825: Epoch 43 
2023-10-26 15:26:53.849077: Current learning rate: 0.00961 
2023-10-26 15:26:57.916162: train_loss -0.7907 
2023-10-26 15:26:57.916676: val_loss -0.8035 
2023-10-26 15:26:57.917056: Pseudo dice [0.8707, 0.9078, 0.9625, 0.6899, 0.9307] 
2023-10-26 15:26:57.917374: Epoch time: 4.07 s 
2023-10-26 15:26:57.917624: Yayy! New best EMA pseudo Dice: 0.8407 
2023-10-26 15:26:59.025469:  
2023-10-26 15:26:59.025775: Epoch 44 
2023-10-26 15:26:59.026033: Current learning rate: 0.0096 
2023-10-26 15:27:03.017233: train_loss -0.7841 
2023-10-26 15:27:03.017630: val_loss -0.8084 
2023-10-26 15:27:03.017901: Pseudo dice [0.8679, 0.9096, 0.9626, 0.7089, 0.9221] 
2023-10-26 15:27:03.018138: Epoch time: 3.99 s 
2023-10-26 15:27:03.018351: Yayy! New best EMA pseudo Dice: 0.844 
2023-10-26 15:27:04.302609:  
2023-10-26 15:27:04.302935: Epoch 45 
2023-10-26 15:27:04.303193: Current learning rate: 0.00959 
2023-10-26 15:27:08.310460: train_loss -0.791 
2023-10-26 15:27:08.310835: val_loss -0.823 
2023-10-26 15:27:08.311442: Pseudo dice [0.8745, 0.9094, 0.9657, 0.708, 0.9269] 
2023-10-26 15:27:08.311827: Epoch time: 4.01 s 
2023-10-26 15:27:08.312132: Yayy! New best EMA pseudo Dice: 0.8473 
2023-10-26 15:27:09.420807:  
2023-10-26 15:27:09.421120: Epoch 46 
2023-10-26 15:27:09.421379: Current learning rate: 0.00959 
2023-10-26 15:27:13.489694: train_loss -0.7854 
2023-10-26 15:27:13.490075: val_loss -0.8092 
2023-10-26 15:27:13.490354: Pseudo dice [0.8687, 0.9111, 0.9641, 0.7352, 0.9252] 
2023-10-26 15:27:13.490597: Epoch time: 4.07 s 
2023-10-26 15:27:13.490829: Yayy! New best EMA pseudo Dice: 0.8507 
2023-10-26 15:27:14.587396:  
2023-10-26 15:27:14.587706: Epoch 47 
2023-10-26 15:27:14.587956: Current learning rate: 0.00958 
2023-10-26 15:27:18.693891: train_loss -0.7733 
2023-10-26 15:27:18.694309: val_loss -0.7896 
2023-10-26 15:27:18.694608: Pseudo dice [0.8698, 0.9007, 0.9609, 0.7353, 0.9258] 
2023-10-26 15:27:18.694856: Epoch time: 4.11 s 
2023-10-26 15:27:18.695082: Yayy! New best EMA pseudo Dice: 0.8534 
2023-10-26 15:27:19.839979:  
2023-10-26 15:27:19.840286: Epoch 48 
2023-10-26 15:27:19.840540: Current learning rate: 0.00957 
2023-10-26 15:27:23.877787: train_loss -0.7795 
2023-10-26 15:27:23.878139: val_loss -0.8009 
2023-10-26 15:27:23.878407: Pseudo dice [0.8696, 0.9029, 0.962, 0.6572, 0.9298] 
2023-10-26 15:27:23.878654: Epoch time: 4.04 s 
2023-10-26 15:27:23.878885: Yayy! New best EMA pseudo Dice: 0.8545 
2023-10-26 15:27:25.037037:  
2023-10-26 15:27:25.037372: Epoch 49 
2023-10-26 15:27:25.037755: Current learning rate: 0.00956 
2023-10-26 15:27:28.916478: train_loss -0.7902 
2023-10-26 15:27:28.916952: val_loss -0.8119 
2023-10-26 15:27:28.917273: Pseudo dice [0.8711, 0.9073, 0.9654, 0.5065, 0.9254] 
2023-10-26 15:27:28.917614: Epoch time: 3.88 s 
2023-10-26 15:27:30.032472:  
2023-10-26 15:27:30.032796: Epoch 50 
2023-10-26 15:27:30.033072: Current learning rate: 0.00955 
2023-10-26 15:27:34.126040: train_loss -0.7721 
2023-10-26 15:27:34.126436: val_loss -0.8027 
2023-10-26 15:27:34.126695: Pseudo dice [0.8474, 0.8964, 0.9626, 0.658, 0.9323] 
2023-10-26 15:27:34.126928: Epoch time: 4.09 s 
2023-10-26 15:27:35.185111:  
2023-10-26 15:27:35.185419: Epoch 51 
2023-10-26 15:27:35.185662: Current learning rate: 0.00954 
2023-10-26 15:27:39.175280: train_loss -0.7649 
2023-10-26 15:27:39.175681: val_loss -0.809 
2023-10-26 15:27:39.175964: Pseudo dice [0.8639, 0.9024, 0.9615, 0.8013, 0.9183] 
2023-10-26 15:27:39.176234: Epoch time: 3.99 s 
2023-10-26 15:27:39.176458: Yayy! New best EMA pseudo Dice: 0.8569 
2023-10-26 15:27:40.475109:  
2023-10-26 15:27:40.475450: Epoch 52 
2023-10-26 15:27:40.475707: Current learning rate: 0.00953 
2023-10-26 15:27:44.541977: train_loss -0.7584 
2023-10-26 15:27:44.542331: val_loss -0.785 
2023-10-26 15:27:44.542609: Pseudo dice [0.8646, 0.8863, 0.9579, 0.5818, 0.9224] 
2023-10-26 15:27:44.542842: Epoch time: 4.07 s 
2023-10-26 15:27:45.594651:  
2023-10-26 15:27:45.594976: Epoch 53 
2023-10-26 15:27:45.595217: Current learning rate: 0.00952 
2023-10-26 15:27:49.636965: train_loss -0.7756 
2023-10-26 15:27:49.637378: val_loss -0.8075 
2023-10-26 15:27:49.637647: Pseudo dice [0.8597, 0.9005, 0.9623, 0.6786, 0.9305] 
2023-10-26 15:27:49.637915: Epoch time: 4.04 s 
2023-10-26 15:27:50.726294:  
2023-10-26 15:27:50.726658: Epoch 54 
2023-10-26 15:27:50.726917: Current learning rate: 0.00951 
2023-10-26 15:27:54.752947: train_loss -0.7831 
2023-10-26 15:27:54.753355: val_loss -0.8041 
2023-10-26 15:27:54.753695: Pseudo dice [0.8665, 0.8994, 0.9626, 0.021, 0.9215] 
2023-10-26 15:27:54.754018: Epoch time: 4.03 s 
2023-10-26 15:27:55.800261:  
2023-10-26 15:27:55.800624: Epoch 55 
2023-10-26 15:27:55.800878: Current learning rate: 0.0095 
2023-10-26 15:27:59.896978: train_loss -0.7811 
2023-10-26 15:27:59.897391: val_loss -0.8091 
2023-10-26 15:27:59.897676: Pseudo dice [0.8673, 0.9133, 0.9625, 0.7269, 0.9246] 
2023-10-26 15:27:59.898005: Epoch time: 4.1 s 
2023-10-26 15:28:00.957356:  
2023-10-26 15:28:00.957698: Epoch 56 
2023-10-26 15:28:00.957955: Current learning rate: 0.00949 
2023-10-26 15:28:04.971468: train_loss -0.7922 
2023-10-26 15:28:04.972021: val_loss -0.797 
2023-10-26 15:28:04.972294: Pseudo dice [0.8589, 0.9068, 0.9642, 0.6986, 0.9187] 
2023-10-26 15:28:04.972549: Epoch time: 4.01 s 
2023-10-26 15:28:06.029310:  
2023-10-26 15:28:06.029618: Epoch 57 
2023-10-26 15:28:06.029891: Current learning rate: 0.00949 
2023-10-26 15:28:10.135692: train_loss -0.7833 
2023-10-26 15:28:10.136133: val_loss -0.7973 
2023-10-26 15:28:10.136408: Pseudo dice [0.8682, 0.9046, 0.9633, 0.7053, 0.9255] 
2023-10-26 15:28:10.136648: Epoch time: 4.11 s 
2023-10-26 15:28:11.375668:  
2023-10-26 15:28:11.375984: Epoch 58 
2023-10-26 15:28:11.376234: Current learning rate: 0.00948 
2023-10-26 15:28:15.518415: train_loss -0.7951 
2023-10-26 15:28:15.518938: val_loss -0.8124 
2023-10-26 15:28:15.519258: Pseudo dice [0.8718, 0.911, 0.9634, 0.7292, 0.9308] 
2023-10-26 15:28:15.519591: Epoch time: 4.14 s 
2023-10-26 15:28:16.589913:  
2023-10-26 15:28:16.590244: Epoch 59 
2023-10-26 15:28:16.590496: Current learning rate: 0.00947 
2023-10-26 15:28:20.574781: train_loss -0.7976 
2023-10-26 15:28:20.575148: val_loss -0.8322 
2023-10-26 15:28:20.575414: Pseudo dice [0.8719, 0.9078, 0.9647, 0.6765, 0.9303] 
2023-10-26 15:28:20.575649: Epoch time: 3.99 s 
2023-10-26 15:28:21.648313:  
2023-10-26 15:28:21.648637: Epoch 60 
2023-10-26 15:28:21.648893: Current learning rate: 0.00946 
2023-10-26 15:28:25.845188: train_loss -0.7833 
2023-10-26 15:28:25.845603: val_loss -0.8054 
2023-10-26 15:28:25.845893: Pseudo dice [0.8657, 0.907, 0.9636, 0.6926, 0.9267] 
2023-10-26 15:28:25.846143: Epoch time: 4.2 s 
2023-10-26 15:28:25.846374: Yayy! New best EMA pseudo Dice: 0.8581 
2023-10-26 15:28:26.988370:  
2023-10-26 15:28:26.988680: Epoch 61 
2023-10-26 15:28:26.988933: Current learning rate: 0.00945 
2023-10-26 15:28:31.164590: train_loss -0.7933 
2023-10-26 15:28:31.165034: val_loss -0.7977 
2023-10-26 15:28:31.165395: Pseudo dice [0.8689, 0.9138, 0.9636, 0.6473, 0.9234] 
2023-10-26 15:28:31.165778: Epoch time: 4.18 s 
2023-10-26 15:28:31.166144: Yayy! New best EMA pseudo Dice: 0.8587 
2023-10-26 15:28:32.299507:  
2023-10-26 15:28:32.299812: Epoch 62 
2023-10-26 15:28:32.300060: Current learning rate: 0.00944 
2023-10-26 15:28:36.423340: train_loss -0.7846 
2023-10-26 15:28:36.423766: val_loss -0.8006 
2023-10-26 15:28:36.424039: Pseudo dice [0.8703, 0.9084, 0.9611, 0.6814, 0.9202] 
2023-10-26 15:28:36.424279: Epoch time: 4.12 s 
2023-10-26 15:28:36.424497: Yayy! New best EMA pseudo Dice: 0.8596 
2023-10-26 15:28:37.559172:  
2023-10-26 15:28:37.559468: Epoch 63 
2023-10-26 15:28:37.559728: Current learning rate: 0.00943 
2023-10-26 15:28:41.493304: train_loss -0.7822 
2023-10-26 15:28:41.493699: val_loss -0.8137 
2023-10-26 15:28:41.493964: Pseudo dice [0.8668, 0.9113, 0.9635, 0.6533, 0.9267] 
2023-10-26 15:28:41.494211: Epoch time: 3.93 s 
2023-10-26 15:28:41.494439: Yayy! New best EMA pseudo Dice: 0.8601 
2023-10-26 15:28:42.865806:  
2023-10-26 15:28:42.866107: Epoch 64 
2023-10-26 15:28:42.866381: Current learning rate: 0.00942 
2023-10-26 15:28:46.931835: train_loss -0.7986 
2023-10-26 15:28:46.932321: val_loss -0.7865 
2023-10-26 15:28:46.932745: Pseudo dice [0.8718, 0.9136, 0.9633, 0.7452, 0.9261] 
2023-10-26 15:28:46.933114: Epoch time: 4.07 s 
2023-10-26 15:28:46.933373: Yayy! New best EMA pseudo Dice: 0.8625 
2023-10-26 15:28:48.077654:  
2023-10-26 15:28:48.078001: Epoch 65 
2023-10-26 15:28:48.078264: Current learning rate: 0.00941 
2023-10-26 15:28:52.074588: train_loss -0.7936 
2023-10-26 15:28:52.074986: val_loss -0.8314 
2023-10-26 15:28:52.075259: Pseudo dice [0.8709, 0.9156, 0.9636, 0.6938, 0.9294] 
2023-10-26 15:28:52.075496: Epoch time: 4.0 s 
2023-10-26 15:28:52.075711: Yayy! New best EMA pseudo Dice: 0.8637 
2023-10-26 15:28:53.208367:  
2023-10-26 15:28:53.208678: Epoch 66 
2023-10-26 15:28:53.208939: Current learning rate: 0.0094 
2023-10-26 15:28:57.176319: train_loss -0.7903 
2023-10-26 15:28:57.176682: val_loss -0.7855 
2023-10-26 15:28:57.176956: Pseudo dice [0.8673, 0.9131, 0.9649, 0.766, 0.9225] 
2023-10-26 15:28:57.177267: Epoch time: 3.97 s 
2023-10-26 15:28:57.177490: Yayy! New best EMA pseudo Dice: 0.866 
2023-10-26 15:28:58.336035:  
2023-10-26 15:28:58.336391: Epoch 67 
2023-10-26 15:28:58.336653: Current learning rate: 0.00939 
2023-10-26 15:29:02.336588: train_loss -0.7896 
2023-10-26 15:29:02.337106: val_loss -0.7958 
2023-10-26 15:29:02.337473: Pseudo dice [0.8638, 0.9133, 0.9635, 0.6478, 0.9273] 
2023-10-26 15:29:02.337746: Epoch time: 4.0 s 
2023-10-26 15:29:03.433200:  
2023-10-26 15:29:03.433501: Epoch 68 
2023-10-26 15:29:03.433748: Current learning rate: 0.00939 
2023-10-26 15:29:07.440408: train_loss -0.7826 
2023-10-26 15:29:07.440795: val_loss -0.8257 
2023-10-26 15:29:07.441072: Pseudo dice [0.8624, 0.9114, 0.9652, 0.5523, 0.9266] 
2023-10-26 15:29:07.441379: Epoch time: 4.01 s 
2023-10-26 15:29:08.554223:  
2023-10-26 15:29:08.554544: Epoch 69 
2023-10-26 15:29:08.554790: Current learning rate: 0.00938 
2023-10-26 15:29:12.640229: train_loss -0.7897 
2023-10-26 15:29:12.640599: val_loss -0.8102 
2023-10-26 15:29:12.640860: Pseudo dice [0.8603, 0.9055, 0.9621, 0.7281, 0.9133] 
2023-10-26 15:29:12.641100: Epoch time: 4.09 s 
2023-10-26 15:29:13.715148:  
2023-10-26 15:29:13.715440: Epoch 70 
2023-10-26 15:29:13.715681: Current learning rate: 0.00937 
2023-10-26 15:29:17.578521: train_loss -0.7885 
2023-10-26 15:29:17.578928: val_loss -0.8117 
2023-10-26 15:29:17.579217: Pseudo dice [0.874, 0.9171, 0.9672, 0.7125, 0.9343] 
2023-10-26 15:29:17.579463: Epoch time: 3.86 s 
2023-10-26 15:29:17.579689: Yayy! New best EMA pseudo Dice: 0.8662 
2023-10-26 15:29:18.901286:  
2023-10-26 15:29:18.901584: Epoch 71 
2023-10-26 15:29:18.901834: Current learning rate: 0.00936 
2023-10-26 15:29:22.819671: train_loss -0.7962 
2023-10-26 15:29:22.820080: val_loss -0.8226 
2023-10-26 15:29:22.820343: Pseudo dice [0.8521, 0.9157, 0.9653, 0.7241, 0.9271] 
2023-10-26 15:29:22.820580: Epoch time: 3.92 s 
2023-10-26 15:29:22.820796: Yayy! New best EMA pseudo Dice: 0.8673 
2023-10-26 15:29:23.979897:  
2023-10-26 15:29:23.980217: Epoch 72 
2023-10-26 15:29:23.980591: Current learning rate: 0.00935 
2023-10-26 15:29:28.039112: train_loss -0.7994 
2023-10-26 15:29:28.039487: val_loss -0.8131 
2023-10-26 15:29:28.039754: Pseudo dice [0.8744, 0.9145, 0.9635, 0.6969, 0.9377] 
2023-10-26 15:29:28.039998: Epoch time: 4.06 s 
2023-10-26 15:29:28.040222: Yayy! New best EMA pseudo Dice: 0.8683 
2023-10-26 15:29:29.199524:  
2023-10-26 15:29:29.199837: Epoch 73 
2023-10-26 15:29:29.200087: Current learning rate: 0.00934 
2023-10-26 15:29:33.145679: train_loss -0.8045 
2023-10-26 15:29:33.146071: val_loss -0.8052 
2023-10-26 15:29:33.146352: Pseudo dice [0.8717, 0.9119, 0.9671, 0.7559, 0.9342] 
2023-10-26 15:29:33.146593: Epoch time: 3.95 s 
2023-10-26 15:29:33.146847: Yayy! New best EMA pseudo Dice: 0.8703 
2023-10-26 15:29:34.310314:  
2023-10-26 15:29:34.310614: Epoch 74 
2023-10-26 15:29:34.310860: Current learning rate: 0.00933 
2023-10-26 15:29:38.330869: train_loss -0.7934 
2023-10-26 15:29:38.331310: val_loss -0.7962 
2023-10-26 15:29:38.331612: Pseudo dice [0.8669, 0.904, 0.9659, 0.8005, 0.9288] 
2023-10-26 15:29:38.331852: Epoch time: 4.02 s 
2023-10-26 15:29:38.332092: Yayy! New best EMA pseudo Dice: 0.8726 
2023-10-26 15:29:39.500640:  
2023-10-26 15:29:39.500953: Epoch 75 
2023-10-26 15:29:39.501208: Current learning rate: 0.00932 
2023-10-26 15:29:43.577788: train_loss -0.7886 
2023-10-26 15:29:43.578252: val_loss -0.8024 
2023-10-26 15:29:43.578674: Pseudo dice [0.8701, 0.9107, 0.9641, 0.7509, 0.9138] 
2023-10-26 15:29:43.579055: Epoch time: 4.08 s 
2023-10-26 15:29:43.579347: Yayy! New best EMA pseudo Dice: 0.8735 
2023-10-26 15:29:44.901539:  
2023-10-26 15:29:44.901844: Epoch 76 
2023-10-26 15:29:44.902116: Current learning rate: 0.00931 
2023-10-26 15:29:48.988563: train_loss -0.7992 
2023-10-26 15:29:48.988946: val_loss -0.793 
2023-10-26 15:29:48.989204: Pseudo dice [0.8697, 0.9043, 0.9657, 0.676, 0.93] 
2023-10-26 15:29:48.989447: Epoch time: 4.09 s 
2023-10-26 15:29:50.091285:  
2023-10-26 15:29:50.091600: Epoch 77 
2023-10-26 15:29:50.091855: Current learning rate: 0.0093 
2023-10-26 15:29:54.137551: train_loss -0.7996 
2023-10-26 15:29:54.137954: val_loss -0.8184 
2023-10-26 15:29:54.138222: Pseudo dice [0.871, 0.9167, 0.9649, 0.7385, 0.9318] 
2023-10-26 15:29:54.138461: Epoch time: 4.05 s 
2023-10-26 15:29:54.138677: Yayy! New best EMA pseudo Dice: 0.8742 
2023-10-26 15:29:55.314216:  
2023-10-26 15:29:55.314544: Epoch 78 
2023-10-26 15:29:55.314833: Current learning rate: 0.0093 
2023-10-26 15:29:59.300141: train_loss -0.794 
2023-10-26 15:29:59.300549: val_loss -0.8095 
2023-10-26 15:29:59.300802: Pseudo dice [0.8773, 0.9114, 0.9642, 0.7888, 0.9323] 
2023-10-26 15:29:59.301034: Epoch time: 3.99 s 
2023-10-26 15:29:59.301249: Yayy! New best EMA pseudo Dice: 0.8763 
2023-10-26 15:30:00.449293:  
2023-10-26 15:30:00.449620: Epoch 79 
2023-10-26 15:30:00.449882: Current learning rate: 0.00929 
2023-10-26 15:30:04.498815: train_loss -0.7992 
2023-10-26 15:30:04.499181: val_loss -0.7993 
2023-10-26 15:30:04.499446: Pseudo dice [0.8686, 0.9117, 0.9653, 0.6275, 0.928] 
2023-10-26 15:30:04.499681: Epoch time: 4.05 s 
2023-10-26 15:30:05.638210:  
2023-10-26 15:30:05.638558: Epoch 80 
2023-10-26 15:30:05.638833: Current learning rate: 0.00928 
2023-10-26 15:30:09.718469: train_loss -0.7974 
2023-10-26 15:30:09.718851: val_loss -0.8221 
2023-10-26 15:30:09.719122: Pseudo dice [0.8703, 0.9137, 0.9661, 0.7041, 0.9295] 
2023-10-26 15:30:09.719347: Epoch time: 4.08 s 
2023-10-26 15:30:10.799126:  
2023-10-26 15:30:10.799446: Epoch 81 
2023-10-26 15:30:10.799707: Current learning rate: 0.00927 
2023-10-26 15:30:14.854303: train_loss -0.788 
2023-10-26 15:30:14.854653: val_loss -0.7977 
2023-10-26 15:30:14.854923: Pseudo dice [0.8752, 0.912, 0.9628, 0.6143, 0.9161] 
2023-10-26 15:30:14.855185: Epoch time: 4.06 s 
2023-10-26 15:30:15.953949:  
2023-10-26 15:30:15.954266: Epoch 82 
2023-10-26 15:30:15.954507: Current learning rate: 0.00926 
2023-10-26 15:30:20.082100: train_loss -0.8017 
2023-10-26 15:30:20.082452: val_loss -0.8166 
2023-10-26 15:30:20.082734: Pseudo dice [0.8719, 0.9071, 0.9643, 0.7963, 0.9168] 
2023-10-26 15:30:20.082969: Epoch time: 4.13 s 
2023-10-26 15:30:21.299225:  
2023-10-26 15:30:21.299563: Epoch 83 
2023-10-26 15:30:21.299820: Current learning rate: 0.00925 
2023-10-26 15:30:25.425647: train_loss -0.7944 
2023-10-26 15:30:25.426013: val_loss -0.7945 
2023-10-26 15:30:25.426287: Pseudo dice [0.87, 0.9109, 0.9651, 0.694, 0.9295] 
2023-10-26 15:30:25.426525: Epoch time: 4.13 s 
2023-10-26 15:30:26.466555:  
2023-10-26 15:30:26.466857: Epoch 84 
2023-10-26 15:30:26.467114: Current learning rate: 0.00924 
2023-10-26 15:30:30.546322: train_loss -0.7957 
2023-10-26 15:30:30.546694: val_loss -0.8135 
2023-10-26 15:30:30.546963: Pseudo dice [0.8578, 0.9112, 0.9622, 0.6919, 0.9303] 
2023-10-26 15:30:30.547203: Epoch time: 4.08 s 
2023-10-26 15:30:31.606822:  
2023-10-26 15:30:31.607153: Epoch 85 
2023-10-26 15:30:31.607452: Current learning rate: 0.00923 
2023-10-26 15:30:35.707044: train_loss -0.7899 
2023-10-26 15:30:35.707476: val_loss -0.7984 
2023-10-26 15:30:35.708009: Pseudo dice [0.8617, 0.9067, 0.9643, 0.6681, 0.9302] 
2023-10-26 15:30:35.708266: Epoch time: 4.1 s 
2023-10-26 15:30:36.773729:  
2023-10-26 15:30:36.774040: Epoch 86 
2023-10-26 15:30:36.774286: Current learning rate: 0.00922 
2023-10-26 15:30:40.889993: train_loss -0.8012 
2023-10-26 15:30:40.890467: val_loss -0.8029 
2023-10-26 15:30:40.890750: Pseudo dice [0.875, 0.9132, 0.9654, 0.7205, 0.9345] 
2023-10-26 15:30:40.890996: Epoch time: 4.12 s 
2023-10-26 15:30:41.914156:  
2023-10-26 15:30:41.914487: Epoch 87 
2023-10-26 15:30:41.914754: Current learning rate: 0.00921 
2023-10-26 15:30:45.772835: train_loss -0.7915 
2023-10-26 15:30:45.773212: val_loss -0.8275 
2023-10-26 15:30:45.773476: Pseudo dice [0.8709, 0.9128, 0.9645, 0.6765, 0.9186] 
2023-10-26 15:30:45.773705: Epoch time: 3.86 s 
2023-10-26 15:30:46.799779:  
2023-10-26 15:30:46.800099: Epoch 88 
2023-10-26 15:30:46.800380: Current learning rate: 0.0092 
2023-10-26 15:30:50.759538: train_loss -0.7993 
2023-10-26 15:30:50.759933: val_loss -0.8126 
2023-10-26 15:30:50.760197: Pseudo dice [0.8699, 0.9129, 0.9659, 0.6597, 0.9364] 
2023-10-26 15:30:50.760433: Epoch time: 3.96 s 
2023-10-26 15:30:51.958773:  
2023-10-26 15:30:51.959108: Epoch 89 
2023-10-26 15:30:51.959363: Current learning rate: 0.0092 
2023-10-26 15:30:55.929375: train_loss -0.7871 
2023-10-26 15:30:55.929800: val_loss -0.8065 
2023-10-26 15:30:55.930079: Pseudo dice [0.8702, 0.8891, 0.9634, 0.7124, 0.9284] 
2023-10-26 15:30:55.930336: Epoch time: 3.97 s 
2023-10-26 15:30:56.960621:  
2023-10-26 15:30:56.960985: Epoch 90 
2023-10-26 15:30:56.961238: Current learning rate: 0.00919 
2023-10-26 15:31:01.187042: train_loss -0.7853 
2023-10-26 15:31:01.187404: val_loss -0.7771 
2023-10-26 15:31:01.187669: Pseudo dice [0.8613, 0.9025, 0.9616, 0.5741, 0.9242] 
2023-10-26 15:31:01.187904: Epoch time: 4.23 s 
2023-10-26 15:31:02.224301:  
2023-10-26 15:31:02.224613: Epoch 91 
2023-10-26 15:31:02.224854: Current learning rate: 0.00918 
2023-10-26 15:31:06.451260: train_loss -0.7921 
2023-10-26 15:31:06.451671: val_loss -0.8018 
2023-10-26 15:31:06.451940: Pseudo dice [0.8716, 0.9126, 0.9639, 0.7058, 0.9317] 
2023-10-26 15:31:06.452174: Epoch time: 4.23 s 
2023-10-26 15:31:07.462416:  
2023-10-26 15:31:07.462735: Epoch 92 
2023-10-26 15:31:07.463022: Current learning rate: 0.00917 
2023-10-26 15:31:11.623066: train_loss -0.8004 
2023-10-26 15:31:11.623444: val_loss -0.8011 
2023-10-26 15:31:11.623712: Pseudo dice [0.8631, 0.9053, 0.9638, 0.7227, 0.9145] 
2023-10-26 15:31:11.623954: Epoch time: 4.16 s 
2023-10-26 15:31:12.644597:  
2023-10-26 15:31:12.644908: Epoch 93 
2023-10-26 15:31:12.645155: Current learning rate: 0.00916 
2023-10-26 15:31:16.719986: train_loss -0.7961 
2023-10-26 15:31:16.720386: val_loss -0.7985 
2023-10-26 15:31:16.720658: Pseudo dice [0.8651, 0.9057, 0.9624, 0.4611, 0.9265] 
2023-10-26 15:31:16.720906: Epoch time: 4.08 s 
2023-10-26 15:31:17.749882:  
2023-10-26 15:31:17.750253: Epoch 94 
2023-10-26 15:31:17.750571: Current learning rate: 0.00915 
2023-10-26 15:31:21.887802: train_loss -0.7856 
2023-10-26 15:31:21.888200: val_loss -0.789 
2023-10-26 15:31:21.888465: Pseudo dice [0.8647, 0.9056, 0.9652, 0.5678, 0.9117] 
2023-10-26 15:31:21.888708: Epoch time: 4.14 s 
2023-10-26 15:31:22.902664:  
2023-10-26 15:31:22.902976: Epoch 95 
2023-10-26 15:31:22.903230: Current learning rate: 0.00914 
2023-10-26 15:31:26.974518: train_loss -0.7915 
2023-10-26 15:31:26.974935: val_loss -0.7975 
2023-10-26 15:31:26.975212: Pseudo dice [0.8735, 0.9087, 0.9631, 0.7343, 0.9357] 
2023-10-26 15:31:26.975451: Epoch time: 4.07 s 
2023-10-26 15:31:28.173707:  
2023-10-26 15:31:28.174038: Epoch 96 
2023-10-26 15:31:28.174293: Current learning rate: 0.00913 
2023-10-26 15:31:32.373093: train_loss -0.7939 
2023-10-26 15:31:32.373498: val_loss -0.8311 
2023-10-26 15:31:32.373772: Pseudo dice [0.8677, 0.9175, 0.9626, 0.4551, 0.93] 
2023-10-26 15:31:32.374020: Epoch time: 4.2 s 
2023-10-26 15:31:33.398693:  
2023-10-26 15:31:33.399020: Epoch 97 
2023-10-26 15:31:33.399263: Current learning rate: 0.00912 
2023-10-26 15:31:37.522009: train_loss -0.8039 
2023-10-26 15:31:37.522396: val_loss -0.8134 
2023-10-26 15:31:37.522653: Pseudo dice [0.8801, 0.9137, 0.9651, 0.7537, 0.9384] 
2023-10-26 15:31:37.522890: Epoch time: 4.12 s 
2023-10-26 15:31:38.582935:  
2023-10-26 15:31:38.583227: Epoch 98 
2023-10-26 15:31:38.583471: Current learning rate: 0.00911 
2023-10-26 15:31:42.668319: train_loss -0.795 
2023-10-26 15:31:42.668729: val_loss -0.8191 
2023-10-26 15:31:42.669014: Pseudo dice [0.8711, 0.9096, 0.9659, 0.7689, 0.9037] 
2023-10-26 15:31:42.669250: Epoch time: 4.09 s 
2023-10-26 15:31:43.711078:  
2023-10-26 15:31:43.711401: Epoch 99 
2023-10-26 15:31:43.711652: Current learning rate: 0.0091 
2023-10-26 15:31:47.878469: train_loss -0.8092 
2023-10-26 15:31:47.878821: val_loss -0.8235 
2023-10-26 15:31:47.879089: Pseudo dice [0.8585, 0.8992, 0.9628, 0.6805, 0.9257] 
2023-10-26 15:31:47.879356: Epoch time: 4.17 s 
2023-10-26 15:31:49.009238:  
2023-10-26 15:31:49.009530: Epoch 100 
2023-10-26 15:31:49.009814: Current learning rate: 0.0091 
2023-10-26 15:31:53.176730: train_loss -0.7925 
2023-10-26 15:31:53.177121: val_loss -0.8046 
2023-10-26 15:31:53.177391: Pseudo dice [0.8649, 0.9164, 0.9627, 0.7003, 0.9279] 
2023-10-26 15:31:53.177633: Epoch time: 4.17 s 
2023-10-26 15:31:54.223824:  
2023-10-26 15:31:54.224129: Epoch 101 
2023-10-26 15:31:54.224381: Current learning rate: 0.00909 
2023-10-26 15:31:58.421258: train_loss -0.8065 
2023-10-26 15:31:58.421778: val_loss -0.7959 
2023-10-26 15:31:58.422169: Pseudo dice [0.8746, 0.9111, 0.9642, 0.6883, 0.9323] 
2023-10-26 15:31:58.422524: Epoch time: 4.2 s 
2023-10-26 15:31:59.464420:  
2023-10-26 15:31:59.464770: Epoch 102 
2023-10-26 15:31:59.465042: Current learning rate: 0.00908 
2023-10-26 15:32:03.519589: train_loss -0.7902 
2023-10-26 15:32:03.519964: val_loss -0.8037 
2023-10-26 15:32:03.520236: Pseudo dice [0.8729, 0.9087, 0.9646, 0.7786, 0.9335] 
2023-10-26 15:32:03.520480: Epoch time: 4.06 s 
2023-10-26 15:32:04.773434:  
2023-10-26 15:32:04.773785: Epoch 103 
2023-10-26 15:32:04.774046: Current learning rate: 0.00907 
2023-10-26 15:32:08.990647: train_loss -0.7864 
2023-10-26 15:32:08.991024: val_loss -0.7928 
2023-10-26 15:32:08.991301: Pseudo dice [0.8714, 0.9112, 0.9655, 0.6416, 0.9321] 
2023-10-26 15:32:08.991533: Epoch time: 4.22 s 
2023-10-26 15:32:10.028103:  
2023-10-26 15:32:10.028396: Epoch 104 
2023-10-26 15:32:10.028647: Current learning rate: 0.00906 
2023-10-26 15:32:14.190271: train_loss -0.7921 
2023-10-26 15:32:14.190668: val_loss -0.8158 
2023-10-26 15:32:14.190933: Pseudo dice [0.8736, 0.915, 0.9643, 0.6941, 0.9316] 
2023-10-26 15:32:14.191194: Epoch time: 4.16 s 
2023-10-26 15:32:15.349567:  
2023-10-26 15:32:15.349892: Epoch 105 
2023-10-26 15:32:15.350430: Current learning rate: 0.00905 
2023-10-26 15:32:19.692034: train_loss -0.797 
2023-10-26 15:32:19.692425: val_loss -0.8167 
2023-10-26 15:32:19.692692: Pseudo dice [0.867, 0.9168, 0.9659, 0.7656, 0.8964] 
2023-10-26 15:32:19.692931: Epoch time: 4.34 s 
2023-10-26 15:32:20.771115:  
2023-10-26 15:32:20.771453: Epoch 106 
2023-10-26 15:32:20.771711: Current learning rate: 0.00904 
2023-10-26 15:32:24.915339: train_loss -0.796 
2023-10-26 15:32:24.915737: val_loss -0.8116 
2023-10-26 15:32:24.916012: Pseudo dice [0.8719, 0.9165, 0.9613, 0.7708, 0.9344] 
2023-10-26 15:32:24.916270: Epoch time: 4.14 s 
2023-10-26 15:32:25.993216:  
2023-10-26 15:32:25.993575: Epoch 107 
2023-10-26 15:32:25.993884: Current learning rate: 0.00903 
2023-10-26 15:32:30.226599: train_loss -0.7935 
2023-10-26 15:32:30.226960: val_loss -0.8028 
2023-10-26 15:32:30.227225: Pseudo dice [0.8655, 0.9151, 0.9638, 0.6872, 0.9183] 
2023-10-26 15:32:30.227462: Epoch time: 4.23 s 
2023-10-26 15:32:31.270779:  
2023-10-26 15:32:31.271091: Epoch 108 
2023-10-26 15:32:31.271350: Current learning rate: 0.00902 
2023-10-26 15:32:35.424270: train_loss -0.7901 
2023-10-26 15:32:35.424659: val_loss -0.7953 
2023-10-26 15:32:35.424942: Pseudo dice [0.8752, 0.9181, 0.9653, 0.8233, 0.9339] 
2023-10-26 15:32:35.425192: Epoch time: 4.15 s 
2023-10-26 15:32:36.639832:  
2023-10-26 15:32:36.640161: Epoch 109 
2023-10-26 15:32:36.640414: Current learning rate: 0.00901 
2023-10-26 15:32:40.809480: train_loss -0.7967 
2023-10-26 15:32:40.809858: val_loss -0.7786 
2023-10-26 15:32:40.810494: Pseudo dice [0.8163, 0.8835, 0.9623, 0.7488, 0.9249] 
2023-10-26 15:32:40.810744: Epoch time: 4.17 s 
2023-10-26 15:32:41.843144:  
2023-10-26 15:32:41.843445: Epoch 110 
2023-10-26 15:32:41.843700: Current learning rate: 0.009 
2023-10-26 15:32:46.071580: train_loss -0.7875 
2023-10-26 15:32:46.072194: val_loss -0.802 
2023-10-26 15:32:46.072526: Pseudo dice [0.8684, 0.9047, 0.9659, 0.6601, 0.929] 
2023-10-26 15:32:46.072811: Epoch time: 4.23 s 
2023-10-26 15:32:47.104108:  
2023-10-26 15:32:47.104405: Epoch 111 
2023-10-26 15:32:47.104650: Current learning rate: 0.009 
2023-10-26 15:32:51.356228: train_loss -0.7962 
2023-10-26 15:32:51.356593: val_loss -0.8049 
2023-10-26 15:32:51.356861: Pseudo dice [0.8715, 0.913, 0.9637, 0.6452, 0.9278] 
2023-10-26 15:32:51.357092: Epoch time: 4.25 s 
2023-10-26 15:32:52.388041:  
2023-10-26 15:32:52.388336: Epoch 112 
2023-10-26 15:32:52.388588: Current learning rate: 0.00899 
2023-10-26 15:32:56.614733: train_loss -0.7977 
2023-10-26 15:32:56.615403: val_loss -0.8187 
2023-10-26 15:32:56.615713: Pseudo dice [0.8716, 0.9162, 0.9666, 0.7967, 0.933] 
2023-10-26 15:32:56.615953: Epoch time: 4.23 s 
2023-10-26 15:32:57.641348:  
2023-10-26 15:32:57.641654: Epoch 113 
2023-10-26 15:32:57.641956: Current learning rate: 0.00898 
2023-10-26 15:33:01.783533: train_loss -0.8004 
2023-10-26 15:33:01.784036: val_loss -0.7798 
2023-10-26 15:33:01.784411: Pseudo dice [0.864, 0.9056, 0.965, 0.4111, 0.9301] 
2023-10-26 15:33:01.784697: Epoch time: 4.14 s 
2023-10-26 15:33:02.827209:  
2023-10-26 15:33:02.827507: Epoch 114 
2023-10-26 15:33:02.827764: Current learning rate: 0.00897 
2023-10-26 15:33:06.977599: train_loss -0.7907 
2023-10-26 15:33:06.977981: val_loss -0.8073 
2023-10-26 15:33:06.978255: Pseudo dice [0.8718, 0.9126, 0.963, 0.7657, 0.9305] 
2023-10-26 15:33:06.978483: Epoch time: 4.15 s 
2023-10-26 15:33:08.009732:  
2023-10-26 15:33:08.010034: Epoch 115 
2023-10-26 15:33:08.010299: Current learning rate: 0.00896 
2023-10-26 15:33:12.155981: train_loss -0.7879 
2023-10-26 15:33:12.156356: val_loss -0.7995 
2023-10-26 15:33:12.156639: Pseudo dice [0.8694, 0.8996, 0.9624, 0.807, 0.9286] 
2023-10-26 15:33:12.156897: Epoch time: 4.15 s 
2023-10-26 15:33:13.403224:  
2023-10-26 15:33:13.403528: Epoch 116 
2023-10-26 15:33:13.403793: Current learning rate: 0.00895 
2023-10-26 15:33:17.609820: train_loss -0.7846 
2023-10-26 15:33:17.610228: val_loss -0.7927 
2023-10-26 15:33:17.610504: Pseudo dice [0.872, 0.9127, 0.9657, 0.5729, 0.9238] 
2023-10-26 15:33:17.610751: Epoch time: 4.21 s 
2023-10-26 15:33:18.670655:  
2023-10-26 15:33:18.670972: Epoch 117 
2023-10-26 15:33:18.671223: Current learning rate: 0.00894 
2023-10-26 15:33:22.781559: train_loss -0.7974 
2023-10-26 15:33:22.782019: val_loss -0.8232 
2023-10-26 15:33:22.782376: Pseudo dice [0.8704, 0.9173, 0.9659, 0.6166, 0.917] 
2023-10-26 15:33:22.782697: Epoch time: 4.11 s 
2023-10-26 15:33:23.842216:  
2023-10-26 15:33:23.842518: Epoch 118 
2023-10-26 15:33:23.842800: Current learning rate: 0.00893 
2023-10-26 15:33:27.958432: train_loss -0.798 
2023-10-26 15:33:27.958800: val_loss -0.7956 
2023-10-26 15:33:27.959076: Pseudo dice [0.8638, 0.9068, 0.9618, 0.7714, 0.9281] 
2023-10-26 15:33:27.959303: Epoch time: 4.12 s 
2023-10-26 15:33:29.008390:  
2023-10-26 15:33:29.008731: Epoch 119 
2023-10-26 15:33:29.008991: Current learning rate: 0.00892 
2023-10-26 15:33:33.136432: train_loss -0.7863 
2023-10-26 15:33:33.136860: val_loss -0.8027 
2023-10-26 15:33:33.137148: Pseudo dice [0.8682, 0.9151, 0.9629, 0.6296, 0.9323] 
2023-10-26 15:33:33.137397: Epoch time: 4.13 s 
2023-10-26 15:33:34.201313:  
2023-10-26 15:33:34.201598: Epoch 120 
2023-10-26 15:33:34.201849: Current learning rate: 0.00891 
2023-10-26 15:33:38.420730: train_loss -0.788 
2023-10-26 15:33:38.421140: val_loss -0.7982 
2023-10-26 15:33:38.421425: Pseudo dice [0.8698, 0.9154, 0.9646, 0.7174, 0.9311] 
2023-10-26 15:33:38.421667: Epoch time: 4.22 s 
2023-10-26 15:33:39.485413:  
2023-10-26 15:33:39.485712: Epoch 121 
2023-10-26 15:33:39.485969: Current learning rate: 0.0089 
2023-10-26 15:33:43.627109: train_loss -0.7999 
2023-10-26 15:33:43.627567: val_loss -0.7949 
2023-10-26 15:33:43.627937: Pseudo dice [0.8731, 0.9148, 0.9664, 0.6216, 0.9253] 
2023-10-26 15:33:43.628256: Epoch time: 4.14 s 
2023-10-26 15:33:44.855920:  
2023-10-26 15:33:44.856233: Epoch 122 
2023-10-26 15:33:44.856485: Current learning rate: 0.00889 
2023-10-26 15:33:49.028618: train_loss -0.7913 
2023-10-26 15:33:49.029057: val_loss -0.7981 
2023-10-26 15:33:49.029336: Pseudo dice [0.8668, 0.9032, 0.9629, 0.6853, 0.9269] 
2023-10-26 15:33:49.029572: Epoch time: 4.17 s 
2023-10-26 15:33:50.078852:  
2023-10-26 15:33:50.079157: Epoch 123 
2023-10-26 15:33:50.079401: Current learning rate: 0.00889 
2023-10-26 15:33:54.207019: train_loss -0.7891 
2023-10-26 15:33:54.207397: val_loss -0.8116 
2023-10-26 15:33:54.207662: Pseudo dice [0.8663, 0.911, 0.9644, 0.7323, 0.9301] 
2023-10-26 15:33:54.207912: Epoch time: 4.13 s 
2023-10-26 15:33:55.293066:  
2023-10-26 15:33:55.293376: Epoch 124 
2023-10-26 15:33:55.293628: Current learning rate: 0.00888 
2023-10-26 15:33:59.545890: train_loss -0.8078 
2023-10-26 15:33:59.546308: val_loss -0.7854 
2023-10-26 15:33:59.546568: Pseudo dice [0.8702, 0.9154, 0.9652, 0.6615, 0.9303] 
2023-10-26 15:33:59.546808: Epoch time: 4.25 s 
2023-10-26 15:34:00.614516:  
2023-10-26 15:34:00.614827: Epoch 125 
2023-10-26 15:34:00.615072: Current learning rate: 0.00887 
2023-10-26 15:34:04.895316: train_loss -0.7946 
2023-10-26 15:34:04.895665: val_loss -0.7871 
2023-10-26 15:34:04.895938: Pseudo dice [0.8639, 0.9096, 0.9646, 0.6418, 0.9257] 
2023-10-26 15:34:04.896173: Epoch time: 4.28 s 
2023-10-26 15:34:05.965270:  
2023-10-26 15:34:05.965629: Epoch 126 
2023-10-26 15:34:05.965944: Current learning rate: 0.00886 
2023-10-26 15:34:10.052449: train_loss -0.8021 
2023-10-26 15:34:10.052878: val_loss -0.8022 
2023-10-26 15:34:10.053154: Pseudo dice [0.8565, 0.899, 0.9628, 0.3533, 0.9323] 
2023-10-26 15:34:10.053458: Epoch time: 4.09 s 
2023-10-26 15:34:11.116162:  
2023-10-26 15:34:11.116460: Epoch 127 
2023-10-26 15:34:11.116715: Current learning rate: 0.00885 
2023-10-26 15:34:15.243057: train_loss -0.8145 
2023-10-26 15:34:15.243462: val_loss -0.8119 
2023-10-26 15:34:15.243727: Pseudo dice [0.8706, 0.9116, 0.9641, 0.7234, 0.9268] 
2023-10-26 15:34:15.243969: Epoch time: 4.13 s 
2023-10-26 15:34:16.320184:  
2023-10-26 15:34:16.320488: Epoch 128 
2023-10-26 15:34:16.320737: Current learning rate: 0.00884 
2023-10-26 15:34:20.399571: train_loss -0.8072 
2023-10-26 15:34:20.400011: val_loss -0.813 
2023-10-26 15:34:20.400295: Pseudo dice [0.8675, 0.924, 0.9642, 0.698, 0.9296] 
2023-10-26 15:34:20.400577: Epoch time: 4.08 s 
2023-10-26 15:34:21.626096:  
2023-10-26 15:34:21.626413: Epoch 129 
2023-10-26 15:34:21.626671: Current learning rate: 0.00883 
2023-10-26 15:34:25.776716: train_loss -0.7971 
2023-10-26 15:34:25.777225: val_loss -0.8213 
2023-10-26 15:34:25.777604: Pseudo dice [0.8708, 0.9229, 0.9647, 0.723, 0.93] 
2023-10-26 15:34:25.777950: Epoch time: 4.15 s 
2023-10-26 15:34:26.862423:  
2023-10-26 15:34:26.862731: Epoch 130 
2023-10-26 15:34:26.862993: Current learning rate: 0.00882 
2023-10-26 15:34:30.911107: train_loss -0.8035 
2023-10-26 15:34:30.911536: val_loss -0.8063 
2023-10-26 15:34:30.911828: Pseudo dice [0.8758, 0.918, 0.9663, 0.6302, 0.9086] 
2023-10-26 15:34:30.912078: Epoch time: 4.05 s 
2023-10-26 15:34:31.977206:  
2023-10-26 15:34:31.977524: Epoch 131 
2023-10-26 15:34:31.977773: Current learning rate: 0.00881 
2023-10-26 15:34:36.074300: train_loss -0.8149 
2023-10-26 15:34:36.074867: val_loss -0.8034 
2023-10-26 15:34:36.075156: Pseudo dice [0.8729, 0.9153, 0.9597, 0.665, 0.9354] 
2023-10-26 15:34:36.075406: Epoch time: 4.1 s 
2023-10-26 15:34:37.141342:  
2023-10-26 15:34:37.141665: Epoch 132 
2023-10-26 15:34:37.141942: Current learning rate: 0.0088 
2023-10-26 15:34:41.256637: train_loss -0.7996 
2023-10-26 15:34:41.257091: val_loss -0.8059 
2023-10-26 15:34:41.257382: Pseudo dice [0.8717, 0.914, 0.9645, 0.5993, 0.9335] 
2023-10-26 15:34:41.257634: Epoch time: 4.12 s 
2023-10-26 15:34:42.353194:  
2023-10-26 15:34:42.353496: Epoch 133 
2023-10-26 15:34:42.353769: Current learning rate: 0.00879 
2023-10-26 15:34:46.442077: train_loss -0.7796 
2023-10-26 15:34:46.442622: val_loss -0.7711 
2023-10-26 15:34:46.443202: Pseudo dice [0.8666, 0.9082, 0.963, 0.6005, 0.9206] 
2023-10-26 15:34:46.443624: Epoch time: 4.09 s 
2023-10-26 15:34:47.547695:  
2023-10-26 15:34:47.547991: Epoch 134 
2023-10-26 15:34:47.548239: Current learning rate: 0.00879 
2023-10-26 15:34:51.570824: train_loss -0.8003 
2023-10-26 15:34:51.571269: val_loss -0.8081 
2023-10-26 15:34:51.571556: Pseudo dice [0.8702, 0.9136, 0.9637, 0.8135, 0.9345] 
2023-10-26 15:34:51.571949: Epoch time: 4.02 s 
2023-10-26 15:34:52.856292:  
2023-10-26 15:34:52.856628: Epoch 135 
2023-10-26 15:34:52.856891: Current learning rate: 0.00878 
2023-10-26 15:34:57.075865: train_loss -0.7995 
2023-10-26 15:34:57.076286: val_loss -0.798 
2023-10-26 15:34:57.076563: Pseudo dice [0.8666, 0.9116, 0.9638, 0.734, 0.9244] 
2023-10-26 15:34:57.076806: Epoch time: 4.22 s 
2023-10-26 15:34:58.157612:  
2023-10-26 15:34:58.158013: Epoch 136 
2023-10-26 15:34:58.158281: Current learning rate: 0.00877 
2023-10-26 15:35:02.351775: train_loss -0.799 
2023-10-26 15:35:02.352195: val_loss -0.8083 
2023-10-26 15:35:02.352469: Pseudo dice [0.8649, 0.9065, 0.9651, 0.7042, 0.9153] 
2023-10-26 15:35:02.352737: Epoch time: 4.19 s 
2023-10-26 15:35:03.460510:  
2023-10-26 15:35:03.460803: Epoch 137 
2023-10-26 15:35:03.461058: Current learning rate: 0.00876 
2023-10-26 15:35:07.593957: train_loss -0.8002 
2023-10-26 15:35:07.594490: val_loss -0.8143 
2023-10-26 15:35:07.594930: Pseudo dice [0.8731, 0.9143, 0.9648, 0.7762, 0.9342] 
2023-10-26 15:35:07.595213: Epoch time: 4.13 s 
2023-10-26 15:35:08.744733:  
2023-10-26 15:35:08.745076: Epoch 138 
2023-10-26 15:35:08.745453: Current learning rate: 0.00875 
2023-10-26 15:35:12.877958: train_loss -0.8008 
2023-10-26 15:35:12.878698: val_loss -0.8096 
2023-10-26 15:35:12.878970: Pseudo dice [0.8687, 0.9063, 0.9629, 0.5231, 0.9235] 
2023-10-26 15:35:12.879346: Epoch time: 4.13 s 
2023-10-26 15:35:13.975671:  
2023-10-26 15:35:13.975976: Epoch 139 
2023-10-26 15:35:13.976231: Current learning rate: 0.00874 
2023-10-26 15:35:18.129347: train_loss -0.7906 
2023-10-26 15:35:18.129711: val_loss -0.8101 
2023-10-26 15:35:18.129984: Pseudo dice [0.8726, 0.908, 0.9616, 0.7857, 0.9272] 
2023-10-26 15:35:18.130217: Epoch time: 4.15 s 
2023-10-26 15:35:19.213225:  
2023-10-26 15:35:19.213557: Epoch 140 
2023-10-26 15:35:19.213823: Current learning rate: 0.00873 
2023-10-26 15:35:23.399921: train_loss -0.7987 
2023-10-26 15:35:23.400341: val_loss -0.8051 
2023-10-26 15:35:23.400613: Pseudo dice [0.8691, 0.9071, 0.9648, 0.7465, 0.9161] 
2023-10-26 15:35:23.400853: Epoch time: 4.19 s 
2023-10-26 15:35:24.467335:  
2023-10-26 15:35:24.467628: Epoch 141 
2023-10-26 15:35:24.467897: Current learning rate: 0.00872 
2023-10-26 15:35:28.501991: train_loss -0.808 
2023-10-26 15:35:28.502373: val_loss -0.8207 
2023-10-26 15:35:28.502631: Pseudo dice [0.8694, 0.9137, 0.9647, 0.7413, 0.9256] 
2023-10-26 15:35:28.502858: Epoch time: 4.04 s 
2023-10-26 15:35:29.738845:  
2023-10-26 15:35:29.739188: Epoch 142 
2023-10-26 15:35:29.739431: Current learning rate: 0.00871 
2023-10-26 15:35:33.927070: train_loss -0.8003 
2023-10-26 15:35:33.927474: val_loss -0.8013 
2023-10-26 15:35:33.927773: Pseudo dice [0.8705, 0.9178, 0.9623, 0.7598, 0.9294] 
2023-10-26 15:35:33.928030: Epoch time: 4.19 s 
2023-10-26 15:35:35.033554:  
2023-10-26 15:35:35.033853: Epoch 143 
2023-10-26 15:35:35.034107: Current learning rate: 0.0087 
2023-10-26 15:35:39.226774: train_loss -0.8054 
2023-10-26 15:35:39.227187: val_loss -0.8047 
2023-10-26 15:35:39.227444: Pseudo dice [0.8678, 0.912, 0.9643, 0.6667, 0.9328] 
2023-10-26 15:35:39.227666: Epoch time: 4.19 s 
2023-10-26 15:35:40.294615:  
2023-10-26 15:35:40.294918: Epoch 144 
2023-10-26 15:35:40.295161: Current learning rate: 0.00869 
2023-10-26 15:35:44.466610: train_loss -0.791 
2023-10-26 15:35:44.467049: val_loss -0.7882 
2023-10-26 15:35:44.467455: Pseudo dice [0.8704, 0.916, 0.9615, 0.6756, 0.9091] 
2023-10-26 15:35:44.468044: Epoch time: 4.17 s 
2023-10-26 15:35:45.555322:  
2023-10-26 15:35:45.555629: Epoch 145 
2023-10-26 15:35:45.555900: Current learning rate: 0.00868 
2023-10-26 15:35:49.682308: train_loss -0.8007 
2023-10-26 15:35:49.682659: val_loss -0.8041 
2023-10-26 15:35:49.682971: Pseudo dice [0.874, 0.9064, 0.9629, 0.6686, 0.9193] 
2023-10-26 15:35:49.683277: Epoch time: 4.13 s 
2023-10-26 15:35:50.782379:  
2023-10-26 15:35:50.782734: Epoch 146 
2023-10-26 15:35:50.783011: Current learning rate: 0.00868 
2023-10-26 15:35:54.911172: train_loss -0.8097 
2023-10-26 15:35:54.911694: val_loss -0.8065 
2023-10-26 15:35:54.912093: Pseudo dice [0.8687, 0.9143, 0.9642, 0.7284, 0.9222] 
2023-10-26 15:35:54.912431: Epoch time: 4.13 s 
2023-10-26 15:35:56.007787:  
2023-10-26 15:35:56.008101: Epoch 147 
2023-10-26 15:35:56.008369: Current learning rate: 0.00867 
2023-10-26 15:36:00.130700: train_loss -0.7957 
2023-10-26 15:36:00.131129: val_loss -0.8207 
2023-10-26 15:36:00.131402: Pseudo dice [0.8743, 0.9129, 0.965, 0.4979, 0.9341] 
2023-10-26 15:36:00.131691: Epoch time: 4.12 s 
2023-10-26 15:36:01.380330:  
2023-10-26 15:36:01.380626: Epoch 148 
2023-10-26 15:36:01.380886: Current learning rate: 0.00866 
2023-10-26 15:36:05.258307: train_loss -0.807 
2023-10-26 15:36:05.258740: val_loss -0.8096 
2023-10-26 15:36:05.259028: Pseudo dice [0.8778, 0.9086, 0.9666, 0.6874, 0.9267] 
2023-10-26 15:36:05.259270: Epoch time: 3.88 s 
2023-10-26 15:36:06.377506:  
2023-10-26 15:36:06.377805: Epoch 149 
2023-10-26 15:36:06.378097: Current learning rate: 0.00865 
2023-10-26 15:36:10.391037: train_loss -0.7965 
2023-10-26 15:36:10.391399: val_loss -0.8151 
2023-10-26 15:36:10.391659: Pseudo dice [0.8637, 0.9113, 0.9631, 0.6605, 0.9274] 
2023-10-26 15:36:10.391905: Epoch time: 4.01 s 
2023-10-26 15:36:11.549787:  
2023-10-26 15:36:11.550122: Epoch 150 
2023-10-26 15:36:11.550387: Current learning rate: 0.00864 
2023-10-26 15:36:15.737837: train_loss -0.81 
2023-10-26 15:36:15.738320: val_loss -0.7979 
2023-10-26 15:36:15.738780: Pseudo dice [0.8671, 0.907, 0.9651, 0.4341, 0.9283] 
2023-10-26 15:36:15.739410: Epoch time: 4.19 s 
2023-10-26 15:36:16.877787:  
2023-10-26 15:36:16.878117: Epoch 151 
2023-10-26 15:36:16.878428: Current learning rate: 0.00863 
2023-10-26 15:36:20.966417: train_loss -0.8081 
2023-10-26 15:36:20.966971: val_loss -0.8138 
2023-10-26 15:36:20.967400: Pseudo dice [0.8627, 0.9062, 0.9623, 0.6223, 0.934] 
2023-10-26 15:36:20.967668: Epoch time: 4.09 s 
2023-10-26 15:36:22.057352:  
2023-10-26 15:36:22.057657: Epoch 152 
2023-10-26 15:36:22.057912: Current learning rate: 0.00862 
2023-10-26 15:36:25.980292: train_loss -0.8099 
2023-10-26 15:36:25.980757: val_loss -0.8093 
2023-10-26 15:36:25.981027: Pseudo dice [0.8615, 0.9143, 0.9642, 0.7494, 0.9289] 
2023-10-26 15:36:25.981285: Epoch time: 3.92 s 
2023-10-26 15:36:27.066034:  
2023-10-26 15:36:27.066338: Epoch 153 
2023-10-26 15:36:27.066593: Current learning rate: 0.00861 
2023-10-26 15:36:31.137579: train_loss -0.8074 
2023-10-26 15:36:31.138020: val_loss -0.8162 
2023-10-26 15:36:31.138283: Pseudo dice [0.8714, 0.9139, 0.9644, 0.6864, 0.9345] 
2023-10-26 15:36:31.138534: Epoch time: 4.07 s 
2023-10-26 15:36:32.442385:  
2023-10-26 15:36:32.442688: Epoch 154 
2023-10-26 15:36:32.442948: Current learning rate: 0.0086 
2023-10-26 15:36:36.659806: train_loss -0.8164 
2023-10-26 15:36:36.660403: val_loss -0.8218 
2023-10-26 15:36:36.660853: Pseudo dice [0.8617, 0.9177, 0.9657, 0.5819, 0.933] 
2023-10-26 15:36:36.661186: Epoch time: 4.22 s 
2023-10-26 15:36:37.812181:  
2023-10-26 15:36:37.812488: Epoch 155 
2023-10-26 15:36:37.812733: Current learning rate: 0.00859 
2023-10-26 15:36:41.914222: train_loss -0.8111 
2023-10-26 15:36:41.914637: val_loss -0.8186 
2023-10-26 15:36:41.915041: Pseudo dice [0.8723, 0.9053, 0.9634, 0.6296, 0.9337] 
2023-10-26 15:36:41.915396: Epoch time: 4.1 s 
2023-10-26 15:36:43.009235:  
2023-10-26 15:36:43.009523: Epoch 156 
2023-10-26 15:36:43.009785: Current learning rate: 0.00858 
2023-10-26 15:36:47.326348: train_loss -0.8049 
2023-10-26 15:36:47.336965: val_loss -0.8162 
2023-10-26 15:36:47.337389: Pseudo dice [0.8709, 0.9182, 0.9658, 0.7253, 0.9328] 
2023-10-26 15:36:47.338463: Epoch time: 4.32 s 
2023-10-26 15:36:48.491230:  
2023-10-26 15:36:48.491540: Epoch 157 
2023-10-26 15:36:48.491791: Current learning rate: 0.00858 
2023-10-26 15:36:52.622780: train_loss -0.8092 
2023-10-26 15:36:52.623182: val_loss -0.814 
2023-10-26 15:36:52.623438: Pseudo dice [0.8665, 0.9108, 0.9644, 0.6732, 0.935] 
2023-10-26 15:36:52.623667: Epoch time: 4.13 s 
2023-10-26 15:36:53.728848:  
2023-10-26 15:36:53.729147: Epoch 158 
2023-10-26 15:36:53.729391: Current learning rate: 0.00857 
2023-10-26 15:36:57.934279: train_loss -0.7978 
2023-10-26 15:36:57.934662: val_loss -0.7971 
2023-10-26 15:36:57.934936: Pseudo dice [0.8651, 0.9163, 0.9653, 0.7251, 0.9257] 
2023-10-26 15:36:57.935176: Epoch time: 4.21 s 
2023-10-26 15:36:59.028786:  
2023-10-26 15:36:59.029076: Epoch 159 
2023-10-26 15:36:59.029320: Current learning rate: 0.00856 
2023-10-26 15:37:03.105726: train_loss -0.8027 
2023-10-26 15:37:03.106101: val_loss -0.8048 
2023-10-26 15:37:03.106394: Pseudo dice [0.8722, 0.9098, 0.963, 0.7833, 0.9313] 
2023-10-26 15:37:03.106644: Epoch time: 4.08 s 
2023-10-26 15:37:04.550231:  
2023-10-26 15:37:04.550617: Epoch 160 
2023-10-26 15:37:04.550865: Current learning rate: 0.00855 
2023-10-26 15:37:08.642164: train_loss -0.812 
2023-10-26 15:37:08.642709: val_loss -0.8278 
2023-10-26 15:37:08.643075: Pseudo dice [0.8752, 0.9106, 0.9642, 0.7054, 0.9361] 
2023-10-26 15:37:08.643370: Epoch time: 4.09 s 
2023-10-26 15:37:09.770771:  
2023-10-26 15:37:09.771116: Epoch 161 
2023-10-26 15:37:09.771369: Current learning rate: 0.00854 
2023-10-26 15:37:13.773392: train_loss -0.8031 
2023-10-26 15:37:13.773781: val_loss -0.8115 
2023-10-26 15:37:13.774146: Pseudo dice [0.866, 0.9112, 0.9609, 0.6243, 0.9328] 
2023-10-26 15:37:13.774387: Epoch time: 4.0 s 
2023-10-26 15:37:14.865994:  
2023-10-26 15:37:14.866345: Epoch 162 
2023-10-26 15:37:14.866592: Current learning rate: 0.00853 
2023-10-26 15:37:18.915709: train_loss -0.7997 
2023-10-26 15:37:18.916092: val_loss -0.816 
2023-10-26 15:37:18.916369: Pseudo dice [0.858, 0.8992, 0.9657, 0.6597, 0.9211] 
2023-10-26 15:37:18.916623: Epoch time: 4.05 s 
2023-10-26 15:37:20.017360:  
2023-10-26 15:37:20.017668: Epoch 163 
2023-10-26 15:37:20.017919: Current learning rate: 0.00852 
2023-10-26 15:37:23.977146: train_loss -0.8063 
2023-10-26 15:37:23.977553: val_loss -0.7967 
2023-10-26 15:37:23.977838: Pseudo dice [0.8732, 0.9104, 0.9657, 0.6881, 0.9303] 
2023-10-26 15:37:23.978084: Epoch time: 3.96 s 
2023-10-26 15:37:25.158259:  
2023-10-26 15:37:25.158560: Epoch 164 
2023-10-26 15:37:25.158813: Current learning rate: 0.00851 
2023-10-26 15:37:29.120172: train_loss -0.8161 
2023-10-26 15:37:29.120583: val_loss -0.8178 
2023-10-26 15:37:29.120844: Pseudo dice [0.8697, 0.9164, 0.9654, 0.7075, 0.9334] 
2023-10-26 15:37:29.121096: Epoch time: 3.96 s 
2023-10-26 15:37:30.192167:  
2023-10-26 15:37:30.192461: Epoch 165 
2023-10-26 15:37:30.192729: Current learning rate: 0.0085 
2023-10-26 15:37:34.591595: train_loss -0.8095 
2023-10-26 15:37:34.704309: val_loss -0.7975 
2023-10-26 15:37:34.704905: Pseudo dice [0.8661, 0.9162, 0.9636, 0.6193, 0.9312] 
2023-10-26 15:37:34.705274: Epoch time: 4.4 s 
2023-10-26 15:37:35.961342:  
2023-10-26 15:37:35.961645: Epoch 166 
2023-10-26 15:37:35.961902: Current learning rate: 0.00849 
2023-10-26 15:37:40.118218: train_loss -0.8164 
2023-10-26 15:37:40.118639: val_loss -0.8087 
2023-10-26 15:37:40.118931: Pseudo dice [0.8701, 0.921, 0.9644, 0.6227, 0.9284] 
2023-10-26 15:37:40.119184: Epoch time: 4.16 s 
2023-10-26 15:37:41.225391:  
2023-10-26 15:37:41.225719: Epoch 167 
2023-10-26 15:37:41.225972: Current learning rate: 0.00848 
2023-10-26 15:37:45.244511: train_loss -0.808 
2023-10-26 15:37:45.259807: val_loss -0.8212 
2023-10-26 15:37:45.260108: Pseudo dice [0.8671, 0.922, 0.9625, 0.377, 0.9248] 
2023-10-26 15:37:45.260347: Epoch time: 4.02 s 
2023-10-26 15:37:46.338339:  
2023-10-26 15:37:46.338653: Epoch 168 
2023-10-26 15:37:46.338904: Current learning rate: 0.00847 
2023-10-26 15:37:50.382399: train_loss -0.8017 
2023-10-26 15:37:50.382830: val_loss -0.8216 
2023-10-26 15:37:50.383126: Pseudo dice [0.8649, 0.9167, 0.966, 0.6199, 0.9315] 
2023-10-26 15:37:50.383418: Epoch time: 4.04 s 
2023-10-26 15:37:51.463518:  
2023-10-26 15:37:51.463835: Epoch 169 
2023-10-26 15:37:51.464086: Current learning rate: 0.00847 
2023-10-26 15:37:55.461989: train_loss -0.8016 
2023-10-26 15:37:55.462355: val_loss -0.8281 
2023-10-26 15:37:55.462613: Pseudo dice [0.8597, 0.9181, 0.9644, 0.3384, 0.9293] 
2023-10-26 15:37:55.462851: Epoch time: 4.0 s 
2023-10-26 15:37:56.543534:  
2023-10-26 15:37:56.543831: Epoch 170 
2023-10-26 15:37:56.544088: Current learning rate: 0.00846 
2023-10-26 15:38:00.617059: train_loss -0.8006 
2023-10-26 15:38:00.617429: val_loss -0.8022 
2023-10-26 15:38:00.617695: Pseudo dice [0.8761, 0.9151, 0.9616, 0.5526, 0.9311] 
2023-10-26 15:38:00.617934: Epoch time: 4.07 s 
2023-10-26 15:38:01.699859:  
2023-10-26 15:38:01.700168: Epoch 171 
2023-10-26 15:38:01.700405: Current learning rate: 0.00845 
2023-10-26 15:38:05.688136: train_loss -0.8016 
2023-10-26 15:38:05.688529: val_loss -0.7852 
2023-10-26 15:38:05.688796: Pseudo dice [0.8659, 0.9147, 0.9635, 0.6275, 0.9318] 
2023-10-26 15:38:05.689045: Epoch time: 3.99 s 
2023-10-26 15:38:06.777699:  
2023-10-26 15:38:06.778012: Epoch 172 
2023-10-26 15:38:06.778263: Current learning rate: 0.00844 
2023-10-26 15:38:10.756373: train_loss -0.7982 
2023-10-26 15:38:10.756796: val_loss -0.7779 
2023-10-26 15:38:10.757080: Pseudo dice [0.8636, 0.9011, 0.9613, 0.7463, 0.9147] 
2023-10-26 15:38:10.757330: Epoch time: 3.98 s 
2023-10-26 15:38:12.034993:  
2023-10-26 15:38:12.035305: Epoch 173 
2023-10-26 15:38:12.035609: Current learning rate: 0.00843 
2023-10-26 15:38:16.139387: train_loss -0.8069 
2023-10-26 15:38:16.139813: val_loss -0.8002 
2023-10-26 15:38:16.140100: Pseudo dice [0.8624, 0.9143, 0.9655, 0.7091, 0.9325] 
2023-10-26 15:38:16.140355: Epoch time: 4.1 s 
2023-10-26 15:38:17.232937:  
2023-10-26 15:38:17.233258: Epoch 174 
2023-10-26 15:38:17.233499: Current learning rate: 0.00842 
2023-10-26 15:38:21.371270: train_loss -0.8173 
2023-10-26 15:38:21.371669: val_loss -0.8178 
2023-10-26 15:38:21.371949: Pseudo dice [0.8668, 0.9115, 0.9642, 0.7429, 0.9373] 
2023-10-26 15:38:21.372197: Epoch time: 4.14 s 
2023-10-26 15:38:22.483990:  
2023-10-26 15:38:22.484293: Epoch 175 
2023-10-26 15:38:22.484562: Current learning rate: 0.00841 
2023-10-26 15:38:26.470573: train_loss -0.8234 
2023-10-26 15:38:26.470971: val_loss -0.8125 
2023-10-26 15:38:26.471305: Pseudo dice [0.8623, 0.9131, 0.9637, 0.717, 0.9236] 
2023-10-26 15:38:26.471637: Epoch time: 3.99 s 
2023-10-26 15:38:27.591248:  
2023-10-26 15:38:27.591585: Epoch 176 
2023-10-26 15:38:27.591837: Current learning rate: 0.0084 
2023-10-26 15:38:31.657054: train_loss -0.8034 
2023-10-26 15:38:31.657455: val_loss -0.7969 
2023-10-26 15:38:31.657722: Pseudo dice [0.8671, 0.909, 0.9645, 0.8107, 0.9336] 
2023-10-26 15:38:31.657981: Epoch time: 4.07 s 
2023-10-26 15:38:32.743068:  
2023-10-26 15:38:32.743373: Epoch 177 
2023-10-26 15:38:32.743663: Current learning rate: 0.00839 
2023-10-26 15:38:36.766891: train_loss -0.8121 
2023-10-26 15:38:36.767352: val_loss -0.8176 
2023-10-26 15:38:36.767630: Pseudo dice [0.854, 0.9072, 0.9638, 0.0, 0.9301] 
2023-10-26 15:38:36.767863: Epoch time: 4.02 s 
2023-10-26 15:38:37.958941:  
2023-10-26 15:38:37.959247: Epoch 178 
2023-10-26 15:38:37.959493: Current learning rate: 0.00838 
2023-10-26 15:38:41.940525: train_loss -0.8056 
2023-10-26 15:38:41.941132: val_loss -0.8073 
2023-10-26 15:38:41.941557: Pseudo dice [0.8772, 0.9107, 0.9673, 0.8058, 0.9361] 
2023-10-26 15:38:41.942324: Epoch time: 3.98 s 
2023-10-26 15:38:43.203692:  
2023-10-26 15:38:43.204004: Epoch 179 
2023-10-26 15:38:43.204254: Current learning rate: 0.00837 
2023-10-26 15:38:47.234743: train_loss -0.8107 
2023-10-26 15:38:47.235136: val_loss -0.8074 
2023-10-26 15:38:47.235422: Pseudo dice [0.8743, 0.9164, 0.965, 0.7794, 0.9332] 
2023-10-26 15:38:47.235658: Epoch time: 4.03 s 
2023-10-26 15:38:48.363748:  
2023-10-26 15:38:48.364064: Epoch 180 
2023-10-26 15:38:48.364345: Current learning rate: 0.00836 
2023-10-26 15:38:52.378995: train_loss -0.8142 
2023-10-26 15:38:52.379722: val_loss -0.8101 
2023-10-26 15:38:52.380067: Pseudo dice [0.8716, 0.9152, 0.9642, 0.7345, 0.9339] 
2023-10-26 15:38:52.380312: Epoch time: 4.02 s 
2023-10-26 15:38:53.489891:  
2023-10-26 15:38:53.490210: Epoch 181 
2023-10-26 15:38:53.490457: Current learning rate: 0.00836 
2023-10-26 15:38:57.562516: train_loss -0.8166 
2023-10-26 15:38:57.562882: val_loss -0.8046 
2023-10-26 15:38:57.563156: Pseudo dice [0.8723, 0.914, 0.9633, 0.7268, 0.9225] 
2023-10-26 15:38:57.563404: Epoch time: 4.07 s 
2023-10-26 15:38:58.651327:  
2023-10-26 15:38:58.651635: Epoch 182 
2023-10-26 15:38:58.651884: Current learning rate: 0.00835 
2023-10-26 15:39:02.629143: train_loss -0.8241 
2023-10-26 15:39:02.629607: val_loss -0.8203 
2023-10-26 15:39:02.629883: Pseudo dice [0.8715, 0.9177, 0.9659, 0.7784, 0.9352] 
2023-10-26 15:39:02.630121: Epoch time: 3.98 s 
2023-10-26 15:39:03.701547:  
2023-10-26 15:39:03.701905: Epoch 183 
2023-10-26 15:39:03.702168: Current learning rate: 0.00834 
2023-10-26 15:39:07.687779: train_loss -0.8219 
2023-10-26 15:39:07.688183: val_loss -0.8081 
2023-10-26 15:39:07.688445: Pseudo dice [0.874, 0.9169, 0.9655, 0.7438, 0.9349] 
2023-10-26 15:39:07.688700: Epoch time: 3.99 s 
2023-10-26 15:39:08.787150:  
2023-10-26 15:39:08.787456: Epoch 184 
2023-10-26 15:39:08.787704: Current learning rate: 0.00833 
2023-10-26 15:39:12.793585: train_loss -0.8247 
2023-10-26 15:39:12.793953: val_loss -0.8062 
2023-10-26 15:39:12.794227: Pseudo dice [0.8686, 0.9205, 0.9627, 0.6344, 0.9316] 
2023-10-26 15:39:12.794468: Epoch time: 4.01 s 
2023-10-26 15:39:14.048471:  
2023-10-26 15:39:14.048789: Epoch 185 
2023-10-26 15:39:14.049049: Current learning rate: 0.00832 
2023-10-26 15:39:18.179533: train_loss -0.819 
2023-10-26 15:39:18.179898: val_loss -0.8292 
2023-10-26 15:39:18.180240: Pseudo dice [0.8726, 0.9194, 0.964, 0.7126, 0.9361] 
2023-10-26 15:39:18.180541: Epoch time: 4.13 s 
2023-10-26 15:39:19.263370:  
2023-10-26 15:39:19.263690: Epoch 186 
2023-10-26 15:39:19.263949: Current learning rate: 0.00831 
2023-10-26 15:39:23.308303: train_loss -0.8235 
2023-10-26 15:39:23.308689: val_loss -0.8052 
2023-10-26 15:39:23.308955: Pseudo dice [0.8684, 0.9137, 0.9649, 0.6357, 0.935] 
2023-10-26 15:39:23.309186: Epoch time: 4.05 s 
2023-10-26 15:39:24.395750:  
2023-10-26 15:39:24.396132: Epoch 187 
2023-10-26 15:39:24.396386: Current learning rate: 0.0083 
2023-10-26 15:39:28.298300: train_loss -0.8218 
2023-10-26 15:39:28.298664: val_loss -0.8101 
2023-10-26 15:39:28.298936: Pseudo dice [0.8711, 0.9129, 0.9637, 0.6504, 0.9268] 
2023-10-26 15:39:28.299172: Epoch time: 3.9 s 
2023-10-26 15:39:29.388933:  
2023-10-26 15:39:29.389256: Epoch 188 
2023-10-26 15:39:29.389515: Current learning rate: 0.00829 
2023-10-26 15:39:33.321207: train_loss -0.8164 
2023-10-26 15:39:33.321626: val_loss -0.7732 
2023-10-26 15:39:33.321903: Pseudo dice [0.8764, 0.9131, 0.9657, 0.6624, 0.9305] 
2023-10-26 15:39:33.322166: Epoch time: 3.93 s 
2023-10-26 15:39:34.413842:  
2023-10-26 15:39:34.414184: Epoch 189 
2023-10-26 15:39:34.414438: Current learning rate: 0.00828 
2023-10-26 15:39:38.577827: train_loss -0.8219 
2023-10-26 15:39:38.578189: val_loss -0.7392 
2023-10-26 15:39:38.578511: Pseudo dice [0.8648, 0.9159, 0.9648, 0.5692, 0.9363] 
2023-10-26 15:39:38.578894: Epoch time: 4.16 s 
2023-10-26 15:39:39.753789:  
2023-10-26 15:39:39.754108: Epoch 190 
2023-10-26 15:39:39.754352: Current learning rate: 0.00827 
2023-10-26 15:39:43.797950: train_loss -0.8217 
2023-10-26 15:39:43.798330: val_loss -0.7812 
2023-10-26 15:39:43.798598: Pseudo dice [0.8714, 0.9169, 0.9632, 0.6967, 0.9317] 
2023-10-26 15:39:43.798837: Epoch time: 4.04 s 
2023-10-26 15:39:45.149419:  
2023-10-26 15:39:45.149722: Epoch 191 
2023-10-26 15:39:45.149975: Current learning rate: 0.00826 
2023-10-26 15:39:49.092086: train_loss -0.8226 
2023-10-26 15:39:49.092526: val_loss -0.8204 
2023-10-26 15:39:49.092862: Pseudo dice [0.8681, 0.9135, 0.9654, 0.7489, 0.9289] 
2023-10-26 15:39:49.093158: Epoch time: 3.94 s 
2023-10-26 15:39:50.214203:  
2023-10-26 15:39:50.214512: Epoch 192 
2023-10-26 15:39:50.214763: Current learning rate: 0.00825 
2023-10-26 15:39:54.250799: train_loss -0.825 
2023-10-26 15:39:54.251215: val_loss -0.7911 
2023-10-26 15:39:54.251490: Pseudo dice [0.8816, 0.9168, 0.9657, 0.6571, 0.9372] 
2023-10-26 15:39:54.251723: Epoch time: 4.04 s 
2023-10-26 15:39:55.350544:  
2023-10-26 15:39:55.350851: Epoch 193 
2023-10-26 15:39:55.351101: Current learning rate: 0.00824 
2023-10-26 15:39:59.429280: train_loss -0.8261 
2023-10-26 15:39:59.429634: val_loss -0.789 
2023-10-26 15:39:59.429893: Pseudo dice [0.8748, 0.9172, 0.9674, 0.6437, 0.9402] 
2023-10-26 15:39:59.430133: Epoch time: 4.08 s 
2023-10-26 15:40:00.557396:  
2023-10-26 15:40:00.557739: Epoch 194 
2023-10-26 15:40:00.558004: Current learning rate: 0.00824 
2023-10-26 15:40:04.602446: train_loss -0.8226 
2023-10-26 15:40:04.602826: val_loss -0.8071 
2023-10-26 15:40:04.603106: Pseudo dice [0.876, 0.9173, 0.9642, 0.6603, 0.9346] 
2023-10-26 15:40:04.603343: Epoch time: 4.05 s 
2023-10-26 15:40:05.702353:  
2023-10-26 15:40:05.702675: Epoch 195 
2023-10-26 15:40:05.702937: Current learning rate: 0.00823 
2023-10-26 15:40:09.778480: train_loss -0.8229 
2023-10-26 15:40:09.781174: val_loss -0.8152 
2023-10-26 15:40:09.781438: Pseudo dice [0.8802, 0.9175, 0.9642, 0.7051, 0.9303] 
2023-10-26 15:40:09.781667: Epoch time: 4.08 s 
2023-10-26 15:40:10.868088:  
2023-10-26 15:40:10.868387: Epoch 196 
2023-10-26 15:40:10.868626: Current learning rate: 0.00822 
2023-10-26 15:40:15.026731: train_loss -0.8312 
2023-10-26 15:40:15.027169: val_loss -0.7822 
2023-10-26 15:40:15.027616: Pseudo dice [0.8703, 0.9172, 0.963, 0.6463, 0.9299] 
2023-10-26 15:40:15.027900: Epoch time: 4.16 s 
2023-10-26 15:40:16.342849:  
2023-10-26 15:40:16.343169: Epoch 197 
2023-10-26 15:40:16.343421: Current learning rate: 0.00821 
2023-10-26 15:40:20.385173: train_loss -0.823 
2023-10-26 15:40:20.385562: val_loss -0.8076 
2023-10-26 15:40:20.385823: Pseudo dice [0.865, 0.9191, 0.9637, 0.7205, 0.9306] 
2023-10-26 15:40:20.386054: Epoch time: 4.04 s 
2023-10-26 15:40:21.489422:  
2023-10-26 15:40:21.489754: Epoch 198 
2023-10-26 15:40:21.490025: Current learning rate: 0.0082 
2023-10-26 15:40:25.481349: train_loss -0.8301 
2023-10-26 15:40:25.481728: val_loss -0.7937 
2023-10-26 15:40:25.482023: Pseudo dice [0.872, 0.9139, 0.9663, 0.6332, 0.9356] 
2023-10-26 15:40:25.482263: Epoch time: 3.99 s 
2023-10-26 15:40:26.572592:  
2023-10-26 15:40:26.572910: Epoch 199 
2023-10-26 15:40:26.573162: Current learning rate: 0.00819 
2023-10-26 15:40:30.706433: train_loss -0.8283 
2023-10-26 15:40:30.706912: val_loss -0.8162 
2023-10-26 15:40:30.707263: Pseudo dice [0.875, 0.9137, 0.9646, 0.6778, 0.9339] 
2023-10-26 15:40:30.707506: Epoch time: 4.13 s 
2023-10-26 15:40:31.884271:  
2023-10-26 15:40:31.884584: Epoch 200 
2023-10-26 15:40:31.884831: Current learning rate: 0.00818 
2023-10-26 15:40:35.932885: train_loss -0.8278 
2023-10-26 15:40:35.933288: val_loss -0.7952 
2023-10-26 15:40:35.933572: Pseudo dice [0.8631, 0.9148, 0.9643, 0.6768, 0.9279] 
2023-10-26 15:40:35.933827: Epoch time: 4.05 s 
2023-10-26 15:40:37.074529:  
2023-10-26 15:40:37.074840: Epoch 201 
2023-10-26 15:40:37.075104: Current learning rate: 0.00817 
2023-10-26 15:40:41.189674: train_loss -0.8256 
2023-10-26 15:40:41.190115: val_loss -0.8084 
2023-10-26 15:40:41.190497: Pseudo dice [0.8721, 0.9204, 0.963, 0.695, 0.9371] 
2023-10-26 15:40:41.190830: Epoch time: 4.12 s 
2023-10-26 15:40:42.299394:  
2023-10-26 15:40:42.299695: Epoch 202 
2023-10-26 15:40:42.299942: Current learning rate: 0.00816 
2023-10-26 15:40:46.367335: train_loss -0.8199 
2023-10-26 15:40:46.367815: val_loss -0.7961 
2023-10-26 15:40:46.368091: Pseudo dice [0.8731, 0.9122, 0.9641, 0.6151, 0.9344] 
2023-10-26 15:40:46.368349: Epoch time: 4.07 s 
2023-10-26 15:40:47.657139:  
2023-10-26 15:40:47.657467: Epoch 203 
2023-10-26 15:40:47.657710: Current learning rate: 0.00815 
2023-10-26 15:40:51.667950: train_loss -0.8138 
2023-10-26 15:40:51.668344: val_loss -0.8085 
2023-10-26 15:40:51.668604: Pseudo dice [0.8732, 0.9107, 0.9633, 0.6876, 0.9299] 
2023-10-26 15:40:51.668837: Epoch time: 4.01 s 
2023-10-26 15:40:52.770469:  
2023-10-26 15:40:52.770786: Epoch 204 
2023-10-26 15:40:52.771188: Current learning rate: 0.00814 
2023-10-26 15:40:56.806768: train_loss -0.8247 
2023-10-26 15:40:56.807173: val_loss -0.8013 
2023-10-26 15:40:56.807455: Pseudo dice [0.869, 0.9122, 0.9658, 0.6271, 0.9339] 
2023-10-26 15:40:56.807692: Epoch time: 4.04 s 
2023-10-26 15:40:57.927134:  
2023-10-26 15:40:57.927448: Epoch 205 
2023-10-26 15:40:57.927713: Current learning rate: 0.00813 
2023-10-26 15:41:02.064837: train_loss -0.8229 
2023-10-26 15:41:02.065228: val_loss -0.8167 
2023-10-26 15:41:02.065686: Pseudo dice [0.8696, 0.9131, 0.9655, 0.6893, 0.9352] 
2023-10-26 15:41:02.065966: Epoch time: 4.14 s 
2023-10-26 15:41:03.152085:  
2023-10-26 15:41:03.152422: Epoch 206 
2023-10-26 15:41:03.152684: Current learning rate: 0.00813 
2023-10-26 15:41:07.269036: train_loss -0.8249 
2023-10-26 15:41:07.269416: val_loss -0.7895 
2023-10-26 15:41:07.269688: Pseudo dice [0.8651, 0.9179, 0.9657, 0.7495, 0.9326] 
2023-10-26 15:41:07.269926: Epoch time: 4.12 s 
2023-10-26 15:41:08.360740:  
2023-10-26 15:41:08.361050: Epoch 207 
2023-10-26 15:41:08.361294: Current learning rate: 0.00812 
2023-10-26 15:41:12.273545: train_loss -0.8279 
2023-10-26 15:41:12.273908: val_loss -0.8212 
2023-10-26 15:41:12.274176: Pseudo dice [0.8793, 0.9131, 0.964, 0.7016, 0.9315] 
2023-10-26 15:41:12.274399: Epoch time: 3.91 s 
2023-10-26 15:41:13.348096:  
2023-10-26 15:41:13.348616: Epoch 208 
2023-10-26 15:41:13.348981: Current learning rate: 0.00811 
2023-10-26 15:41:17.431801: train_loss -0.8258 
2023-10-26 15:41:17.432175: val_loss -0.8097 
2023-10-26 15:41:17.432462: Pseudo dice [0.8711, 0.9103, 0.9646, 0.6663, 0.9332] 
2023-10-26 15:41:17.432700: Epoch time: 4.08 s 
2023-10-26 15:41:18.469583:  
2023-10-26 15:41:18.469910: Epoch 209 
2023-10-26 15:41:18.470169: Current learning rate: 0.0081 
2023-10-26 15:41:22.698998: train_loss -0.8135 
2023-10-26 15:41:22.699392: val_loss -0.7965 
2023-10-26 15:41:22.699663: Pseudo dice [0.8781, 0.9114, 0.9635, 0.6834, 0.9328] 
2023-10-26 15:41:22.699932: Epoch time: 4.23 s 
2023-10-26 15:41:23.893485:  
2023-10-26 15:41:23.893833: Epoch 210 
2023-10-26 15:41:23.894101: Current learning rate: 0.00809 
2023-10-26 15:41:27.895807: train_loss -0.8151 
2023-10-26 15:41:27.896181: val_loss -0.8162 
2023-10-26 15:41:27.896457: Pseudo dice [0.8732, 0.9209, 0.964, 0.7202, 0.9363] 
2023-10-26 15:41:27.896692: Epoch time: 4.0 s 
2023-10-26 15:41:28.934713:  
2023-10-26 15:41:28.935032: Epoch 211 
2023-10-26 15:41:28.935297: Current learning rate: 0.00808 
2023-10-26 15:41:33.045292: train_loss -0.8215 
2023-10-26 15:41:33.045652: val_loss -0.8126 
2023-10-26 15:41:33.045939: Pseudo dice [0.8721, 0.9167, 0.9634, 0.6899, 0.9343] 
2023-10-26 15:41:33.046173: Epoch time: 4.11 s 
2023-10-26 15:41:34.132167:  
2023-10-26 15:41:34.132472: Epoch 212 
2023-10-26 15:41:34.132749: Current learning rate: 0.00807 
2023-10-26 15:41:38.332371: train_loss -0.8312 
2023-10-26 15:41:38.332750: val_loss -0.8154 
2023-10-26 15:41:38.333035: Pseudo dice [0.8741, 0.9152, 0.9647, 0.7758, 0.9363] 
2023-10-26 15:41:38.333303: Epoch time: 4.2 s 
2023-10-26 15:41:39.375498:  
2023-10-26 15:41:39.375859: Epoch 213 
2023-10-26 15:41:39.376198: Current learning rate: 0.00806 
2023-10-26 15:41:43.362811: train_loss -0.822 
2023-10-26 15:41:43.363209: val_loss -0.8048 
2023-10-26 15:41:43.363480: Pseudo dice [0.8763, 0.9091, 0.9655, 0.7025, 0.938] 
2023-10-26 15:41:43.363714: Epoch time: 3.99 s 
2023-10-26 15:41:44.403219:  
2023-10-26 15:41:44.403507: Epoch 214 
2023-10-26 15:41:44.403749: Current learning rate: 0.00805 
2023-10-26 15:41:48.609291: train_loss -0.8294 
2023-10-26 15:41:48.609702: val_loss -0.7961 
2023-10-26 15:41:48.610085: Pseudo dice [0.8818, 0.9105, 0.9645, 0.6129, 0.9348] 
2023-10-26 15:41:48.610405: Epoch time: 4.21 s 
2023-10-26 15:41:49.655931:  
2023-10-26 15:41:49.656223: Epoch 215 
2023-10-26 15:41:49.656462: Current learning rate: 0.00804 
2023-10-26 15:41:53.973383: train_loss -0.8114 
2023-10-26 15:41:53.973968: val_loss -0.8057 
2023-10-26 15:41:53.974404: Pseudo dice [0.8674, 0.9152, 0.9632, 0.6543, 0.932] 
2023-10-26 15:41:53.974682: Epoch time: 4.32 s 
2023-10-26 15:41:55.213213:  
2023-10-26 15:41:55.213568: Epoch 216 
2023-10-26 15:41:55.213834: Current learning rate: 0.00803 
2023-10-26 15:41:59.430346: train_loss -0.8218 
2023-10-26 15:41:59.430707: val_loss -0.753 
2023-10-26 15:41:59.430970: Pseudo dice [0.87, 0.9148, 0.9642, 0.7256, 0.9353] 
2023-10-26 15:41:59.431203: Epoch time: 4.22 s 
2023-10-26 15:42:00.479993:  
2023-10-26 15:42:00.480328: Epoch 217 
2023-10-26 15:42:00.480579: Current learning rate: 0.00802 
2023-10-26 15:42:04.710246: train_loss -0.8177 
2023-10-26 15:42:04.710642: val_loss -0.8226 
2023-10-26 15:42:04.710921: Pseudo dice [0.8771, 0.9159, 0.964, 0.846, 0.937] 
2023-10-26 15:42:04.711157: Epoch time: 4.23 s 
2023-10-26 15:42:04.711379: Yayy! New best EMA pseudo Dice: 0.8777 
2023-10-26 15:42:05.826763:  
2023-10-26 15:42:05.827078: Epoch 218 
2023-10-26 15:42:05.827328: Current learning rate: 0.00801 
2023-10-26 15:42:10.013226: train_loss -0.8215 
2023-10-26 15:42:10.013601: val_loss -0.8015 
2023-10-26 15:42:10.013861: Pseudo dice [0.8666, 0.9078, 0.9637, 0.6747, 0.9311] 
2023-10-26 15:42:10.014104: Epoch time: 4.19 s 
2023-10-26 15:42:11.055917:  
2023-10-26 15:42:11.056227: Epoch 219 
2023-10-26 15:42:11.056468: Current learning rate: 0.00801 
2023-10-26 15:42:15.099500: train_loss -0.8217 
2023-10-26 15:42:15.099901: val_loss -0.7716 
2023-10-26 15:42:15.100185: Pseudo dice [0.8736, 0.9165, 0.9648, 0.6495, 0.9018] 
2023-10-26 15:42:15.100425: Epoch time: 4.04 s 
2023-10-26 15:42:16.156967:  
2023-10-26 15:42:16.157320: Epoch 220 
2023-10-26 15:42:16.157631: Current learning rate: 0.008 
2023-10-26 15:42:20.265553: train_loss -0.8287 
2023-10-26 15:42:20.266042: val_loss -0.7868 
2023-10-26 15:42:20.266534: Pseudo dice [0.87, 0.9179, 0.9649, 0.6116, 0.9338] 
2023-10-26 15:42:20.266894: Epoch time: 4.11 s 
2023-10-26 15:42:21.321557:  
2023-10-26 15:42:21.321892: Epoch 221 
2023-10-26 15:42:21.322168: Current learning rate: 0.00799 
2023-10-26 15:42:25.441836: train_loss -0.8253 
2023-10-26 15:42:25.442249: val_loss -0.7965 
2023-10-26 15:42:25.442514: Pseudo dice [0.8723, 0.9144, 0.9668, 0.6667, 0.9372] 
2023-10-26 15:42:25.442761: Epoch time: 4.12 s 
2023-10-26 15:42:26.497007:  
2023-10-26 15:42:26.497323: Epoch 222 
2023-10-26 15:42:26.497581: Current learning rate: 0.00798 
2023-10-26 15:42:30.622824: train_loss -0.8221 
2023-10-26 15:42:30.623214: val_loss -0.7968 
2023-10-26 15:42:30.623492: Pseudo dice [0.8749, 0.9171, 0.9659, 0.6262, 0.9346] 
2023-10-26 15:42:30.623732: Epoch time: 4.13 s 
2023-10-26 15:42:31.894571:  
2023-10-26 15:42:31.894919: Epoch 223 
2023-10-26 15:42:31.895241: Current learning rate: 0.00797 
2023-10-26 15:42:36.061355: train_loss -0.8319 
2023-10-26 15:42:36.061922: val_loss -0.7959 
2023-10-26 15:42:36.062243: Pseudo dice [0.8687, 0.9179, 0.9648, 0.5919, 0.934] 
2023-10-26 15:42:36.062522: Epoch time: 4.17 s 
2023-10-26 15:42:37.125969:  
2023-10-26 15:42:37.126294: Epoch 224 
2023-10-26 15:42:37.126545: Current learning rate: 0.00796 
2023-10-26 15:42:41.333925: train_loss -0.8346 
2023-10-26 15:42:41.334320: val_loss -0.8252 
2023-10-26 15:42:41.334605: Pseudo dice [0.8642, 0.9187, 0.9655, 0.6993, 0.9377] 
2023-10-26 15:42:41.334835: Epoch time: 4.21 s 
2023-10-26 15:42:42.383149:  
2023-10-26 15:42:42.383445: Epoch 225 
2023-10-26 15:42:42.383698: Current learning rate: 0.00795 
2023-10-26 15:42:46.491645: train_loss -0.8335 
2023-10-26 15:42:46.492071: val_loss -0.7904 
2023-10-26 15:42:46.492357: Pseudo dice [0.869, 0.9175, 0.9666, 0.5504, 0.9306] 
2023-10-26 15:42:46.492614: Epoch time: 4.11 s 
2023-10-26 15:42:47.531935:  
2023-10-26 15:42:47.532266: Epoch 226 
2023-10-26 15:42:47.532527: Current learning rate: 0.00794 
2023-10-26 15:42:51.649014: train_loss -0.8224 
2023-10-26 15:42:51.649407: val_loss -0.8022 
2023-10-26 15:42:51.649739: Pseudo dice [0.8642, 0.9177, 0.9642, 0.6804, 0.9364] 
2023-10-26 15:42:51.650024: Epoch time: 4.12 s 
2023-10-26 15:42:52.725504:  
2023-10-26 15:42:52.725806: Epoch 227 
2023-10-26 15:42:52.726066: Current learning rate: 0.00793 
2023-10-26 15:42:56.909640: train_loss -0.8256 
2023-10-26 15:42:56.910031: val_loss -0.8052 
2023-10-26 15:42:56.910476: Pseudo dice [0.8701, 0.9154, 0.965, 0.7386, 0.9337] 
2023-10-26 15:42:56.910819: Epoch time: 4.18 s 
2023-10-26 15:42:57.950161:  
2023-10-26 15:42:57.950456: Epoch 228 
2023-10-26 15:42:57.950706: Current learning rate: 0.00792 
2023-10-26 15:43:02.086651: train_loss -0.813 
2023-10-26 15:43:02.087057: val_loss -0.8033 
2023-10-26 15:43:02.087324: Pseudo dice [0.8718, 0.9153, 0.9651, 0.6452, 0.9376] 
2023-10-26 15:43:02.087567: Epoch time: 4.14 s 
2023-10-26 15:43:03.333404:  
2023-10-26 15:43:03.333718: Epoch 229 
2023-10-26 15:43:03.333989: Current learning rate: 0.00791 
2023-10-26 15:43:07.440239: train_loss -0.8208 
2023-10-26 15:43:07.440717: val_loss -0.8166 
2023-10-26 15:43:07.441002: Pseudo dice [0.8662, 0.9127, 0.9623, 0.6034, 0.9292] 
2023-10-26 15:43:07.441314: Epoch time: 4.11 s 
2023-10-26 15:43:08.477359:  
2023-10-26 15:43:08.477675: Epoch 230 
2023-10-26 15:43:08.477921: Current learning rate: 0.0079 
2023-10-26 15:43:12.645252: train_loss -0.8281 
2023-10-26 15:43:12.645648: val_loss -0.8103 
2023-10-26 15:43:12.645927: Pseudo dice [0.875, 0.918, 0.965, 0.7588, 0.9316] 
2023-10-26 15:43:12.646169: Epoch time: 4.17 s 
2023-10-26 15:43:13.724631:  
2023-10-26 15:43:13.724941: Epoch 231 
2023-10-26 15:43:13.725200: Current learning rate: 0.00789 
2023-10-26 15:43:17.711848: train_loss -0.8043 
2023-10-26 15:43:17.712232: val_loss -0.8018 
2023-10-26 15:43:17.712496: Pseudo dice [0.8519, 0.9022, 0.9595, 0.0, 0.9278] 
2023-10-26 15:43:17.712732: Epoch time: 3.99 s 
2023-10-26 15:43:18.784642:  
2023-10-26 15:43:18.784951: Epoch 232 
2023-10-26 15:43:18.785209: Current learning rate: 0.00789 
2023-10-26 15:43:22.897197: train_loss -0.76 
2023-10-26 15:43:22.897646: val_loss -0.8097 
2023-10-26 15:43:22.897915: Pseudo dice [0.8658, 0.9142, 0.9662, 0.0, 0.935] 
2023-10-26 15:43:22.898211: Epoch time: 4.11 s 
2023-10-26 15:43:23.936397:  
2023-10-26 15:43:23.936715: Epoch 233 
2023-10-26 15:43:23.936972: Current learning rate: 0.00788 
2023-10-26 15:43:28.159332: train_loss -0.7691 
2023-10-26 15:43:28.159840: val_loss -0.8068 
2023-10-26 15:43:28.160361: Pseudo dice [0.8681, 0.9166, 0.9656, 0.0, 0.9324] 
2023-10-26 15:43:28.160733: Epoch time: 4.22 s 
2023-10-26 15:43:29.204709:  
2023-10-26 15:43:29.205016: Epoch 234 
2023-10-26 15:43:29.205268: Current learning rate: 0.00787 
2023-10-26 15:43:33.303931: train_loss -0.7681 
2023-10-26 15:43:33.304303: val_loss -0.798 
2023-10-26 15:43:33.304567: Pseudo dice [0.868, 0.9161, 0.9648, 0.0, 0.9397] 
2023-10-26 15:43:33.304797: Epoch time: 4.1 s 
2023-10-26 15:43:34.344519:  
2023-10-26 15:43:34.344893: Epoch 235 
2023-10-26 15:43:34.345216: Current learning rate: 0.00786 
2023-10-26 15:43:38.504781: train_loss -0.7918 
2023-10-26 15:43:38.505232: val_loss -0.7985 
2023-10-26 15:43:38.505515: Pseudo dice [0.8543, 0.8908, 0.9575, 0.5755, 0.93] 
2023-10-26 15:43:38.505756: Epoch time: 4.16 s 
2023-10-26 15:43:39.736658:  
2023-10-26 15:43:39.737007: Epoch 236 
2023-10-26 15:43:39.737255: Current learning rate: 0.00785 
2023-10-26 15:43:43.861590: train_loss -0.8105 
2023-10-26 15:43:43.862052: val_loss -0.8165 
2023-10-26 15:43:43.862336: Pseudo dice [0.8697, 0.9149, 0.9643, 0.7856, 0.9372] 
2023-10-26 15:43:43.862581: Epoch time: 4.13 s 
2023-10-26 15:43:44.904555:  
2023-10-26 15:43:44.904871: Epoch 237 
2023-10-26 15:43:44.905133: Current learning rate: 0.00784 
2023-10-26 15:43:48.761958: train_loss -0.8117 
2023-10-26 15:43:48.762345: val_loss -0.803 
2023-10-26 15:43:48.762636: Pseudo dice [0.8622, 0.9123, 0.9667, 0.5728, 0.9304] 
2023-10-26 15:43:48.762910: Epoch time: 3.86 s 
2023-10-26 15:43:49.803007:  
2023-10-26 15:43:49.803332: Epoch 238 
2023-10-26 15:43:49.803583: Current learning rate: 0.00783 
2023-10-26 15:43:53.884216: train_loss -0.8212 
2023-10-26 15:43:53.884624: val_loss -0.8165 
2023-10-26 15:43:53.884906: Pseudo dice [0.861, 0.9011, 0.9625, 0.829, 0.9326] 
2023-10-26 15:43:53.885153: Epoch time: 4.08 s 
2023-10-26 15:43:54.953245:  
2023-10-26 15:43:54.953552: Epoch 239 
2023-10-26 15:43:54.953795: Current learning rate: 0.00782 
2023-10-26 15:43:59.062504: train_loss -0.808 
2023-10-26 15:43:59.062883: val_loss -0.7559 
2023-10-26 15:43:59.063151: Pseudo dice [0.8733, 0.9161, 0.9666, 0.7801, 0.9367] 
2023-10-26 15:43:59.063377: Epoch time: 4.11 s 
2023-10-26 15:44:00.113469:  
2023-10-26 15:44:00.113766: Epoch 240 
2023-10-26 15:44:00.114016: Current learning rate: 0.00781 
2023-10-26 15:44:04.092189: train_loss -0.8257 
2023-10-26 15:44:04.092584: val_loss -0.7999 
2023-10-26 15:44:04.092846: Pseudo dice [0.8622, 0.9176, 0.9641, 0.7112, 0.9237] 
2023-10-26 15:44:04.093123: Epoch time: 3.98 s 
2023-10-26 15:44:05.208554:  
2023-10-26 15:44:05.208855: Epoch 241 
2023-10-26 15:44:05.209116: Current learning rate: 0.0078 
2023-10-26 15:44:09.336294: train_loss -0.7994 
2023-10-26 15:44:09.336662: val_loss -0.8124 
2023-10-26 15:44:09.336942: Pseudo dice [0.8642, 0.8947, 0.9591, 0.7852, 0.9255] 
2023-10-26 15:44:09.337195: Epoch time: 4.13 s 
2023-10-26 15:44:10.397112:  
2023-10-26 15:44:10.397405: Epoch 242 
2023-10-26 15:44:10.397653: Current learning rate: 0.00779 
2023-10-26 15:44:14.539119: train_loss -0.8134 
2023-10-26 15:44:14.539516: val_loss -0.8361 
2023-10-26 15:44:14.539790: Pseudo dice [0.8701, 0.9168, 0.9655, 0.8329, 0.9379] 
2023-10-26 15:44:14.540044: Epoch time: 4.14 s 
2023-10-26 15:44:15.798387:  
2023-10-26 15:44:15.798673: Epoch 243 
2023-10-26 15:44:15.798923: Current learning rate: 0.00778 
2023-10-26 15:44:19.753942: train_loss -0.8081 
2023-10-26 15:44:19.754554: val_loss -0.7904 
2023-10-26 15:44:19.755001: Pseudo dice [0.873, 0.915, 0.9649, 0.6999, 0.9342] 
2023-10-26 15:44:19.755429: Epoch time: 3.96 s 
2023-10-26 15:44:20.936031:  
2023-10-26 15:44:20.936339: Epoch 244 
2023-10-26 15:44:20.936590: Current learning rate: 0.00777 
2023-10-26 15:44:25.053108: train_loss -0.8247 
2023-10-26 15:44:25.053488: val_loss -0.8228 
2023-10-26 15:44:25.053756: Pseudo dice [0.8739, 0.9158, 0.9644, 0.7462, 0.9293] 
2023-10-26 15:44:25.054061: Epoch time: 4.12 s 
2023-10-26 15:44:26.123296:  
2023-10-26 15:44:26.123604: Epoch 245 
2023-10-26 15:44:26.123855: Current learning rate: 0.00777 
2023-10-26 15:44:30.187423: train_loss -0.8139 
2023-10-26 15:44:30.187936: val_loss -0.7912 
2023-10-26 15:44:30.188247: Pseudo dice [0.8545, 0.8962, 0.9613, 0.5872, 0.9268] 
2023-10-26 15:44:30.188540: Epoch time: 4.06 s 
2023-10-26 15:44:31.250388:  
2023-10-26 15:44:31.250722: Epoch 246 
2023-10-26 15:44:31.250969: Current learning rate: 0.00776 
2023-10-26 15:44:35.094929: train_loss -0.8163 
2023-10-26 15:44:35.095311: val_loss -0.8309 
2023-10-26 15:44:35.095575: Pseudo dice [0.8761, 0.9113, 0.9631, 0.7724, 0.9349] 
2023-10-26 15:44:35.095814: Epoch time: 3.85 s 
2023-10-26 15:44:36.155324:  
2023-10-26 15:44:36.155651: Epoch 247 
2023-10-26 15:44:36.155905: Current learning rate: 0.00775 
2023-10-26 15:44:40.214905: train_loss -0.8221 
2023-10-26 15:44:40.215270: val_loss -0.8418 
2023-10-26 15:44:40.215532: Pseudo dice [0.8647, 0.9214, 0.9633, 0.7887, 0.9331] 
2023-10-26 15:44:40.215762: Epoch time: 4.06 s 
2023-10-26 15:44:41.270384:  
2023-10-26 15:44:41.270696: Epoch 248 
2023-10-26 15:44:41.270956: Current learning rate: 0.00774 
2023-10-26 15:44:45.216391: train_loss -0.8231 
2023-10-26 15:44:45.216788: val_loss -0.8164 
2023-10-26 15:44:45.217063: Pseudo dice [0.8722, 0.9118, 0.9632, 0.7239, 0.9381] 
2023-10-26 15:44:45.217298: Epoch time: 3.95 s 
2023-10-26 15:44:46.268408:  
2023-10-26 15:44:46.268715: Epoch 249 
2023-10-26 15:44:46.268953: Current learning rate: 0.00773 
2023-10-26 15:44:50.330704: train_loss -0.8257 
2023-10-26 15:44:50.331090: val_loss -0.8293 
2023-10-26 15:44:50.331368: Pseudo dice [0.8752, 0.9137, 0.9663, 0.8054, 0.9344] 
2023-10-26 15:44:50.331597: Epoch time: 4.06 s 
2023-10-26 15:44:51.645435:  
2023-10-26 15:44:51.645766: Epoch 250 
2023-10-26 15:44:51.646018: Current learning rate: 0.00772 
2023-10-26 15:44:55.721820: train_loss -0.8243 
2023-10-26 15:44:55.722224: val_loss -0.7995 
2023-10-26 15:44:55.722488: Pseudo dice [0.8785, 0.9194, 0.9656, 0.7603, 0.9344] 
2023-10-26 15:44:55.722727: Epoch time: 4.08 s 
2023-10-26 15:44:56.831398:  
2023-10-26 15:44:56.831776: Epoch 251 
2023-10-26 15:44:56.832033: Current learning rate: 0.00771 
2023-10-26 15:45:00.920806: train_loss -0.8296 
2023-10-26 15:45:00.921386: val_loss -0.8278 
2023-10-26 15:45:00.921671: Pseudo dice [0.8658, 0.9088, 0.9639, 0.5406, 0.9336] 
2023-10-26 15:45:00.921951: Epoch time: 4.09 s 
2023-10-26 15:45:02.007691:  
2023-10-26 15:45:02.008000: Epoch 252 
2023-10-26 15:45:02.008247: Current learning rate: 0.0077 
2023-10-26 15:45:06.094690: train_loss -0.8364 
2023-10-26 15:45:06.095096: val_loss -0.7964 
2023-10-26 15:45:06.095364: Pseudo dice [0.8641, 0.9069, 0.9663, 0.611, 0.9224] 
2023-10-26 15:45:06.095611: Epoch time: 4.09 s 
2023-10-26 15:45:07.233450:  
2023-10-26 15:45:07.233780: Epoch 253 
2023-10-26 15:45:07.234041: Current learning rate: 0.00769 
2023-10-26 15:45:11.349273: train_loss -0.8365 
2023-10-26 15:45:11.349718: val_loss -0.7935 
2023-10-26 15:45:11.350003: Pseudo dice [0.8729, 0.9169, 0.9646, 0.6438, 0.9384] 
2023-10-26 15:45:11.350253: Epoch time: 4.12 s 
2023-10-26 15:45:12.419620:  
2023-10-26 15:45:12.419946: Epoch 254 
2023-10-26 15:45:12.420201: Current learning rate: 0.00768 
2023-10-26 15:45:16.475677: train_loss -0.8333 
2023-10-26 15:45:16.476101: val_loss -0.8335 
2023-10-26 15:45:16.476424: Pseudo dice [0.8735, 0.9138, 0.9664, 0.7197, 0.9344] 
2023-10-26 15:45:16.476673: Epoch time: 4.06 s 
2023-10-26 15:45:17.536129:  
2023-10-26 15:45:17.536435: Epoch 255 
2023-10-26 15:45:17.536684: Current learning rate: 0.00767 
2023-10-26 15:45:21.629423: train_loss -0.8254 
2023-10-26 15:45:21.629812: val_loss -0.8106 
2023-10-26 15:45:21.630125: Pseudo dice [0.869, 0.9147, 0.9667, 0.6457, 0.9344] 
2023-10-26 15:45:21.630438: Epoch time: 4.09 s 
2023-10-26 15:45:22.942486:  
2023-10-26 15:45:22.942794: Epoch 256 
2023-10-26 15:45:22.943058: Current learning rate: 0.00766 
2023-10-26 15:45:27.015934: train_loss -0.8319 
2023-10-26 15:45:27.016420: val_loss -0.7934 
2023-10-26 15:45:27.016894: Pseudo dice [0.872, 0.9123, 0.9629, 0.7111, 0.9378] 
2023-10-26 15:45:27.017160: Epoch time: 4.07 s 
2023-10-26 15:45:28.079023:  
2023-10-26 15:45:28.079355: Epoch 257 
2023-10-26 15:45:28.079600: Current learning rate: 0.00765 
2023-10-26 15:45:32.069988: train_loss -0.834 
2023-10-26 15:45:32.070338: val_loss -0.7766 
2023-10-26 15:45:32.070601: Pseudo dice [0.8651, 0.9166, 0.9647, 0.715, 0.9374] 
2023-10-26 15:45:32.070826: Epoch time: 3.99 s 
2023-10-26 15:45:33.144632:  
2023-10-26 15:45:33.144964: Epoch 258 
2023-10-26 15:45:33.145221: Current learning rate: 0.00764 
2023-10-26 15:45:37.234721: train_loss -0.8184 
2023-10-26 15:45:37.235135: val_loss -0.8133 
2023-10-26 15:45:37.235406: Pseudo dice [0.8641, 0.9159, 0.9641, 0.7648, 0.9358] 
2023-10-26 15:45:37.235642: Epoch time: 4.09 s 
2023-10-26 15:45:38.357070:  
2023-10-26 15:45:38.357398: Epoch 259 
2023-10-26 15:45:38.357675: Current learning rate: 0.00764 
2023-10-26 15:45:42.557914: train_loss -0.8271 
2023-10-26 15:45:42.558583: val_loss -0.8127 
2023-10-26 15:45:42.559027: Pseudo dice [0.876, 0.9147, 0.9622, 0.8079, 0.9358] 
2023-10-26 15:45:42.559455: Epoch time: 4.2 s 
2023-10-26 15:45:43.615896:  
2023-10-26 15:45:43.616216: Epoch 260 
2023-10-26 15:45:43.616501: Current learning rate: 0.00763 
2023-10-26 15:45:47.389114: train_loss -0.8259 
2023-10-26 15:45:47.389586: val_loss -0.8114 
2023-10-26 15:45:47.389976: Pseudo dice [0.8669, 0.9079, 0.9626, 0.8151, 0.9297] 
2023-10-26 15:45:47.390242: Epoch time: 3.77 s 
2023-10-26 15:45:47.390488: Yayy! New best EMA pseudo Dice: 0.8778 
2023-10-26 15:45:48.604706:  
2023-10-26 15:45:48.605050: Epoch 261 
2023-10-26 15:45:48.605296: Current learning rate: 0.00762 
2023-10-26 15:45:52.599234: train_loss -0.8225 
2023-10-26 15:45:52.599638: val_loss -0.8335 
2023-10-26 15:45:52.599992: Pseudo dice [0.8701, 0.9127, 0.9629, 0.85, 0.9353] 
2023-10-26 15:45:52.600260: Epoch time: 4.0 s 
2023-10-26 15:45:52.600507: Yayy! New best EMA pseudo Dice: 0.8807 
2023-10-26 15:45:53.739861:  
2023-10-26 15:45:53.740192: Epoch 262 
2023-10-26 15:45:53.740440: Current learning rate: 0.00761 
2023-10-26 15:45:57.599161: train_loss -0.8221 
2023-10-26 15:45:57.599530: val_loss -0.7917 
2023-10-26 15:45:57.599787: Pseudo dice [0.8693, 0.9115, 0.9664, 0.7309, 0.9368] 
2023-10-26 15:45:57.600024: Epoch time: 3.86 s 
2023-10-26 15:45:57.600233: Yayy! New best EMA pseudo Dice: 0.8809 
2023-10-26 15:45:58.916638:  
2023-10-26 15:45:58.916953: Epoch 263 
2023-10-26 15:45:58.917202: Current learning rate: 0.0076 
2023-10-26 15:46:02.878090: train_loss -0.83 
2023-10-26 15:46:02.878468: val_loss -0.8124 
2023-10-26 15:46:02.878748: Pseudo dice [0.8722, 0.9171, 0.9648, 0.777, 0.9381] 
2023-10-26 15:46:02.879009: Epoch time: 3.96 s 
2023-10-26 15:46:02.879248: Yayy! New best EMA pseudo Dice: 0.8822 
2023-10-26 15:46:04.025653:  
2023-10-26 15:46:04.025973: Epoch 264 
2023-10-26 15:46:04.026229: Current learning rate: 0.00759 
2023-10-26 15:46:08.091997: train_loss -0.8309 
2023-10-26 15:46:08.092449: val_loss -0.8202 
2023-10-26 15:46:08.092719: Pseudo dice [0.868, 0.9131, 0.9654, 0.5358, 0.9348] 
2023-10-26 15:46:08.092983: Epoch time: 4.07 s 
2023-10-26 15:46:09.167660:  
2023-10-26 15:46:09.168035: Epoch 265 
2023-10-26 15:46:09.168283: Current learning rate: 0.00758 
2023-10-26 15:46:13.321520: train_loss -0.8174 
2023-10-26 15:46:13.321924: val_loss -0.7986 
2023-10-26 15:46:13.322188: Pseudo dice [0.8702, 0.9001, 0.9658, 0.6303, 0.9325] 
2023-10-26 15:46:13.322427: Epoch time: 4.15 s 
2023-10-26 15:46:14.382305:  
2023-10-26 15:46:14.382634: Epoch 266 
2023-10-26 15:46:14.382893: Current learning rate: 0.00757 
2023-10-26 15:46:18.417761: train_loss -0.8209 
2023-10-26 15:46:18.418170: val_loss -0.7982 
2023-10-26 15:46:18.418629: Pseudo dice [0.8712, 0.9198, 0.9635, 0.7662, 0.9308] 
2023-10-26 15:46:18.418912: Epoch time: 4.04 s 
2023-10-26 15:46:19.496269:  
2023-10-26 15:46:19.496650: Epoch 267 
2023-10-26 15:46:19.496991: Current learning rate: 0.00756 
2023-10-26 15:46:23.527411: train_loss -0.8269 
2023-10-26 15:46:23.527903: val_loss -0.785 
2023-10-26 15:46:23.528365: Pseudo dice [0.8748, 0.911, 0.964, 0.7474, 0.9354] 
2023-10-26 15:46:23.528629: Epoch time: 4.03 s 
2023-10-26 15:46:24.594473:  
2023-10-26 15:46:24.594793: Epoch 268 
2023-10-26 15:46:24.595033: Current learning rate: 0.00755 
2023-10-26 15:46:28.619807: train_loss -0.8273 
2023-10-26 15:46:28.620221: val_loss -0.7832 
2023-10-26 15:46:28.620492: Pseudo dice [0.8719, 0.9112, 0.9632, 0.7354, 0.9253] 
2023-10-26 15:46:28.620739: Epoch time: 4.03 s 
2023-10-26 15:46:29.860925:  
2023-10-26 15:46:29.861246: Epoch 269 
2023-10-26 15:46:29.861495: Current learning rate: 0.00754 
2023-10-26 15:46:33.970711: train_loss -0.8227 
2023-10-26 15:46:33.971111: val_loss -0.8209 
2023-10-26 15:46:33.971395: Pseudo dice [0.8558, 0.9132, 0.9625, 0.3195, 0.9365] 
2023-10-26 15:46:33.971641: Epoch time: 4.11 s 
2023-10-26 15:46:35.111402:  
2023-10-26 15:46:35.111739: Epoch 270 
2023-10-26 15:46:35.111997: Current learning rate: 0.00753 
2023-10-26 15:46:39.183169: train_loss -0.823 
2023-10-26 15:46:39.183602: val_loss -0.8069 
2023-10-26 15:46:39.183879: Pseudo dice [0.8706, 0.9136, 0.9638, 0.6044, 0.9321] 
2023-10-26 15:46:39.184132: Epoch time: 4.07 s 
2023-10-26 15:46:40.378913:  
2023-10-26 15:46:40.379318: Epoch 271 
2023-10-26 15:46:40.379655: Current learning rate: 0.00752 
2023-10-26 15:46:44.425206: train_loss -0.8301 
2023-10-26 15:46:44.425627: val_loss -0.8012 
2023-10-26 15:46:44.425891: Pseudo dice [0.8723, 0.9106, 0.963, 0.6581, 0.9227] 
2023-10-26 15:46:44.426136: Epoch time: 4.05 s 
2023-10-26 15:46:45.497262:  
2023-10-26 15:46:45.497565: Epoch 272 
2023-10-26 15:46:45.497821: Current learning rate: 0.00751 
2023-10-26 15:46:49.618536: train_loss -0.8293 
2023-10-26 15:46:49.618944: val_loss -0.7781 
2023-10-26 15:46:49.619209: Pseudo dice [0.8697, 0.9065, 0.9628, 0.7756, 0.9334] 
2023-10-26 15:46:49.619444: Epoch time: 4.12 s 
2023-10-26 15:46:50.686332:  
2023-10-26 15:46:50.686715: Epoch 273 
2023-10-26 15:46:50.687050: Current learning rate: 0.00751 
2023-10-26 15:46:54.681759: train_loss -0.82 
2023-10-26 15:46:54.682191: val_loss -0.8031 
2023-10-26 15:46:54.682456: Pseudo dice [0.8869, 0.9123, 0.9665, 0.6126, 0.9238] 
2023-10-26 15:46:54.682686: Epoch time: 4.0 s 
2023-10-26 15:46:55.769887:  
2023-10-26 15:46:55.770189: Epoch 274 
2023-10-26 15:46:55.770452: Current learning rate: 0.0075 
2023-10-26 15:46:59.754503: train_loss -0.8068 
2023-10-26 15:46:59.755004: val_loss -0.7923 
2023-10-26 15:46:59.755317: Pseudo dice [0.8608, 0.903, 0.964, 0.7673, 0.9316] 
2023-10-26 15:46:59.755602: Epoch time: 3.99 s 
2023-10-26 15:47:00.828584:  
2023-10-26 15:47:00.828901: Epoch 275 
2023-10-26 15:47:00.829153: Current learning rate: 0.00749 
2023-10-26 15:47:04.856477: train_loss -0.8177 
2023-10-26 15:47:04.856838: val_loss -0.7522 
2023-10-26 15:47:04.857106: Pseudo dice [0.8727, 0.9042, 0.9633, 0.7702, 0.8444] 
2023-10-26 15:47:04.857332: Epoch time: 4.03 s 
2023-10-26 15:47:06.131333:  
2023-10-26 15:47:06.131646: Epoch 276 
2023-10-26 15:47:06.131907: Current learning rate: 0.00748 
2023-10-26 15:47:10.265658: train_loss -0.8137 
2023-10-26 15:47:10.266075: val_loss -0.8091 
2023-10-26 15:47:10.266336: Pseudo dice [0.8693, 0.9101, 0.9656, 0.7522, 0.9366] 
2023-10-26 15:47:10.266579: Epoch time: 4.13 s 
2023-10-26 15:47:11.363518:  
2023-10-26 15:47:11.363844: Epoch 277 
2023-10-26 15:47:11.364096: Current learning rate: 0.00747 
2023-10-26 15:47:15.488791: train_loss -0.8184 
2023-10-26 15:47:15.489198: val_loss -0.8383 
2023-10-26 15:47:15.489749: Pseudo dice [0.8731, 0.9204, 0.9615, 0.8385, 0.932] 
2023-10-26 15:47:15.490084: Epoch time: 4.13 s 
2023-10-26 15:47:16.561082:  
2023-10-26 15:47:16.561395: Epoch 278 
2023-10-26 15:47:16.561638: Current learning rate: 0.00746 
2023-10-26 15:47:20.446978: train_loss -0.8133 
2023-10-26 15:47:20.447348: val_loss -0.8114 
2023-10-26 15:47:20.447618: Pseudo dice [0.8718, 0.9081, 0.9625, 0.7211, 0.9339] 
2023-10-26 15:47:20.447849: Epoch time: 3.89 s 
2023-10-26 15:47:21.530578:  
2023-10-26 15:47:21.530942: Epoch 279 
2023-10-26 15:47:21.531194: Current learning rate: 0.00745 
2023-10-26 15:47:25.558089: train_loss -0.8289 
2023-10-26 15:47:25.558442: val_loss -0.8247 
2023-10-26 15:47:25.558718: Pseudo dice [0.8483, 0.8975, 0.9633, 0.3934, 0.931] 
2023-10-26 15:47:25.558961: Epoch time: 4.03 s 
2023-10-26 15:47:26.620520:  
2023-10-26 15:47:26.620825: Epoch 280 
2023-10-26 15:47:26.621093: Current learning rate: 0.00744 
2023-10-26 15:47:30.608961: train_loss -0.8174 
2023-10-26 15:47:30.609326: val_loss -0.8063 
2023-10-26 15:47:30.609594: Pseudo dice [0.8615, 0.9133, 0.9628, 0.7986, 0.9323] 
2023-10-26 15:47:30.609843: Epoch time: 3.99 s 
2023-10-26 15:47:31.670046:  
2023-10-26 15:47:31.670339: Epoch 281 
2023-10-26 15:47:31.670585: Current learning rate: 0.00743 
2023-10-26 15:47:35.652018: train_loss -0.8035 
2023-10-26 15:47:35.652484: val_loss -0.8057 
2023-10-26 15:47:35.652961: Pseudo dice [0.8694, 0.8982, 0.9626, 0.8557, 0.9273] 
2023-10-26 15:47:35.653316: Epoch time: 3.98 s 
2023-10-26 15:47:36.950352:  
2023-10-26 15:47:36.950653: Epoch 282 
2023-10-26 15:47:36.950913: Current learning rate: 0.00742 
2023-10-26 15:47:41.039394: train_loss -0.7995 
2023-10-26 15:47:41.039843: val_loss -0.8022 
2023-10-26 15:47:41.040159: Pseudo dice [0.8602, 0.9002, 0.9617, 0.7829, 0.926] 
2023-10-26 15:47:41.040406: Epoch time: 4.09 s 
2023-10-26 15:47:42.151615:  
2023-10-26 15:47:42.152113: Epoch 283 
2023-10-26 15:47:42.152368: Current learning rate: 0.00741 
2023-10-26 15:47:46.285068: train_loss -0.8074 
2023-10-26 15:47:46.285460: val_loss -0.8168 
2023-10-26 15:47:46.285729: Pseudo dice [0.8712, 0.9048, 0.9644, 0.7087, 0.9305] 
2023-10-26 15:47:46.285992: Epoch time: 4.13 s 
2023-10-26 15:47:47.360568:  
2023-10-26 15:47:47.360884: Epoch 284 
2023-10-26 15:47:47.361127: Current learning rate: 0.0074 
2023-10-26 15:47:51.419014: train_loss -0.8218 
2023-10-26 15:47:51.419424: val_loss -0.8096 
2023-10-26 15:47:51.419695: Pseudo dice [0.8724, 0.9117, 0.9659, 0.7318, 0.9384] 
2023-10-26 15:47:51.419940: Epoch time: 4.06 s 
2023-10-26 15:47:52.486399:  
2023-10-26 15:47:52.486708: Epoch 285 
2023-10-26 15:47:52.486959: Current learning rate: 0.00739 
2023-10-26 15:47:56.447454: train_loss -0.8268 
2023-10-26 15:47:56.447818: val_loss -0.8149 
2023-10-26 15:47:56.448090: Pseudo dice [0.8594, 0.9075, 0.9663, 0.7426, 0.9335] 
2023-10-26 15:47:56.448324: Epoch time: 3.96 s 
2023-10-26 15:47:57.555555:  
2023-10-26 15:47:57.555869: Epoch 286 
2023-10-26 15:47:57.556121: Current learning rate: 0.00738 
2023-10-26 15:48:01.684300: train_loss -0.826 
2023-10-26 15:48:01.684671: val_loss -0.7907 
2023-10-26 15:48:01.684946: Pseudo dice [0.8593, 0.9102, 0.9634, 0.6172, 0.9292] 
2023-10-26 15:48:01.685179: Epoch time: 4.13 s 
2023-10-26 15:48:02.822650:  
2023-10-26 15:48:02.823242: Epoch 287 
2023-10-26 15:48:02.823544: Current learning rate: 0.00738 
2023-10-26 15:48:06.836957: train_loss -0.8291 
2023-10-26 15:48:06.837307: val_loss -0.7965 
2023-10-26 15:48:06.837558: Pseudo dice [0.869, 0.919, 0.9667, 0.5857, 0.9349] 
2023-10-26 15:48:06.837783: Epoch time: 4.01 s 
2023-10-26 15:48:07.906614:  
2023-10-26 15:48:07.906935: Epoch 288 
2023-10-26 15:48:07.907186: Current learning rate: 0.00737 
2023-10-26 15:48:11.888747: train_loss -0.828 
2023-10-26 15:48:11.889263: val_loss -0.7731 
2023-10-26 15:48:11.889684: Pseudo dice [0.8654, 0.9111, 0.966, 0.6616, 0.9323] 
2023-10-26 15:48:11.890012: Epoch time: 3.98 s 
2023-10-26 15:48:13.165668:  
2023-10-26 15:48:13.165973: Epoch 289 
2023-10-26 15:48:13.166222: Current learning rate: 0.00736 
2023-10-26 15:48:16.946750: train_loss -0.8267 
2023-10-26 15:48:16.947184: val_loss -0.8282 
2023-10-26 15:48:16.947474: Pseudo dice [0.8667, 0.9129, 0.965, 0.8681, 0.9207] 
2023-10-26 15:48:16.947721: Epoch time: 3.78 s 
2023-10-26 15:48:18.037722:  
2023-10-26 15:48:18.038034: Epoch 290 
2023-10-26 15:48:18.038278: Current learning rate: 0.00735 
2023-10-26 15:48:21.903350: train_loss -0.828 
2023-10-26 15:48:21.903933: val_loss -0.8286 
2023-10-26 15:48:21.904301: Pseudo dice [0.8768, 0.9158, 0.9661, 0.85, 0.929] 
2023-10-26 15:48:21.904587: Epoch time: 3.87 s 
2023-10-26 15:48:22.979963:  
2023-10-26 15:48:22.980272: Epoch 291 
2023-10-26 15:48:22.980557: Current learning rate: 0.00734 
2023-10-26 15:48:26.913483: train_loss -0.8322 
2023-10-26 15:48:26.913892: val_loss -0.7987 
2023-10-26 15:48:26.914163: Pseudo dice [0.8707, 0.917, 0.9649, 0.7323, 0.913] 
2023-10-26 15:48:26.914406: Epoch time: 3.93 s 
2023-10-26 15:48:27.993935:  
2023-10-26 15:48:27.994254: Epoch 292 
2023-10-26 15:48:27.994512: Current learning rate: 0.00733 
2023-10-26 15:48:31.999565: train_loss -0.8334 
2023-10-26 15:48:32: val_loss -0.8153 
2023-10-26 15:48:32.000276: Pseudo dice [0.8693, 0.9214, 0.9645, 0.7439, 0.9376] 
2023-10-26 15:48:32.000530: Epoch time: 4.01 s 
2023-10-26 15:48:33.179603:  
2023-10-26 15:48:33.179913: Epoch 293 
2023-10-26 15:48:33.180169: Current learning rate: 0.00732 
2023-10-26 15:48:37.145494: train_loss -0.8385 
2023-10-26 15:48:37.145931: val_loss -0.8061 
2023-10-26 15:48:37.146246: Pseudo dice [0.8658, 0.9181, 0.9672, 0.644, 0.9312] 
2023-10-26 15:48:37.146527: Epoch time: 3.97 s 
2023-10-26 15:48:38.281208:  
2023-10-26 15:48:38.281520: Epoch 294 
2023-10-26 15:48:38.281766: Current learning rate: 0.00731 
2023-10-26 15:48:42.294698: train_loss -0.8371 
2023-10-26 15:48:42.295093: val_loss -0.8179 
2023-10-26 15:48:42.295362: Pseudo dice [0.8736, 0.9195, 0.9654, 0.6642, 0.9365] 
2023-10-26 15:48:42.295596: Epoch time: 4.01 s 
2023-10-26 15:48:43.567612:  
2023-10-26 15:48:43.567933: Epoch 295 
2023-10-26 15:48:43.568186: Current learning rate: 0.0073 
2023-10-26 15:48:47.714018: train_loss -0.8352 
2023-10-26 15:48:47.714771: val_loss -0.8005 
2023-10-26 15:48:47.715164: Pseudo dice [0.8683, 0.9021, 0.9647, 0.7283, 0.9363] 
2023-10-26 15:48:47.715437: Epoch time: 4.15 s 
2023-10-26 15:48:48.837559:  
2023-10-26 15:48:48.837887: Epoch 296 
2023-10-26 15:48:48.838141: Current learning rate: 0.00729 
2023-10-26 15:48:52.925272: train_loss -0.8341 
2023-10-26 15:48:52.925660: val_loss -0.8154 
2023-10-26 15:48:52.925938: Pseudo dice [0.8691, 0.9147, 0.9646, 0.7304, 0.9301] 
2023-10-26 15:48:52.926180: Epoch time: 4.09 s 
2023-10-26 15:48:54.063885:  
2023-10-26 15:48:54.064196: Epoch 297 
2023-10-26 15:48:54.064451: Current learning rate: 0.00728 
2023-10-26 15:48:58.091810: train_loss -0.8381 
2023-10-26 15:48:58.092240: val_loss -0.7866 
2023-10-26 15:48:58.092518: Pseudo dice [0.8695, 0.9169, 0.9649, 0.7366, 0.9349] 
2023-10-26 15:48:58.092760: Epoch time: 4.03 s 
2023-10-26 15:48:59.192409:  
2023-10-26 15:48:59.192884: Epoch 298 
2023-10-26 15:48:59.193166: Current learning rate: 0.00727 
2023-10-26 15:49:03.208318: train_loss -0.8351 
2023-10-26 15:49:03.208915: val_loss -0.8053 
2023-10-26 15:49:03.209413: Pseudo dice [0.8753, 0.9067, 0.9649, 0.7368, 0.9391] 
2023-10-26 15:49:03.209761: Epoch time: 4.02 s 
2023-10-26 15:49:04.298854:  
2023-10-26 15:49:04.299185: Epoch 299 
2023-10-26 15:49:04.299430: Current learning rate: 0.00726 
2023-10-26 15:49:08.367568: train_loss -0.8321 
2023-10-26 15:49:08.368028: val_loss -0.8102 
2023-10-26 15:49:08.368289: Pseudo dice [0.8739, 0.9152, 0.9651, 0.7808, 0.9357] 
2023-10-26 15:49:08.368520: Epoch time: 4.07 s 
2023-10-26 15:49:09.648645:  
2023-10-26 15:49:09.648950: Epoch 300 
2023-10-26 15:49:09.649197: Current learning rate: 0.00725 
2023-10-26 15:49:13.732563: train_loss -0.8198 
2023-10-26 15:49:13.732997: val_loss -0.823 
2023-10-26 15:49:13.733268: Pseudo dice [0.8632, 0.9133, 0.9644, 0.7567, 0.9345] 
2023-10-26 15:49:13.733508: Epoch time: 4.08 s 
2023-10-26 15:49:14.814152:  
2023-10-26 15:49:14.814458: Epoch 301 
2023-10-26 15:49:14.814706: Current learning rate: 0.00724 
2023-10-26 15:49:18.819948: train_loss -0.8254 
2023-10-26 15:49:18.820290: val_loss -0.834 
2023-10-26 15:49:18.820544: Pseudo dice [0.8725, 0.9174, 0.9671, 0.6855, 0.9342] 
2023-10-26 15:49:18.820778: Epoch time: 4.01 s 
2023-10-26 15:49:20.085475:  
2023-10-26 15:49:20.085787: Epoch 302 
2023-10-26 15:49:20.086032: Current learning rate: 0.00724 
2023-10-26 15:49:24.011529: train_loss -0.8345 
2023-10-26 15:49:24.011966: val_loss -0.7853 
2023-10-26 15:49:24.012312: Pseudo dice [0.8734, 0.9178, 0.9602, 0.7091, 0.9331] 
2023-10-26 15:49:24.012554: Epoch time: 3.93 s 
2023-10-26 15:49:25.126598:  
2023-10-26 15:49:25.126913: Epoch 303 
2023-10-26 15:49:25.127187: Current learning rate: 0.00723 
2023-10-26 15:49:29.126336: train_loss -0.8257 
2023-10-26 15:49:29.126760: val_loss -0.8182 
2023-10-26 15:49:29.127104: Pseudo dice [0.8614, 0.9084, 0.9656, 0.0739, 0.9351] 
2023-10-26 15:49:29.127372: Epoch time: 4.0 s 
2023-10-26 15:49:30.208071:  
2023-10-26 15:49:30.208382: Epoch 304 
2023-10-26 15:49:30.208638: Current learning rate: 0.00722 
2023-10-26 15:49:34.131600: train_loss -0.8278 
2023-10-26 15:49:34.131956: val_loss -0.8124 
2023-10-26 15:49:34.132238: Pseudo dice [0.8662, 0.9163, 0.9667, 0.6151, 0.936] 
2023-10-26 15:49:34.132482: Epoch time: 3.92 s 
2023-10-26 15:49:35.206997:  
2023-10-26 15:49:35.207294: Epoch 305 
2023-10-26 15:49:35.207541: Current learning rate: 0.00721 
2023-10-26 15:49:39.275093: train_loss -0.8407 
2023-10-26 15:49:39.275493: val_loss -0.8086 
2023-10-26 15:49:39.275767: Pseudo dice [0.8743, 0.9188, 0.9672, 0.6434, 0.9364] 
2023-10-26 15:49:39.276022: Epoch time: 4.07 s 
2023-10-26 15:49:40.350132:  
2023-10-26 15:49:40.350512: Epoch 306 
2023-10-26 15:49:40.350752: Current learning rate: 0.0072 
2023-10-26 15:49:44.377830: train_loss -0.8336 
2023-10-26 15:49:44.378184: val_loss -0.8154 
2023-10-26 15:49:44.378439: Pseudo dice [0.8637, 0.913, 0.9664, 0.4559, 0.9285] 
2023-10-26 15:49:44.378679: Epoch time: 4.03 s 
2023-10-26 15:49:45.432621:  
2023-10-26 15:49:45.432926: Epoch 307 
2023-10-26 15:49:45.433171: Current learning rate: 0.00719 
2023-10-26 15:49:49.517733: train_loss -0.8268 
2023-10-26 15:49:49.518244: val_loss -0.8087 
2023-10-26 15:49:49.518619: Pseudo dice [0.8676, 0.9186, 0.9654, 0.5273, 0.934] 
2023-10-26 15:49:49.518950: Epoch time: 4.09 s 
2023-10-26 15:49:50.586445:  
2023-10-26 15:49:50.586752: Epoch 308 
2023-10-26 15:49:50.587009: Current learning rate: 0.00718 
2023-10-26 15:49:54.656131: train_loss -0.8282 
2023-10-26 15:49:54.656487: val_loss -0.8152 
2023-10-26 15:49:54.656748: Pseudo dice [0.8726, 0.9207, 0.9632, 0.66, 0.9306] 
2023-10-26 15:49:54.656987: Epoch time: 4.07 s 
2023-10-26 15:49:55.949539:  
2023-10-26 15:49:55.949847: Epoch 309 
2023-10-26 15:49:55.950099: Current learning rate: 0.00717 
2023-10-26 15:50:00.088317: train_loss -0.8277 
2023-10-26 15:50:00.088675: val_loss -0.8202 
2023-10-26 15:50:00.088954: Pseudo dice [0.8745, 0.9157, 0.965, 0.6552, 0.9393] 
2023-10-26 15:50:00.089195: Epoch time: 4.14 s 
2023-10-26 15:50:01.178272:  
2023-10-26 15:50:01.178611: Epoch 310 
2023-10-26 15:50:01.178917: Current learning rate: 0.00716 
2023-10-26 15:50:05.264250: train_loss -0.8203 
2023-10-26 15:50:05.264631: val_loss -0.7949 
2023-10-26 15:50:05.264912: Pseudo dice [0.8642, 0.9148, 0.9643, 0.679, 0.9291] 
2023-10-26 15:50:05.265147: Epoch time: 4.09 s 
2023-10-26 15:50:06.361306:  
2023-10-26 15:50:06.361598: Epoch 311 
2023-10-26 15:50:06.361841: Current learning rate: 0.00715 
2023-10-26 15:50:10.471412: train_loss -0.8132 
2023-10-26 15:50:10.471772: val_loss -0.8096 
2023-10-26 15:50:10.472059: Pseudo dice [0.8677, 0.9055, 0.9625, 0.5921, 0.9222] 
2023-10-26 15:50:10.472286: Epoch time: 4.11 s 
2023-10-26 15:50:11.531485:  
2023-10-26 15:50:11.531820: Epoch 312 
2023-10-26 15:50:11.532082: Current learning rate: 0.00714 
2023-10-26 15:50:15.659334: train_loss -0.8241 
2023-10-26 15:50:15.659729: val_loss -0.8104 
2023-10-26 15:50:15.660003: Pseudo dice [0.8774, 0.9142, 0.9665, 0.656, 0.9366] 
2023-10-26 15:50:15.660253: Epoch time: 4.13 s 
2023-10-26 15:50:16.730747:  
2023-10-26 15:50:16.731056: Epoch 313 
2023-10-26 15:50:16.731328: Current learning rate: 0.00713 
2023-10-26 15:50:20.884024: train_loss -0.8301 
2023-10-26 15:50:20.884422: val_loss -0.8015 
2023-10-26 15:50:20.884699: Pseudo dice [0.8626, 0.9089, 0.9649, 0.6946, 0.934] 
2023-10-26 15:50:20.884944: Epoch time: 4.15 s 
2023-10-26 15:50:21.959859:  
2023-10-26 15:50:21.960165: Epoch 314 
2023-10-26 15:50:21.960416: Current learning rate: 0.00712 
2023-10-26 15:50:26.101177: train_loss -0.8128 
2023-10-26 15:50:26.101730: val_loss -0.7822 
2023-10-26 15:50:26.102142: Pseudo dice [0.8685, 0.9064, 0.9638, 0.6271, 0.9193] 
2023-10-26 15:50:26.102445: Epoch time: 4.14 s 
2023-10-26 15:50:27.358362:  
2023-10-26 15:50:27.358682: Epoch 315 
2023-10-26 15:50:27.358943: Current learning rate: 0.00711 
2023-10-26 15:50:31.332210: train_loss -0.8243 
2023-10-26 15:50:31.332569: val_loss -0.8139 
2023-10-26 15:50:31.332851: Pseudo dice [0.8643, 0.8993, 0.9635, 0.6332, 0.9266] 
2023-10-26 15:50:31.333097: Epoch time: 3.97 s 
2023-10-26 15:50:32.451971:  
2023-10-26 15:50:32.452362: Epoch 316 
2023-10-26 15:50:32.452632: Current learning rate: 0.0071 
2023-10-26 15:50:36.597486: train_loss -0.8308 
2023-10-26 15:50:36.597993: val_loss -0.8031 
2023-10-26 15:50:36.598294: Pseudo dice [0.8594, 0.9178, 0.9632, 0.2413, 0.9327] 
2023-10-26 15:50:36.598637: Epoch time: 4.15 s 
2023-10-26 15:50:37.681328:  
2023-10-26 15:50:37.681631: Epoch 317 
2023-10-26 15:50:37.681899: Current learning rate: 0.0071 
2023-10-26 15:50:41.763767: train_loss -0.8298 
2023-10-26 15:50:41.764120: val_loss -0.8004 
2023-10-26 15:50:41.764371: Pseudo dice [0.863, 0.9113, 0.965, 0.1464, 0.9317] 
2023-10-26 15:50:41.764597: Epoch time: 4.08 s 
2023-10-26 15:50:42.955320:  
2023-10-26 15:50:42.955629: Epoch 318 
2023-10-26 15:50:42.955878: Current learning rate: 0.00709 
2023-10-26 15:50:47.061833: train_loss -0.8258 
2023-10-26 15:50:47.062560: val_loss -0.7962 
2023-10-26 15:50:47.063054: Pseudo dice [0.8645, 0.9022, 0.9633, 0.6967, 0.9322] 
2023-10-26 15:50:47.063401: Epoch time: 4.11 s 
2023-10-26 15:50:48.179238:  
2023-10-26 15:50:48.179573: Epoch 319 
2023-10-26 15:50:48.179821: Current learning rate: 0.00708 
2023-10-26 15:50:52.328139: train_loss -0.8295 
2023-10-26 15:50:52.328480: val_loss -0.806 
2023-10-26 15:50:52.328732: Pseudo dice [0.8793, 0.916, 0.9657, 0.7338, 0.9411] 
2023-10-26 15:50:52.328959: Epoch time: 4.15 s 
2023-10-26 15:50:53.396382:  
2023-10-26 15:50:53.396669: Epoch 320 
2023-10-26 15:50:53.396940: Current learning rate: 0.00707 
2023-10-26 15:50:57.491582: train_loss -0.8377 
2023-10-26 15:50:57.491956: val_loss -0.8008 
2023-10-26 15:50:57.492245: Pseudo dice [0.8659, 0.9149, 0.9654, 0.6884, 0.931] 
2023-10-26 15:50:57.492471: Epoch time: 4.1 s 
2023-10-26 15:50:58.759760:  
2023-10-26 15:50:58.760098: Epoch 321 
2023-10-26 15:50:58.760350: Current learning rate: 0.00706 
2023-10-26 15:51:02.945119: train_loss -0.8266 
2023-10-26 15:51:02.945530: val_loss -0.8104 
2023-10-26 15:51:02.945804: Pseudo dice [0.8674, 0.9134, 0.9613, 0.7058, 0.9258] 
2023-10-26 15:51:02.946264: Epoch time: 4.19 s 
2023-10-26 15:51:04.144675:  
2023-10-26 15:51:04.144998: Epoch 322 
2023-10-26 15:51:04.145250: Current learning rate: 0.00705 
2023-10-26 15:51:08.034932: train_loss -0.8383 
2023-10-26 15:51:08.035332: val_loss -0.8274 
2023-10-26 15:51:08.035646: Pseudo dice [0.8718, 0.9146, 0.9646, 0.7852, 0.9312] 
2023-10-26 15:51:08.035955: Epoch time: 3.89 s 
2023-10-26 15:51:09.111015:  
2023-10-26 15:51:09.111324: Epoch 323 
2023-10-26 15:51:09.111572: Current learning rate: 0.00704 
2023-10-26 15:51:13.221083: train_loss -0.8317 
2023-10-26 15:51:13.221456: val_loss -0.8012 
2023-10-26 15:51:13.221861: Pseudo dice [0.8707, 0.9147, 0.9646, 0.7175, 0.9326] 
2023-10-26 15:51:13.222358: Epoch time: 4.11 s 
2023-10-26 15:51:14.358696:  
2023-10-26 15:51:14.359004: Epoch 324 
2023-10-26 15:51:14.359257: Current learning rate: 0.00703 
2023-10-26 15:51:18.458749: train_loss -0.8265 
2023-10-26 15:51:18.459163: val_loss -0.8091 
2023-10-26 15:51:18.459433: Pseudo dice [0.8676, 0.9114, 0.9668, 0.7184, 0.9335] 
2023-10-26 15:51:18.459669: Epoch time: 4.1 s 
2023-10-26 15:51:19.539402:  
2023-10-26 15:51:19.539716: Epoch 325 
2023-10-26 15:51:19.539958: Current learning rate: 0.00702 
2023-10-26 15:51:23.679325: train_loss -0.837 
2023-10-26 15:51:23.679686: val_loss -0.8112 
2023-10-26 15:51:23.679951: Pseudo dice [0.8711, 0.9132, 0.9658, 0.6378, 0.936] 
2023-10-26 15:51:23.680182: Epoch time: 4.14 s 
2023-10-26 15:51:24.814721:  
2023-10-26 15:51:24.815032: Epoch 326 
2023-10-26 15:51:24.815293: Current learning rate: 0.00701 
2023-10-26 15:51:28.915675: train_loss -0.8289 
2023-10-26 15:51:28.916184: val_loss -0.8318 
2023-10-26 15:51:28.916672: Pseudo dice [0.8691, 0.9061, 0.967, 0.6798, 0.9315] 
2023-10-26 15:51:28.917037: Epoch time: 4.1 s 
2023-10-26 15:51:30.004406:  
2023-10-26 15:51:30.004720: Epoch 327 
2023-10-26 15:51:30.004971: Current learning rate: 0.007 
2023-10-26 15:51:34.097955: train_loss -0.8385 
2023-10-26 15:51:34.098350: val_loss -0.8254 
2023-10-26 15:51:34.098712: Pseudo dice [0.8732, 0.9135, 0.9681, 0.6749, 0.9326] 
2023-10-26 15:51:34.099097: Epoch time: 4.09 s 
2023-10-26 15:51:35.322248:  
2023-10-26 15:51:35.322545: Epoch 328 
2023-10-26 15:51:35.322784: Current learning rate: 0.00699 
2023-10-26 15:51:39.488978: train_loss -0.836 
2023-10-26 15:51:39.489377: val_loss -0.8083 
2023-10-26 15:51:39.489648: Pseudo dice [0.8685, 0.9085, 0.9642, 0.633, 0.9295] 
2023-10-26 15:51:39.489896: Epoch time: 4.17 s 
2023-10-26 15:51:40.569385:  
2023-10-26 15:51:40.569689: Epoch 329 
2023-10-26 15:51:40.569951: Current learning rate: 0.00698 
2023-10-26 15:51:44.664501: train_loss -0.8279 
2023-10-26 15:51:44.664884: val_loss -0.7994 
2023-10-26 15:51:44.665154: Pseudo dice [0.8675, 0.9055, 0.9645, 0.7076, 0.9322] 
2023-10-26 15:51:44.665406: Epoch time: 4.1 s 
2023-10-26 15:51:45.762336:  
2023-10-26 15:51:45.762653: Epoch 330 
2023-10-26 15:51:45.762907: Current learning rate: 0.00697 
2023-10-26 15:51:49.703422: train_loss -0.8314 
2023-10-26 15:51:49.703795: val_loss -0.8108 
2023-10-26 15:51:49.704073: Pseudo dice [0.87, 0.9166, 0.9644, 0.8444, 0.9315] 
2023-10-26 15:51:49.704302: Epoch time: 3.94 s 
2023-10-26 15:51:50.762946:  
2023-10-26 15:51:50.763257: Epoch 331 
2023-10-26 15:51:50.763506: Current learning rate: 0.00696 
2023-10-26 15:51:54.686273: train_loss -0.8373 
2023-10-26 15:51:54.686668: val_loss -0.8038 
2023-10-26 15:51:54.687071: Pseudo dice [0.8708, 0.9154, 0.965, 0.6692, 0.9371] 
2023-10-26 15:51:54.687518: Epoch time: 3.92 s 
2023-10-26 15:51:55.758776:  
2023-10-26 15:51:55.759104: Epoch 332 
2023-10-26 15:51:55.759353: Current learning rate: 0.00696 
2023-10-26 15:51:59.888638: train_loss -0.8252 
2023-10-26 15:51:59.889039: val_loss -0.7814 
2023-10-26 15:51:59.889298: Pseudo dice [0.8635, 0.9116, 0.963, 0.4842, 0.9358] 
2023-10-26 15:51:59.889527: Epoch time: 4.13 s 
2023-10-26 15:52:01.024631:  
2023-10-26 15:52:01.024958: Epoch 333 
2023-10-26 15:52:01.025203: Current learning rate: 0.00695 
2023-10-26 15:52:05.035182: train_loss -0.8229 
2023-10-26 15:52:05.035570: val_loss -0.8061 
2023-10-26 15:52:05.035830: Pseudo dice [0.874, 0.9148, 0.9634, 0.5476, 0.9364] 
2023-10-26 15:52:05.036072: Epoch time: 4.01 s 
2023-10-26 15:52:06.250020:  
2023-10-26 15:52:06.250337: Epoch 334 
2023-10-26 15:52:06.250587: Current learning rate: 0.00694 
2023-10-26 15:52:10.384283: train_loss -0.8293 
2023-10-26 15:52:10.384660: val_loss -0.804 
2023-10-26 15:52:10.384944: Pseudo dice [0.8563, 0.9169, 0.965, 0.5367, 0.928] 
2023-10-26 15:52:10.385188: Epoch time: 4.13 s 
2023-10-26 15:52:11.462183:  
2023-10-26 15:52:11.462502: Epoch 335 
2023-10-26 15:52:11.462758: Current learning rate: 0.00693 
2023-10-26 15:52:15.513914: train_loss -0.8392 
2023-10-26 15:52:15.514306: val_loss -0.7882 
2023-10-26 15:52:15.514569: Pseudo dice [0.8669, 0.9163, 0.9654, 0.7102, 0.9359] 
2023-10-26 15:52:15.514807: Epoch time: 4.05 s 
2023-10-26 15:52:16.597661:  
2023-10-26 15:52:16.597986: Epoch 336 
2023-10-26 15:52:16.598236: Current learning rate: 0.00692 
2023-10-26 15:52:20.704369: train_loss -0.8423 
2023-10-26 15:52:20.704766: val_loss -0.7456 
2023-10-26 15:52:20.705045: Pseudo dice [0.871, 0.9149, 0.9653, 0.5172, 0.9368] 
2023-10-26 15:52:20.705273: Epoch time: 4.11 s 
2023-10-26 15:52:21.834638:  
2023-10-26 15:52:21.835032: Epoch 337 
2023-10-26 15:52:21.835393: Current learning rate: 0.00691 
2023-10-26 15:52:25.916949: train_loss -0.835 
2023-10-26 15:52:25.917302: val_loss -0.809 
2023-10-26 15:52:25.917564: Pseudo dice [0.873, 0.9203, 0.966, 0.6833, 0.928] 
2023-10-26 15:52:25.917804: Epoch time: 4.08 s 
2023-10-26 15:52:26.999818:  
2023-10-26 15:52:27.000109: Epoch 338 
2023-10-26 15:52:27.000354: Current learning rate: 0.0069 
2023-10-26 15:52:31.211416: train_loss -0.8363 
2023-10-26 15:52:31.211839: val_loss -0.8026 
2023-10-26 15:52:31.212112: Pseudo dice [0.8589, 0.9121, 0.9656, 0.6688, 0.9369] 
2023-10-26 15:52:31.212341: Epoch time: 4.21 s 
2023-10-26 15:52:32.332480:  
2023-10-26 15:52:32.332781: Epoch 339 
2023-10-26 15:52:32.333038: Current learning rate: 0.00689 
2023-10-26 15:52:36.525566: train_loss -0.8214 
2023-10-26 15:52:36.525934: val_loss -0.7908 
2023-10-26 15:52:36.526200: Pseudo dice [0.8709, 0.9162, 0.9664, 0.6146, 0.9362] 
2023-10-26 15:52:36.526432: Epoch time: 4.19 s 
2023-10-26 15:52:37.760379:  
2023-10-26 15:52:37.760699: Epoch 340 
2023-10-26 15:52:37.760972: Current learning rate: 0.00688 
2023-10-26 15:52:41.989044: train_loss -0.8425 
2023-10-26 15:52:41.989448: val_loss -0.7941 
2023-10-26 15:52:41.989715: Pseudo dice [0.8677, 0.9165, 0.9666, 0.6762, 0.9338] 
2023-10-26 15:52:41.989955: Epoch time: 4.23 s 
2023-10-26 15:52:43.085447:  
2023-10-26 15:52:43.085745: Epoch 341 
2023-10-26 15:52:43.086000: Current learning rate: 0.00687 
2023-10-26 15:52:47.260459: train_loss -0.8344 
2023-10-26 15:52:47.260818: val_loss -0.8 
2023-10-26 15:52:47.261089: Pseudo dice [0.8643, 0.914, 0.9657, 0.6248, 0.9352] 
2023-10-26 15:52:47.261319: Epoch time: 4.18 s 
2023-10-26 15:52:48.339581:  
2023-10-26 15:52:48.339897: Epoch 342 
2023-10-26 15:52:48.340138: Current learning rate: 0.00686 
2023-10-26 15:52:52.494988: train_loss -0.8186 
2023-10-26 15:52:52.495383: val_loss -0.7981 
2023-10-26 15:52:52.495646: Pseudo dice [0.8792, 0.9055, 0.9662, 0.5203, 0.9337] 
2023-10-26 15:52:52.495891: Epoch time: 4.16 s 
2023-10-26 15:52:53.579718:  
2023-10-26 15:52:53.580024: Epoch 343 
2023-10-26 15:52:53.580265: Current learning rate: 0.00685 
2023-10-26 15:52:57.777681: train_loss -0.8223 
2023-10-26 15:52:57.778049: val_loss -0.8028 
2023-10-26 15:52:57.778312: Pseudo dice [0.8731, 0.9179, 0.9674, 0.6427, 0.9312] 
2023-10-26 15:52:57.778543: Epoch time: 4.2 s 
2023-10-26 15:52:58.905089:  
2023-10-26 15:52:58.905399: Epoch 344 
2023-10-26 15:52:58.905657: Current learning rate: 0.00684 
2023-10-26 15:53:03.139069: train_loss -0.8344 
2023-10-26 15:53:03.139436: val_loss -0.7777 
2023-10-26 15:53:03.139704: Pseudo dice [0.8665, 0.9158, 0.9654, 0.6238, 0.9382] 
2023-10-26 15:53:03.139937: Epoch time: 4.23 s 
2023-10-26 15:53:04.219617:  
2023-10-26 15:53:04.219925: Epoch 345 
2023-10-26 15:53:04.220176: Current learning rate: 0.00683 
2023-10-26 15:53:08.340357: train_loss -0.8341 
2023-10-26 15:53:08.340972: val_loss -0.8032 
2023-10-26 15:53:08.341272: Pseudo dice [0.8715, 0.913, 0.9638, 0.742, 0.9338] 
2023-10-26 15:53:08.341515: Epoch time: 4.12 s 
2023-10-26 15:53:09.421002:  
2023-10-26 15:53:09.421297: Epoch 346 
2023-10-26 15:53:09.421539: Current learning rate: 0.00682 
2023-10-26 15:53:13.527325: train_loss -0.8322 
2023-10-26 15:53:13.527748: val_loss -0.7684 
2023-10-26 15:53:13.528023: Pseudo dice [0.8622, 0.905, 0.9625, 0.7632, 0.9309] 
2023-10-26 15:53:13.528268: Epoch time: 4.11 s 
2023-10-26 15:53:14.776562:  
2023-10-26 15:53:14.776886: Epoch 347 
2023-10-26 15:53:14.777139: Current learning rate: 0.00681 
2023-10-26 15:53:18.939260: train_loss -0.8393 
2023-10-26 15:53:18.939638: val_loss -0.8074 
2023-10-26 15:53:18.939903: Pseudo dice [0.8555, 0.911, 0.9634, 0.0045, 0.9336] 
2023-10-26 15:53:18.940147: Epoch time: 4.16 s 
2023-10-26 15:53:20.023801:  
2023-10-26 15:53:20.024108: Epoch 348 
2023-10-26 15:53:20.024357: Current learning rate: 0.0068 
2023-10-26 15:53:24.198729: train_loss -0.8298 
2023-10-26 15:53:24.199092: val_loss -0.8097 
2023-10-26 15:53:24.199350: Pseudo dice [0.8688, 0.9128, 0.9663, 0.7416, 0.9317] 
2023-10-26 15:53:24.199573: Epoch time: 4.18 s 
2023-10-26 15:53:25.275599:  
2023-10-26 15:53:25.275897: Epoch 349 
2023-10-26 15:53:25.276149: Current learning rate: 0.0068 
2023-10-26 15:53:29.484069: train_loss -0.8384 
2023-10-26 15:53:29.484475: val_loss -0.8216 
2023-10-26 15:53:29.484758: Pseudo dice [0.8734, 0.9144, 0.966, 0.7375, 0.9334] 
2023-10-26 15:53:29.485009: Epoch time: 4.21 s 
2023-10-26 15:53:30.659691:  
2023-10-26 15:53:30.660027: Epoch 350 
2023-10-26 15:53:30.660280: Current learning rate: 0.00679 
2023-10-26 15:53:34.789061: train_loss -0.8342 
2023-10-26 15:53:34.789487: val_loss -0.8224 
2023-10-26 15:53:34.789772: Pseudo dice [0.87, 0.9183, 0.9671, 0.6569, 0.9375] 
2023-10-26 15:53:34.790025: Epoch time: 4.13 s 
2023-10-26 15:53:35.866382:  
2023-10-26 15:53:35.866683: Epoch 351 
2023-10-26 15:53:35.866928: Current learning rate: 0.00678 
2023-10-26 15:53:39.963845: train_loss -0.833 
2023-10-26 15:53:39.964261: val_loss -0.8107 
2023-10-26 15:53:39.964524: Pseudo dice [0.8689, 0.9105, 0.9653, 0.7709, 0.9225] 
2023-10-26 15:53:39.964769: Epoch time: 4.1 s 
2023-10-26 15:53:41.071499:  
2023-10-26 15:53:41.071808: Epoch 352 
2023-10-26 15:53:41.072052: Current learning rate: 0.00677 
2023-10-26 15:53:45.128045: train_loss -0.8267 
2023-10-26 15:53:45.128463: val_loss -0.8156 
2023-10-26 15:53:45.128816: Pseudo dice [0.8678, 0.9174, 0.9641, 0.7264, 0.9111] 
2023-10-26 15:53:45.129136: Epoch time: 4.06 s 
2023-10-26 15:53:46.411787:  
2023-10-26 15:53:46.412095: Epoch 353 
2023-10-26 15:53:46.412340: Current learning rate: 0.00676 
2023-10-26 15:53:50.676814: train_loss -0.8337 
2023-10-26 15:53:50.677530: val_loss -0.779 
2023-10-26 15:53:50.677818: Pseudo dice [0.8642, 0.9103, 0.9667, 0.4815, 0.9361] 
2023-10-26 15:53:50.678060: Epoch time: 4.27 s 
2023-10-26 15:53:51.773635:  
2023-10-26 15:53:51.773949: Epoch 354 
2023-10-26 15:53:51.774197: Current learning rate: 0.00675 
2023-10-26 15:53:55.833691: train_loss -0.8273 
2023-10-26 15:53:55.834132: val_loss -0.8133 
2023-10-26 15:53:55.834444: Pseudo dice [0.8739, 0.9098, 0.9643, 0.2885, 0.9336] 
2023-10-26 15:53:55.834802: Epoch time: 4.06 s 
2023-10-26 15:53:56.949788:  
2023-10-26 15:53:56.950154: Epoch 355 
2023-10-26 15:53:56.950469: Current learning rate: 0.00674 
2023-10-26 15:54:01.000371: train_loss -0.8334 
2023-10-26 15:54:01.000783: val_loss -0.8148 
2023-10-26 15:54:01.001082: Pseudo dice [0.8727, 0.9101, 0.9623, 0.8285, 0.9323] 
2023-10-26 15:54:01.001323: Epoch time: 4.05 s 
2023-10-26 15:54:02.103174:  
2023-10-26 15:54:02.103516: Epoch 356 
2023-10-26 15:54:02.103759: Current learning rate: 0.00673 
2023-10-26 15:54:06.302467: train_loss -0.8281 
2023-10-26 15:54:06.303008: val_loss -0.8332 
2023-10-26 15:54:06.303422: Pseudo dice [0.8674, 0.9198, 0.9663, 0.6889, 0.9363] 
2023-10-26 15:54:06.304024: Epoch time: 4.2 s 
2023-10-26 15:54:07.397345:  
2023-10-26 15:54:07.397643: Epoch 357 
2023-10-26 15:54:07.397900: Current learning rate: 0.00672 
2023-10-26 15:54:11.513404: train_loss -0.8397 
2023-10-26 15:54:11.513805: val_loss -0.8175 
2023-10-26 15:54:11.514090: Pseudo dice [0.873, 0.9135, 0.9622, 0.7541, 0.9359] 
2023-10-26 15:54:11.514332: Epoch time: 4.12 s 
2023-10-26 15:54:12.605766:  
2023-10-26 15:54:12.606077: Epoch 358 
2023-10-26 15:54:12.606338: Current learning rate: 0.00671 
2023-10-26 15:54:16.699733: train_loss -0.8468 
2023-10-26 15:54:16.700160: val_loss -0.8061 
2023-10-26 15:54:16.700425: Pseudo dice [0.8702, 0.9139, 0.9636, 0.7314, 0.9335] 
2023-10-26 15:54:16.700661: Epoch time: 4.09 s 
2023-10-26 15:54:17.966688:  
2023-10-26 15:54:17.967055: Epoch 359 
2023-10-26 15:54:17.967360: Current learning rate: 0.0067 
2023-10-26 15:54:21.827886: train_loss -0.8326 
2023-10-26 15:54:21.828261: val_loss -0.8375 
2023-10-26 15:54:21.828541: Pseudo dice [0.8719, 0.9104, 0.964, 0.7761, 0.9322] 
2023-10-26 15:54:21.828779: Epoch time: 3.86 s 
2023-10-26 15:54:22.922038:  
2023-10-26 15:54:22.922344: Epoch 360 
2023-10-26 15:54:22.922588: Current learning rate: 0.00669 
2023-10-26 15:54:26.923540: train_loss -0.8336 
2023-10-26 15:54:26.923955: val_loss -0.7751 
2023-10-26 15:54:26.924251: Pseudo dice [0.8689, 0.9135, 0.9632, 0.3373, 0.9356] 
2023-10-26 15:54:26.924496: Epoch time: 4.0 s 
2023-10-26 15:54:28.030833:  
2023-10-26 15:54:28.031149: Epoch 361 
2023-10-26 15:54:28.031403: Current learning rate: 0.00668 
2023-10-26 15:54:32.157366: train_loss -0.836 
2023-10-26 15:54:32.157741: val_loss -0.7748 
2023-10-26 15:54:32.158015: Pseudo dice [0.8609, 0.9091, 0.9626, 0.6575, 0.9285] 
2023-10-26 15:54:32.158246: Epoch time: 4.13 s 
2023-10-26 15:54:33.273993:  
2023-10-26 15:54:33.274336: Epoch 362 
2023-10-26 15:54:33.274604: Current learning rate: 0.00667 
2023-10-26 15:54:37.338091: train_loss -0.8462 
2023-10-26 15:54:37.338703: val_loss -0.8107 
2023-10-26 15:54:37.339003: Pseudo dice [0.8692, 0.9117, 0.965, 0.6425, 0.9357] 
2023-10-26 15:54:37.339288: Epoch time: 4.06 s 
2023-10-26 15:54:38.437665:  
2023-10-26 15:54:38.437982: Epoch 363 
2023-10-26 15:54:38.438232: Current learning rate: 0.00666 
2023-10-26 15:54:42.567942: train_loss -0.8331 
2023-10-26 15:54:42.568318: val_loss -0.8037 
2023-10-26 15:54:42.568601: Pseudo dice [0.8615, 0.9181, 0.966, 0.3537, 0.9353] 
2023-10-26 15:54:42.568842: Epoch time: 4.13 s 
2023-10-26 15:54:43.673763:  
2023-10-26 15:54:43.674064: Epoch 364 
2023-10-26 15:54:43.674313: Current learning rate: 0.00665 
2023-10-26 15:54:47.716109: train_loss -0.8413 
2023-10-26 15:54:47.716813: val_loss -0.8161 
2023-10-26 15:54:47.717257: Pseudo dice [0.8733, 0.9148, 0.9657, 0.628, 0.937] 
2023-10-26 15:54:47.717556: Epoch time: 4.04 s 
2023-10-26 15:54:49.020196:  
2023-10-26 15:54:49.020490: Epoch 365 
2023-10-26 15:54:49.020752: Current learning rate: 0.00665 
2023-10-26 15:54:53.183622: train_loss -0.8395 
2023-10-26 15:54:53.184025: val_loss -0.7885 
2023-10-26 15:54:53.184302: Pseudo dice [0.867, 0.9084, 0.9656, 0.5197, 0.9332] 
2023-10-26 15:54:53.184541: Epoch time: 4.16 s 
2023-10-26 15:54:54.345030:  
2023-10-26 15:54:54.345351: Epoch 366 
2023-10-26 15:54:54.345608: Current learning rate: 0.00664 
2023-10-26 15:54:58.452607: train_loss -0.8354 
2023-10-26 15:54:58.453056: val_loss -0.7943 
2023-10-26 15:54:58.453336: Pseudo dice [0.8639, 0.9151, 0.9642, 0.494, 0.9334] 
2023-10-26 15:54:58.453585: Epoch time: 4.11 s 
2023-10-26 15:54:59.548935:  
2023-10-26 15:54:59.549230: Epoch 367 
2023-10-26 15:54:59.549484: Current learning rate: 0.00663 
2023-10-26 15:55:03.670064: train_loss -0.8359 
2023-10-26 15:55:03.670439: val_loss -0.8266 
2023-10-26 15:55:03.670708: Pseudo dice [0.867, 0.9117, 0.9657, 0.658, 0.9368] 
2023-10-26 15:55:03.670944: Epoch time: 4.12 s 
2023-10-26 15:55:04.802686:  
2023-10-26 15:55:04.802988: Epoch 368 
2023-10-26 15:55:04.803235: Current learning rate: 0.00662 
2023-10-26 15:55:08.973846: train_loss -0.8464 
2023-10-26 15:55:08.974239: val_loss -0.8067 
2023-10-26 15:55:08.974508: Pseudo dice [0.8722, 0.9007, 0.9651, 0.6559, 0.9362] 
2023-10-26 15:55:08.974741: Epoch time: 4.17 s 
2023-10-26 15:55:10.069518:  
2023-10-26 15:55:10.069816: Epoch 369 
2023-10-26 15:55:10.070062: Current learning rate: 0.00661 
2023-10-26 15:55:14.204659: train_loss -0.8389 
2023-10-26 15:55:14.205139: val_loss -0.804 
2023-10-26 15:55:14.205669: Pseudo dice [0.8693, 0.915, 0.9658, 0.6201, 0.934] 
2023-10-26 15:55:14.206021: Epoch time: 4.14 s 
2023-10-26 15:55:15.313922:  
2023-10-26 15:55:15.314222: Epoch 370 
2023-10-26 15:55:15.314478: Current learning rate: 0.0066 
2023-10-26 15:55:19.266604: train_loss -0.8491 
2023-10-26 15:55:19.267003: val_loss -0.7942 
2023-10-26 15:55:19.267262: Pseudo dice [0.8709, 0.9117, 0.9637, 0.5917, 0.9294] 
2023-10-26 15:55:19.267499: Epoch time: 3.95 s 
2023-10-26 15:55:20.372979:  
2023-10-26 15:55:20.373276: Epoch 371 
2023-10-26 15:55:20.373518: Current learning rate: 0.00659 
2023-10-26 15:55:24.202671: train_loss -0.8377 
2023-10-26 15:55:24.203067: val_loss -0.8037 
2023-10-26 15:55:24.203338: Pseudo dice [0.8682, 0.9151, 0.9667, 0.6547, 0.932] 
2023-10-26 15:55:24.203573: Epoch time: 3.83 s 
2023-10-26 15:55:25.495390:  
2023-10-26 15:55:25.495706: Epoch 372 
2023-10-26 15:55:25.495960: Current learning rate: 0.00658 
2023-10-26 15:55:29.527366: train_loss -0.8368 
2023-10-26 15:55:29.527780: val_loss -0.8115 
2023-10-26 15:55:29.528061: Pseudo dice [0.864, 0.8934, 0.9652, 0.343, 0.9349] 
2023-10-26 15:55:29.528300: Epoch time: 4.03 s 
2023-10-26 15:55:30.652987:  
2023-10-26 15:55:30.653289: Epoch 373 
2023-10-26 15:55:30.653536: Current learning rate: 0.00657 
2023-10-26 15:55:34.555147: train_loss -0.8427 
2023-10-26 15:55:34.555520: val_loss -0.8225 
2023-10-26 15:55:34.555798: Pseudo dice [0.8686, 0.9105, 0.9655, 0.569, 0.9335] 
2023-10-26 15:55:34.556041: Epoch time: 3.9 s 
2023-10-26 15:55:35.664629:  
2023-10-26 15:55:35.664924: Epoch 374 
2023-10-26 15:55:35.665173: Current learning rate: 0.00656 
2023-10-26 15:55:39.757696: train_loss -0.8367 
2023-10-26 15:55:39.758086: val_loss -0.8221 
2023-10-26 15:55:39.758358: Pseudo dice [0.8572, 0.91, 0.9644, 0.6145, 0.9277] 
2023-10-26 15:55:39.758604: Epoch time: 4.09 s 
2023-10-26 15:55:40.865420:  
2023-10-26 15:55:40.865718: Epoch 375 
2023-10-26 15:55:40.865961: Current learning rate: 0.00655 
2023-10-26 15:55:45.005440: train_loss -0.8386 
2023-10-26 15:55:45.005906: val_loss -0.8362 
2023-10-26 15:55:45.006377: Pseudo dice [0.8671, 0.9125, 0.9662, 0.5592, 0.9366] 
2023-10-26 15:55:45.006662: Epoch time: 4.14 s 
2023-10-26 15:55:46.134288:  
2023-10-26 15:55:46.134685: Epoch 376 
2023-10-26 15:55:46.134977: Current learning rate: 0.00654 
2023-10-26 15:55:50.323980: train_loss -0.8407 
2023-10-26 15:55:50.324411: val_loss -0.7857 
2023-10-26 15:55:50.324885: Pseudo dice [0.8651, 0.9033, 0.9669, 0.559, 0.9335] 
2023-10-26 15:55:50.325217: Epoch time: 4.19 s 
2023-10-26 15:55:51.452355:  
2023-10-26 15:55:51.452724: Epoch 377 
2023-10-26 15:55:51.453088: Current learning rate: 0.00653 
2023-10-26 15:55:55.465279: train_loss -0.8288 
2023-10-26 15:55:55.465856: val_loss -0.8157 
2023-10-26 15:55:55.466320: Pseudo dice [0.8719, 0.9164, 0.963, 0.6976, 0.9311] 
2023-10-26 15:55:55.466621: Epoch time: 4.01 s 
2023-10-26 15:55:56.794500:  
2023-10-26 15:55:56.794914: Epoch 378 
2023-10-26 15:55:56.795338: Current learning rate: 0.00652 
2023-10-26 15:56:00.812893: train_loss -0.8327 
2023-10-26 15:56:00.813426: val_loss -0.7911 
2023-10-26 15:56:00.813855: Pseudo dice [0.8598, 0.9118, 0.9655, 0.5194, 0.9259] 
2023-10-26 15:56:00.814152: Epoch time: 4.02 s 
2023-10-26 15:56:01.940253:  
2023-10-26 15:56:01.940568: Epoch 379 
2023-10-26 15:56:01.940820: Current learning rate: 0.00651 
2023-10-26 15:56:05.870129: train_loss -0.8401 
2023-10-26 15:56:05.870555: val_loss -0.8132 
2023-10-26 15:56:05.870850: Pseudo dice [0.8737, 0.9064, 0.9645, 0.7186, 0.9306] 
2023-10-26 15:56:05.871106: Epoch time: 3.93 s 
2023-10-26 15:56:06.976623:  
2023-10-26 15:56:06.976919: Epoch 380 
2023-10-26 15:56:06.977170: Current learning rate: 0.0065 
2023-10-26 15:56:11.010325: train_loss -0.8358 
2023-10-26 15:56:11.010729: val_loss -0.8027 
2023-10-26 15:56:11.011014: Pseudo dice [0.8664, 0.9218, 0.9634, 0.5453, 0.9179] 
2023-10-26 15:56:11.011241: Epoch time: 4.03 s 
2023-10-26 15:56:12.121091:  
2023-10-26 15:56:12.121393: Epoch 381 
2023-10-26 15:56:12.121642: Current learning rate: 0.00649 
2023-10-26 15:56:15.959678: train_loss -0.8356 
2023-10-26 15:56:15.960095: val_loss -0.7934 
2023-10-26 15:56:15.960370: Pseudo dice [0.8734, 0.9144, 0.9669, 0.5081, 0.9364] 
2023-10-26 15:56:15.960610: Epoch time: 3.84 s 
2023-10-26 15:56:17.152681:  
2023-10-26 15:56:17.152996: Epoch 382 
2023-10-26 15:56:17.153245: Current learning rate: 0.00648 
2023-10-26 15:56:21.115242: train_loss -0.8366 
2023-10-26 15:56:21.115617: val_loss -0.7913 
2023-10-26 15:56:21.115900: Pseudo dice [0.8692, 0.9156, 0.964, 0.4796, 0.9287] 
2023-10-26 15:56:21.116147: Epoch time: 3.96 s 
2023-10-26 15:56:22.252129:  
2023-10-26 15:56:22.252434: Epoch 383 
2023-10-26 15:56:22.252684: Current learning rate: 0.00648 
2023-10-26 15:56:26.269574: train_loss -0.8381 
2023-10-26 15:56:26.269984: val_loss -0.8044 
2023-10-26 15:56:26.270295: Pseudo dice [0.8725, 0.9109, 0.9638, 0.523, 0.9211] 
2023-10-26 15:56:26.270545: Epoch time: 4.02 s 
2023-10-26 15:56:27.594362:  
2023-10-26 15:56:27.594691: Epoch 384 
2023-10-26 15:56:27.594945: Current learning rate: 0.00647 
2023-10-26 15:56:31.576027: train_loss -0.8392 
2023-10-26 15:56:31.576509: val_loss -0.8309 
2023-10-26 15:56:31.576773: Pseudo dice [0.8628, 0.9121, 0.9646, 0.5099, 0.9338] 
2023-10-26 15:56:31.577017: Epoch time: 3.98 s 
2023-10-26 15:56:32.706777:  
2023-10-26 15:56:32.707085: Epoch 385 
2023-10-26 15:56:32.707335: Current learning rate: 0.00646 
2023-10-26 15:56:36.699526: train_loss -0.8389 
2023-10-26 15:56:36.699882: val_loss -0.8328 
2023-10-26 15:56:36.700150: Pseudo dice [0.8726, 0.9187, 0.9669, 0.4818, 0.9353] 
2023-10-26 15:56:36.700387: Epoch time: 3.99 s 
2023-10-26 15:56:37.812392:  
2023-10-26 15:56:37.812700: Epoch 386 
2023-10-26 15:56:37.812951: Current learning rate: 0.00645 
2023-10-26 15:56:41.859664: train_loss -0.8373 
2023-10-26 15:56:41.860097: val_loss -0.8181 
2023-10-26 15:56:41.860358: Pseudo dice [0.8644, 0.9139, 0.9646, 0.4247, 0.9269] 
2023-10-26 15:56:41.860603: Epoch time: 4.05 s 
2023-10-26 15:56:42.995558:  
2023-10-26 15:56:42.995882: Epoch 387 
2023-10-26 15:56:42.996133: Current learning rate: 0.00644 
2023-10-26 15:56:47.068415: train_loss -0.8346 
2023-10-26 15:56:47.068809: val_loss -0.8243 
2023-10-26 15:56:47.069106: Pseudo dice [0.8717, 0.9138, 0.9659, 0.6087, 0.9381] 
2023-10-26 15:56:47.069338: Epoch time: 4.07 s 
2023-10-26 15:56:48.201937:  
2023-10-26 15:56:48.202241: Epoch 388 
2023-10-26 15:56:48.202495: Current learning rate: 0.00643 
2023-10-26 15:56:52.251190: train_loss -0.8394 
2023-10-26 15:56:52.251676: val_loss -0.8012 
2023-10-26 15:56:52.251968: Pseudo dice [0.871, 0.9137, 0.9657, 0.6154, 0.9274] 
2023-10-26 15:56:52.252231: Epoch time: 4.05 s 
2023-10-26 15:56:53.366968:  
2023-10-26 15:56:53.367270: Epoch 389 
2023-10-26 15:56:53.367525: Current learning rate: 0.00642 
2023-10-26 15:56:57.285577: train_loss -0.8452 
2023-10-26 15:56:57.285962: val_loss -0.7775 
2023-10-26 15:56:57.286231: Pseudo dice [0.8637, 0.9149, 0.9658, 0.6717, 0.9327] 
2023-10-26 15:56:57.286475: Epoch time: 3.92 s 
2023-10-26 15:56:58.592462:  
2023-10-26 15:56:58.592753: Epoch 390 
2023-10-26 15:56:58.593008: Current learning rate: 0.00641 
2023-10-26 15:57:02.591256: train_loss -0.8436 
2023-10-26 15:57:02.591708: val_loss -0.8162 
2023-10-26 15:57:02.592079: Pseudo dice [0.8726, 0.9175, 0.9658, 0.56, 0.9372] 
2023-10-26 15:57:02.592563: Epoch time: 4.0 s 
2023-10-26 15:57:03.714831:  
2023-10-26 15:57:03.715182: Epoch 391 
2023-10-26 15:57:03.715464: Current learning rate: 0.0064 
2023-10-26 15:57:07.818621: train_loss -0.8356 
2023-10-26 15:57:07.819206: val_loss -0.7849 
2023-10-26 15:57:07.819482: Pseudo dice [0.8739, 0.9104, 0.9663, 0.657, 0.9318] 
2023-10-26 15:57:07.819785: Epoch time: 4.1 s 
2023-10-26 15:57:08.938598:  
2023-10-26 15:57:08.939008: Epoch 392 
2023-10-26 15:57:08.939253: Current learning rate: 0.00639 
2023-10-26 15:57:12.942139: train_loss -0.8395 
2023-10-26 15:57:12.942526: val_loss -0.8115 
2023-10-26 15:57:12.942830: Pseudo dice [0.8726, 0.9193, 0.966, 0.719, 0.9383] 
2023-10-26 15:57:12.943353: Epoch time: 4.0 s 
2023-10-26 15:57:14.057792:  
2023-10-26 15:57:14.058087: Epoch 393 
2023-10-26 15:57:14.058328: Current learning rate: 0.00638 
2023-10-26 15:57:18.005797: train_loss -0.8375 
2023-10-26 15:57:18.006267: val_loss -0.8016 
2023-10-26 15:57:18.006552: Pseudo dice [0.8677, 0.9193, 0.9664, 0.7622, 0.9382] 
2023-10-26 15:57:18.006801: Epoch time: 3.95 s 
2023-10-26 15:57:19.135115:  
2023-10-26 15:57:19.135420: Epoch 394 
2023-10-26 15:57:19.135661: Current learning rate: 0.00637 
2023-10-26 15:57:23.267127: train_loss -0.8361 
2023-10-26 15:57:23.267601: val_loss -0.8118 
2023-10-26 15:57:23.267980: Pseudo dice [0.8704, 0.9149, 0.9675, 0.7107, 0.9415] 
2023-10-26 15:57:23.268304: Epoch time: 4.13 s 
2023-10-26 15:57:24.536258:  
2023-10-26 15:57:24.536659: Epoch 395 
2023-10-26 15:57:24.536903: Current learning rate: 0.00636 
2023-10-26 15:57:28.648801: train_loss -0.8422 
2023-10-26 15:57:28.649444: val_loss -0.8214 
2023-10-26 15:57:28.649808: Pseudo dice [0.8576, 0.917, 0.9668, 0.4708, 0.9292] 
2023-10-26 15:57:28.650118: Epoch time: 4.11 s 
2023-10-26 15:57:29.958574:  
2023-10-26 15:57:29.958901: Epoch 396 
2023-10-26 15:57:29.959165: Current learning rate: 0.00635 
2023-10-26 15:57:34.052182: train_loss -0.8357 
2023-10-26 15:57:34.052544: val_loss -0.7722 
2023-10-26 15:57:34.052803: Pseudo dice [0.8534, 0.9096, 0.9609, 0.092, 0.9358] 
2023-10-26 15:57:34.053047: Epoch time: 4.09 s 
2023-10-26 15:57:35.167648:  
2023-10-26 15:57:35.167951: Epoch 397 
2023-10-26 15:57:35.168200: Current learning rate: 0.00634 
2023-10-26 15:57:39.130327: train_loss -0.8324 
2023-10-26 15:57:39.130758: val_loss -0.8132 
2023-10-26 15:57:39.131046: Pseudo dice [0.8727, 0.9119, 0.9658, 0.6961, 0.9368] 
2023-10-26 15:57:39.131303: Epoch time: 3.96 s 
2023-10-26 15:57:40.318696:  
2023-10-26 15:57:40.319010: Epoch 398 
2023-10-26 15:57:40.319262: Current learning rate: 0.00633 
2023-10-26 15:57:44.266840: train_loss -0.8445 
2023-10-26 15:57:44.267305: val_loss -0.8051 
2023-10-26 15:57:44.267577: Pseudo dice [0.8696, 0.9161, 0.966, 0.692, 0.9349] 
2023-10-26 15:57:44.267806: Epoch time: 3.95 s 
2023-10-26 15:57:45.406146:  
2023-10-26 15:57:45.406436: Epoch 399 
2023-10-26 15:57:45.406678: Current learning rate: 0.00632 
2023-10-26 15:57:49.417642: train_loss -0.8486 
2023-10-26 15:57:49.418082: val_loss -0.7908 
2023-10-26 15:57:49.418358: Pseudo dice [0.8666, 0.9138, 0.967, 0.6898, 0.9372] 
2023-10-26 15:57:49.418601: Epoch time: 4.01 s 
2023-10-26 15:57:50.631464:  
2023-10-26 15:57:50.631775: Epoch 400 
2023-10-26 15:57:50.632038: Current learning rate: 0.00631 
2023-10-26 15:57:54.801918: train_loss -0.8408 
2023-10-26 15:57:54.802301: val_loss -0.79 
2023-10-26 15:57:54.802567: Pseudo dice [0.8701, 0.916, 0.9656, 0.7337, 0.9326] 
2023-10-26 15:57:54.802891: Epoch time: 4.17 s 
2023-10-26 15:57:55.951101:  
2023-10-26 15:57:55.951399: Epoch 401 
2023-10-26 15:57:55.951663: Current learning rate: 0.0063 
2023-10-26 15:57:59.999239: train_loss -0.8373 
2023-10-26 15:57:59.999659: val_loss -0.8138 
2023-10-26 15:57:59.999935: Pseudo dice [0.8712, 0.9073, 0.965, 0.7858, 0.9348] 
2023-10-26 15:58:00.000190: Epoch time: 4.05 s 
2023-10-26 15:58:01.300563:  
2023-10-26 15:58:01.300907: Epoch 402 
2023-10-26 15:58:01.301163: Current learning rate: 0.0063 
2023-10-26 15:58:05.345892: train_loss -0.8373 
2023-10-26 15:58:05.346243: val_loss -0.7872 
2023-10-26 15:58:05.346514: Pseudo dice [0.8642, 0.9117, 0.9637, 0.2883, 0.9401] 
2023-10-26 15:58:05.346753: Epoch time: 4.05 s 
2023-10-26 15:58:06.462510:  
2023-10-26 15:58:06.462830: Epoch 403 
2023-10-26 15:58:06.463099: Current learning rate: 0.00629 
2023-10-26 15:58:10.468854: train_loss -0.8418 
2023-10-26 15:58:10.469268: val_loss -0.8273 
2023-10-26 15:58:10.469537: Pseudo dice [0.8713, 0.9124, 0.9655, 0.6996, 0.9312] 
2023-10-26 15:58:10.469773: Epoch time: 4.01 s 
2023-10-26 15:58:11.592669:  
2023-10-26 15:58:11.592989: Epoch 404 
2023-10-26 15:58:11.593235: Current learning rate: 0.00628 
2023-10-26 15:58:15.714330: train_loss -0.8352 
2023-10-26 15:58:15.714720: val_loss -0.8254 
2023-10-26 15:58:15.714998: Pseudo dice [0.871, 0.9153, 0.9665, 0.734, 0.9349] 
2023-10-26 15:58:15.715230: Epoch time: 4.12 s 
2023-10-26 15:58:16.855996:  
2023-10-26 15:58:16.856306: Epoch 405 
2023-10-26 15:58:16.856560: Current learning rate: 0.00627 
2023-10-26 15:58:20.928661: train_loss -0.8385 
2023-10-26 15:58:20.929330: val_loss -0.8074 
2023-10-26 15:58:20.929799: Pseudo dice [0.8646, 0.9028, 0.9652, 0.5822, 0.9318] 
2023-10-26 15:58:20.930155: Epoch time: 4.07 s 
2023-10-26 15:58:22.048268:  
2023-10-26 15:58:22.048575: Epoch 406 
2023-10-26 15:58:22.048816: Current learning rate: 0.00626 
2023-10-26 15:58:26.075840: train_loss -0.8494 
2023-10-26 15:58:26.076200: val_loss -0.8089 
2023-10-26 15:58:26.076472: Pseudo dice [0.8691, 0.9137, 0.9646, 0.6949, 0.9347] 
2023-10-26 15:58:26.076705: Epoch time: 4.03 s 
2023-10-26 15:58:27.195998:  
2023-10-26 15:58:27.196290: Epoch 407 
2023-10-26 15:58:27.196536: Current learning rate: 0.00625 
2023-10-26 15:58:31.244432: train_loss -0.8458 
2023-10-26 15:58:31.244820: val_loss -0.8237 
2023-10-26 15:58:31.245098: Pseudo dice [0.8707, 0.9111, 0.9651, 0.7916, 0.9348] 
2023-10-26 15:58:31.245333: Epoch time: 4.05 s 
2023-10-26 15:58:32.572904:  
2023-10-26 15:58:32.573227: Epoch 408 
2023-10-26 15:58:32.573488: Current learning rate: 0.00624 
2023-10-26 15:58:36.578523: train_loss -0.8353 
2023-10-26 15:58:36.578930: val_loss -0.8269 
2023-10-26 15:58:36.579194: Pseudo dice [0.8634, 0.9172, 0.9661, 0.7731, 0.9346] 
2023-10-26 15:58:36.579430: Epoch time: 4.01 s 
2023-10-26 15:58:37.711830:  
2023-10-26 15:58:37.712154: Epoch 409 
2023-10-26 15:58:37.712393: Current learning rate: 0.00623 
2023-10-26 15:58:41.752997: train_loss -0.8424 
2023-10-26 15:58:41.753345: val_loss -0.813 
2023-10-26 15:58:41.753608: Pseudo dice [0.8592, 0.9198, 0.9653, 0.6951, 0.9187] 
2023-10-26 15:58:41.753825: Epoch time: 4.04 s 
2023-10-26 15:58:42.873616:  
2023-10-26 15:58:42.873928: Epoch 410 
2023-10-26 15:58:42.874194: Current learning rate: 0.00622 
2023-10-26 15:58:46.835093: train_loss -0.8455 
2023-10-26 15:58:46.835584: val_loss -0.8142 
2023-10-26 15:58:46.835999: Pseudo dice [0.8636, 0.9183, 0.9654, 0.619, 0.9315] 
2023-10-26 15:58:46.836491: Epoch time: 3.96 s 
2023-10-26 15:58:47.913415:  
2023-10-26 15:58:47.913721: Epoch 411 
2023-10-26 15:58:47.913968: Current learning rate: 0.00621 
2023-10-26 15:58:52.017647: train_loss -0.8388 
2023-10-26 15:58:52.018039: val_loss -0.814 
2023-10-26 15:58:52.018312: Pseudo dice [0.8716, 0.9101, 0.9647, 0.6639, 0.938] 
2023-10-26 15:58:52.018567: Epoch time: 4.1 s 
2023-10-26 15:58:53.096228:  
2023-10-26 15:58:53.096548: Epoch 412 
2023-10-26 15:58:53.096800: Current learning rate: 0.0062 
2023-10-26 15:58:57.111440: train_loss -0.8426 
2023-10-26 15:58:57.111846: val_loss -0.8042 
2023-10-26 15:58:57.112144: Pseudo dice [0.8766, 0.9172, 0.9673, 0.6655, 0.9335] 
2023-10-26 15:58:57.112397: Epoch time: 4.02 s 
2023-10-26 15:58:58.216487:  
2023-10-26 15:58:58.216808: Epoch 413 
2023-10-26 15:58:58.217061: Current learning rate: 0.00619 
2023-10-26 15:59:02.248141: train_loss -0.8396 
2023-10-26 15:59:02.248531: val_loss -0.8074 
2023-10-26 15:59:02.248961: Pseudo dice [0.8683, 0.908, 0.9674, 0.7178, 0.9325] 
2023-10-26 15:59:02.249256: Epoch time: 4.03 s 
2023-10-26 15:59:03.347392:  
2023-10-26 15:59:03.347708: Epoch 414 
2023-10-26 15:59:03.347956: Current learning rate: 0.00618 
2023-10-26 15:59:07.357234: train_loss -0.8527 
2023-10-26 15:59:07.357659: val_loss -0.807 
2023-10-26 15:59:07.357936: Pseudo dice [0.8739, 0.9196, 0.9662, 0.732, 0.9378] 
2023-10-26 15:59:07.358197: Epoch time: 4.01 s 
2023-10-26 15:59:08.615023:  
2023-10-26 15:59:08.615355: Epoch 415 
2023-10-26 15:59:08.615615: Current learning rate: 0.00617 
2023-10-26 15:59:12.569920: train_loss -0.8519 
2023-10-26 15:59:12.570296: val_loss -0.7909 
2023-10-26 15:59:12.570564: Pseudo dice [0.8702, 0.912, 0.9669, 0.714, 0.9381] 
2023-10-26 15:59:12.570810: Epoch time: 3.96 s 
2023-10-26 15:59:13.641136:  
2023-10-26 15:59:13.641439: Epoch 416 
2023-10-26 15:59:13.641694: Current learning rate: 0.00616 
2023-10-26 15:59:17.656772: train_loss -0.8474 
2023-10-26 15:59:17.657485: val_loss -0.7915 
2023-10-26 15:59:17.657819: Pseudo dice [0.8787, 0.9091, 0.9669, 0.6707, 0.9296] 
2023-10-26 15:59:17.658155: Epoch time: 4.02 s 
2023-10-26 15:59:18.735804:  
2023-10-26 15:59:18.736113: Epoch 417 
2023-10-26 15:59:18.736363: Current learning rate: 0.00615 
2023-10-26 15:59:22.848222: train_loss -0.8493 
2023-10-26 15:59:22.848654: val_loss -0.7814 
2023-10-26 15:59:22.849201: Pseudo dice [0.8649, 0.9087, 0.9648, 0.6783, 0.9361] 
2023-10-26 15:59:22.849464: Epoch time: 4.11 s 
2023-10-26 15:59:24.018976:  
2023-10-26 15:59:24.019287: Epoch 418 
2023-10-26 15:59:24.019539: Current learning rate: 0.00614 
2023-10-26 15:59:28.009626: train_loss -0.8478 
2023-10-26 15:59:28.010051: val_loss -0.7754 
2023-10-26 15:59:28.010328: Pseudo dice [0.8632, 0.9068, 0.9665, 0.699, 0.9338] 
2023-10-26 15:59:28.010580: Epoch time: 3.99 s 
2023-10-26 15:59:29.095572:  
2023-10-26 15:59:29.095885: Epoch 419 
2023-10-26 15:59:29.096139: Current learning rate: 0.00613 
2023-10-26 15:59:33.140044: train_loss -0.8447 
2023-10-26 15:59:33.140402: val_loss -0.7952 
2023-10-26 15:59:33.140663: Pseudo dice [0.8727, 0.9159, 0.9642, 0.6053, 0.9343] 
2023-10-26 15:59:33.140910: Epoch time: 4.05 s 
2023-10-26 15:59:34.202376:  
2023-10-26 15:59:34.202672: Epoch 420 
2023-10-26 15:59:34.202930: Current learning rate: 0.00612 
2023-10-26 15:59:38.142429: train_loss -0.8399 
2023-10-26 15:59:38.142846: val_loss -0.7976 
2023-10-26 15:59:38.143126: Pseudo dice [0.8654, 0.9153, 0.9654, 0.4267, 0.9354] 
2023-10-26 15:59:38.143371: Epoch time: 3.94 s 
2023-10-26 15:59:39.300880:  
2023-10-26 15:59:39.301188: Epoch 421 
2023-10-26 15:59:39.301432: Current learning rate: 0.00612 
2023-10-26 15:59:43.309470: train_loss -0.8475 
2023-10-26 15:59:43.309916: val_loss -0.813 
2023-10-26 15:59:43.310170: Pseudo dice [0.8713, 0.9214, 0.9655, 0.5455, 0.9349] 
2023-10-26 15:59:43.310462: Epoch time: 4.01 s 
2023-10-26 15:59:44.579000:  
2023-10-26 15:59:44.579319: Epoch 422 
2023-10-26 15:59:44.579565: Current learning rate: 0.00611 
2023-10-26 15:59:48.696430: train_loss -0.8419 
2023-10-26 15:59:48.696815: val_loss -0.8053 
2023-10-26 15:59:48.697086: Pseudo dice [0.8658, 0.9126, 0.9658, 0.6381, 0.9255] 
2023-10-26 15:59:48.697319: Epoch time: 4.12 s 
2023-10-26 15:59:49.789528:  
2023-10-26 15:59:49.789855: Epoch 423 
2023-10-26 15:59:49.790107: Current learning rate: 0.0061 
2023-10-26 15:59:53.932721: train_loss -0.8456 
2023-10-26 15:59:53.933167: val_loss -0.8001 
2023-10-26 15:59:53.933475: Pseudo dice [0.8655, 0.9196, 0.9672, 0.6073, 0.9337] 
2023-10-26 15:59:53.933717: Epoch time: 4.14 s 
2023-10-26 15:59:55.020834:  
2023-10-26 15:59:55.021176: Epoch 424 
2023-10-26 15:59:55.021433: Current learning rate: 0.00609 
2023-10-26 15:59:59.164778: train_loss -0.8453 
2023-10-26 15:59:59.165230: val_loss -0.8182 
2023-10-26 15:59:59.165691: Pseudo dice [0.8656, 0.9139, 0.9663, 0.5249, 0.9368] 
2023-10-26 15:59:59.165982: Epoch time: 4.14 s 
2023-10-26 16:00:00.240209:  
2023-10-26 16:00:00.240540: Epoch 425 
2023-10-26 16:00:00.240784: Current learning rate: 0.00608 
2023-10-26 16:00:04.252094: train_loss -0.8376 
2023-10-26 16:00:04.252453: val_loss -0.81 
2023-10-26 16:00:04.252712: Pseudo dice [0.8627, 0.916, 0.9654, 0.4938, 0.9353] 
2023-10-26 16:00:04.252963: Epoch time: 4.01 s 
2023-10-26 16:00:05.313410:  
2023-10-26 16:00:05.313703: Epoch 426 
2023-10-26 16:00:05.313962: Current learning rate: 0.00607 
2023-10-26 16:00:09.370931: train_loss -0.8382 
2023-10-26 16:00:09.371386: val_loss -0.8113 
2023-10-26 16:00:09.371668: Pseudo dice [0.8761, 0.9138, 0.9664, 0.6355, 0.9343] 
2023-10-26 16:00:09.371908: Epoch time: 4.06 s 
2023-10-26 16:00:10.438286:  
2023-10-26 16:00:10.438595: Epoch 427 
2023-10-26 16:00:10.438844: Current learning rate: 0.00606 
2023-10-26 16:00:14.606753: train_loss -0.8488 
2023-10-26 16:00:14.607169: val_loss -0.8029 
2023-10-26 16:00:14.607436: Pseudo dice [0.8653, 0.9179, 0.9647, 0.5878, 0.932] 
2023-10-26 16:00:14.607685: Epoch time: 4.17 s 
2023-10-26 16:00:15.837068:  
2023-10-26 16:00:15.837388: Epoch 428 
2023-10-26 16:00:15.837650: Current learning rate: 0.00605 
2023-10-26 16:00:19.995187: train_loss -0.8284 
2023-10-26 16:00:19.995575: val_loss -0.8144 
2023-10-26 16:00:19.995863: Pseudo dice [0.8663, 0.9141, 0.9637, 0.6762, 0.9324] 
2023-10-26 16:00:19.996102: Epoch time: 4.16 s 
2023-10-26 16:00:21.055549:  
2023-10-26 16:00:21.055865: Epoch 429 
2023-10-26 16:00:21.056128: Current learning rate: 0.00604 
2023-10-26 16:00:25.129173: train_loss -0.8328 
2023-10-26 16:00:25.129538: val_loss -0.8012 
2023-10-26 16:00:25.129802: Pseudo dice [0.8532, 0.9115, 0.9656, 0.381, 0.9259] 
2023-10-26 16:00:25.130051: Epoch time: 4.07 s 
2023-10-26 16:00:26.210608:  
2023-10-26 16:00:26.210944: Epoch 430 
2023-10-26 16:00:26.211204: Current learning rate: 0.00603 
2023-10-26 16:00:30.278642: train_loss -0.8328 
2023-10-26 16:00:30.279026: val_loss -0.8167 
2023-10-26 16:00:30.279335: Pseudo dice [0.8748, 0.9114, 0.9641, 0.5084, 0.9402] 
2023-10-26 16:00:30.279583: Epoch time: 4.07 s 
2023-10-26 16:00:31.357675:  
2023-10-26 16:00:31.357997: Epoch 431 
2023-10-26 16:00:31.358255: Current learning rate: 0.00602 
2023-10-26 16:00:35.501759: train_loss -0.8366 
2023-10-26 16:00:35.502136: val_loss -0.8081 
2023-10-26 16:00:35.502545: Pseudo dice [0.8668, 0.9084, 0.9631, 0.668, 0.9332] 
2023-10-26 16:00:35.502788: Epoch time: 4.14 s 
2023-10-26 16:00:36.584432:  
2023-10-26 16:00:36.584735: Epoch 432 
2023-10-26 16:00:36.584985: Current learning rate: 0.00601 
2023-10-26 16:00:40.713556: train_loss -0.8423 
2023-10-26 16:00:40.713921: val_loss -0.8059 
2023-10-26 16:00:40.714195: Pseudo dice [0.8714, 0.9168, 0.9662, 0.6099, 0.9378] 
2023-10-26 16:00:40.714422: Epoch time: 4.13 s 
2023-10-26 16:00:41.802608:  
2023-10-26 16:00:41.802906: Epoch 433 
2023-10-26 16:00:41.803156: Current learning rate: 0.006 
2023-10-26 16:00:45.828693: train_loss -0.8412 
2023-10-26 16:00:45.829092: val_loss -0.8204 
2023-10-26 16:00:45.829347: Pseudo dice [0.8608, 0.9176, 0.9655, 0.2727, 0.9314] 
2023-10-26 16:00:45.829584: Epoch time: 4.03 s 
2023-10-26 16:00:46.911373:  
2023-10-26 16:00:46.911664: Epoch 434 
2023-10-26 16:00:46.911924: Current learning rate: 0.00599 
2023-10-26 16:00:51.078624: train_loss -0.8346 
2023-10-26 16:00:51.079010: val_loss -0.8009 
2023-10-26 16:00:51.079293: Pseudo dice [0.8675, 0.9126, 0.9647, 0.4858, 0.934] 
2023-10-26 16:00:51.079541: Epoch time: 4.17 s 
2023-10-26 16:00:52.364666:  
2023-10-26 16:00:52.364983: Epoch 435 
2023-10-26 16:00:52.365228: Current learning rate: 0.00598 
2023-10-26 16:00:56.540012: train_loss -0.8391 
2023-10-26 16:00:56.540418: val_loss -0.7991 
2023-10-26 16:00:56.540704: Pseudo dice [0.8713, 0.9166, 0.965, 0.6987, 0.9324] 
2023-10-26 16:00:56.540951: Epoch time: 4.18 s 
2023-10-26 16:00:57.627364:  
2023-10-26 16:00:57.627664: Epoch 436 
2023-10-26 16:00:57.627922: Current learning rate: 0.00597 
2023-10-26 16:01:01.705126: train_loss -0.8453 
2023-10-26 16:01:01.705555: val_loss -0.8025 
2023-10-26 16:01:01.705927: Pseudo dice [0.8733, 0.9164, 0.9658, 0.7242, 0.9389] 
2023-10-26 16:01:01.706401: Epoch time: 4.08 s 
2023-10-26 16:01:02.799642:  
2023-10-26 16:01:02.799958: Epoch 437 
2023-10-26 16:01:02.800209: Current learning rate: 0.00596 
2023-10-26 16:01:06.901941: train_loss -0.8331 
2023-10-26 16:01:06.913471: val_loss -0.8243 
2023-10-26 16:01:06.913759: Pseudo dice [0.8737, 0.9095, 0.9653, 0.6221, 0.9254] 
2023-10-26 16:01:06.913998: Epoch time: 4.1 s 
2023-10-26 16:01:08.013361:  
2023-10-26 16:01:08.013665: Epoch 438 
2023-10-26 16:01:08.013924: Current learning rate: 0.00595 
2023-10-26 16:01:11.972957: train_loss -0.8413 
2023-10-26 16:01:11.973338: val_loss -0.8317 
2023-10-26 16:01:11.973597: Pseudo dice [0.8741, 0.9152, 0.9667, 0.6766, 0.9345] 
2023-10-26 16:01:11.973837: Epoch time: 3.96 s 
2023-10-26 16:01:13.115419:  
2023-10-26 16:01:13.115716: Epoch 439 
2023-10-26 16:01:13.115963: Current learning rate: 0.00594 
2023-10-26 16:01:17.248409: train_loss -0.8466 
2023-10-26 16:01:17.248808: val_loss -0.8234 
2023-10-26 16:01:17.249070: Pseudo dice [0.8704, 0.9036, 0.9656, 0.5721, 0.9394] 
2023-10-26 16:01:17.249308: Epoch time: 4.13 s 
2023-10-26 16:01:18.320972:  
2023-10-26 16:01:18.321277: Epoch 440 
2023-10-26 16:01:18.321560: Current learning rate: 0.00593 
2023-10-26 16:01:22.415003: train_loss -0.8331 
2023-10-26 16:01:22.415366: val_loss -0.8078 
2023-10-26 16:01:22.415618: Pseudo dice [0.8649, 0.9135, 0.9667, 0.6907, 0.934] 
2023-10-26 16:01:22.415842: Epoch time: 4.09 s 
2023-10-26 16:01:23.743034:  
2023-10-26 16:01:23.743348: Epoch 441 
2023-10-26 16:01:23.743600: Current learning rate: 0.00592 
2023-10-26 16:01:27.865168: train_loss -0.8335 
2023-10-26 16:01:27.865568: val_loss -0.826 
2023-10-26 16:01:27.865839: Pseudo dice [0.8681, 0.9155, 0.9669, 0.6679, 0.9318] 
2023-10-26 16:01:27.866079: Epoch time: 4.12 s 
2023-10-26 16:01:28.980528:  
2023-10-26 16:01:28.980844: Epoch 442 
2023-10-26 16:01:28.981114: Current learning rate: 0.00592 
2023-10-26 16:01:33.266207: train_loss -0.8416 
2023-10-26 16:01:33.266676: val_loss -0.7945 
2023-10-26 16:01:33.267110: Pseudo dice [0.872, 0.9086, 0.9662, 0.6133, 0.9362] 
2023-10-26 16:01:33.267421: Epoch time: 4.29 s 
2023-10-26 16:01:34.337241:  
2023-10-26 16:01:34.337576: Epoch 443 
2023-10-26 16:01:34.337820: Current learning rate: 0.00591 
2023-10-26 16:01:38.380918: train_loss -0.8323 
2023-10-26 16:01:38.381334: val_loss -0.7948 
2023-10-26 16:01:38.381602: Pseudo dice [0.86, 0.908, 0.9628, 0.6208, 0.9333] 
2023-10-26 16:01:38.381841: Epoch time: 4.04 s 
2023-10-26 16:01:39.446454:  
2023-10-26 16:01:39.446756: Epoch 444 
2023-10-26 16:01:39.447003: Current learning rate: 0.0059 
2023-10-26 16:01:43.629629: train_loss -0.8276 
2023-10-26 16:01:43.630010: val_loss -0.8136 
2023-10-26 16:01:43.630275: Pseudo dice [0.8625, 0.9166, 0.9641, 0.6443, 0.9257] 
2023-10-26 16:01:43.630506: Epoch time: 4.18 s 
2023-10-26 16:01:44.684589:  
2023-10-26 16:01:44.684902: Epoch 445 
2023-10-26 16:01:44.685161: Current learning rate: 0.00589 
2023-10-26 16:01:48.780637: train_loss -0.8325 
2023-10-26 16:01:48.781072: val_loss -0.7517 
2023-10-26 16:01:48.781338: Pseudo dice [0.8725, 0.9093, 0.9655, 0.6564, 0.929] 
2023-10-26 16:01:48.781572: Epoch time: 4.1 s 
2023-10-26 16:01:49.838307:  
2023-10-26 16:01:49.838604: Epoch 446 
2023-10-26 16:01:49.838850: Current learning rate: 0.00588 
2023-10-26 16:01:53.910851: train_loss -0.8405 
2023-10-26 16:01:53.911246: val_loss -0.8282 
2023-10-26 16:01:53.911505: Pseudo dice [0.8644, 0.9105, 0.9655, 0.4868, 0.9367] 
2023-10-26 16:01:53.911756: Epoch time: 4.07 s 
2023-10-26 16:01:54.976895:  
2023-10-26 16:01:54.977209: Epoch 447 
2023-10-26 16:01:54.977455: Current learning rate: 0.00587 
2023-10-26 16:01:58.964703: train_loss -0.8437 
2023-10-26 16:01:58.965118: val_loss -0.8044 
2023-10-26 16:01:58.965376: Pseudo dice [0.8794, 0.9134, 0.9651, 0.6772, 0.9353] 
2023-10-26 16:01:58.965608: Epoch time: 3.99 s 
2023-10-26 16:02:00.223189:  
2023-10-26 16:02:00.223508: Epoch 448 
2023-10-26 16:02:00.223758: Current learning rate: 0.00586 
2023-10-26 16:02:04.294776: train_loss -0.8358 
2023-10-26 16:02:04.295201: val_loss -0.8075 
2023-10-26 16:02:04.295514: Pseudo dice [0.8528, 0.9031, 0.9635, 0.0, 0.9322] 
2023-10-26 16:02:04.295791: Epoch time: 4.07 s 
2023-10-26 16:02:05.408608:  
2023-10-26 16:02:05.409059: Epoch 449 
2023-10-26 16:02:05.409315: Current learning rate: 0.00585 
2023-10-26 16:02:09.554868: train_loss -0.8094 
2023-10-26 16:02:09.555406: val_loss -0.7768 
2023-10-26 16:02:09.555717: Pseudo dice [0.8677, 0.9083, 0.9599, 0.6384, 0.9263] 
2023-10-26 16:02:09.555977: Epoch time: 4.15 s 
2023-10-26 16:02:10.714275:  
2023-10-26 16:02:10.714597: Epoch 450 
2023-10-26 16:02:10.714857: Current learning rate: 0.00584 
2023-10-26 16:02:14.894966: train_loss -0.8215 
2023-10-26 16:02:14.895388: val_loss -0.8326 
2023-10-26 16:02:14.895656: Pseudo dice [0.8729, 0.9168, 0.9644, 0.4557, 0.9333] 
2023-10-26 16:02:14.895917: Epoch time: 4.18 s 
2023-10-26 16:02:15.984205:  
2023-10-26 16:02:15.984519: Epoch 451 
2023-10-26 16:02:15.984771: Current learning rate: 0.00583 
2023-10-26 16:02:20.067473: train_loss -0.8305 
2023-10-26 16:02:20.067943: val_loss -0.8358 
2023-10-26 16:02:20.068434: Pseudo dice [0.8694, 0.9041, 0.9648, 0.5758, 0.9359] 
2023-10-26 16:02:20.068861: Epoch time: 4.08 s 
2023-10-26 16:02:21.157448:  
2023-10-26 16:02:21.157774: Epoch 452 
2023-10-26 16:02:21.158036: Current learning rate: 0.00582 
2023-10-26 16:02:25.127714: train_loss -0.8332 
2023-10-26 16:02:25.128161: val_loss -0.8317 
2023-10-26 16:02:25.128440: Pseudo dice [0.8801, 0.9191, 0.9648, 0.7612, 0.9413] 
2023-10-26 16:02:25.128683: Epoch time: 3.97 s 
2023-10-26 16:02:26.258920:  
2023-10-26 16:02:26.259247: Epoch 453 
2023-10-26 16:02:26.259507: Current learning rate: 0.00581 
2023-10-26 16:02:30.307957: train_loss -0.8319 
2023-10-26 16:02:30.308399: val_loss -0.817 
2023-10-26 16:02:30.308669: Pseudo dice [0.8782, 0.9182, 0.9653, 0.7651, 0.9359] 
2023-10-26 16:02:30.308918: Epoch time: 4.05 s 
2023-10-26 16:02:31.415669:  
2023-10-26 16:02:31.415975: Epoch 454 
2023-10-26 16:02:31.416229: Current learning rate: 0.0058 
2023-10-26 16:02:35.427769: train_loss -0.8318 
2023-10-26 16:02:35.428140: val_loss -0.8215 
2023-10-26 16:02:35.428400: Pseudo dice [0.8764, 0.9165, 0.9648, 0.6546, 0.9353] 
2023-10-26 16:02:35.428630: Epoch time: 4.01 s 
2023-10-26 16:02:36.707215:  
2023-10-26 16:02:36.707555: Epoch 455 
2023-10-26 16:02:36.707853: Current learning rate: 0.00579 
2023-10-26 16:02:40.790764: train_loss -0.8294 
2023-10-26 16:02:40.791168: val_loss -0.7673 
2023-10-26 16:02:40.791434: Pseudo dice [0.8632, 0.9149, 0.9639, 0.6161, 0.9285] 
2023-10-26 16:02:40.791686: Epoch time: 4.08 s 
2023-10-26 16:02:41.869899:  
2023-10-26 16:02:41.870201: Epoch 456 
2023-10-26 16:02:41.870450: Current learning rate: 0.00578 
2023-10-26 16:02:45.749443: train_loss -0.8374 
2023-10-26 16:02:45.749806: val_loss -0.8129 
2023-10-26 16:02:45.750063: Pseudo dice [0.8738, 0.9091, 0.9644, 0.7755, 0.933] 
2023-10-26 16:02:45.750363: Epoch time: 3.88 s 
2023-10-26 16:02:46.842108:  
2023-10-26 16:02:46.842398: Epoch 457 
2023-10-26 16:02:46.842648: Current learning rate: 0.00577 
2023-10-26 16:02:51.049486: train_loss -0.8338 
2023-10-26 16:02:51.049929: val_loss -0.7818 
2023-10-26 16:02:51.050200: Pseudo dice [0.8661, 0.9137, 0.9661, 0.7516, 0.9283] 
2023-10-26 16:02:51.050442: Epoch time: 4.21 s 
2023-10-26 16:02:52.126985:  
2023-10-26 16:02:52.127326: Epoch 458 
2023-10-26 16:02:52.127573: Current learning rate: 0.00576 
2023-10-26 16:02:56.145512: train_loss -0.8491 
2023-10-26 16:02:56.145930: val_loss -0.7808 
2023-10-26 16:02:56.146192: Pseudo dice [0.8711, 0.9206, 0.9649, 0.7476, 0.9299] 
2023-10-26 16:02:56.146434: Epoch time: 4.02 s 
2023-10-26 16:02:57.223769:  
2023-10-26 16:02:57.224115: Epoch 459 
2023-10-26 16:02:57.224420: Current learning rate: 0.00575 
2023-10-26 16:03:01.053662: train_loss -0.8345 
2023-10-26 16:03:01.054095: val_loss -0.7871 
2023-10-26 16:03:01.054360: Pseudo dice [0.8609, 0.9145, 0.9659, 0.6442, 0.928] 
2023-10-26 16:03:01.054602: Epoch time: 3.83 s 
2023-10-26 16:03:02.237485:  
2023-10-26 16:03:02.237803: Epoch 460 
2023-10-26 16:03:02.238072: Current learning rate: 0.00574 
2023-10-26 16:03:06.286677: train_loss -0.8421 
2023-10-26 16:03:06.287100: val_loss -0.779 
2023-10-26 16:03:06.287382: Pseudo dice [0.8713, 0.9168, 0.9669, 0.6699, 0.9399] 
2023-10-26 16:03:06.287631: Epoch time: 4.05 s 
2023-10-26 16:03:07.365151:  
2023-10-26 16:03:07.365474: Epoch 461 
2023-10-26 16:03:07.365732: Current learning rate: 0.00573 
2023-10-26 16:03:11.452242: train_loss -0.835 
2023-10-26 16:03:11.452637: val_loss -0.8025 
2023-10-26 16:03:11.452924: Pseudo dice [0.8703, 0.912, 0.9644, 0.7292, 0.9373] 
2023-10-26 16:03:11.453158: Epoch time: 4.09 s 
2023-10-26 16:03:12.723675:  
2023-10-26 16:03:12.724013: Epoch 462 
2023-10-26 16:03:12.724273: Current learning rate: 0.00572 
2023-10-26 16:03:16.759424: train_loss -0.84 
2023-10-26 16:03:16.759833: val_loss -0.8191 
2023-10-26 16:03:16.760120: Pseudo dice [0.8672, 0.916, 0.9666, 0.7765, 0.9352] 
2023-10-26 16:03:16.760368: Epoch time: 4.04 s 
2023-10-26 16:03:17.885533:  
2023-10-26 16:03:17.885855: Epoch 463 
2023-10-26 16:03:17.886114: Current learning rate: 0.00571 
2023-10-26 16:03:22.052258: train_loss -0.8451 
2023-10-26 16:03:22.052624: val_loss -0.8227 
2023-10-26 16:03:22.052899: Pseudo dice [0.866, 0.9109, 0.9637, 0.7942, 0.9326] 
2023-10-26 16:03:22.053136: Epoch time: 4.17 s 
2023-10-26 16:03:23.128237:  
2023-10-26 16:03:23.128675: Epoch 464 
2023-10-26 16:03:23.128947: Current learning rate: 0.0057 
2023-10-26 16:03:27.235385: train_loss -0.8435 
2023-10-26 16:03:27.235792: val_loss -0.7837 
2023-10-26 16:03:27.236058: Pseudo dice [0.8714, 0.9204, 0.964, 0.538, 0.931] 
2023-10-26 16:03:27.236312: Epoch time: 4.11 s 
2023-10-26 16:03:28.335869:  
2023-10-26 16:03:28.336205: Epoch 465 
2023-10-26 16:03:28.336458: Current learning rate: 0.0057 
2023-10-26 16:03:32.382700: train_loss -0.8341 
2023-10-26 16:03:32.383137: val_loss -0.8028 
2023-10-26 16:03:32.383412: Pseudo dice [0.8699, 0.9193, 0.9659, 0.6632, 0.934] 
2023-10-26 16:03:32.383660: Epoch time: 4.05 s 
2023-10-26 16:03:33.593816:  
2023-10-26 16:03:33.594186: Epoch 466 
2023-10-26 16:03:33.594479: Current learning rate: 0.00569 
2023-10-26 16:03:37.637196: train_loss -0.8423 
2023-10-26 16:03:37.637584: val_loss -0.8079 
2023-10-26 16:03:37.637848: Pseudo dice [0.8691, 0.9101, 0.9653, 0.7362, 0.93] 
2023-10-26 16:03:37.638096: Epoch time: 4.04 s 
2023-10-26 16:03:38.730780:  
2023-10-26 16:03:38.731082: Epoch 467 
2023-10-26 16:03:38.731335: Current learning rate: 0.00568 
2023-10-26 16:03:42.979314: train_loss -0.8491 
2023-10-26 16:03:42.979669: val_loss -0.8128 
2023-10-26 16:03:42.979960: Pseudo dice [0.8703, 0.9164, 0.9677, 0.7075, 0.9353] 
2023-10-26 16:03:42.980204: Epoch time: 4.25 s 
2023-10-26 16:03:44.087280:  
2023-10-26 16:03:44.087608: Epoch 468 
2023-10-26 16:03:44.087868: Current learning rate: 0.00567 
2023-10-26 16:03:48.258713: train_loss -0.8342 
2023-10-26 16:03:48.259135: val_loss -0.7997 
2023-10-26 16:03:48.259434: Pseudo dice [0.8728, 0.9162, 0.9649, 0.7164, 0.9397] 
2023-10-26 16:03:48.259673: Epoch time: 4.17 s 
2023-10-26 16:03:49.517737:  
2023-10-26 16:03:49.518054: Epoch 469 
2023-10-26 16:03:49.518298: Current learning rate: 0.00566 
2023-10-26 16:03:53.702517: train_loss -0.8468 
2023-10-26 16:03:53.702893: val_loss -0.7882 
2023-10-26 16:03:53.703160: Pseudo dice [0.8646, 0.9159, 0.967, 0.5067, 0.9356] 
2023-10-26 16:03:53.703398: Epoch time: 4.19 s 
2023-10-26 16:03:54.763918:  
2023-10-26 16:03:54.764220: Epoch 470 
2023-10-26 16:03:54.764469: Current learning rate: 0.00565 
2023-10-26 16:03:58.718153: train_loss -0.8286 
2023-10-26 16:03:58.718522: val_loss -0.8068 
2023-10-26 16:03:58.718785: Pseudo dice [0.8686, 0.9186, 0.9638, 0.4406, 0.9322] 
2023-10-26 16:03:58.719034: Epoch time: 3.95 s 
2023-10-26 16:03:59.850672:  
2023-10-26 16:03:59.850995: Epoch 471 
2023-10-26 16:03:59.851247: Current learning rate: 0.00564 
2023-10-26 16:04:03.938204: train_loss -0.8362 
2023-10-26 16:04:03.938724: val_loss -0.8258 
2023-10-26 16:04:03.939247: Pseudo dice [0.8675, 0.9212, 0.9643, 0.6436, 0.9335] 
2023-10-26 16:04:03.939717: Epoch time: 4.09 s 
2023-10-26 16:04:05.016407:  
2023-10-26 16:04:05.016727: Epoch 472 
2023-10-26 16:04:05.016991: Current learning rate: 0.00563 
2023-10-26 16:04:09.081021: train_loss -0.844 
2023-10-26 16:04:09.081426: val_loss -0.8085 
2023-10-26 16:04:09.081693: Pseudo dice [0.8709, 0.9204, 0.9669, 0.6138, 0.9369] 
2023-10-26 16:04:09.081931: Epoch time: 4.07 s 
2023-10-26 16:04:10.275616:  
2023-10-26 16:04:10.275957: Epoch 473 
2023-10-26 16:04:10.276224: Current learning rate: 0.00562 
2023-10-26 16:04:14.324050: train_loss -0.8469 
2023-10-26 16:04:14.324409: val_loss -0.7988 
2023-10-26 16:04:14.324672: Pseudo dice [0.8723, 0.9168, 0.9665, 0.6203, 0.9343] 
2023-10-26 16:04:14.324904: Epoch time: 4.05 s 
2023-10-26 16:04:15.403910:  
2023-10-26 16:04:15.404226: Epoch 474 
2023-10-26 16:04:15.404489: Current learning rate: 0.00561 
2023-10-26 16:04:19.400672: train_loss -0.8387 
2023-10-26 16:04:19.401031: val_loss -0.8134 
2023-10-26 16:04:19.401288: Pseudo dice [0.8729, 0.9197, 0.9645, 0.6555, 0.9336] 
2023-10-26 16:04:19.401514: Epoch time: 4.0 s 
2023-10-26 16:04:20.476053:  
2023-10-26 16:04:20.476359: Epoch 475 
2023-10-26 16:04:20.476622: Current learning rate: 0.0056 
2023-10-26 16:04:24.529997: train_loss -0.84 
2023-10-26 16:04:24.530535: val_loss -0.8041 
2023-10-26 16:04:24.530907: Pseudo dice [0.8738, 0.9172, 0.9655, 0.6806, 0.9335] 
2023-10-26 16:04:24.531182: Epoch time: 4.05 s 
2023-10-26 16:04:25.807755:  
2023-10-26 16:04:25.808104: Epoch 476 
2023-10-26 16:04:25.808383: Current learning rate: 0.00559 
2023-10-26 16:04:29.822345: train_loss -0.8443 
2023-10-26 16:04:29.822718: val_loss -0.8122 
2023-10-26 16:04:29.822985: Pseudo dice [0.8716, 0.9208, 0.9666, 0.6154, 0.934] 
2023-10-26 16:04:29.823221: Epoch time: 4.02 s 
2023-10-26 16:04:30.888982:  
2023-10-26 16:04:30.889304: Epoch 477 
2023-10-26 16:04:30.889559: Current learning rate: 0.00558 
2023-10-26 16:04:35.038573: train_loss -0.8401 
2023-10-26 16:04:35.038958: val_loss -0.8206 
2023-10-26 16:04:35.039218: Pseudo dice [0.8719, 0.921, 0.9668, 0.6756, 0.9352] 
2023-10-26 16:04:35.039442: Epoch time: 4.15 s 
2023-10-26 16:04:36.108979:  
2023-10-26 16:04:36.109288: Epoch 478 
2023-10-26 16:04:36.109537: Current learning rate: 0.00557 
2023-10-26 16:04:40.223936: train_loss -0.8413 
2023-10-26 16:04:40.224413: val_loss -0.8169 
2023-10-26 16:04:40.224887: Pseudo dice [0.8637, 0.9138, 0.9649, 0.653, 0.9355] 
2023-10-26 16:04:40.225181: Epoch time: 4.12 s 
2023-10-26 16:04:41.358585:  
2023-10-26 16:04:41.358903: Epoch 479 
2023-10-26 16:04:41.359152: Current learning rate: 0.00556 
2023-10-26 16:04:45.462870: train_loss -0.8566 
2023-10-26 16:04:45.463297: val_loss -0.836 
2023-10-26 16:04:45.463688: Pseudo dice [0.8774, 0.9164, 0.9678, 0.6675, 0.9328] 
2023-10-26 16:04:45.464017: Epoch time: 4.1 s 
2023-10-26 16:04:46.549370:  
2023-10-26 16:04:46.549666: Epoch 480 
2023-10-26 16:04:46.549922: Current learning rate: 0.00555 
2023-10-26 16:04:50.581317: train_loss -0.8507 
2023-10-26 16:04:50.581823: val_loss -0.8448 
2023-10-26 16:04:50.582154: Pseudo dice [0.8688, 0.9169, 0.9667, 0.6958, 0.9365] 
2023-10-26 16:04:50.582438: Epoch time: 4.03 s 
2023-10-26 16:04:51.696643:  
2023-10-26 16:04:51.696997: Epoch 481 
2023-10-26 16:04:51.697238: Current learning rate: 0.00554 
2023-10-26 16:04:55.787351: train_loss -0.8427 
2023-10-26 16:04:55.787750: val_loss -0.7923 
2023-10-26 16:04:55.788033: Pseudo dice [0.8698, 0.9154, 0.9658, 0.5493, 0.9334] 
2023-10-26 16:04:55.788266: Epoch time: 4.09 s 
2023-10-26 16:04:56.866565:  
2023-10-26 16:04:56.866869: Epoch 482 
2023-10-26 16:04:56.867126: Current learning rate: 0.00553 
2023-10-26 16:05:00.881627: train_loss -0.8497 
2023-10-26 16:05:00.882036: val_loss -0.8289 
2023-10-26 16:05:00.882291: Pseudo dice [0.8739, 0.9165, 0.9678, 0.5558, 0.9366] 
2023-10-26 16:05:00.882518: Epoch time: 4.02 s 
2023-10-26 16:05:02.141745:  
2023-10-26 16:05:02.142069: Epoch 483 
2023-10-26 16:05:02.142312: Current learning rate: 0.00552 
2023-10-26 16:05:06.247648: train_loss -0.8519 
2023-10-26 16:05:06.248096: val_loss -0.7929 
2023-10-26 16:05:06.248362: Pseudo dice [0.8704, 0.916, 0.9659, 0.5602, 0.9386] 
2023-10-26 16:05:06.248598: Epoch time: 4.11 s 
2023-10-26 16:05:07.355278:  
2023-10-26 16:05:07.355575: Epoch 484 
2023-10-26 16:05:07.355816: Current learning rate: 0.00551 
2023-10-26 16:05:11.472584: train_loss -0.8309 
2023-10-26 16:05:11.473022: val_loss -0.7897 
2023-10-26 16:05:11.473477: Pseudo dice [0.8698, 0.9073, 0.9649, 0.5582, 0.9278] 
2023-10-26 16:05:11.473784: Epoch time: 4.12 s 
2023-10-26 16:05:12.572143:  
2023-10-26 16:05:12.572454: Epoch 485 
2023-10-26 16:05:12.572694: Current learning rate: 0.0055 
2023-10-26 16:05:16.605005: train_loss -0.8384 
2023-10-26 16:05:16.605832: val_loss -0.8225 
2023-10-26 16:05:16.606264: Pseudo dice [0.8709, 0.9155, 0.9657, 0.403, 0.9362] 
2023-10-26 16:05:16.606748: Epoch time: 4.03 s 
2023-10-26 16:05:17.713888:  
2023-10-26 16:05:17.714205: Epoch 486 
2023-10-26 16:05:17.714454: Current learning rate: 0.00549 
2023-10-26 16:05:21.707222: train_loss -0.8342 
2023-10-26 16:05:21.707696: val_loss -0.8295 
2023-10-26 16:05:21.708018: Pseudo dice [0.8727, 0.9191, 0.9649, 0.6356, 0.9355] 
2023-10-26 16:05:21.708296: Epoch time: 3.99 s 
2023-10-26 16:05:22.899671:  
2023-10-26 16:05:22.899976: Epoch 487 
2023-10-26 16:05:22.900220: Current learning rate: 0.00548 
2023-10-26 16:05:26.863101: train_loss -0.8385 
2023-10-26 16:05:26.863546: val_loss -0.8199 
2023-10-26 16:05:26.863944: Pseudo dice [0.8761, 0.9096, 0.9668, 0.6815, 0.9268] 
2023-10-26 16:05:26.864430: Epoch time: 3.96 s 
2023-10-26 16:05:27.951423:  
2023-10-26 16:05:27.951723: Epoch 488 
2023-10-26 16:05:27.951971: Current learning rate: 0.00547 
2023-10-26 16:05:31.916600: train_loss -0.8445 
2023-10-26 16:05:31.917012: val_loss -0.828 
2023-10-26 16:05:31.917280: Pseudo dice [0.8764, 0.9175, 0.9662, 0.6967, 0.9363] 
2023-10-26 16:05:31.917516: Epoch time: 3.97 s 
2023-10-26 16:05:33.182563:  
2023-10-26 16:05:33.182910: Epoch 489 
2023-10-26 16:05:33.183180: Current learning rate: 0.00546 
2023-10-26 16:05:37.254136: train_loss -0.8525 
2023-10-26 16:05:37.254559: val_loss -0.7803 
2023-10-26 16:05:37.254825: Pseudo dice [0.873, 0.9168, 0.9655, 0.6626, 0.9337] 
2023-10-26 16:05:37.255114: Epoch time: 4.07 s 
2023-10-26 16:05:38.341692:  
2023-10-26 16:05:38.342012: Epoch 490 
2023-10-26 16:05:38.342278: Current learning rate: 0.00546 
2023-10-26 16:05:42.318035: train_loss -0.8484 
2023-10-26 16:05:42.318410: val_loss -0.8202 
2023-10-26 16:05:42.318681: Pseudo dice [0.8687, 0.9066, 0.9652, 0.6704, 0.9164] 
2023-10-26 16:05:42.318928: Epoch time: 3.98 s 
2023-10-26 16:05:43.408916:  
2023-10-26 16:05:43.409238: Epoch 491 
2023-10-26 16:05:43.409493: Current learning rate: 0.00545 
2023-10-26 16:05:47.430882: train_loss -0.8283 
2023-10-26 16:05:47.431285: val_loss -0.7894 
2023-10-26 16:05:47.431560: Pseudo dice [0.8656, 0.8947, 0.9601, 0.5997, 0.9289] 
2023-10-26 16:05:47.431860: Epoch time: 4.02 s 
2023-10-26 16:05:48.569560:  
2023-10-26 16:05:48.569893: Epoch 492 
2023-10-26 16:05:48.570158: Current learning rate: 0.00544 
2023-10-26 16:05:52.585735: train_loss -0.821 
2023-10-26 16:05:52.586600: val_loss -0.8288 
2023-10-26 16:05:52.586921: Pseudo dice [0.8782, 0.9197, 0.9669, 0.6428, 0.9339] 
2023-10-26 16:05:52.587149: Epoch time: 4.02 s 
2023-10-26 16:05:53.668524:  
2023-10-26 16:05:53.668831: Epoch 493 
2023-10-26 16:05:53.669102: Current learning rate: 0.00543 
2023-10-26 16:05:57.660265: train_loss -0.8279 
2023-10-26 16:05:57.660685: val_loss -0.809 
2023-10-26 16:05:57.660958: Pseudo dice [0.8754, 0.9103, 0.9661, 0.6395, 0.9349] 
2023-10-26 16:05:57.661202: Epoch time: 3.99 s 
2023-10-26 16:05:58.748774:  
2023-10-26 16:05:58.749085: Epoch 494 
2023-10-26 16:05:58.749357: Current learning rate: 0.00542 
2023-10-26 16:06:02.788796: train_loss -0.8444 
2023-10-26 16:06:02.789211: val_loss -0.8002 
2023-10-26 16:06:02.789572: Pseudo dice [0.8757, 0.9152, 0.966, 0.5945, 0.9384] 
2023-10-26 16:06:02.790064: Epoch time: 4.04 s 
2023-10-26 16:06:03.869321:  
2023-10-26 16:06:03.869649: Epoch 495 
2023-10-26 16:06:03.869910: Current learning rate: 0.00541 
2023-10-26 16:06:08.057956: train_loss -0.8385 
2023-10-26 16:06:08.058324: val_loss -0.8229 
2023-10-26 16:06:08.058588: Pseudo dice [0.8692, 0.9156, 0.9642, 0.4916, 0.9371] 
2023-10-26 16:06:08.058820: Epoch time: 4.19 s 
2023-10-26 16:06:09.340131:  
2023-10-26 16:06:09.340435: Epoch 496 
2023-10-26 16:06:09.340680: Current learning rate: 0.0054 
2023-10-26 16:06:13.445558: train_loss -0.8353 
2023-10-26 16:06:13.445949: val_loss -0.8348 
2023-10-26 16:06:13.446229: Pseudo dice [0.8524, 0.9143, 0.9677, 0.3804, 0.9268] 
2023-10-26 16:06:13.446482: Epoch time: 4.11 s 
2023-10-26 16:06:14.545290:  
2023-10-26 16:06:14.545597: Epoch 497 
2023-10-26 16:06:14.545863: Current learning rate: 0.00539 
2023-10-26 16:06:18.662911: train_loss -0.8515 
2023-10-26 16:06:18.663314: val_loss -0.8311 
2023-10-26 16:06:18.663604: Pseudo dice [0.8738, 0.9222, 0.9679, 0.6603, 0.9352] 
2023-10-26 16:06:18.663851: Epoch time: 4.12 s 
2023-10-26 16:06:19.751525:  
2023-10-26 16:06:19.751838: Epoch 498 
2023-10-26 16:06:19.752088: Current learning rate: 0.00538 
2023-10-26 16:06:23.951799: train_loss -0.8399 
2023-10-26 16:06:23.952159: val_loss -0.7896 
2023-10-26 16:06:23.952428: Pseudo dice [0.8705, 0.907, 0.9666, 0.4473, 0.9352] 
2023-10-26 16:06:23.952664: Epoch time: 4.2 s 
2023-10-26 16:06:25.046114:  
2023-10-26 16:06:25.046409: Epoch 499 
2023-10-26 16:06:25.046653: Current learning rate: 0.00537 
2023-10-26 16:06:29.179620: train_loss -0.8507 
2023-10-26 16:06:29.180008: val_loss -0.7947 
2023-10-26 16:06:29.180290: Pseudo dice [0.868, 0.9149, 0.9673, 0.6286, 0.9289] 
2023-10-26 16:06:29.180543: Epoch time: 4.13 s 
2023-10-26 16:06:30.374779:  
2023-10-26 16:06:30.375154: Epoch 500 
2023-10-26 16:06:30.375466: Current learning rate: 0.00536 
2023-10-26 16:06:34.450765: train_loss -0.8463 
2023-10-26 16:06:34.451249: val_loss -0.7776 
2023-10-26 16:06:34.451532: Pseudo dice [0.867, 0.9155, 0.9637, 0.573, 0.9351] 
2023-10-26 16:06:34.451783: Epoch time: 4.08 s 
2023-10-26 16:06:35.551651:  
2023-10-26 16:06:35.551983: Epoch 501 
2023-10-26 16:06:35.552246: Current learning rate: 0.00535 
2023-10-26 16:06:39.580007: train_loss -0.8331 
2023-10-26 16:06:39.580405: val_loss -0.7942 
2023-10-26 16:06:39.580680: Pseudo dice [0.8715, 0.9143, 0.9658, 0.7407, 0.9309] 
2023-10-26 16:06:39.580936: Epoch time: 4.03 s 
2023-10-26 16:06:40.673084:  
2023-10-26 16:06:40.673402: Epoch 502 
2023-10-26 16:06:40.673653: Current learning rate: 0.00534 
2023-10-26 16:06:44.589005: train_loss -0.8387 
2023-10-26 16:06:44.589421: val_loss -0.8011 
2023-10-26 16:06:44.589696: Pseudo dice [0.8773, 0.9187, 0.9654, 0.6461, 0.9343] 
2023-10-26 16:06:44.589941: Epoch time: 3.92 s 
2023-10-26 16:06:45.867303:  
2023-10-26 16:06:45.867613: Epoch 503 
2023-10-26 16:06:45.867864: Current learning rate: 0.00533 
2023-10-26 16:06:49.700367: train_loss -0.8453 
2023-10-26 16:06:49.700732: val_loss -0.8212 
2023-10-26 16:06:49.701007: Pseudo dice [0.8705, 0.9177, 0.9645, 0.6236, 0.9346] 
2023-10-26 16:06:49.701241: Epoch time: 3.83 s 
2023-10-26 16:06:50.808812:  
2023-10-26 16:06:50.809129: Epoch 504 
2023-10-26 16:06:50.809412: Current learning rate: 0.00532 
2023-10-26 16:06:54.843805: train_loss -0.8443 
2023-10-26 16:06:54.844161: val_loss -0.7871 
2023-10-26 16:06:54.844407: Pseudo dice [0.8716, 0.9149, 0.9668, 0.4162, 0.9372] 
2023-10-26 16:06:54.844648: Epoch time: 4.04 s 
2023-10-26 16:06:55.953528:  
2023-10-26 16:06:55.953826: Epoch 505 
2023-10-26 16:06:55.954069: Current learning rate: 0.00531 
2023-10-26 16:07:00.013035: train_loss -0.8445 
2023-10-26 16:07:00.013432: val_loss -0.8133 
2023-10-26 16:07:00.013700: Pseudo dice [0.8667, 0.9112, 0.9654, 0.4627, 0.9297] 
2023-10-26 16:07:00.013943: Epoch time: 4.06 s 
2023-10-26 16:07:01.105001:  
2023-10-26 16:07:01.105313: Epoch 506 
2023-10-26 16:07:01.105568: Current learning rate: 0.0053 
2023-10-26 16:07:05.064048: train_loss -0.8379 
2023-10-26 16:07:05.064429: val_loss -0.8154 
2023-10-26 16:07:05.064710: Pseudo dice [0.8738, 0.916, 0.9644, 0.6449, 0.9334] 
2023-10-26 16:07:05.064943: Epoch time: 3.96 s 
2023-10-26 16:07:06.160451:  
2023-10-26 16:07:06.160752: Epoch 507 
2023-10-26 16:07:06.161003: Current learning rate: 0.00529 
2023-10-26 16:07:10.159296: train_loss -0.848 
2023-10-26 16:07:10.159742: val_loss -0.8355 
2023-10-26 16:07:10.160122: Pseudo dice [0.874, 0.9207, 0.9674, 0.6718, 0.9312] 
2023-10-26 16:07:10.160401: Epoch time: 4.0 s 
2023-10-26 16:07:11.252244:  
2023-10-26 16:07:11.252544: Epoch 508 
2023-10-26 16:07:11.252783: Current learning rate: 0.00528 
2023-10-26 16:07:15.318680: train_loss -0.8551 
2023-10-26 16:07:15.319073: val_loss -0.7998 
2023-10-26 16:07:15.319330: Pseudo dice [0.8686, 0.9176, 0.9671, 0.6386, 0.9341] 
2023-10-26 16:07:15.319556: Epoch time: 4.07 s 
2023-10-26 16:07:16.581781:  
2023-10-26 16:07:16.582092: Epoch 509 
2023-10-26 16:07:16.582355: Current learning rate: 0.00527 
2023-10-26 16:07:20.631006: train_loss -0.8548 
2023-10-26 16:07:20.631633: val_loss -0.8323 
2023-10-26 16:07:20.631988: Pseudo dice [0.8755, 0.9224, 0.9663, 0.61, 0.9329] 
2023-10-26 16:07:20.632298: Epoch time: 4.05 s 
2023-10-26 16:07:21.717863:  
2023-10-26 16:07:21.718188: Epoch 510 
2023-10-26 16:07:21.718441: Current learning rate: 0.00526 
2023-10-26 16:07:25.760722: train_loss -0.8502 
2023-10-26 16:07:25.761122: val_loss -0.8147 
2023-10-26 16:07:25.761386: Pseudo dice [0.8661, 0.911, 0.9676, 0.6084, 0.9321] 
2023-10-26 16:07:25.761611: Epoch time: 4.04 s 
2023-10-26 16:07:26.854578:  
2023-10-26 16:07:26.854889: Epoch 511 
2023-10-26 16:07:26.855140: Current learning rate: 0.00525 
2023-10-26 16:07:30.652364: train_loss -0.8517 
2023-10-26 16:07:30.652840: val_loss -0.8065 
2023-10-26 16:07:30.653125: Pseudo dice [0.8748, 0.9199, 0.9665, 0.6392, 0.9362] 
2023-10-26 16:07:30.653360: Epoch time: 3.8 s 
2023-10-26 16:07:31.754696:  
2023-10-26 16:07:31.755078: Epoch 512 
2023-10-26 16:07:31.755327: Current learning rate: 0.00524 
2023-10-26 16:07:35.782661: train_loss -0.8499 
2023-10-26 16:07:35.783072: val_loss -0.8189 
2023-10-26 16:07:35.783339: Pseudo dice [0.8712, 0.9167, 0.9674, 0.5769, 0.9331] 
2023-10-26 16:07:35.783637: Epoch time: 4.03 s 
2023-10-26 16:07:36.870895:  
2023-10-26 16:07:36.871219: Epoch 513 
2023-10-26 16:07:36.871475: Current learning rate: 0.00523 
2023-10-26 16:07:40.843602: train_loss -0.8416 
2023-10-26 16:07:40.843967: val_loss -0.8091 
2023-10-26 16:07:40.844248: Pseudo dice [0.8707, 0.9166, 0.966, 0.6073, 0.9384] 
2023-10-26 16:07:40.844523: Epoch time: 3.97 s 
2023-10-26 16:07:41.983066:  
2023-10-26 16:07:41.983375: Epoch 514 
2023-10-26 16:07:41.983623: Current learning rate: 0.00522 
2023-10-26 16:07:45.922429: train_loss -0.8474 
2023-10-26 16:07:45.922797: val_loss -0.8181 
2023-10-26 16:07:45.923067: Pseudo dice [0.8665, 0.9168, 0.9672, 0.6816, 0.9362] 
2023-10-26 16:07:45.923301: Epoch time: 3.94 s 
2023-10-26 16:07:47.011804:  
2023-10-26 16:07:47.012119: Epoch 515 
2023-10-26 16:07:47.012375: Current learning rate: 0.00521 
2023-10-26 16:07:50.927101: train_loss -0.8618 
2023-10-26 16:07:50.927493: val_loss -0.7983 
2023-10-26 16:07:50.927750: Pseudo dice [0.8668, 0.9136, 0.9682, 0.678, 0.9351] 
2023-10-26 16:07:50.927999: Epoch time: 3.92 s 
2023-10-26 16:07:52.213759:  
2023-10-26 16:07:52.214077: Epoch 516 
2023-10-26 16:07:52.214325: Current learning rate: 0.0052 
2023-10-26 16:07:56.285864: train_loss -0.8439 
2023-10-26 16:07:56.286238: val_loss -0.82 
2023-10-26 16:07:56.286497: Pseudo dice [0.8757, 0.9184, 0.9661, 0.4871, 0.933] 
2023-10-26 16:07:56.286724: Epoch time: 4.07 s 
2023-10-26 16:07:57.368711:  
2023-10-26 16:07:57.369018: Epoch 517 
2023-10-26 16:07:57.369263: Current learning rate: 0.00519 
2023-10-26 16:08:01.639721: train_loss -0.8563 
2023-10-26 16:08:01.640319: val_loss -0.8079 
2023-10-26 16:08:01.640614: Pseudo dice [0.876, 0.9226, 0.9668, 0.4289, 0.9377] 
2023-10-26 16:08:01.640846: Epoch time: 4.27 s 
2023-10-26 16:08:02.738562:  
2023-10-26 16:08:02.738884: Epoch 518 
2023-10-26 16:08:02.739240: Current learning rate: 0.00518 
2023-10-26 16:08:06.731302: train_loss -0.8436 
2023-10-26 16:08:06.731658: val_loss -0.8367 
2023-10-26 16:08:06.731917: Pseudo dice [0.8771, 0.9224, 0.9648, 0.7212, 0.9321] 
2023-10-26 16:08:06.732137: Epoch time: 3.99 s 
2023-10-26 16:08:07.863859:  
2023-10-26 16:08:07.864171: Epoch 519 
2023-10-26 16:08:07.864420: Current learning rate: 0.00518 
2023-10-26 16:08:11.714057: train_loss -0.8474 
2023-10-26 16:08:11.714425: val_loss -0.7865 
2023-10-26 16:08:11.714681: Pseudo dice [0.8707, 0.9188, 0.9667, 0.5587, 0.9364] 
2023-10-26 16:08:11.714920: Epoch time: 3.85 s 
2023-10-26 16:08:12.786673:  
2023-10-26 16:08:12.787007: Epoch 520 
2023-10-26 16:08:12.787254: Current learning rate: 0.00517 
2023-10-26 16:08:16.695741: train_loss -0.8507 
2023-10-26 16:08:16.696079: val_loss -0.8232 
2023-10-26 16:08:16.696332: Pseudo dice [0.8749, 0.9212, 0.9663, 0.6698, 0.9387] 
2023-10-26 16:08:16.696553: Epoch time: 3.91 s 
2023-10-26 16:08:17.766438:  
2023-10-26 16:08:17.766742: Epoch 521 
2023-10-26 16:08:17.766994: Current learning rate: 0.00516 
2023-10-26 16:08:21.869009: train_loss -0.8499 
2023-10-26 16:08:21.869400: val_loss -0.7797 
2023-10-26 16:08:21.869714: Pseudo dice [0.864, 0.9176, 0.9662, 0.4325, 0.9324] 
2023-10-26 16:08:21.869977: Epoch time: 4.1 s 
2023-10-26 16:08:22.951638:  
2023-10-26 16:08:22.951949: Epoch 522 
2023-10-26 16:08:22.952199: Current learning rate: 0.00515 
2023-10-26 16:08:27.200144: train_loss -0.8514 
2023-10-26 16:08:27.200522: val_loss -0.8026 
2023-10-26 16:08:27.200791: Pseudo dice [0.8685, 0.9168, 0.967, 0.649, 0.9346] 
2023-10-26 16:08:27.201046: Epoch time: 4.25 s 
2023-10-26 16:08:28.451656:  
2023-10-26 16:08:28.452070: Epoch 523 
2023-10-26 16:08:28.452317: Current learning rate: 0.00514 
2023-10-26 16:08:32.693494: train_loss -0.8519 
2023-10-26 16:08:32.693852: val_loss -0.6916 
2023-10-26 16:08:32.694111: Pseudo dice [0.872, 0.9192, 0.9664, 0.6982, 0.6238] 
2023-10-26 16:08:32.694330: Epoch time: 4.24 s 
2023-10-26 16:08:33.771771:  
2023-10-26 16:08:33.772081: Epoch 524 
2023-10-26 16:08:33.772324: Current learning rate: 0.00513 
2023-10-26 16:08:38.008317: train_loss -0.8498 
2023-10-26 16:08:38.008686: val_loss -0.8291 
2023-10-26 16:08:38.008951: Pseudo dice [0.8748, 0.9159, 0.967, 0.7187, 0.9343] 
2023-10-26 16:08:38.009193: Epoch time: 4.24 s 
2023-10-26 16:08:39.085022:  
2023-10-26 16:08:39.085321: Epoch 525 
2023-10-26 16:08:39.085557: Current learning rate: 0.00512 
2023-10-26 16:08:43.301947: train_loss -0.8482 
2023-10-26 16:08:43.302584: val_loss -0.8279 
2023-10-26 16:08:43.302879: Pseudo dice [0.8721, 0.9125, 0.9677, 0.6344, 0.9384] 
2023-10-26 16:08:43.303210: Epoch time: 4.22 s 
2023-10-26 16:08:44.390941:  
2023-10-26 16:08:44.391259: Epoch 526 
2023-10-26 16:08:44.391522: Current learning rate: 0.00511 
2023-10-26 16:08:48.495431: train_loss -0.8479 
2023-10-26 16:08:48.495818: val_loss -0.8246 
2023-10-26 16:08:48.496084: Pseudo dice [0.8674, 0.9157, 0.9658, 0.6633, 0.9353] 
2023-10-26 16:08:48.496337: Epoch time: 4.11 s 
2023-10-26 16:08:49.579318:  
2023-10-26 16:08:49.579618: Epoch 527 
2023-10-26 16:08:49.579877: Current learning rate: 0.0051 
2023-10-26 16:08:53.747474: train_loss -0.8346 
2023-10-26 16:08:53.747929: val_loss -0.819 
2023-10-26 16:08:53.748216: Pseudo dice [0.8709, 0.9182, 0.9668, 0.5383, 0.9334] 
2023-10-26 16:08:53.748463: Epoch time: 4.17 s 
2023-10-26 16:08:54.830597:  
2023-10-26 16:08:54.830901: Epoch 528 
2023-10-26 16:08:54.831160: Current learning rate: 0.00509 
2023-10-26 16:08:58.931561: train_loss -0.8475 
2023-10-26 16:08:58.932145: val_loss -0.7891 
2023-10-26 16:08:58.932416: Pseudo dice [0.8701, 0.9076, 0.9665, 0.5601, 0.9347] 
2023-10-26 16:08:58.932665: Epoch time: 4.1 s 
2023-10-26 16:09:00.197893:  
2023-10-26 16:09:00.198195: Epoch 529 
2023-10-26 16:09:00.198447: Current learning rate: 0.00508 
2023-10-26 16:09:04.490062: train_loss -0.8526 
2023-10-26 16:09:04.490419: val_loss -0.8238 
2023-10-26 16:09:04.490678: Pseudo dice [0.8693, 0.9156, 0.9669, 0.6617, 0.9341] 
2023-10-26 16:09:04.490923: Epoch time: 4.29 s 
2023-10-26 16:09:05.586375:  
2023-10-26 16:09:05.586671: Epoch 530 
2023-10-26 16:09:05.586925: Current learning rate: 0.00507 
2023-10-26 16:09:09.735715: train_loss -0.8445 
2023-10-26 16:09:09.736112: val_loss -0.8228 
2023-10-26 16:09:09.736398: Pseudo dice [0.8785, 0.9119, 0.966, 0.666, 0.934] 
2023-10-26 16:09:09.736639: Epoch time: 4.15 s 
2023-10-26 16:09:10.842461:  
2023-10-26 16:09:10.842788: Epoch 531 
2023-10-26 16:09:10.843050: Current learning rate: 0.00506 
2023-10-26 16:09:15.088001: train_loss -0.8531 
2023-10-26 16:09:15.088414: val_loss -0.8185 
2023-10-26 16:09:15.088688: Pseudo dice [0.8702, 0.9179, 0.9669, 0.6561, 0.9325] 
2023-10-26 16:09:15.088942: Epoch time: 4.25 s 
2023-10-26 16:09:16.219500:  
2023-10-26 16:09:16.219815: Epoch 532 
2023-10-26 16:09:16.220066: Current learning rate: 0.00505 
2023-10-26 16:09:20.326929: train_loss -0.8526 
2023-10-26 16:09:20.327311: val_loss -0.8161 
2023-10-26 16:09:20.327571: Pseudo dice [0.8669, 0.9152, 0.9667, 0.3673, 0.9345] 
2023-10-26 16:09:20.327842: Epoch time: 4.11 s 
2023-10-26 16:09:21.455676:  
2023-10-26 16:09:21.455975: Epoch 533 
2023-10-26 16:09:21.456216: Current learning rate: 0.00504 
2023-10-26 16:09:25.583357: train_loss -0.8446 
2023-10-26 16:09:25.583738: val_loss -0.7917 
2023-10-26 16:09:25.583998: Pseudo dice [0.8658, 0.9148, 0.9674, 0.3951, 0.9369] 
2023-10-26 16:09:25.584224: Epoch time: 4.13 s 
2023-10-26 16:09:26.690160:  
2023-10-26 16:09:26.690465: Epoch 534 
2023-10-26 16:09:26.690712: Current learning rate: 0.00503 
2023-10-26 16:09:30.710689: train_loss -0.8553 
2023-10-26 16:09:30.711187: val_loss -0.7873 
2023-10-26 16:09:30.711597: Pseudo dice [0.8688, 0.9106, 0.9655, 0.3713, 0.9333] 
2023-10-26 16:09:30.711939: Epoch time: 4.02 s 
2023-10-26 16:09:31.812715:  
2023-10-26 16:09:31.813056: Epoch 535 
2023-10-26 16:09:31.813432: Current learning rate: 0.00502 
2023-10-26 16:09:35.915327: train_loss -0.8471 
2023-10-26 16:09:35.915990: val_loss -0.7885 
2023-10-26 16:09:35.916290: Pseudo dice [0.8635, 0.9138, 0.9677, 0.4591, 0.9336] 
2023-10-26 16:09:35.916533: Epoch time: 4.1 s 
2023-10-26 16:09:37.216184:  
2023-10-26 16:09:37.216484: Epoch 536 
2023-10-26 16:09:37.216743: Current learning rate: 0.00501 
2023-10-26 16:09:41.270613: train_loss -0.8492 
2023-10-26 16:09:41.271051: val_loss -0.7894 
2023-10-26 16:09:41.271323: Pseudo dice [0.8675, 0.9061, 0.9666, 0.4415, 0.9366] 
2023-10-26 16:09:41.271580: Epoch time: 4.06 s 
2023-10-26 16:09:42.423899:  
2023-10-26 16:09:42.424208: Epoch 537 
2023-10-26 16:09:42.424469: Current learning rate: 0.005 
2023-10-26 16:09:46.477921: train_loss -0.8349 
2023-10-26 16:09:46.478726: val_loss -0.798 
2023-10-26 16:09:46.479045: Pseudo dice [0.8786, 0.9162, 0.9635, 0.6432, 0.9351] 
2023-10-26 16:09:46.479327: Epoch time: 4.05 s 
2023-10-26 16:09:47.576328:  
2023-10-26 16:09:47.576612: Epoch 538 
2023-10-26 16:09:47.576853: Current learning rate: 0.00499 
2023-10-26 16:09:51.466999: train_loss -0.848 
2023-10-26 16:09:51.467406: val_loss -0.8229 
2023-10-26 16:09:51.467688: Pseudo dice [0.8692, 0.9053, 0.9648, 0.4768, 0.9261] 
2023-10-26 16:09:51.467929: Epoch time: 3.89 s 
2023-10-26 16:09:52.558408:  
2023-10-26 16:09:52.558708: Epoch 539 
2023-10-26 16:09:52.558985: Current learning rate: 0.00498 
2023-10-26 16:09:56.465377: train_loss -0.8411 
2023-10-26 16:09:56.465916: val_loss -0.7815 
2023-10-26 16:09:56.466344: Pseudo dice [0.8596, 0.9111, 0.9665, 0.4525, 0.9354] 
2023-10-26 16:09:56.466655: Epoch time: 3.91 s 
2023-10-26 16:09:57.576853:  
2023-10-26 16:09:57.577168: Epoch 540 
2023-10-26 16:09:57.577416: Current learning rate: 0.00497 
2023-10-26 16:10:01.432571: train_loss -0.8409 
2023-10-26 16:10:01.433072: val_loss -0.7777 
2023-10-26 16:10:01.433599: Pseudo dice [0.863, 0.9132, 0.9667, 0.531, 0.9331] 
2023-10-26 16:10:01.433998: Epoch time: 3.86 s 
2023-10-26 16:10:02.549408:  
2023-10-26 16:10:02.549723: Epoch 541 
2023-10-26 16:10:02.549997: Current learning rate: 0.00496 
2023-10-26 16:10:06.672239: train_loss -0.8473 
2023-10-26 16:10:06.672616: val_loss -0.7761 
2023-10-26 16:10:06.672884: Pseudo dice [0.8692, 0.9132, 0.9651, 0.6078, 0.9329] 
2023-10-26 16:10:06.673127: Epoch time: 4.12 s 
2023-10-26 16:10:07.775034:  
2023-10-26 16:10:07.775340: Epoch 542 
2023-10-26 16:10:07.775584: Current learning rate: 0.00495 
2023-10-26 16:10:11.843053: train_loss -0.8508 
2023-10-26 16:10:11.843461: val_loss -0.7849 
2023-10-26 16:10:11.843729: Pseudo dice [0.8636, 0.9161, 0.9656, 0.46, 0.9311] 
2023-10-26 16:10:11.843971: Epoch time: 4.07 s 
2023-10-26 16:10:13.177966:  
2023-10-26 16:10:13.178269: Epoch 543 
2023-10-26 16:10:13.178536: Current learning rate: 0.00494 
2023-10-26 16:10:17.210882: train_loss -0.8453 
2023-10-26 16:10:17.211264: val_loss -0.7763 
2023-10-26 16:10:17.211527: Pseudo dice [0.8626, 0.9144, 0.9651, 0.4781, 0.9334] 
2023-10-26 16:10:17.211763: Epoch time: 4.03 s 
2023-10-26 16:10:18.306121:  
2023-10-26 16:10:18.306418: Epoch 544 
2023-10-26 16:10:18.306657: Current learning rate: 0.00493 
2023-10-26 16:10:22.370611: train_loss -0.847 
2023-10-26 16:10:22.371045: val_loss -0.7844 
2023-10-26 16:10:22.371315: Pseudo dice [0.8657, 0.9148, 0.9669, 0.5095, 0.9331] 
2023-10-26 16:10:22.371546: Epoch time: 4.07 s 
2023-10-26 16:10:23.465637:  
2023-10-26 16:10:23.465958: Epoch 545 
2023-10-26 16:10:23.466213: Current learning rate: 0.00492 
2023-10-26 16:10:27.526212: train_loss -0.8489 
2023-10-26 16:10:27.526629: val_loss -0.8107 
2023-10-26 16:10:27.526905: Pseudo dice [0.8761, 0.9216, 0.9681, 0.7603, 0.9372] 
2023-10-26 16:10:27.527151: Epoch time: 4.06 s 
2023-10-26 16:10:28.681107:  
2023-10-26 16:10:28.681392: Epoch 546 
2023-10-26 16:10:28.681637: Current learning rate: 0.00491 
2023-10-26 16:10:32.848059: train_loss -0.8483 
2023-10-26 16:10:32.848433: val_loss -0.8052 
2023-10-26 16:10:32.848700: Pseudo dice [0.8712, 0.9188, 0.9646, 0.7498, 0.928] 
2023-10-26 16:10:32.848931: Epoch time: 4.17 s 
2023-10-26 16:10:33.933006:  
2023-10-26 16:10:33.933306: Epoch 547 
2023-10-26 16:10:33.933560: Current learning rate: 0.0049 
2023-10-26 16:10:38.097856: train_loss -0.8557 
2023-10-26 16:10:38.098224: val_loss -0.7819 
2023-10-26 16:10:38.098479: Pseudo dice [0.8626, 0.9106, 0.9672, 0.5239, 0.9298] 
2023-10-26 16:10:38.098707: Epoch time: 4.17 s 
2023-10-26 16:10:39.197236:  
2023-10-26 16:10:39.197536: Epoch 548 
2023-10-26 16:10:39.197790: Current learning rate: 0.00489 
2023-10-26 16:10:43.223114: train_loss -0.8531 
2023-10-26 16:10:43.223514: val_loss -0.8266 
2023-10-26 16:10:43.223775: Pseudo dice [0.8744, 0.9143, 0.9657, 0.4525, 0.9375] 
2023-10-26 16:10:43.224024: Epoch time: 4.03 s 
2023-10-26 16:10:44.493911:  
2023-10-26 16:10:44.494226: Epoch 549 
2023-10-26 16:10:44.494481: Current learning rate: 0.00488 
2023-10-26 16:10:48.577655: train_loss -0.8496 
2023-10-26 16:10:48.578022: val_loss -0.8077 
2023-10-26 16:10:48.578281: Pseudo dice [0.8675, 0.9159, 0.9663, 0.5421, 0.9377] 
2023-10-26 16:10:48.578514: Epoch time: 4.08 s 
2023-10-26 16:10:49.764706:  
2023-10-26 16:10:49.765015: Epoch 550 
2023-10-26 16:10:49.765255: Current learning rate: 0.00487 
2023-10-26 16:10:53.784099: train_loss -0.8538 
2023-10-26 16:10:53.784473: val_loss -0.8049 
2023-10-26 16:10:53.784736: Pseudo dice [0.8734, 0.9136, 0.9655, 0.6136, 0.9361] 
2023-10-26 16:10:53.784970: Epoch time: 4.02 s 
2023-10-26 16:10:54.886031:  
2023-10-26 16:10:54.886317: Epoch 551 
2023-10-26 16:10:54.886564: Current learning rate: 0.00486 
2023-10-26 16:10:59.016750: train_loss -0.8542 
2023-10-26 16:10:59.017220: val_loss -0.8243 
2023-10-26 16:10:59.017547: Pseudo dice [0.8586, 0.9089, 0.9648, 0.3721, 0.9339] 
2023-10-26 16:10:59.017927: Epoch time: 4.13 s 
2023-10-26 16:11:00.110385:  
2023-10-26 16:11:00.110685: Epoch 552 
2023-10-26 16:11:00.110946: Current learning rate: 0.00485 
2023-10-26 16:11:04.108994: train_loss -0.8363 
2023-10-26 16:11:04.109428: val_loss -0.8096 
2023-10-26 16:11:04.109699: Pseudo dice [0.8693, 0.9095, 0.9631, 0.7408, 0.9309] 
2023-10-26 16:11:04.109938: Epoch time: 4.0 s 
2023-10-26 16:11:05.238183:  
2023-10-26 16:11:05.238472: Epoch 553 
2023-10-26 16:11:05.238714: Current learning rate: 0.00484 
2023-10-26 16:11:09.328932: train_loss -0.8444 
2023-10-26 16:11:09.329305: val_loss -0.8113 
2023-10-26 16:11:09.329582: Pseudo dice [0.8605, 0.9173, 0.9664, 0.5829, 0.9304] 
2023-10-26 16:11:09.329809: Epoch time: 4.09 s 
2023-10-26 16:11:10.428068:  
2023-10-26 16:11:10.428381: Epoch 554 
2023-10-26 16:11:10.428633: Current learning rate: 0.00484 
2023-10-26 16:11:14.432469: train_loss -0.8518 
2023-10-26 16:11:14.432926: val_loss -0.8141 
2023-10-26 16:11:14.433218: Pseudo dice [0.8685, 0.912, 0.9673, 0.6841, 0.9391] 
2023-10-26 16:11:14.433445: Epoch time: 4.0 s 
2023-10-26 16:11:15.521051:  
2023-10-26 16:11:15.521370: Epoch 555 
2023-10-26 16:11:15.521612: Current learning rate: 0.00483 
2023-10-26 16:11:19.582724: train_loss -0.8452 
2023-10-26 16:11:19.583143: val_loss -0.8068 
2023-10-26 16:11:19.583412: Pseudo dice [0.8686, 0.9185, 0.9665, 0.6553, 0.9344] 
2023-10-26 16:11:19.583673: Epoch time: 4.06 s 
2023-10-26 16:11:20.871000:  
2023-10-26 16:11:20.871332: Epoch 556 
2023-10-26 16:11:20.871586: Current learning rate: 0.00482 
2023-10-26 16:11:24.920951: train_loss -0.8574 
2023-10-26 16:11:24.921342: val_loss -0.807 
2023-10-26 16:11:24.921610: Pseudo dice [0.8709, 0.9176, 0.9658, 0.647, 0.9367] 
2023-10-26 16:11:24.921834: Epoch time: 4.05 s 
2023-10-26 16:11:26.017032:  
2023-10-26 16:11:26.017327: Epoch 557 
2023-10-26 16:11:26.017567: Current learning rate: 0.00481 
2023-10-26 16:11:30.054417: train_loss -0.856 
2023-10-26 16:11:30.054819: val_loss -0.8221 
2023-10-26 16:11:30.055093: Pseudo dice [0.8676, 0.9223, 0.965, 0.4714, 0.9369] 
2023-10-26 16:11:30.055348: Epoch time: 4.04 s 
2023-10-26 16:11:31.152188:  
2023-10-26 16:11:31.152501: Epoch 558 
2023-10-26 16:11:31.152755: Current learning rate: 0.0048 
2023-10-26 16:11:35.267015: train_loss -0.8466 
2023-10-26 16:11:35.267377: val_loss -0.8108 
2023-10-26 16:11:35.267643: Pseudo dice [0.8603, 0.9177, 0.9661, 0.5663, 0.9313] 
2023-10-26 16:11:35.267870: Epoch time: 4.12 s 
2023-10-26 16:11:36.371220:  
2023-10-26 16:11:36.371511: Epoch 559 
2023-10-26 16:11:36.371751: Current learning rate: 0.00479 
2023-10-26 16:11:40.325027: train_loss -0.8561 
2023-10-26 16:11:40.325388: val_loss -0.8146 
2023-10-26 16:11:40.325641: Pseudo dice [0.8633, 0.9154, 0.9667, 0.6004, 0.9351] 
2023-10-26 16:11:40.325895: Epoch time: 3.95 s 
2023-10-26 16:11:41.492418:  
2023-10-26 16:11:41.492706: Epoch 560 
2023-10-26 16:11:41.492947: Current learning rate: 0.00478 
2023-10-26 16:11:45.635610: train_loss -0.8411 
2023-10-26 16:11:45.636043: val_loss -0.7584 
2023-10-26 16:11:45.636319: Pseudo dice [0.8645, 0.912, 0.9661, 0.468, 0.9351] 
2023-10-26 16:11:45.636577: Epoch time: 4.14 s 
2023-10-26 16:11:46.732134:  
2023-10-26 16:11:46.732430: Epoch 561 
2023-10-26 16:11:46.732675: Current learning rate: 0.00477 
2023-10-26 16:11:50.718415: train_loss -0.8515 
2023-10-26 16:11:50.718778: val_loss -0.7727 
2023-10-26 16:11:50.719050: Pseudo dice [0.859, 0.9096, 0.9653, 0.5366, 0.9342] 
2023-10-26 16:11:50.719290: Epoch time: 3.99 s 
2023-10-26 16:11:52.009020:  
2023-10-26 16:11:52.009341: Epoch 562 
2023-10-26 16:11:52.009589: Current learning rate: 0.00476 
2023-10-26 16:11:56.153915: train_loss -0.8534 
2023-10-26 16:11:56.154315: val_loss -0.7866 
2023-10-26 16:11:56.154581: Pseudo dice [0.8653, 0.9115, 0.9668, 0.5247, 0.9372] 
2023-10-26 16:11:56.154817: Epoch time: 4.15 s 
2023-10-26 16:11:57.256584:  
2023-10-26 16:11:57.256918: Epoch 563 
2023-10-26 16:11:57.257199: Current learning rate: 0.00475 
2023-10-26 16:12:01.399362: train_loss -0.8629 
2023-10-26 16:12:01.399749: val_loss -0.8013 
2023-10-26 16:12:01.400039: Pseudo dice [0.8743, 0.9146, 0.9671, 0.6494, 0.9374] 
2023-10-26 16:12:01.400275: Epoch time: 4.14 s 
2023-10-26 16:12:02.535502:  
2023-10-26 16:12:02.535821: Epoch 564 
2023-10-26 16:12:02.536080: Current learning rate: 0.00474 
2023-10-26 16:12:06.790669: train_loss -0.8549 
2023-10-26 16:12:06.791166: val_loss -0.8131 
2023-10-26 16:12:06.791487: Pseudo dice [0.8704, 0.9159, 0.9653, 0.636, 0.9322] 
2023-10-26 16:12:06.791772: Epoch time: 4.26 s 
2023-10-26 16:12:07.900231:  
2023-10-26 16:12:07.900531: Epoch 565 
2023-10-26 16:12:07.900773: Current learning rate: 0.00473 
2023-10-26 16:12:11.971215: train_loss -0.8573 
2023-10-26 16:12:11.971589: val_loss -0.8059 
2023-10-26 16:12:11.971853: Pseudo dice [0.8686, 0.9175, 0.9662, 0.5542, 0.9357] 
2023-10-26 16:12:11.972103: Epoch time: 4.07 s 
2023-10-26 16:12:13.072378:  
2023-10-26 16:12:13.073108: Epoch 566 
2023-10-26 16:12:13.073566: Current learning rate: 0.00472 
2023-10-26 16:12:17.276858: train_loss -0.851 
2023-10-26 16:12:17.277524: val_loss -0.786 
2023-10-26 16:12:17.277994: Pseudo dice [0.8671, 0.9162, 0.9651, 0.5533, 0.9322] 
2023-10-26 16:12:17.278373: Epoch time: 4.21 s 
2023-10-26 16:12:18.392226:  
2023-10-26 16:12:18.392533: Epoch 567 
2023-10-26 16:12:18.392789: Current learning rate: 0.00471 
2023-10-26 16:12:22.432032: train_loss -0.8531 
2023-10-26 16:12:22.432477: val_loss -0.806 
2023-10-26 16:12:22.432860: Pseudo dice [0.8693, 0.9142, 0.9658, 0.5534, 0.9401] 
2023-10-26 16:12:22.433167: Epoch time: 4.04 s 
2023-10-26 16:12:23.600047:  
2023-10-26 16:12:23.600367: Epoch 568 
2023-10-26 16:12:23.600609: Current learning rate: 0.0047 
2023-10-26 16:12:27.579568: train_loss -0.8477 
2023-10-26 16:12:27.579958: val_loss -0.8021 
2023-10-26 16:12:27.580224: Pseudo dice [0.8697, 0.9147, 0.9671, 0.6104, 0.9376] 
2023-10-26 16:12:27.580464: Epoch time: 3.98 s 
2023-10-26 16:12:29.087903:  
2023-10-26 16:12:29.088224: Epoch 569 
2023-10-26 16:12:29.088485: Current learning rate: 0.00469 
2023-10-26 16:12:33.204294: train_loss -0.8599 
2023-10-26 16:12:33.204703: val_loss -0.8145 
2023-10-26 16:12:33.204964: Pseudo dice [0.8749, 0.9187, 0.965, 0.5289, 0.9392] 
2023-10-26 16:12:33.205205: Epoch time: 4.12 s 
2023-10-26 16:12:34.302976:  
2023-10-26 16:12:34.303269: Epoch 570 
2023-10-26 16:12:34.303515: Current learning rate: 0.00468 
2023-10-26 16:12:38.360288: train_loss -0.8503 
2023-10-26 16:12:38.360701: val_loss -0.7924 
2023-10-26 16:12:38.360976: Pseudo dice [0.8675, 0.9187, 0.9674, 0.5069, 0.9335] 
2023-10-26 16:12:38.361394: Epoch time: 4.06 s 
2023-10-26 16:12:39.473294:  
2023-10-26 16:12:39.473584: Epoch 571 
2023-10-26 16:12:39.473831: Current learning rate: 0.00467 
2023-10-26 16:12:43.544970: train_loss -0.8486 
2023-10-26 16:12:43.545347: val_loss -0.7924 
2023-10-26 16:12:43.545636: Pseudo dice [0.8683, 0.9148, 0.9647, 0.594, 0.9374] 
2023-10-26 16:12:43.545890: Epoch time: 4.07 s 
2023-10-26 16:12:44.682808:  
2023-10-26 16:12:44.683159: Epoch 572 
2023-10-26 16:12:44.683473: Current learning rate: 0.00466 
2023-10-26 16:12:48.744379: train_loss -0.8427 
2023-10-26 16:12:48.744774: val_loss -0.7394 
2023-10-26 16:12:48.745104: Pseudo dice [0.8695, 0.9041, 0.9655, 0.4366, 0.9337] 
2023-10-26 16:12:48.745426: Epoch time: 4.06 s 
2023-10-26 16:12:49.975370:  
2023-10-26 16:12:49.975671: Epoch 573 
2023-10-26 16:12:49.975954: Current learning rate: 0.00465 
2023-10-26 16:12:53.913816: train_loss -0.8514 
2023-10-26 16:12:53.914194: val_loss -0.7773 
2023-10-26 16:12:53.914467: Pseudo dice [0.8619, 0.9151, 0.9657, 0.4816, 0.937] 
2023-10-26 16:12:53.914698: Epoch time: 3.94 s 
2023-10-26 16:12:55.017936:  
2023-10-26 16:12:55.018226: Epoch 574 
2023-10-26 16:12:55.018475: Current learning rate: 0.00464 
2023-10-26 16:12:59.012409: train_loss -0.8479 
2023-10-26 16:12:59.012784: val_loss -0.8179 
2023-10-26 16:12:59.013073: Pseudo dice [0.8683, 0.9144, 0.9665, 0.7497, 0.9333] 
2023-10-26 16:12:59.013352: Epoch time: 4.0 s 
2023-10-26 16:13:00.119968:  
2023-10-26 16:13:00.120291: Epoch 575 
2023-10-26 16:13:00.120549: Current learning rate: 0.00463 
2023-10-26 16:13:04.253966: train_loss -0.854 
2023-10-26 16:13:04.254384: val_loss -0.7977 
2023-10-26 16:13:04.254806: Pseudo dice [0.8677, 0.9153, 0.9653, 0.7667, 0.9282] 
2023-10-26 16:13:04.255114: Epoch time: 4.13 s 
2023-10-26 16:13:05.550828:  
2023-10-26 16:13:05.551138: Epoch 576 
2023-10-26 16:13:05.551394: Current learning rate: 0.00462 
2023-10-26 16:13:09.388504: train_loss -0.8535 
2023-10-26 16:13:09.388917: val_loss -0.7956 
2023-10-26 16:13:09.389190: Pseudo dice [0.8724, 0.9128, 0.9657, 0.6699, 0.9357] 
2023-10-26 16:13:09.389435: Epoch time: 3.84 s 
2023-10-26 16:13:10.496386:  
2023-10-26 16:13:10.496716: Epoch 577 
2023-10-26 16:13:10.496962: Current learning rate: 0.00461 
2023-10-26 16:13:14.593257: train_loss -0.8506 
2023-10-26 16:13:14.593615: val_loss -0.7856 
2023-10-26 16:13:14.593900: Pseudo dice [0.8611, 0.9059, 0.964, 0.5455, 0.9335] 
2023-10-26 16:13:14.594131: Epoch time: 4.1 s 
2023-10-26 16:13:15.711637:  
2023-10-26 16:13:15.711944: Epoch 578 
2023-10-26 16:13:15.712187: Current learning rate: 0.0046 
2023-10-26 16:13:19.845125: train_loss -0.8491 
2023-10-26 16:13:19.845561: val_loss -0.7982 
2023-10-26 16:13:19.845854: Pseudo dice [0.8712, 0.9129, 0.9667, 0.6028, 0.9338] 
2023-10-26 16:13:19.846115: Epoch time: 4.13 s 
2023-10-26 16:13:21.005807:  
2023-10-26 16:13:21.006238: Epoch 579 
2023-10-26 16:13:21.006491: Current learning rate: 0.00459 
2023-10-26 16:13:25.126365: train_loss -0.8496 
2023-10-26 16:13:25.126729: val_loss -0.7814 
2023-10-26 16:13:25.126997: Pseudo dice [0.864, 0.9145, 0.9661, 0.5326, 0.9249] 
2023-10-26 16:13:25.127235: Epoch time: 4.12 s 
2023-10-26 16:13:26.299226:  
2023-10-26 16:13:26.299523: Epoch 580 
2023-10-26 16:13:26.299769: Current learning rate: 0.00458 
2023-10-26 16:13:30.404959: train_loss -0.8571 
2023-10-26 16:13:30.405327: val_loss -0.7932 
2023-10-26 16:13:30.405600: Pseudo dice [0.867, 0.9118, 0.9662, 0.5535, 0.9353] 
2023-10-26 16:13:30.405840: Epoch time: 4.11 s 
2023-10-26 16:13:31.509610:  
2023-10-26 16:13:31.509902: Epoch 581 
2023-10-26 16:13:31.510142: Current learning rate: 0.00457 
2023-10-26 16:13:35.587001: train_loss -0.8612 
2023-10-26 16:13:35.587410: val_loss -0.788 
2023-10-26 16:13:35.587678: Pseudo dice [0.8665, 0.9146, 0.9657, 0.5284, 0.9342] 
2023-10-26 16:13:35.587924: Epoch time: 4.08 s 
2023-10-26 16:13:36.887893:  
2023-10-26 16:13:36.888199: Epoch 582 
2023-10-26 16:13:36.888462: Current learning rate: 0.00456 
2023-10-26 16:13:41.046216: train_loss -0.8477 
2023-10-26 16:13:41.046616: val_loss -0.8077 
2023-10-26 16:13:41.046983: Pseudo dice [0.8688, 0.9173, 0.9663, 0.5224, 0.9366] 
2023-10-26 16:13:41.047379: Epoch time: 4.16 s 
2023-10-26 16:13:42.155410:  
2023-10-26 16:13:42.155706: Epoch 583 
2023-10-26 16:13:42.155957: Current learning rate: 0.00455 
2023-10-26 16:13:46.097711: train_loss -0.851 
2023-10-26 16:13:46.098095: val_loss -0.7801 
2023-10-26 16:13:46.098361: Pseudo dice [0.8659, 0.9169, 0.9658, 0.568, 0.9331] 
2023-10-26 16:13:46.098583: Epoch time: 3.94 s 
2023-10-26 16:13:47.252257:  
2023-10-26 16:13:47.252550: Epoch 584 
2023-10-26 16:13:47.252797: Current learning rate: 0.00454 
2023-10-26 16:13:51.369862: train_loss -0.8524 
2023-10-26 16:13:51.370251: val_loss -0.8053 
2023-10-26 16:13:51.370510: Pseudo dice [0.8731, 0.917, 0.9668, 0.6537, 0.9365] 
2023-10-26 16:13:51.370753: Epoch time: 4.12 s 
2023-10-26 16:13:52.504783:  
2023-10-26 16:13:52.505084: Epoch 585 
2023-10-26 16:13:52.505336: Current learning rate: 0.00453 
2023-10-26 16:13:56.439363: train_loss -0.8509 
2023-10-26 16:13:56.439745: val_loss -0.7697 
2023-10-26 16:13:56.440027: Pseudo dice [0.8725, 0.918, 0.9661, 0.5688, 0.9352] 
2023-10-26 16:13:56.440282: Epoch time: 3.94 s 
2023-10-26 16:13:57.538722:  
2023-10-26 16:13:57.539029: Epoch 586 
2023-10-26 16:13:57.539275: Current learning rate: 0.00452 
2023-10-26 16:14:01.592526: train_loss -0.8487 
2023-10-26 16:14:01.593014: val_loss -0.8095 
2023-10-26 16:14:01.593345: Pseudo dice [0.8645, 0.9094, 0.9649, 0.0, 0.9382] 
2023-10-26 16:14:01.593678: Epoch time: 4.05 s 
2023-10-26 16:14:02.723966:  
2023-10-26 16:14:02.724283: Epoch 587 
2023-10-26 16:14:02.724543: Current learning rate: 0.00451 
2023-10-26 16:14:06.749542: train_loss -0.8315 
2023-10-26 16:14:06.749922: val_loss -0.822 
2023-10-26 16:14:06.750185: Pseudo dice [0.8694, 0.9217, 0.9657, 0.756, 0.9349] 
2023-10-26 16:14:06.750415: Epoch time: 4.03 s 
2023-10-26 16:14:07.867338:  
2023-10-26 16:14:07.867681: Epoch 588 
2023-10-26 16:14:07.867939: Current learning rate: 0.0045 
2023-10-26 16:14:11.860273: train_loss -0.842 
2023-10-26 16:14:11.860645: val_loss -0.7732 
2023-10-26 16:14:11.861059: Pseudo dice [0.8686, 0.9134, 0.9651, 0.5265, 0.932] 
2023-10-26 16:14:11.861304: Epoch time: 3.99 s 
2023-10-26 16:14:13.156054:  
2023-10-26 16:14:13.156376: Epoch 589 
2023-10-26 16:14:13.156637: Current learning rate: 0.00449 
2023-10-26 16:14:17.189338: train_loss -0.8498 
2023-10-26 16:14:17.189811: val_loss -0.7771 
2023-10-26 16:14:17.190146: Pseudo dice [0.8583, 0.9133, 0.966, 0.5331, 0.9304] 
2023-10-26 16:14:17.190421: Epoch time: 4.03 s 
2023-10-26 16:14:18.294449:  
2023-10-26 16:14:18.294759: Epoch 590 
2023-10-26 16:14:18.295010: Current learning rate: 0.00448 
2023-10-26 16:14:22.284493: train_loss -0.8452 
2023-10-26 16:14:22.284944: val_loss -0.7862 
2023-10-26 16:14:22.285219: Pseudo dice [0.8673, 0.9149, 0.9648, 0.5981, 0.9366] 
2023-10-26 16:14:22.285466: Epoch time: 3.99 s 
2023-10-26 16:14:23.409203:  
2023-10-26 16:14:23.409580: Epoch 591 
2023-10-26 16:14:23.409902: Current learning rate: 0.00447 
2023-10-26 16:14:27.437810: train_loss -0.853 
2023-10-26 16:14:27.438575: val_loss -0.7869 
2023-10-26 16:14:27.439066: Pseudo dice [0.8602, 0.9143, 0.9659, 0.5925, 0.9337] 
2023-10-26 16:14:27.439358: Epoch time: 4.03 s 
2023-10-26 16:14:28.624471:  
2023-10-26 16:14:28.624794: Epoch 592 
2023-10-26 16:14:28.625056: Current learning rate: 0.00446 
2023-10-26 16:14:32.617375: train_loss -0.8163 
2023-10-26 16:14:32.617758: val_loss -0.8112 
2023-10-26 16:14:32.618036: Pseudo dice [0.8634, 0.9048, 0.9618, 0.0, 0.9343] 
2023-10-26 16:14:32.618287: Epoch time: 3.99 s 
2023-10-26 16:14:33.736811:  
2023-10-26 16:14:33.737123: Epoch 593 
2023-10-26 16:14:33.737382: Current learning rate: 0.00445 
2023-10-26 16:14:37.837181: train_loss -0.7944 
2023-10-26 16:14:37.837549: val_loss -0.8119 
2023-10-26 16:14:37.837808: Pseudo dice [0.8649, 0.9097, 0.963, 0.754, 0.9342] 
2023-10-26 16:14:37.838043: Epoch time: 4.1 s 
2023-10-26 16:14:39.020909:  
2023-10-26 16:14:39.021226: Epoch 594 
2023-10-26 16:14:39.021473: Current learning rate: 0.00444 
2023-10-26 16:14:43.028835: train_loss -0.824 
2023-10-26 16:14:43.029234: val_loss -0.7996 
2023-10-26 16:14:43.029491: Pseudo dice [0.8674, 0.9171, 0.9672, 0.594, 0.9316] 
2023-10-26 16:14:43.029722: Epoch time: 4.01 s 
2023-10-26 16:14:44.321920:  
2023-10-26 16:14:44.322228: Epoch 595 
2023-10-26 16:14:44.322487: Current learning rate: 0.00443 
2023-10-26 16:14:48.165234: train_loss -0.8255 
2023-10-26 16:14:48.165605: val_loss -0.7875 
2023-10-26 16:14:48.165869: Pseudo dice [0.8708, 0.917, 0.9648, 0.7495, 0.9361] 
2023-10-26 16:14:48.166110: Epoch time: 3.84 s 
2023-10-26 16:14:49.278844:  
2023-10-26 16:14:49.279161: Epoch 596 
2023-10-26 16:14:49.279405: Current learning rate: 0.00442 
2023-10-26 16:14:53.097357: train_loss -0.8192 
2023-10-26 16:14:53.097847: val_loss -0.7797 
2023-10-26 16:14:53.098140: Pseudo dice [0.8652, 0.9037, 0.9646, 0.5382, 0.9343] 
2023-10-26 16:14:53.098390: Epoch time: 3.82 s 
2023-10-26 16:14:54.230192:  
2023-10-26 16:14:54.230503: Epoch 597 
2023-10-26 16:14:54.230756: Current learning rate: 0.00441 
2023-10-26 16:14:58.011856: train_loss -0.8298 
2023-10-26 16:14:58.012694: val_loss -0.8199 
2023-10-26 16:14:58.013073: Pseudo dice [0.8614, 0.9142, 0.9659, 0.6265, 0.9275] 
2023-10-26 16:14:58.013359: Epoch time: 3.78 s 
2023-10-26 16:14:59.118325:  
2023-10-26 16:14:59.118639: Epoch 598 
2023-10-26 16:14:59.118897: Current learning rate: 0.0044 
2023-10-26 16:15:03.148901: train_loss -0.8382 
2023-10-26 16:15:03.149529: val_loss -0.7832 
2023-10-26 16:15:03.149891: Pseudo dice [0.8717, 0.9174, 0.9653, 0.6178, 0.9339] 
2023-10-26 16:15:03.150217: Epoch time: 4.03 s 
2023-10-26 16:15:04.255307:  
2023-10-26 16:15:04.255615: Epoch 599 
2023-10-26 16:15:04.255867: Current learning rate: 0.00439 
2023-10-26 16:15:08.232217: train_loss -0.843 
2023-10-26 16:15:08.232667: val_loss -0.7851 
2023-10-26 16:15:08.233093: Pseudo dice [0.863, 0.9122, 0.9649, 0.5342, 0.9334] 
2023-10-26 16:15:08.233381: Epoch time: 3.98 s 
2023-10-26 16:15:09.444241:  
2023-10-26 16:15:09.444547: Epoch 600 
2023-10-26 16:15:09.444793: Current learning rate: 0.00438 
2023-10-26 16:15:13.444582: train_loss -0.8404 
2023-10-26 16:15:13.444969: val_loss -0.8078 
2023-10-26 16:15:13.445242: Pseudo dice [0.8689, 0.9162, 0.9661, 0.605, 0.935] 
2023-10-26 16:15:13.445483: Epoch time: 4.0 s 
2023-10-26 16:15:14.744209:  
2023-10-26 16:15:14.744512: Epoch 601 
2023-10-26 16:15:14.744759: Current learning rate: 0.00437 
2023-10-26 16:15:18.822356: train_loss -0.8582 
2023-10-26 16:15:18.822737: val_loss -0.7991 
2023-10-26 16:15:18.823010: Pseudo dice [0.8626, 0.9176, 0.9672, 0.5349, 0.9353] 
2023-10-26 16:15:18.823237: Epoch time: 4.08 s 
2023-10-26 16:15:19.934189:  
2023-10-26 16:15:19.934491: Epoch 602 
2023-10-26 16:15:19.934736: Current learning rate: 0.00436 
2023-10-26 16:15:24.016568: train_loss -0.8501 
2023-10-26 16:15:24.017022: val_loss -0.7825 
2023-10-26 16:15:24.017469: Pseudo dice [0.8701, 0.9129, 0.9649, 0.1891, 0.9356] 
2023-10-26 16:15:24.017764: Epoch time: 4.08 s 
2023-10-26 16:15:25.155425:  
2023-10-26 16:15:25.155715: Epoch 603 
2023-10-26 16:15:25.155964: Current learning rate: 0.00435 
2023-10-26 16:15:29.216880: train_loss -0.8477 
2023-10-26 16:15:29.217278: val_loss -0.797 
2023-10-26 16:15:29.217567: Pseudo dice [0.876, 0.9201, 0.9641, 0.4781, 0.9379] 
2023-10-26 16:15:29.217810: Epoch time: 4.06 s 
2023-10-26 16:15:30.343699:  
2023-10-26 16:15:30.344013: Epoch 604 
2023-10-26 16:15:30.344259: Current learning rate: 0.00434 
2023-10-26 16:15:34.233819: train_loss -0.8494 
2023-10-26 16:15:34.234193: val_loss -0.8073 
2023-10-26 16:15:34.234484: Pseudo dice [0.8631, 0.9164, 0.9676, 0.5065, 0.9359] 
2023-10-26 16:15:34.234731: Epoch time: 3.89 s 
2023-10-26 16:15:35.335351:  
2023-10-26 16:15:35.335668: Epoch 605 
2023-10-26 16:15:35.335919: Current learning rate: 0.00433 
2023-10-26 16:15:39.449908: train_loss -0.8561 
2023-10-26 16:15:39.450256: val_loss -0.7798 
2023-10-26 16:15:39.450509: Pseudo dice [0.8638, 0.9139, 0.9654, 0.354, 0.935] 
2023-10-26 16:15:39.450734: Epoch time: 4.12 s 
2023-10-26 16:15:40.552141:  
2023-10-26 16:15:40.552456: Epoch 606 
2023-10-26 16:15:40.552712: Current learning rate: 0.00432 
2023-10-26 16:15:44.639752: train_loss -0.8529 
2023-10-26 16:15:44.640131: val_loss -0.7714 
2023-10-26 16:15:44.640393: Pseudo dice [0.8597, 0.915, 0.9661, 0.4052, 0.9294] 
2023-10-26 16:15:44.640621: Epoch time: 4.09 s 
2023-10-26 16:15:45.736356:  
2023-10-26 16:15:45.736647: Epoch 607 
2023-10-26 16:15:45.736897: Current learning rate: 0.00431 
2023-10-26 16:15:49.583986: train_loss -0.8466 
2023-10-26 16:15:49.584354: val_loss -0.7984 
2023-10-26 16:15:49.584607: Pseudo dice [0.8733, 0.9198, 0.9651, 0.4806, 0.9336] 
2023-10-26 16:15:49.584828: Epoch time: 3.85 s 
2023-10-26 16:15:50.853397:  
2023-10-26 16:15:50.853704: Epoch 608 
2023-10-26 16:15:50.853970: Current learning rate: 0.0043 
2023-10-26 16:15:55.049817: train_loss -0.8508 
2023-10-26 16:15:55.050348: val_loss -0.7998 
2023-10-26 16:15:55.050729: Pseudo dice [0.8716, 0.9192, 0.9661, 0.4868, 0.9374] 
2023-10-26 16:15:55.051025: Epoch time: 4.2 s 
2023-10-26 16:15:56.148063:  
2023-10-26 16:15:56.148394: Epoch 609 
2023-10-26 16:15:56.148640: Current learning rate: 0.00429 
2023-10-26 16:16:00.420893: train_loss -0.8506 
2023-10-26 16:16:00.421273: val_loss -0.7913 
2023-10-26 16:16:00.421545: Pseudo dice [0.8607, 0.9141, 0.9659, 0.4075, 0.9325] 
2023-10-26 16:16:00.421774: Epoch time: 4.27 s 
2023-10-26 16:16:01.545478:  
2023-10-26 16:16:01.545807: Epoch 610 
2023-10-26 16:16:01.546061: Current learning rate: 0.00429 
2023-10-26 16:16:05.550936: train_loss -0.8602 
2023-10-26 16:16:05.551315: val_loss -0.7827 
2023-10-26 16:16:05.551578: Pseudo dice [0.8683, 0.9166, 0.9642, 0.5216, 0.9335] 
2023-10-26 16:16:05.551808: Epoch time: 4.01 s 
2023-10-26 16:16:06.658253:  
2023-10-26 16:16:06.658565: Epoch 611 
2023-10-26 16:16:06.658817: Current learning rate: 0.00428 
2023-10-26 16:16:10.828272: train_loss -0.8578 
2023-10-26 16:16:10.828640: val_loss -0.7878 
2023-10-26 16:16:10.828920: Pseudo dice [0.8648, 0.9091, 0.9658, 0.4337, 0.9351] 
2023-10-26 16:16:10.829170: Epoch time: 4.17 s 
2023-10-26 16:16:11.929390:  
2023-10-26 16:16:11.929790: Epoch 612 
2023-10-26 16:16:11.930120: Current learning rate: 0.00427 
2023-10-26 16:16:16.143616: train_loss -0.8591 
2023-10-26 16:16:16.144019: val_loss -0.8024 
2023-10-26 16:16:16.144294: Pseudo dice [0.8694, 0.9103, 0.9658, 0.3651, 0.9294] 
2023-10-26 16:16:16.144538: Epoch time: 4.22 s 
2023-10-26 16:16:17.248446:  
2023-10-26 16:16:17.248768: Epoch 613 
2023-10-26 16:16:17.249029: Current learning rate: 0.00426 
2023-10-26 16:16:21.324714: train_loss -0.8456 
2023-10-26 16:16:21.325124: val_loss -0.8078 
2023-10-26 16:16:21.325412: Pseudo dice [0.8719, 0.9159, 0.9656, 0.417, 0.9354] 
2023-10-26 16:16:21.325658: Epoch time: 4.08 s 
2023-10-26 16:16:22.606064:  
2023-10-26 16:16:22.606392: Epoch 614 
2023-10-26 16:16:22.606645: Current learning rate: 0.00425 
2023-10-26 16:16:26.645315: train_loss -0.8467 
2023-10-26 16:16:26.645741: val_loss -0.7783 
2023-10-26 16:16:26.646011: Pseudo dice [0.8684, 0.912, 0.9672, 0.4663, 0.9372] 
2023-10-26 16:16:26.646250: Epoch time: 4.04 s 
2023-10-26 16:16:27.750802:  
2023-10-26 16:16:27.751118: Epoch 615 
2023-10-26 16:16:27.751376: Current learning rate: 0.00424 
2023-10-26 16:16:31.975509: train_loss -0.8593 
2023-10-26 16:16:31.975862: val_loss -0.8211 
2023-10-26 16:16:31.976135: Pseudo dice [0.8769, 0.9159, 0.9662, 0.6112, 0.9385] 
2023-10-26 16:16:31.976363: Epoch time: 4.23 s 
2023-10-26 16:16:33.086977:  
2023-10-26 16:16:33.087293: Epoch 616 
2023-10-26 16:16:33.087551: Current learning rate: 0.00423 
2023-10-26 16:16:37.374861: train_loss -0.8531 
2023-10-26 16:16:37.375263: val_loss -0.79 
2023-10-26 16:16:37.375536: Pseudo dice [0.8668, 0.9155, 0.9666, 0.5652, 0.9341] 
2023-10-26 16:16:37.375781: Epoch time: 4.29 s 
2023-10-26 16:16:38.479432:  
2023-10-26 16:16:38.479752: Epoch 617 
2023-10-26 16:16:38.480001: Current learning rate: 0.00422 
2023-10-26 16:16:42.661632: train_loss -0.86 
2023-10-26 16:16:42.662016: val_loss -0.7829 
2023-10-26 16:16:42.662291: Pseudo dice [0.8698, 0.9125, 0.9654, 0.648, 0.9347] 
2023-10-26 16:16:42.662535: Epoch time: 4.18 s 
2023-10-26 16:16:43.761195:  
2023-10-26 16:16:43.761487: Epoch 618 
2023-10-26 16:16:43.761754: Current learning rate: 0.00421 
2023-10-26 16:16:48.043691: train_loss -0.849 
2023-10-26 16:16:48.044048: val_loss -0.7811 
2023-10-26 16:16:48.044307: Pseudo dice [0.8681, 0.9065, 0.9652, 0.7034, 0.9337] 
2023-10-26 16:16:48.044529: Epoch time: 4.28 s 
2023-10-26 16:16:49.154543:  
2023-10-26 16:16:49.154853: Epoch 619 
2023-10-26 16:16:49.155122: Current learning rate: 0.0042 
2023-10-26 16:16:53.428704: train_loss -0.8394 
2023-10-26 16:16:53.429090: val_loss -0.7921 
2023-10-26 16:16:53.429354: Pseudo dice [0.8776, 0.9156, 0.965, 0.6586, 0.9298] 
2023-10-26 16:16:53.429580: Epoch time: 4.27 s 
2023-10-26 16:16:54.546298:  
2023-10-26 16:16:54.546602: Epoch 620 
2023-10-26 16:16:54.546848: Current learning rate: 0.00419 
2023-10-26 16:16:58.642486: train_loss -0.8482 
2023-10-26 16:16:58.642843: val_loss -0.8035 
2023-10-26 16:16:58.643109: Pseudo dice [0.8735, 0.9214, 0.9638, 0.557, 0.9328] 
2023-10-26 16:16:58.643333: Epoch time: 4.1 s 
2023-10-26 16:16:59.900866:  
2023-10-26 16:16:59.901185: Epoch 621 
2023-10-26 16:16:59.901440: Current learning rate: 0.00418 
2023-10-26 16:17:04.013816: train_loss -0.8423 
2023-10-26 16:17:04.014174: val_loss -0.7924 
2023-10-26 16:17:04.014426: Pseudo dice [0.8623, 0.9071, 0.9653, 0.5634, 0.9329] 
2023-10-26 16:17:04.014652: Epoch time: 4.11 s 
2023-10-26 16:17:05.119484:  
2023-10-26 16:17:05.119803: Epoch 622 
2023-10-26 16:17:05.120064: Current learning rate: 0.00417 
2023-10-26 16:17:09.360503: train_loss -0.8373 
2023-10-26 16:17:09.360847: val_loss -0.792 
2023-10-26 16:17:09.361118: Pseudo dice [0.8627, 0.9091, 0.9634, 0.4452, 0.9322] 
2023-10-26 16:17:09.361351: Epoch time: 4.24 s 
2023-10-26 16:17:10.484371:  
2023-10-26 16:17:10.484681: Epoch 623 
2023-10-26 16:17:10.484953: Current learning rate: 0.00416 
2023-10-26 16:17:14.449195: train_loss -0.853 
2023-10-26 16:17:14.449556: val_loss -0.7827 
2023-10-26 16:17:14.449814: Pseudo dice [0.8653, 0.9181, 0.9669, 0.6244, 0.9365] 
2023-10-26 16:17:14.450048: Epoch time: 3.97 s 
2023-10-26 16:17:15.553456:  
2023-10-26 16:17:15.553770: Epoch 624 
2023-10-26 16:17:15.554030: Current learning rate: 0.00415 
2023-10-26 16:17:19.503826: train_loss -0.862 
2023-10-26 16:17:19.504235: val_loss -0.7738 
2023-10-26 16:17:19.504495: Pseudo dice [0.8767, 0.903, 0.9642, 0.3329, 0.9372] 
2023-10-26 16:17:19.504727: Epoch time: 3.95 s 
2023-10-26 16:17:20.612371:  
2023-10-26 16:17:20.612687: Epoch 625 
2023-10-26 16:17:20.612945: Current learning rate: 0.00414 
2023-10-26 16:17:24.873791: train_loss -0.8432 
2023-10-26 16:17:24.874218: val_loss -0.7596 
2023-10-26 16:17:24.874482: Pseudo dice [0.8601, 0.9115, 0.9638, 0.6052, 0.9314] 
2023-10-26 16:17:24.874814: Epoch time: 4.26 s 
2023-10-26 16:17:26.004216:  
2023-10-26 16:17:26.004534: Epoch 626 
2023-10-26 16:17:26.004792: Current learning rate: 0.00413 
2023-10-26 16:17:30.216258: train_loss -0.8505 
2023-10-26 16:17:30.216689: val_loss -0.7674 
2023-10-26 16:17:30.216963: Pseudo dice [0.872, 0.9164, 0.9665, 0.5616, 0.8535] 
2023-10-26 16:17:30.217203: Epoch time: 4.21 s 
2023-10-26 16:17:31.500707:  
2023-10-26 16:17:31.501439: Epoch 627 
2023-10-26 16:17:31.501801: Current learning rate: 0.00412 
2023-10-26 16:17:35.609860: train_loss -0.8518 
2023-10-26 16:17:35.610261: val_loss -0.7917 
2023-10-26 16:17:35.610526: Pseudo dice [0.8736, 0.9185, 0.9647, 0.6716, 0.9128] 
2023-10-26 16:17:35.610761: Epoch time: 4.11 s 
2023-10-26 16:17:36.739465:  
2023-10-26 16:17:36.739763: Epoch 628 
2023-10-26 16:17:36.740038: Current learning rate: 0.00411 
2023-10-26 16:17:40.953572: train_loss -0.8459 
2023-10-26 16:17:40.953920: val_loss -0.7923 
2023-10-26 16:17:40.954189: Pseudo dice [0.8684, 0.9121, 0.9641, 0.7307, 0.9392] 
2023-10-26 16:17:40.954419: Epoch time: 4.21 s 
2023-10-26 16:17:42.056613:  
2023-10-26 16:17:42.056926: Epoch 629 
2023-10-26 16:17:42.057197: Current learning rate: 0.0041 
2023-10-26 16:17:46.167996: train_loss -0.8449 
2023-10-26 16:17:46.168472: val_loss -0.7747 
2023-10-26 16:17:46.168846: Pseudo dice [0.8747, 0.9169, 0.9637, 0.6602, 0.9354] 
2023-10-26 16:17:46.169363: Epoch time: 4.11 s 
2023-10-26 16:17:47.278193:  
2023-10-26 16:17:47.278651: Epoch 630 
2023-10-26 16:17:47.278914: Current learning rate: 0.00409 
2023-10-26 16:17:51.435879: train_loss -0.8525 
2023-10-26 16:17:51.436459: val_loss -0.7982 
2023-10-26 16:17:51.436714: Pseudo dice [0.87, 0.919, 0.9671, 0.6644, 0.9328] 
2023-10-26 16:17:51.437040: Epoch time: 4.16 s 
2023-10-26 16:17:52.552625:  
2023-10-26 16:17:52.552987: Epoch 631 
2023-10-26 16:17:52.553281: Current learning rate: 0.00408 
2023-10-26 16:17:56.607440: train_loss -0.8553 
2023-10-26 16:17:56.607849: val_loss -0.8104 
2023-10-26 16:17:56.608134: Pseudo dice [0.8566, 0.9124, 0.9649, 0.4968, 0.9326] 
2023-10-26 16:17:56.608380: Epoch time: 4.06 s 
2023-10-26 16:17:57.710147:  
2023-10-26 16:17:57.710440: Epoch 632 
2023-10-26 16:17:57.710687: Current learning rate: 0.00407 
2023-10-26 16:18:01.761963: train_loss -0.8389 
2023-10-26 16:18:01.762319: val_loss -0.7839 
2023-10-26 16:18:01.762566: Pseudo dice [0.8722, 0.913, 0.9644, 0.5833, 0.9348] 
2023-10-26 16:18:01.762809: Epoch time: 4.05 s 
2023-10-26 16:18:02.881673:  
2023-10-26 16:18:02.882046: Epoch 633 
2023-10-26 16:18:02.882397: Current learning rate: 0.00406 
2023-10-26 16:18:07.105890: train_loss -0.8465 
2023-10-26 16:18:07.106261: val_loss -0.7763 
2023-10-26 16:18:07.106531: Pseudo dice [0.864, 0.9101, 0.9656, 0.7249, 0.924] 
2023-10-26 16:18:07.106768: Epoch time: 4.22 s 
2023-10-26 16:18:08.382829:  
2023-10-26 16:18:08.383195: Epoch 634 
2023-10-26 16:18:08.383456: Current learning rate: 0.00405 
2023-10-26 16:18:12.585069: train_loss -0.8411 
2023-10-26 16:18:12.585487: val_loss -0.8128 
2023-10-26 16:18:12.585762: Pseudo dice [0.8703, 0.9161, 0.9667, 0.4729, 0.9355] 
2023-10-26 16:18:12.586036: Epoch time: 4.2 s 
2023-10-26 16:18:13.756751:  
2023-10-26 16:18:13.757073: Epoch 635 
2023-10-26 16:18:13.757343: Current learning rate: 0.00404 
2023-10-26 16:18:17.950868: train_loss -0.8486 
2023-10-26 16:18:17.951274: val_loss -0.8204 
2023-10-26 16:18:17.951562: Pseudo dice [0.8688, 0.9127, 0.9674, 0.5057, 0.9364] 
2023-10-26 16:18:17.951808: Epoch time: 4.19 s 
2023-10-26 16:18:19.061696:  
2023-10-26 16:18:19.062012: Epoch 636 
2023-10-26 16:18:19.062261: Current learning rate: 0.00403 
2023-10-26 16:18:23.299650: train_loss -0.8515 
2023-10-26 16:18:23.300176: val_loss -0.7674 
2023-10-26 16:18:23.300581: Pseudo dice [0.8644, 0.9156, 0.9666, 0.5066, 0.9371] 
2023-10-26 16:18:23.300865: Epoch time: 4.24 s 
2023-10-26 16:18:24.406132:  
2023-10-26 16:18:24.406423: Epoch 637 
2023-10-26 16:18:24.406667: Current learning rate: 0.00402 
2023-10-26 16:18:28.448623: train_loss -0.8493 
2023-10-26 16:18:28.448984: val_loss -0.8078 
2023-10-26 16:18:28.449248: Pseudo dice [0.8679, 0.9152, 0.9682, 0.5324, 0.94] 
2023-10-26 16:18:28.449474: Epoch time: 4.04 s 
2023-10-26 16:18:29.548737:  
2023-10-26 16:18:29.549050: Epoch 638 
2023-10-26 16:18:29.549319: Current learning rate: 0.00401 
2023-10-26 16:18:33.699688: train_loss -0.8556 
2023-10-26 16:18:33.700158: val_loss -0.7745 
2023-10-26 16:18:33.700559: Pseudo dice [0.87, 0.9133, 0.9675, 0.4802, 0.9312] 
2023-10-26 16:18:33.700842: Epoch time: 4.15 s 
2023-10-26 16:18:34.851280:  
2023-10-26 16:18:34.851577: Epoch 639 
2023-10-26 16:18:34.851845: Current learning rate: 0.004 
2023-10-26 16:18:38.833497: train_loss -0.8514 
2023-10-26 16:18:38.833896: val_loss -0.7982 
2023-10-26 16:18:38.834160: Pseudo dice [0.8768, 0.9181, 0.9669, 0.5888, 0.9368] 
2023-10-26 16:18:38.834394: Epoch time: 3.98 s 
2023-10-26 16:18:40.185273:  
2023-10-26 16:18:40.185593: Epoch 640 
2023-10-26 16:18:40.185850: Current learning rate: 0.00399 
2023-10-26 16:18:44.367122: train_loss -0.8505 
2023-10-26 16:18:44.367495: val_loss -0.7867 
2023-10-26 16:18:44.367760: Pseudo dice [0.8653, 0.9182, 0.9689, 0.3852, 0.932] 
2023-10-26 16:18:44.368005: Epoch time: 4.18 s 
2023-10-26 16:18:45.474171:  
2023-10-26 16:18:45.474482: Epoch 641 
2023-10-26 16:18:45.474721: Current learning rate: 0.00398 
2023-10-26 16:18:49.568541: train_loss -0.8531 
2023-10-26 16:18:49.568922: val_loss -0.7605 
2023-10-26 16:18:49.569191: Pseudo dice [0.8754, 0.9184, 0.9659, 0.4322, 0.9375] 
2023-10-26 16:18:49.569440: Epoch time: 4.09 s 
2023-10-26 16:18:50.761813:  
2023-10-26 16:18:50.762164: Epoch 642 
2023-10-26 16:18:50.762450: Current learning rate: 0.00397 
2023-10-26 16:18:54.914065: train_loss -0.8534 
2023-10-26 16:18:54.914520: val_loss -0.8067 
2023-10-26 16:18:54.914804: Pseudo dice [0.8638, 0.9098, 0.9667, 0.5251, 0.9327] 
2023-10-26 16:18:54.915060: Epoch time: 4.15 s 
2023-10-26 16:18:56.017834:  
2023-10-26 16:18:56.018160: Epoch 643 
2023-10-26 16:18:56.018413: Current learning rate: 0.00396 
2023-10-26 16:19:00.126968: train_loss -0.8452 
2023-10-26 16:19:00.127321: val_loss -0.8068 
2023-10-26 16:19:00.127588: Pseudo dice [0.8664, 0.9184, 0.9667, 0.4453, 0.9344] 
2023-10-26 16:19:00.127823: Epoch time: 4.11 s 
2023-10-26 16:19:01.248569:  
2023-10-26 16:19:01.248888: Epoch 644 
2023-10-26 16:19:01.249131: Current learning rate: 0.00395 
2023-10-26 16:19:05.414382: train_loss -0.8434 
2023-10-26 16:19:05.414772: val_loss -0.7706 
2023-10-26 16:19:05.415044: Pseudo dice [0.8583, 0.9098, 0.9675, 0.6111, 0.9333] 
2023-10-26 16:19:05.415277: Epoch time: 4.17 s 
2023-10-26 16:19:06.524812:  
2023-10-26 16:19:06.525140: Epoch 645 
2023-10-26 16:19:06.525382: Current learning rate: 0.00394 
2023-10-26 16:19:10.388216: train_loss -0.8613 
2023-10-26 16:19:10.388564: val_loss -0.7899 
2023-10-26 16:19:10.388813: Pseudo dice [0.8653, 0.9146, 0.9657, 0.6384, 0.9318] 
2023-10-26 16:19:10.389046: Epoch time: 3.86 s 
2023-10-26 16:19:11.785342:  
2023-10-26 16:19:11.785711: Epoch 646 
2023-10-26 16:19:11.786017: Current learning rate: 0.00393 
2023-10-26 16:19:15.996817: train_loss -0.8396 
2023-10-26 16:19:15.997186: val_loss -0.8038 
2023-10-26 16:19:15.997451: Pseudo dice [0.8715, 0.9181, 0.9653, 0.5655, 0.9338] 
2023-10-26 16:19:15.997672: Epoch time: 4.21 s 
2023-10-26 16:19:17.124367:  
2023-10-26 16:19:17.124692: Epoch 647 
2023-10-26 16:19:17.124946: Current learning rate: 0.00392 
2023-10-26 16:19:21.288096: train_loss -0.844 
2023-10-26 16:19:21.288515: val_loss -0.7989 
2023-10-26 16:19:21.288787: Pseudo dice [0.8667, 0.923, 0.9657, 0.6819, 0.9297] 
2023-10-26 16:19:21.289026: Epoch time: 4.16 s 
2023-10-26 16:19:22.432256:  
2023-10-26 16:19:22.432571: Epoch 648 
2023-10-26 16:19:22.432817: Current learning rate: 0.00391 
2023-10-26 16:19:26.542974: train_loss -0.8519 
2023-10-26 16:19:26.543339: val_loss -0.7761 
2023-10-26 16:19:26.543592: Pseudo dice [0.8685, 0.9142, 0.9675, 0.476, 0.9378] 
2023-10-26 16:19:26.543821: Epoch time: 4.11 s 
2023-10-26 16:19:27.641233:  
2023-10-26 16:19:27.641533: Epoch 649 
2023-10-26 16:19:27.641771: Current learning rate: 0.0039 
2023-10-26 16:19:31.781529: train_loss -0.8496 
2023-10-26 16:19:31.781883: val_loss -0.785 
2023-10-26 16:19:31.782148: Pseudo dice [0.8702, 0.9135, 0.9672, 0.5591, 0.9358] 
2023-10-26 16:19:31.782385: Epoch time: 4.14 s 
2023-10-26 16:19:32.982459:  
2023-10-26 16:19:32.982816: Epoch 650 
2023-10-26 16:19:32.983083: Current learning rate: 0.00389 
2023-10-26 16:19:37.249267: train_loss -0.8445 
2023-10-26 16:19:37.249671: val_loss -0.7881 
2023-10-26 16:19:37.249943: Pseudo dice [0.8702, 0.9141, 0.9641, 0.5182, 0.9329] 
2023-10-26 16:19:37.250184: Epoch time: 4.27 s 
2023-10-26 16:19:38.361567:  
2023-10-26 16:19:38.361904: Epoch 651 
2023-10-26 16:19:38.362165: Current learning rate: 0.00388 
2023-10-26 16:19:42.457299: train_loss -0.8421 
2023-10-26 16:19:42.457757: val_loss -0.7536 
2023-10-26 16:19:42.458030: Pseudo dice [0.8626, 0.9086, 0.9655, 0.2981, 0.935] 
2023-10-26 16:19:42.458266: Epoch time: 4.1 s 
2023-10-26 16:19:43.598564:  
2023-10-26 16:19:43.598918: Epoch 652 
2023-10-26 16:19:43.599183: Current learning rate: 0.00387 
2023-10-26 16:19:47.896239: train_loss -0.8455 
2023-10-26 16:19:47.896645: val_loss -0.7822 
2023-10-26 16:19:47.896922: Pseudo dice [0.8667, 0.9169, 0.9681, 0.4544, 0.9354] 
2023-10-26 16:19:47.897163: Epoch time: 4.3 s 
2023-10-26 16:19:49.049108:  
2023-10-26 16:19:49.049469: Epoch 653 
2023-10-26 16:19:49.049737: Current learning rate: 0.00386 
2023-10-26 16:19:53.168849: train_loss -0.8464 
2023-10-26 16:19:53.169262: val_loss -0.7852 
2023-10-26 16:19:53.169526: Pseudo dice [0.8653, 0.9167, 0.9682, 0.4087, 0.936] 
2023-10-26 16:19:53.170016: Epoch time: 4.12 s 
2023-10-26 16:19:54.282331:  
2023-10-26 16:19:54.282643: Epoch 654 
2023-10-26 16:19:54.282899: Current learning rate: 0.00385 
2023-10-26 16:19:58.279026: train_loss -0.8504 
2023-10-26 16:19:58.279552: val_loss -0.8123 
2023-10-26 16:19:58.279835: Pseudo dice [0.8681, 0.9171, 0.9664, 0.523, 0.9349] 
2023-10-26 16:19:58.280064: Epoch time: 4.0 s 
2023-10-26 16:19:59.418654:  
2023-10-26 16:19:59.418969: Epoch 655 
2023-10-26 16:19:59.419224: Current learning rate: 0.00384 
2023-10-26 16:20:03.558901: train_loss -0.8545 
2023-10-26 16:20:03.559291: val_loss -0.8096 
2023-10-26 16:20:03.559560: Pseudo dice [0.8666, 0.911, 0.9661, 0.5166, 0.9352] 
2023-10-26 16:20:03.559816: Epoch time: 4.14 s 
2023-10-26 16:20:04.665766:  
2023-10-26 16:20:04.666111: Epoch 656 
2023-10-26 16:20:04.666379: Current learning rate: 0.00383 
2023-10-26 16:20:08.851870: train_loss -0.8387 
2023-10-26 16:20:08.852280: val_loss -0.7981 
2023-10-26 16:20:08.852556: Pseudo dice [0.8636, 0.9136, 0.9672, 0.5417, 0.9345] 
2023-10-26 16:20:08.852794: Epoch time: 4.19 s 
2023-10-26 16:20:09.952775:  
2023-10-26 16:20:09.953111: Epoch 657 
2023-10-26 16:20:09.953358: Current learning rate: 0.00382 
2023-10-26 16:20:14.107603: train_loss -0.8648 
2023-10-26 16:20:14.107963: val_loss -0.7804 
2023-10-26 16:20:14.108220: Pseudo dice [0.864, 0.9105, 0.9665, 0.5416, 0.9298] 
2023-10-26 16:20:14.108445: Epoch time: 4.16 s 
2023-10-26 16:20:15.207983:  
2023-10-26 16:20:15.208277: Epoch 658 
2023-10-26 16:20:15.208533: Current learning rate: 0.00381 
2023-10-26 16:20:19.369335: train_loss -0.8583 
2023-10-26 16:20:19.369733: val_loss -0.8028 
2023-10-26 16:20:19.370013: Pseudo dice [0.8624, 0.9171, 0.9664, 0.4228, 0.9353] 
2023-10-26 16:20:19.370256: Epoch time: 4.16 s 
2023-10-26 16:20:20.686368:  
2023-10-26 16:20:20.686690: Epoch 659 
2023-10-26 16:20:20.686951: Current learning rate: 0.0038 
2023-10-26 16:20:24.831937: train_loss -0.8606 
2023-10-26 16:20:24.832354: val_loss -0.7998 
2023-10-26 16:20:24.832674: Pseudo dice [0.8702, 0.9159, 0.966, 0.4613, 0.9326] 
2023-10-26 16:20:24.832928: Epoch time: 4.15 s 
2023-10-26 16:20:25.986298:  
2023-10-26 16:20:25.986607: Epoch 660 
2023-10-26 16:20:25.986854: Current learning rate: 0.00379 
2023-10-26 16:20:30.066563: train_loss -0.8625 
2023-10-26 16:20:30.066958: val_loss -0.7978 
2023-10-26 16:20:30.067229: Pseudo dice [0.8716, 0.9194, 0.9666, 0.4441, 0.9301] 
2023-10-26 16:20:30.067467: Epoch time: 4.08 s 
2023-10-26 16:20:31.176185:  
2023-10-26 16:20:31.176518: Epoch 661 
2023-10-26 16:20:31.176766: Current learning rate: 0.00378 
2023-10-26 16:20:35.321936: train_loss -0.8459 
2023-10-26 16:20:35.322331: val_loss -0.7797 
2023-10-26 16:20:35.322596: Pseudo dice [0.8685, 0.9125, 0.9675, 0.4047, 0.931] 
2023-10-26 16:20:35.322837: Epoch time: 4.15 s 
2023-10-26 16:20:36.429788:  
2023-10-26 16:20:36.430135: Epoch 662 
2023-10-26 16:20:36.430389: Current learning rate: 0.00377 
2023-10-26 16:20:40.626578: train_loss -0.8553 
2023-10-26 16:20:40.627281: val_loss -0.7999 
2023-10-26 16:20:40.627785: Pseudo dice [0.8726, 0.9158, 0.9656, 0.6017, 0.9381] 
2023-10-26 16:20:40.628102: Epoch time: 4.2 s 
2023-10-26 16:20:41.738267:  
2023-10-26 16:20:41.738563: Epoch 663 
2023-10-26 16:20:41.738803: Current learning rate: 0.00376 
2023-10-26 16:20:45.872277: train_loss -0.8584 
2023-10-26 16:20:45.872650: val_loss -0.8102 
2023-10-26 16:20:45.872919: Pseudo dice [0.8734, 0.9226, 0.9647, 0.6271, 0.9363] 
2023-10-26 16:20:45.873154: Epoch time: 4.13 s 
2023-10-26 16:20:46.980516:  
2023-10-26 16:20:46.980819: Epoch 664 
2023-10-26 16:20:46.981063: Current learning rate: 0.00375 
2023-10-26 16:20:50.986567: train_loss -0.852 
2023-10-26 16:20:50.987010: val_loss -0.8047 
2023-10-26 16:20:50.987286: Pseudo dice [0.8712, 0.9132, 0.9628, 0.4154, 0.9263] 
2023-10-26 16:20:50.987529: Epoch time: 4.01 s 
2023-10-26 16:20:52.267672:  
2023-10-26 16:20:52.268003: Epoch 665 
2023-10-26 16:20:52.268249: Current learning rate: 0.00374 
2023-10-26 16:20:56.511337: train_loss -0.8554 
2023-10-26 16:20:56.511716: val_loss -0.7929 
2023-10-26 16:20:56.512421: Pseudo dice [0.8673, 0.9166, 0.9683, 0.4873, 0.9329] 
2023-10-26 16:20:56.512850: Epoch time: 4.24 s 
2023-10-26 16:20:57.616766:  
2023-10-26 16:20:57.617135: Epoch 666 
2023-10-26 16:20:57.617387: Current learning rate: 0.00373 
2023-10-26 16:21:01.921264: train_loss -0.8597 
2023-10-26 16:21:01.921626: val_loss -0.7988 
2023-10-26 16:21:01.921892: Pseudo dice [0.8677, 0.9157, 0.9662, 0.5051, 0.9354] 
2023-10-26 16:21:01.922126: Epoch time: 4.31 s 
2023-10-26 16:21:03.074454:  
2023-10-26 16:21:03.074755: Epoch 667 
2023-10-26 16:21:03.075010: Current learning rate: 0.00372 
2023-10-26 16:21:07.257757: train_loss -0.8584 
2023-10-26 16:21:07.258141: val_loss -0.7656 
2023-10-26 16:21:07.258404: Pseudo dice [0.8687, 0.9172, 0.9661, 0.5721, 0.9307] 
2023-10-26 16:21:07.258658: Epoch time: 4.18 s 
2023-10-26 16:21:08.398345:  
2023-10-26 16:21:08.398633: Epoch 668 
2023-10-26 16:21:08.398880: Current learning rate: 0.00371 
2023-10-26 16:21:12.537332: train_loss -0.8596 
2023-10-26 16:21:12.537716: val_loss -0.8056 
2023-10-26 16:21:12.538003: Pseudo dice [0.869, 0.9161, 0.9683, 0.6533, 0.9401] 
2023-10-26 16:21:12.538239: Epoch time: 4.14 s 
2023-10-26 16:21:13.656680:  
2023-10-26 16:21:13.657009: Epoch 669 
2023-10-26 16:21:13.657280: Current learning rate: 0.0037 
2023-10-26 16:21:17.769318: train_loss -0.8612 
2023-10-26 16:21:17.769678: val_loss -0.795 
2023-10-26 16:21:17.770011: Pseudo dice [0.8713, 0.9201, 0.9665, 0.6145, 0.9384] 
2023-10-26 16:21:17.770397: Epoch time: 4.11 s 
2023-10-26 16:21:18.888010:  
2023-10-26 16:21:18.888322: Epoch 670 
2023-10-26 16:21:18.888575: Current learning rate: 0.00369 
2023-10-26 16:21:23.125916: train_loss -0.8492 
2023-10-26 16:21:23.126298: val_loss -0.7946 
2023-10-26 16:21:23.126562: Pseudo dice [0.8699, 0.9089, 0.9665, 0.6759, 0.9305] 
2023-10-26 16:21:23.126795: Epoch time: 4.24 s 
2023-10-26 16:21:24.318860:  
2023-10-26 16:21:24.319170: Epoch 671 
2023-10-26 16:21:24.319424: Current learning rate: 0.00368 
2023-10-26 16:21:28.474906: train_loss -0.8594 
2023-10-26 16:21:28.475264: val_loss -0.7922 
2023-10-26 16:21:28.475524: Pseudo dice [0.8647, 0.9192, 0.9663, 0.5822, 0.9321] 
2023-10-26 16:21:28.475748: Epoch time: 4.16 s 
2023-10-26 16:21:29.868451:  
2023-10-26 16:21:29.868770: Epoch 672 
2023-10-26 16:21:29.869030: Current learning rate: 0.00367 
2023-10-26 16:21:33.958052: train_loss -0.8624 
2023-10-26 16:21:33.958459: val_loss -0.8028 
2023-10-26 16:21:33.958730: Pseudo dice [0.8709, 0.9176, 0.9659, 0.4916, 0.9361] 
2023-10-26 16:21:33.958987: Epoch time: 4.09 s 
2023-10-26 16:21:35.096473:  
2023-10-26 16:21:35.096762: Epoch 673 
2023-10-26 16:21:35.097023: Current learning rate: 0.00366 
2023-10-26 16:21:39.227767: train_loss -0.8538 
2023-10-26 16:21:39.228127: val_loss -0.8023 
2023-10-26 16:21:39.228403: Pseudo dice [0.8664, 0.904, 0.9641, 0.7215, 0.9332] 
2023-10-26 16:21:39.228640: Epoch time: 4.13 s 
2023-10-26 16:21:40.376154:  
2023-10-26 16:21:40.376448: Epoch 674 
2023-10-26 16:21:40.376703: Current learning rate: 0.00365 
2023-10-26 16:21:44.444535: train_loss -0.8571 
2023-10-26 16:21:44.444963: val_loss -0.8013 
2023-10-26 16:21:44.445292: Pseudo dice [0.8703, 0.9179, 0.9664, 0.6795, 0.9393] 
2023-10-26 16:21:44.445565: Epoch time: 4.07 s 
2023-10-26 16:21:45.587244:  
2023-10-26 16:21:45.587565: Epoch 675 
2023-10-26 16:21:45.587812: Current learning rate: 0.00364 
2023-10-26 16:21:49.698983: train_loss -0.8563 
2023-10-26 16:21:49.699356: val_loss -0.8086 
2023-10-26 16:21:49.699633: Pseudo dice [0.8699, 0.9207, 0.9649, 0.0, 0.9349] 
2023-10-26 16:21:49.699869: Epoch time: 4.11 s 
2023-10-26 16:21:50.838410:  
2023-10-26 16:21:50.838719: Epoch 676 
2023-10-26 16:21:50.838971: Current learning rate: 0.00363 
2023-10-26 16:21:54.792679: train_loss -0.8438 
2023-10-26 16:21:54.793103: val_loss -0.8169 
2023-10-26 16:21:54.793569: Pseudo dice [0.8714, 0.9136, 0.9664, 0.7222, 0.9369] 
2023-10-26 16:21:54.793854: Epoch time: 3.95 s 
2023-10-26 16:21:55.929791:  
2023-10-26 16:21:55.930106: Epoch 677 
2023-10-26 16:21:55.930349: Current learning rate: 0.00362 
2023-10-26 16:21:59.896000: train_loss -0.8594 
2023-10-26 16:21:59.896453: val_loss -0.7957 
2023-10-26 16:21:59.896742: Pseudo dice [0.8697, 0.9115, 0.9652, 0.7005, 0.9356] 
2023-10-26 16:21:59.897004: Epoch time: 3.97 s 
2023-10-26 16:22:01.284997:  
2023-10-26 16:22:01.285304: Epoch 678 
2023-10-26 16:22:01.285563: Current learning rate: 0.00361 
2023-10-26 16:22:05.372216: train_loss -0.8652 
2023-10-26 16:22:05.372627: val_loss -0.7788 
2023-10-26 16:22:05.372918: Pseudo dice [0.8623, 0.9135, 0.9663, 0.6321, 0.9326] 
2023-10-26 16:22:05.373163: Epoch time: 4.09 s 
2023-10-26 16:22:06.516790:  
2023-10-26 16:22:06.517154: Epoch 679 
2023-10-26 16:22:06.517482: Current learning rate: 0.0036 
2023-10-26 16:22:10.689917: train_loss -0.8645 
2023-10-26 16:22:10.690287: val_loss -0.8207 
2023-10-26 16:22:10.690552: Pseudo dice [0.8722, 0.9169, 0.9652, 0.6843, 0.9396] 
2023-10-26 16:22:10.690779: Epoch time: 4.17 s 
2023-10-26 16:22:11.834022:  
2023-10-26 16:22:11.834339: Epoch 680 
2023-10-26 16:22:11.834589: Current learning rate: 0.00359 
2023-10-26 16:22:15.957066: train_loss -0.8532 
2023-10-26 16:22:15.957499: val_loss -0.8065 
2023-10-26 16:22:15.957783: Pseudo dice [0.8697, 0.9108, 0.9645, 0.6832, 0.9273] 
2023-10-26 16:22:15.958037: Epoch time: 4.12 s 
2023-10-26 16:22:17.108596:  
2023-10-26 16:22:17.108941: Epoch 681 
2023-10-26 16:22:17.109202: Current learning rate: 0.00358 
2023-10-26 16:22:21.181330: train_loss -0.8468 
2023-10-26 16:22:21.181699: val_loss -0.8232 
2023-10-26 16:22:21.181965: Pseudo dice [0.8761, 0.9233, 0.9635, 0.7352, 0.9364] 
2023-10-26 16:22:21.182205: Epoch time: 4.07 s 
2023-10-26 16:22:22.318096:  
2023-10-26 16:22:22.318411: Epoch 682 
2023-10-26 16:22:22.318665: Current learning rate: 0.00357 
2023-10-26 16:22:26.409353: train_loss -0.8528 
2023-10-26 16:22:26.409747: val_loss -0.7996 
2023-10-26 16:22:26.410012: Pseudo dice [0.8693, 0.9189, 0.9646, 0.7276, 0.9374] 
2023-10-26 16:22:26.410251: Epoch time: 4.09 s 
2023-10-26 16:22:27.573720:  
2023-10-26 16:22:27.574029: Epoch 683 
2023-10-26 16:22:27.574273: Current learning rate: 0.00356 
2023-10-26 16:22:31.566527: train_loss -0.8497 
2023-10-26 16:22:31.566891: val_loss -0.8361 
2023-10-26 16:22:31.567154: Pseudo dice [0.8722, 0.9166, 0.9661, 0.6195, 0.933] 
2023-10-26 16:22:31.567405: Epoch time: 3.99 s 
2023-10-26 16:22:32.850980:  
2023-10-26 16:22:32.851285: Epoch 684 
2023-10-26 16:22:32.851535: Current learning rate: 0.00355 
2023-10-26 16:22:36.929663: train_loss -0.8552 
2023-10-26 16:22:36.930160: val_loss -0.7825 
2023-10-26 16:22:36.930541: Pseudo dice [0.8705, 0.9161, 0.9656, 0.6594, 0.934] 
2023-10-26 16:22:36.930958: Epoch time: 4.08 s 
2023-10-26 16:22:38.072115:  
2023-10-26 16:22:38.072444: Epoch 685 
2023-10-26 16:22:38.072697: Current learning rate: 0.00354 
2023-10-26 16:22:42.123419: train_loss -0.842 
2023-10-26 16:22:42.123790: val_loss -0.8066 
2023-10-26 16:22:42.124067: Pseudo dice [0.8694, 0.9119, 0.9651, 0.773, 0.9338] 
2023-10-26 16:22:42.124403: Epoch time: 4.05 s 
2023-10-26 16:22:43.260721:  
2023-10-26 16:22:43.261019: Epoch 686 
2023-10-26 16:22:43.261281: Current learning rate: 0.00353 
2023-10-26 16:22:47.274261: train_loss -0.8455 
2023-10-26 16:22:47.274628: val_loss -0.7849 
2023-10-26 16:22:47.274896: Pseudo dice [0.869, 0.9145, 0.9648, 0.7275, 0.9377] 
2023-10-26 16:22:47.275129: Epoch time: 4.01 s 
2023-10-26 16:22:48.393702:  
2023-10-26 16:22:48.394011: Epoch 687 
2023-10-26 16:22:48.394274: Current learning rate: 0.00352 
2023-10-26 16:22:52.334691: train_loss -0.8585 
2023-10-26 16:22:52.335095: val_loss -0.7999 
2023-10-26 16:22:52.335368: Pseudo dice [0.8756, 0.915, 0.9653, 0.7458, 0.9355] 
2023-10-26 16:22:52.335608: Epoch time: 3.94 s 
2023-10-26 16:22:53.451721:  
2023-10-26 16:22:53.452043: Epoch 688 
2023-10-26 16:22:53.452289: Current learning rate: 0.00351 
2023-10-26 16:22:57.525260: train_loss -0.8651 
2023-10-26 16:22:57.525619: val_loss -0.8003 
2023-10-26 16:22:57.525883: Pseudo dice [0.8614, 0.9118, 0.9664, 0.5619, 0.9316] 
2023-10-26 16:22:57.526127: Epoch time: 4.07 s 
2023-10-26 16:22:58.647291:  
2023-10-26 16:22:58.660393: Epoch 689 
2023-10-26 16:22:58.660764: Current learning rate: 0.0035 
2023-10-26 16:23:02.771577: train_loss -0.856 
2023-10-26 16:23:02.771958: val_loss -0.8234 
2023-10-26 16:23:02.772221: Pseudo dice [0.8625, 0.9147, 0.9657, 0.7466, 0.9313] 
2023-10-26 16:23:02.772455: Epoch time: 4.12 s 
2023-10-26 16:23:04.100927:  
2023-10-26 16:23:04.101246: Epoch 690 
2023-10-26 16:23:04.101549: Current learning rate: 0.00349 
2023-10-26 16:23:08.206177: train_loss -0.8606 
2023-10-26 16:23:08.206587: val_loss -0.8182 
2023-10-26 16:23:08.206861: Pseudo dice [0.8673, 0.9173, 0.9672, 0.679, 0.936] 
2023-10-26 16:23:08.207112: Epoch time: 4.11 s 
2023-10-26 16:23:09.365793:  
2023-10-26 16:23:09.366147: Epoch 691 
2023-10-26 16:23:09.366462: Current learning rate: 0.00348 
2023-10-26 16:23:13.477769: train_loss -0.8499 
2023-10-26 16:23:13.478166: val_loss -0.8186 
2023-10-26 16:23:13.478441: Pseudo dice [0.859, 0.9139, 0.9623, 0.5354, 0.9297] 
2023-10-26 16:23:13.478686: Epoch time: 4.11 s 
2023-10-26 16:23:14.651508:  
2023-10-26 16:23:14.651808: Epoch 692 
2023-10-26 16:23:14.652071: Current learning rate: 0.00346 
2023-10-26 16:23:18.815316: train_loss -0.8361 
2023-10-26 16:23:18.815850: val_loss -0.8074 
2023-10-26 16:23:18.816121: Pseudo dice [0.865, 0.9137, 0.9632, 0.7727, 0.9328] 
2023-10-26 16:23:18.816354: Epoch time: 4.16 s 
2023-10-26 16:23:20.043068:  
2023-10-26 16:23:20.043407: Epoch 693 
2023-10-26 16:23:20.043669: Current learning rate: 0.00345 
2023-10-26 16:23:24.318087: train_loss -0.8364 
2023-10-26 16:23:24.319503: val_loss -0.8026 
2023-10-26 16:23:24.320066: Pseudo dice [0.8796, 0.9177, 0.967, 0.6064, 0.9268] 
2023-10-26 16:23:24.320513: Epoch time: 4.28 s 
2023-10-26 16:23:25.500576:  
2023-10-26 16:23:25.501021: Epoch 694 
2023-10-26 16:23:25.501444: Current learning rate: 0.00344 
2023-10-26 16:23:29.629334: train_loss -0.8505 
2023-10-26 16:23:29.630356: val_loss -0.814 
2023-10-26 16:23:29.630641: Pseudo dice [0.8772, 0.9151, 0.9672, 0.8116, 0.9379] 
2023-10-26 16:23:29.630925: Epoch time: 4.13 s 
2023-10-26 16:23:30.832187:  
2023-10-26 16:23:30.832495: Epoch 695 
2023-10-26 16:23:30.832750: Current learning rate: 0.00343 
2023-10-26 16:23:34.771398: train_loss -0.8536 
2023-10-26 16:23:34.771928: val_loss -0.7791 
2023-10-26 16:23:34.772207: Pseudo dice [0.8676, 0.9142, 0.9665, 0.5502, 0.9359] 
2023-10-26 16:23:34.772473: Epoch time: 3.94 s 
2023-10-26 16:23:35.934211:  
2023-10-26 16:23:35.934505: Epoch 696 
2023-10-26 16:23:35.934751: Current learning rate: 0.00342 
2023-10-26 16:23:40.020401: train_loss -0.8466 
2023-10-26 16:23:40.020844: val_loss -0.7887 
2023-10-26 16:23:40.021113: Pseudo dice [0.8685, 0.9155, 0.9676, 0.5012, 0.9363] 
2023-10-26 16:23:40.021383: Epoch time: 4.09 s 
2023-10-26 16:23:41.340114:  
2023-10-26 16:23:41.340453: Epoch 697 
2023-10-26 16:23:41.340712: Current learning rate: 0.00341 
2023-10-26 16:23:45.461222: train_loss -0.864 
2023-10-26 16:23:45.461620: val_loss -0.8025 
2023-10-26 16:23:45.461895: Pseudo dice [0.8747, 0.921, 0.9671, 0.59, 0.9368] 
2023-10-26 16:23:45.462134: Epoch time: 4.12 s 
2023-10-26 16:23:46.665845:  
2023-10-26 16:23:46.666171: Epoch 698 
2023-10-26 16:23:46.666435: Current learning rate: 0.0034 
2023-10-26 16:23:50.822258: train_loss -0.8596 
2023-10-26 16:23:50.822660: val_loss -0.8081 
2023-10-26 16:23:50.822960: Pseudo dice [0.8714, 0.919, 0.9654, 0.5942, 0.932] 
2023-10-26 16:23:50.823222: Epoch time: 4.16 s 
2023-10-26 16:23:51.973138:  
2023-10-26 16:23:51.973468: Epoch 699 
2023-10-26 16:23:51.973718: Current learning rate: 0.00339 
2023-10-26 16:23:56.041363: train_loss -0.8551 
2023-10-26 16:23:56.041756: val_loss -0.8023 
2023-10-26 16:23:56.042034: Pseudo dice [0.8692, 0.9212, 0.9682, 0.611, 0.9364] 
2023-10-26 16:23:56.042288: Epoch time: 4.07 s 
2023-10-26 16:23:57.274817:  
2023-10-26 16:23:57.275139: Epoch 700 
2023-10-26 16:23:57.275397: Current learning rate: 0.00338 
2023-10-26 16:24:01.088703: train_loss -0.8516 
2023-10-26 16:24:01.089070: val_loss -0.7924 
2023-10-26 16:24:01.089333: Pseudo dice [0.866, 0.9148, 0.9681, 0.5821, 0.9351] 
2023-10-26 16:24:01.089562: Epoch time: 3.81 s 
2023-10-26 16:24:02.217638:  
2023-10-26 16:24:02.217930: Epoch 701 
2023-10-26 16:24:02.218301: Current learning rate: 0.00337 
2023-10-26 16:24:06.228856: train_loss -0.861 
2023-10-26 16:24:06.229228: val_loss -0.7843 
2023-10-26 16:24:06.229489: Pseudo dice [0.8689, 0.9144, 0.9674, 0.6304, 0.9363] 
2023-10-26 16:24:06.229719: Epoch time: 4.01 s 
2023-10-26 16:24:07.378417:  
2023-10-26 16:24:07.378718: Epoch 702 
2023-10-26 16:24:07.378973: Current learning rate: 0.00336 
2023-10-26 16:24:11.507110: train_loss -0.8585 
2023-10-26 16:24:11.507501: val_loss -0.8033 
2023-10-26 16:24:11.507780: Pseudo dice [0.8655, 0.9216, 0.9655, 0.608, 0.9317] 
2023-10-26 16:24:11.508029: Epoch time: 4.13 s 
2023-10-26 16:24:12.856525:  
2023-10-26 16:24:12.856961: Epoch 703 
2023-10-26 16:24:12.857236: Current learning rate: 0.00335 
2023-10-26 16:24:17.034514: train_loss -0.8507 
2023-10-26 16:24:17.034928: val_loss -0.7897 
2023-10-26 16:24:17.035195: Pseudo dice [0.8686, 0.9157, 0.965, 0.678, 0.9342] 
2023-10-26 16:24:17.035439: Epoch time: 4.18 s 
2023-10-26 16:24:18.197019:  
2023-10-26 16:24:18.197332: Epoch 704 
2023-10-26 16:24:18.197582: Current learning rate: 0.00334 
2023-10-26 16:24:22.301062: train_loss -0.8651 
2023-10-26 16:24:22.301470: val_loss -0.7941 
2023-10-26 16:24:22.301726: Pseudo dice [0.8654, 0.9107, 0.9662, 0.6627, 0.9357] 
2023-10-26 16:24:22.301971: Epoch time: 4.1 s 
2023-10-26 16:24:23.448418:  
2023-10-26 16:24:23.448763: Epoch 705 
2023-10-26 16:24:23.449028: Current learning rate: 0.00333 
2023-10-26 16:24:27.604462: train_loss -0.8655 
2023-10-26 16:24:27.604880: val_loss -0.7893 
2023-10-26 16:24:27.605150: Pseudo dice [0.867, 0.919, 0.9661, 0.5732, 0.936] 
2023-10-26 16:24:27.605407: Epoch time: 4.16 s 
2023-10-26 16:24:28.735823:  
2023-10-26 16:24:28.736143: Epoch 706 
2023-10-26 16:24:28.736389: Current learning rate: 0.00332 
2023-10-26 16:24:32.887065: train_loss -0.8641 
2023-10-26 16:24:32.887476: val_loss -0.7492 
2023-10-26 16:24:32.887737: Pseudo dice [0.8684, 0.9162, 0.9667, 0.3777, 0.9341] 
2023-10-26 16:24:32.887988: Epoch time: 4.15 s 
2023-10-26 16:24:34.063293:  
2023-10-26 16:24:34.063599: Epoch 707 
2023-10-26 16:24:34.063842: Current learning rate: 0.00331 
2023-10-26 16:24:38.132824: train_loss -0.8593 
2023-10-26 16:24:38.133251: val_loss -0.791 
2023-10-26 16:24:38.133673: Pseudo dice [0.8662, 0.9148, 0.9643, 0.5809, 0.9342] 
2023-10-26 16:24:38.133988: Epoch time: 4.07 s 
2023-10-26 16:24:39.257917:  
2023-10-26 16:24:39.258254: Epoch 708 
2023-10-26 16:24:39.258523: Current learning rate: 0.0033 
2023-10-26 16:24:43.425580: train_loss -0.8565 
2023-10-26 16:24:43.426122: val_loss -0.7751 
2023-10-26 16:24:43.426428: Pseudo dice [0.8665, 0.913, 0.9651, 0.5028, 0.9341] 
2023-10-26 16:24:43.426676: Epoch time: 4.17 s 
2023-10-26 16:24:44.711463:  
2023-10-26 16:24:44.711773: Epoch 709 
2023-10-26 16:24:44.712027: Current learning rate: 0.00329 
2023-10-26 16:24:48.745489: train_loss -0.8593 
2023-10-26 16:24:48.745889: val_loss -0.7851 
2023-10-26 16:24:48.746146: Pseudo dice [0.8628, 0.916, 0.9671, 0.4732, 0.9293] 
2023-10-26 16:24:48.746378: Epoch time: 4.03 s 
2023-10-26 16:24:49.873467:  
2023-10-26 16:24:49.873774: Epoch 710 
2023-10-26 16:24:49.874035: Current learning rate: 0.00328 
2023-10-26 16:24:54.125623: train_loss -0.8638 
2023-10-26 16:24:54.126012: val_loss -0.8162 
2023-10-26 16:24:54.126280: Pseudo dice [0.8749, 0.9189, 0.9652, 0.6359, 0.9344] 
2023-10-26 16:24:54.126522: Epoch time: 4.25 s 
2023-10-26 16:24:55.245071:  
2023-10-26 16:24:55.245371: Epoch 711 
2023-10-26 16:24:55.245624: Current learning rate: 0.00327 
2023-10-26 16:24:59.467209: train_loss -0.8606 
2023-10-26 16:24:59.467563: val_loss -0.8041 
2023-10-26 16:24:59.467840: Pseudo dice [0.8692, 0.9177, 0.9661, 0.6951, 0.9345] 
2023-10-26 16:24:59.468390: Epoch time: 4.22 s 
2023-10-26 16:25:00.586611:  
2023-10-26 16:25:00.587171: Epoch 712 
2023-10-26 16:25:00.587455: Current learning rate: 0.00326 
2023-10-26 16:25:04.769071: train_loss -0.8583 
2023-10-26 16:25:04.769456: val_loss -0.7724 
2023-10-26 16:25:04.769721: Pseudo dice [0.8645, 0.9107, 0.9653, 0.5723, 0.9291] 
2023-10-26 16:25:04.770174: Epoch time: 4.18 s 
2023-10-26 16:25:05.893179:  
2023-10-26 16:25:05.893510: Epoch 713 
2023-10-26 16:25:05.893781: Current learning rate: 0.00325 
2023-10-26 16:25:10.029689: train_loss -0.8531 
2023-10-26 16:25:10.030209: val_loss -0.8081 
2023-10-26 16:25:10.030538: Pseudo dice [0.87, 0.9126, 0.9655, 0.4689, 0.9354] 
2023-10-26 16:25:10.030780: Epoch time: 4.14 s 
2023-10-26 16:25:11.147756:  
2023-10-26 16:25:11.148076: Epoch 714 
2023-10-26 16:25:11.148318: Current learning rate: 0.00324 
2023-10-26 16:25:15.385279: train_loss -0.8653 
2023-10-26 16:25:15.385655: val_loss -0.7933 
2023-10-26 16:25:15.385927: Pseudo dice [0.8572, 0.9144, 0.9667, 0.4919, 0.9284] 
2023-10-26 16:25:15.386161: Epoch time: 4.24 s 
2023-10-26 16:25:16.676550:  
2023-10-26 16:25:16.676856: Epoch 715 
2023-10-26 16:25:16.677116: Current learning rate: 0.00323 
2023-10-26 16:25:20.843446: train_loss -0.8641 
2023-10-26 16:25:20.843806: val_loss -0.7983 
2023-10-26 16:25:20.844080: Pseudo dice [0.8693, 0.9161, 0.9652, 0.4992, 0.9364] 
2023-10-26 16:25:20.844325: Epoch time: 4.17 s 
2023-10-26 16:25:21.998783:  
2023-10-26 16:25:21.999095: Epoch 716 
2023-10-26 16:25:21.999344: Current learning rate: 0.00322 
2023-10-26 16:25:25.940189: train_loss -0.8711 
2023-10-26 16:25:25.940602: val_loss -0.7915 
2023-10-26 16:25:25.940865: Pseudo dice [0.8649, 0.9127, 0.9669, 0.4583, 0.9372] 
2023-10-26 16:25:25.941106: Epoch time: 3.94 s 
2023-10-26 16:25:27.069918:  
2023-10-26 16:25:27.070230: Epoch 717 
2023-10-26 16:25:27.070488: Current learning rate: 0.00321 
2023-10-26 16:25:31.067456: train_loss -0.8598 
2023-10-26 16:25:31.067819: val_loss -0.7735 
2023-10-26 16:25:31.068090: Pseudo dice [0.8647, 0.912, 0.9655, 0.5931, 0.934] 
2023-10-26 16:25:31.068322: Epoch time: 4.0 s 
2023-10-26 16:25:32.192019:  
2023-10-26 16:25:32.192334: Epoch 718 
2023-10-26 16:25:32.192581: Current learning rate: 0.0032 
2023-10-26 16:25:36.227631: train_loss -0.8634 
2023-10-26 16:25:36.228091: val_loss -0.7866 
2023-10-26 16:25:36.228363: Pseudo dice [0.8688, 0.9105, 0.9673, 0.5, 0.9335] 
2023-10-26 16:25:36.228607: Epoch time: 4.04 s 
2023-10-26 16:25:37.367142:  
2023-10-26 16:25:37.367453: Epoch 719 
2023-10-26 16:25:37.367710: Current learning rate: 0.00319 
2023-10-26 16:25:41.575311: train_loss -0.8668 
2023-10-26 16:25:41.575724: val_loss -0.7812 
2023-10-26 16:25:41.576016: Pseudo dice [0.8639, 0.9109, 0.9659, 0.4869, 0.9335] 
2023-10-26 16:25:41.576257: Epoch time: 4.21 s 
2023-10-26 16:25:42.703332:  
2023-10-26 16:25:42.703636: Epoch 720 
2023-10-26 16:25:42.703887: Current learning rate: 0.00318 
2023-10-26 16:25:46.802411: train_loss -0.8611 
2023-10-26 16:25:46.802796: val_loss -0.7889 
2023-10-26 16:25:46.803070: Pseudo dice [0.8704, 0.9136, 0.9659, 0.5467, 0.9385] 
2023-10-26 16:25:46.803303: Epoch time: 4.1 s 
2023-10-26 16:25:48.085390:  
2023-10-26 16:25:48.085724: Epoch 721 
2023-10-26 16:25:48.085987: Current learning rate: 0.00317 
2023-10-26 16:25:52.264943: train_loss -0.8593 
2023-10-26 16:25:52.265300: val_loss -0.7793 
2023-10-26 16:25:52.265598: Pseudo dice [0.8676, 0.9098, 0.9622, 0.5759, 0.9323] 
2023-10-26 16:25:52.265834: Epoch time: 4.18 s 
2023-10-26 16:25:53.390145:  
2023-10-26 16:25:53.390466: Epoch 722 
2023-10-26 16:25:53.390723: Current learning rate: 0.00316 
2023-10-26 16:25:57.605910: train_loss -0.861 
2023-10-26 16:25:57.606300: val_loss -0.8012 
2023-10-26 16:25:57.606565: Pseudo dice [0.8708, 0.9186, 0.9677, 0.465, 0.9359] 
2023-10-26 16:25:57.606802: Epoch time: 4.22 s 
2023-10-26 16:25:58.736367:  
2023-10-26 16:25:58.736678: Epoch 723 
2023-10-26 16:25:58.736945: Current learning rate: 0.00315 
2023-10-26 16:26:02.946005: train_loss -0.8498 
2023-10-26 16:26:02.946384: val_loss -0.7704 
2023-10-26 16:26:02.946653: Pseudo dice [0.8697, 0.9143, 0.966, 0.574, 0.9363] 
2023-10-26 16:26:02.946888: Epoch time: 4.21 s 
2023-10-26 16:26:04.097467:  
2023-10-26 16:26:04.097947: Epoch 724 
2023-10-26 16:26:04.098199: Current learning rate: 0.00314 
2023-10-26 16:26:08.329142: train_loss -0.8538 
2023-10-26 16:26:08.329538: val_loss -0.7927 
2023-10-26 16:26:08.329814: Pseudo dice [0.8681, 0.9171, 0.9668, 0.6573, 0.9343] 
2023-10-26 16:26:08.330284: Epoch time: 4.23 s 
2023-10-26 16:26:09.514476:  
2023-10-26 16:26:09.514807: Epoch 725 
2023-10-26 16:26:09.515069: Current learning rate: 0.00313 
2023-10-26 16:26:13.655548: train_loss -0.8635 
2023-10-26 16:26:13.655999: val_loss -0.7841 
2023-10-26 16:26:13.656278: Pseudo dice [0.8699, 0.9107, 0.9645, 0.6815, 0.9329] 
2023-10-26 16:26:13.656528: Epoch time: 4.14 s 
2023-10-26 16:26:14.798224:  
2023-10-26 16:26:14.798619: Epoch 726 
2023-10-26 16:26:14.798958: Current learning rate: 0.00312 
2023-10-26 16:26:18.954963: train_loss -0.861 
2023-10-26 16:26:18.955383: val_loss -0.8219 
2023-10-26 16:26:18.955755: Pseudo dice [0.8696, 0.9129, 0.9626, 0.6563, 0.9343] 
2023-10-26 16:26:18.956329: Epoch time: 4.16 s 
2023-10-26 16:26:20.083738:  
2023-10-26 16:26:20.084076: Epoch 727 
2023-10-26 16:26:20.084358: Current learning rate: 0.00311 
2023-10-26 16:26:24.141799: train_loss -0.8463 
2023-10-26 16:26:24.142201: val_loss -0.789 
2023-10-26 16:26:24.142461: Pseudo dice [0.8683, 0.9058, 0.9629, 0.6497, 0.9318] 
2023-10-26 16:26:24.142689: Epoch time: 4.06 s 
2023-10-26 16:26:25.425667:  
2023-10-26 16:26:25.426004: Epoch 728 
2023-10-26 16:26:25.426268: Current learning rate: 0.0031 
2023-10-26 16:26:29.502517: train_loss -0.8595 
2023-10-26 16:26:29.502937: val_loss -0.8222 
2023-10-26 16:26:29.503207: Pseudo dice [0.8721, 0.9135, 0.9649, 0.6194, 0.9329] 
2023-10-26 16:26:29.503453: Epoch time: 4.08 s 
2023-10-26 16:26:30.678456:  
2023-10-26 16:26:30.678777: Epoch 729 
2023-10-26 16:26:30.679035: Current learning rate: 0.00309 
2023-10-26 16:26:34.844328: train_loss -0.865 
2023-10-26 16:26:34.844730: val_loss -0.8015 
2023-10-26 16:26:34.845015: Pseudo dice [0.8721, 0.9147, 0.9669, 0.693, 0.935] 
2023-10-26 16:26:34.845257: Epoch time: 4.17 s 
2023-10-26 16:26:35.979550:  
2023-10-26 16:26:35.979861: Epoch 730 
2023-10-26 16:26:35.980123: Current learning rate: 0.00308 
2023-10-26 16:26:40.175121: train_loss -0.8617 
2023-10-26 16:26:40.175472: val_loss -0.7798 
2023-10-26 16:26:40.175723: Pseudo dice [0.8665, 0.9077, 0.9659, 0.5914, 0.9382] 
2023-10-26 16:26:40.175958: Epoch time: 4.2 s 
2023-10-26 16:26:41.307803:  
2023-10-26 16:26:41.308132: Epoch 731 
2023-10-26 16:26:41.308386: Current learning rate: 0.00307 
2023-10-26 16:26:45.483989: train_loss -0.8646 
2023-10-26 16:26:45.484385: val_loss -0.7964 
2023-10-26 16:26:45.484653: Pseudo dice [0.871, 0.9088, 0.9653, 0.5092, 0.9358] 
2023-10-26 16:26:45.484890: Epoch time: 4.18 s 
2023-10-26 16:26:46.608708:  
2023-10-26 16:26:46.609023: Epoch 732 
2023-10-26 16:26:46.609271: Current learning rate: 0.00306 
2023-10-26 16:26:50.791838: train_loss -0.8578 
2023-10-26 16:26:50.792261: val_loss -0.8068 
2023-10-26 16:26:50.792537: Pseudo dice [0.8703, 0.9153, 0.9631, 0.5504, 0.9331] 
2023-10-26 16:26:50.792778: Epoch time: 4.18 s 
2023-10-26 16:26:51.925220:  
2023-10-26 16:26:51.925528: Epoch 733 
2023-10-26 16:26:51.925778: Current learning rate: 0.00305 
2023-10-26 16:26:56.143608: train_loss -0.8598 
2023-10-26 16:26:56.144090: val_loss -0.8123 
2023-10-26 16:26:56.144403: Pseudo dice [0.8768, 0.9212, 0.9674, 0.6631, 0.9348] 
2023-10-26 16:26:56.144763: Epoch time: 4.22 s 
2023-10-26 16:26:57.430895:  
2023-10-26 16:26:57.431218: Epoch 734 
2023-10-26 16:26:57.431482: Current learning rate: 0.00304 
2023-10-26 16:27:01.598406: train_loss -0.8556 
2023-10-26 16:27:01.598867: val_loss -0.7924 
2023-10-26 16:27:01.599262: Pseudo dice [0.8703, 0.9135, 0.9685, 0.4251, 0.9324] 
2023-10-26 16:27:01.599531: Epoch time: 4.17 s 
2023-10-26 16:27:02.733556:  
2023-10-26 16:27:02.733933: Epoch 735 
2023-10-26 16:27:02.734246: Current learning rate: 0.00303 
2023-10-26 16:27:06.943030: train_loss -0.8542 
2023-10-26 16:27:06.943431: val_loss -0.8028 
2023-10-26 16:27:06.943707: Pseudo dice [0.8685, 0.9161, 0.966, 0.5078, 0.9342] 
2023-10-26 16:27:06.943972: Epoch time: 4.21 s 
2023-10-26 16:27:08.143067:  
2023-10-26 16:27:08.143421: Epoch 736 
2023-10-26 16:27:08.143685: Current learning rate: 0.00302 
2023-10-26 16:27:12.396847: train_loss -0.8634 
2023-10-26 16:27:12.397254: val_loss -0.8278 
2023-10-26 16:27:12.397526: Pseudo dice [0.8705, 0.9112, 0.9669, 0.6335, 0.9349] 
2023-10-26 16:27:12.397761: Epoch time: 4.25 s 
2023-10-26 16:27:13.525680:  
2023-10-26 16:27:13.526005: Epoch 737 
2023-10-26 16:27:13.526263: Current learning rate: 0.00301 
2023-10-26 16:27:17.809272: train_loss -0.859 
2023-10-26 16:27:17.809751: val_loss -0.8281 
2023-10-26 16:27:17.810102: Pseudo dice [0.8629, 0.9124, 0.9645, 0.5705, 0.9347] 
2023-10-26 16:27:17.810342: Epoch time: 4.28 s 
2023-10-26 16:27:18.953152:  
2023-10-26 16:27:18.953463: Epoch 738 
2023-10-26 16:27:18.953710: Current learning rate: 0.003 
2023-10-26 16:27:23.092463: train_loss -0.8514 
2023-10-26 16:27:23.092856: val_loss -0.8243 
2023-10-26 16:27:23.093154: Pseudo dice [0.8654, 0.9117, 0.9671, 0.6491, 0.936] 
2023-10-26 16:27:23.093404: Epoch time: 4.14 s 
2023-10-26 16:27:24.218094:  
2023-10-26 16:27:24.218407: Epoch 739 
2023-10-26 16:27:24.218659: Current learning rate: 0.00299 
2023-10-26 16:27:28.370202: train_loss -0.8574 
2023-10-26 16:27:28.370618: val_loss -0.801 
2023-10-26 16:27:28.370904: Pseudo dice [0.8683, 0.9162, 0.9665, 0.6443, 0.9363] 
2023-10-26 16:27:28.371162: Epoch time: 4.15 s 
2023-10-26 16:27:29.653213:  
2023-10-26 16:27:29.653522: Epoch 740 
2023-10-26 16:27:29.653769: Current learning rate: 0.00297 
2023-10-26 16:27:33.777729: train_loss -0.8538 
2023-10-26 16:27:33.778320: val_loss -0.803 
2023-10-26 16:27:33.778674: Pseudo dice [0.8702, 0.9148, 0.9653, 0.6139, 0.9346] 
2023-10-26 16:27:33.778970: Epoch time: 4.13 s 
2023-10-26 16:27:34.931258:  
2023-10-26 16:27:34.931599: Epoch 741 
2023-10-26 16:27:34.931849: Current learning rate: 0.00296 
2023-10-26 16:27:39.069765: train_loss -0.8547 
2023-10-26 16:27:39.070245: val_loss -0.7954 
2023-10-26 16:27:39.070498: Pseudo dice [0.8706, 0.9161, 0.9659, 0.6472, 0.9376] 
2023-10-26 16:27:39.070725: Epoch time: 4.14 s 
2023-10-26 16:27:40.187348:  
2023-10-26 16:27:40.187642: Epoch 742 
2023-10-26 16:27:40.187891: Current learning rate: 0.00295 
2023-10-26 16:27:44.425726: train_loss -0.8554 
2023-10-26 16:27:44.426080: val_loss -0.7941 
2023-10-26 16:27:44.426340: Pseudo dice [0.8664, 0.9131, 0.9662, 0.5392, 0.933] 
2023-10-26 16:27:44.426573: Epoch time: 4.24 s 
2023-10-26 16:27:45.552938:  
2023-10-26 16:27:45.553274: Epoch 743 
2023-10-26 16:27:45.553554: Current learning rate: 0.00294 
2023-10-26 16:27:49.573036: train_loss -0.8619 
2023-10-26 16:27:49.573440: val_loss -0.7929 
2023-10-26 16:27:49.573709: Pseudo dice [0.8644, 0.917, 0.9676, 0.5119, 0.9315] 
2023-10-26 16:27:49.573949: Epoch time: 4.02 s 
2023-10-26 16:27:50.699628:  
2023-10-26 16:27:50.699927: Epoch 744 
2023-10-26 16:27:50.700184: Current learning rate: 0.00293 
2023-10-26 16:27:54.840898: train_loss -0.858 
2023-10-26 16:27:54.841787: val_loss -0.8045 
2023-10-26 16:27:54.842063: Pseudo dice [0.8669, 0.9169, 0.9678, 0.5782, 0.9309] 
2023-10-26 16:27:54.842307: Epoch time: 4.14 s 
2023-10-26 16:27:56.089280:  
2023-10-26 16:27:56.089638: Epoch 745 
2023-10-26 16:27:56.089936: Current learning rate: 0.00292 
2023-10-26 16:28:00.125316: train_loss -0.8694 
2023-10-26 16:28:00.125666: val_loss -0.7957 
2023-10-26 16:28:00.125932: Pseudo dice [0.8644, 0.9116, 0.9676, 0.4775, 0.9375] 
2023-10-26 16:28:00.126170: Epoch time: 4.04 s 
2023-10-26 16:28:01.456439:  
2023-10-26 16:28:01.456752: Epoch 746 
2023-10-26 16:28:01.457032: Current learning rate: 0.00291 
2023-10-26 16:28:05.624036: train_loss -0.8691 
2023-10-26 16:28:05.624424: val_loss -0.7911 
2023-10-26 16:28:05.624695: Pseudo dice [0.8677, 0.9162, 0.969, 0.5722, 0.9336] 
2023-10-26 16:28:05.624929: Epoch time: 4.17 s 
2023-10-26 16:28:06.757298:  
2023-10-26 16:28:06.757646: Epoch 747 
2023-10-26 16:28:06.757914: Current learning rate: 0.0029 
2023-10-26 16:28:10.964349: train_loss -0.8625 
2023-10-26 16:28:10.964725: val_loss -0.7869 
2023-10-26 16:28:10.964990: Pseudo dice [0.8681, 0.9074, 0.9675, 0.5445, 0.9314] 
2023-10-26 16:28:10.965220: Epoch time: 4.21 s 
2023-10-26 16:28:12.088973:  
2023-10-26 16:28:12.089284: Epoch 748 
2023-10-26 16:28:12.089551: Current learning rate: 0.00289 
2023-10-26 16:28:17.694547: train_loss -0.8634 
2023-10-26 16:28:17.695171: val_loss -0.7777 
2023-10-26 16:28:17.695622: Pseudo dice [0.8623, 0.9126, 0.9673, 0.5149, 0.9307] 
2023-10-26 16:28:17.695911: Epoch time: 5.61 s 
2023-10-26 16:28:18.887343:  
2023-10-26 16:28:18.887672: Epoch 749 
2023-10-26 16:28:18.887951: Current learning rate: 0.00288 
2023-10-26 16:28:23.012042: train_loss -0.8641 
2023-10-26 16:28:23.012395: val_loss -0.8085 
2023-10-26 16:28:23.012650: Pseudo dice [0.8734, 0.9164, 0.9662, 0.5461, 0.9362] 
2023-10-26 16:28:23.012870: Epoch time: 4.13 s 
2023-10-26 16:28:24.280891:  
2023-10-26 16:28:24.281209: Epoch 750 
2023-10-26 16:28:24.281476: Current learning rate: 0.00287 
2023-10-26 16:28:28.375130: train_loss -0.8675 
2023-10-26 16:28:28.375477: val_loss -0.7887 
2023-10-26 16:28:28.375748: Pseudo dice [0.8663, 0.916, 0.9669, 0.5321, 0.933] 
2023-10-26 16:28:28.376000: Epoch time: 4.09 s 
2023-10-26 16:28:29.505635:  
2023-10-26 16:28:29.505934: Epoch 751 
2023-10-26 16:28:29.506182: Current learning rate: 0.00286 
2023-10-26 16:28:33.561088: train_loss -0.8664 
2023-10-26 16:28:33.561456: val_loss -0.7945 
2023-10-26 16:28:33.561756: Pseudo dice [0.873, 0.9074, 0.9662, 0.5544, 0.9357] 
2023-10-26 16:28:33.562148: Epoch time: 4.06 s 
2023-10-26 16:28:34.859174:  
2023-10-26 16:28:34.859489: Epoch 752 
2023-10-26 16:28:34.859744: Current learning rate: 0.00285 
2023-10-26 16:28:38.972588: train_loss -0.8705 
2023-10-26 16:28:38.973003: val_loss -0.7923 
2023-10-26 16:28:38.973269: Pseudo dice [0.8683, 0.9148, 0.9647, 0.5666, 0.9371] 
2023-10-26 16:28:38.973502: Epoch time: 4.11 s 
2023-10-26 16:28:40.117603:  
2023-10-26 16:28:40.117919: Epoch 753 
2023-10-26 16:28:40.118193: Current learning rate: 0.00284 
2023-10-26 16:28:44.154762: train_loss -0.8704 
2023-10-26 16:28:44.155144: val_loss -0.8011 
2023-10-26 16:28:44.155409: Pseudo dice [0.8684, 0.9141, 0.9676, 0.6298, 0.938] 
2023-10-26 16:28:44.155649: Epoch time: 4.04 s 
2023-10-26 16:28:45.298345:  
2023-10-26 16:28:45.298659: Epoch 754 
2023-10-26 16:28:45.298924: Current learning rate: 0.00283 
2023-10-26 16:28:49.382445: train_loss -0.8601 
2023-10-26 16:28:49.382792: val_loss -0.7869 
2023-10-26 16:28:49.383065: Pseudo dice [0.8691, 0.9134, 0.9659, 0.62, 0.9314] 
2023-10-26 16:28:49.383296: Epoch time: 4.08 s 
2023-10-26 16:28:50.547259:  
2023-10-26 16:28:50.547580: Epoch 755 
2023-10-26 16:28:50.547866: Current learning rate: 0.00282 
2023-10-26 16:28:54.620209: train_loss -0.859 
2023-10-26 16:28:54.620564: val_loss -0.7944 
2023-10-26 16:28:54.620821: Pseudo dice [0.8717, 0.9168, 0.9664, 0.7083, 0.9361] 
2023-10-26 16:28:54.621043: Epoch time: 4.07 s 
2023-10-26 16:28:55.743437:  
2023-10-26 16:28:55.743741: Epoch 756 
2023-10-26 16:28:55.744017: Current learning rate: 0.00281 
2023-10-26 16:28:59.899663: train_loss -0.8667 
2023-10-26 16:28:59.900075: val_loss -0.7881 
2023-10-26 16:28:59.900342: Pseudo dice [0.8725, 0.9129, 0.9653, 0.6671, 0.9346] 
2023-10-26 16:28:59.900586: Epoch time: 4.16 s 
2023-10-26 16:29:01.042071:  
2023-10-26 16:29:01.042448: Epoch 757 
2023-10-26 16:29:01.042787: Current learning rate: 0.0028 
2023-10-26 16:29:05.157039: train_loss -0.8695 
2023-10-26 16:29:05.157436: val_loss -0.8157 
2023-10-26 16:29:05.157700: Pseudo dice [0.8692, 0.9147, 0.9671, 0.6254, 0.9379] 
2023-10-26 16:29:05.157946: Epoch time: 4.12 s 
2023-10-26 16:29:06.321440:  
2023-10-26 16:29:06.321764: Epoch 758 
2023-10-26 16:29:06.322033: Current learning rate: 0.00279 
2023-10-26 16:29:10.607092: train_loss -0.8652 
2023-10-26 16:29:10.607507: val_loss -0.7921 
2023-10-26 16:29:10.607787: Pseudo dice [0.8609, 0.9122, 0.9658, 0.4497, 0.9341] 
2023-10-26 16:29:10.608046: Epoch time: 4.29 s 
2023-10-26 16:29:11.754740:  
2023-10-26 16:29:11.755069: Epoch 759 
2023-10-26 16:29:11.755317: Current learning rate: 0.00278 
2023-10-26 16:29:15.917020: train_loss -0.8686 
2023-10-26 16:29:15.917396: val_loss -0.8144 
2023-10-26 16:29:15.917740: Pseudo dice [0.8669, 0.9152, 0.965, 0.4906, 0.9336] 
2023-10-26 16:29:15.918116: Epoch time: 4.16 s 
2023-10-26 16:29:17.073399:  
2023-10-26 16:29:17.073709: Epoch 760 
2023-10-26 16:29:17.073961: Current learning rate: 0.00277 
2023-10-26 16:29:21.183840: train_loss -0.8687 
2023-10-26 16:29:21.184232: val_loss -0.8083 
2023-10-26 16:29:21.184494: Pseudo dice [0.872, 0.9139, 0.9644, 0.5953, 0.9349] 
2023-10-26 16:29:21.184755: Epoch time: 4.11 s 
2023-10-26 16:29:22.320307:  
2023-10-26 16:29:22.320618: Epoch 761 
2023-10-26 16:29:22.320879: Current learning rate: 0.00276 
2023-10-26 16:29:26.394012: train_loss -0.8828 
2023-10-26 16:29:26.394389: val_loss -0.8084 
2023-10-26 16:29:26.394651: Pseudo dice [0.8698, 0.9121, 0.9685, 0.5726, 0.9373] 
2023-10-26 16:29:26.394888: Epoch time: 4.07 s 
2023-10-26 16:29:27.591784:  
2023-10-26 16:29:27.592191: Epoch 762 
2023-10-26 16:29:27.592585: Current learning rate: 0.00275 
2023-10-26 16:29:31.699087: train_loss -0.8667 
2023-10-26 16:29:31.699448: val_loss -0.8063 
2023-10-26 16:29:31.699722: Pseudo dice [0.866, 0.9118, 0.9674, 0.5934, 0.9342] 
2023-10-26 16:29:31.699949: Epoch time: 4.11 s 
2023-10-26 16:29:32.936092:  
2023-10-26 16:29:32.936408: Epoch 763 
2023-10-26 16:29:32.936676: Current learning rate: 0.00274 
2023-10-26 16:29:37.064443: train_loss -0.8683 
2023-10-26 16:29:37.064845: val_loss -0.7848 
2023-10-26 16:29:37.065121: Pseudo dice [0.86, 0.91, 0.9663, 0.7092, 0.9318] 
2023-10-26 16:29:37.065356: Epoch time: 4.13 s 
2023-10-26 16:29:38.206812:  
2023-10-26 16:29:38.207128: Epoch 764 
2023-10-26 16:29:38.207378: Current learning rate: 0.00273 
2023-10-26 16:29:42.318154: train_loss -0.865 
2023-10-26 16:29:42.318532: val_loss -0.7767 
2023-10-26 16:29:42.318796: Pseudo dice [0.8661, 0.9141, 0.9658, 0.5804, 0.9354] 
2023-10-26 16:29:42.319046: Epoch time: 4.11 s 
2023-10-26 16:29:43.690596:  
2023-10-26 16:29:43.691077: Epoch 765 
2023-10-26 16:29:43.691401: Current learning rate: 0.00272 
2023-10-26 16:29:47.778518: train_loss -0.8652 
2023-10-26 16:29:47.778941: val_loss -0.7782 
2023-10-26 16:29:47.779214: Pseudo dice [0.8613, 0.9153, 0.9671, 0.6407, 0.9337] 
2023-10-26 16:29:47.779452: Epoch time: 4.09 s 
2023-10-26 16:29:48.964733:  
2023-10-26 16:29:48.965078: Epoch 766 
2023-10-26 16:29:48.965330: Current learning rate: 0.00271 
2023-10-26 16:29:52.941327: train_loss -0.8671 
2023-10-26 16:29:52.941698: val_loss -0.7998 
2023-10-26 16:29:52.941968: Pseudo dice [0.8685, 0.9159, 0.9666, 0.6571, 0.9355] 
2023-10-26 16:29:52.942202: Epoch time: 3.98 s 
2023-10-26 16:29:54.127269:  
2023-10-26 16:29:54.127616: Epoch 767 
2023-10-26 16:29:54.127883: Current learning rate: 0.0027 
2023-10-26 16:29:58.095169: train_loss -0.8667 
2023-10-26 16:29:58.095532: val_loss -0.7809 
2023-10-26 16:29:58.095792: Pseudo dice [0.8696, 0.9101, 0.9664, 0.6383, 0.9315] 
2023-10-26 16:29:58.096042: Epoch time: 3.97 s 
2023-10-26 16:29:59.322562:  
2023-10-26 16:29:59.322882: Epoch 768 
2023-10-26 16:29:59.323140: Current learning rate: 0.00268 
2023-10-26 16:30:03.500555: train_loss -0.8705 
2023-10-26 16:30:03.500951: val_loss -0.7779 
2023-10-26 16:30:03.501215: Pseudo dice [0.8633, 0.9133, 0.9653, 0.6432, 0.9315] 
2023-10-26 16:30:03.501465: Epoch time: 4.18 s 
2023-10-26 16:30:04.643898:  
2023-10-26 16:30:04.644219: Epoch 769 
2023-10-26 16:30:04.644501: Current learning rate: 0.00267 
2023-10-26 16:30:08.778195: train_loss -0.8752 
2023-10-26 16:30:08.778553: val_loss -0.7981 
2023-10-26 16:30:08.778817: Pseudo dice [0.8698, 0.9186, 0.9649, 0.5385, 0.9366] 
2023-10-26 16:30:08.779049: Epoch time: 4.13 s 
2023-10-26 16:30:09.961911:  
2023-10-26 16:30:09.962211: Epoch 770 
2023-10-26 16:30:09.962479: Current learning rate: 0.00266 
2023-10-26 16:30:13.837755: train_loss -0.872 
2023-10-26 16:30:13.838128: val_loss -0.7894 
2023-10-26 16:30:13.838383: Pseudo dice [0.8692, 0.915, 0.9647, 0.6641, 0.9318] 
2023-10-26 16:30:13.838615: Epoch time: 3.88 s 
2023-10-26 16:30:15.139626:  
2023-10-26 16:30:15.139925: Epoch 771 
2023-10-26 16:30:15.140182: Current learning rate: 0.00265 
2023-10-26 16:30:19.279765: train_loss -0.8668 
2023-10-26 16:30:19.280179: val_loss -0.8043 
2023-10-26 16:30:19.280461: Pseudo dice [0.8703, 0.9168, 0.9655, 0.7123, 0.937] 
2023-10-26 16:30:19.280696: Epoch time: 4.14 s 
2023-10-26 16:30:20.456431:  
2023-10-26 16:30:20.456729: Epoch 772 
2023-10-26 16:30:20.456991: Current learning rate: 0.00264 
2023-10-26 16:30:24.502551: train_loss -0.8742 
2023-10-26 16:30:24.503108: val_loss -0.7745 
2023-10-26 16:30:24.503405: Pseudo dice [0.8698, 0.914, 0.9644, 0.6521, 0.9352] 
2023-10-26 16:30:24.503648: Epoch time: 4.05 s 
2023-10-26 16:30:25.637699:  
2023-10-26 16:30:25.637993: Epoch 773 
2023-10-26 16:30:25.638254: Current learning rate: 0.00263 
2023-10-26 16:30:29.892639: train_loss -0.8662 
2023-10-26 16:30:29.893116: val_loss -0.7894 
2023-10-26 16:30:29.893411: Pseudo dice [0.8633, 0.912, 0.9657, 0.7895, 0.9328] 
2023-10-26 16:30:29.893658: Epoch time: 4.26 s 
2023-10-26 16:30:31.056346:  
2023-10-26 16:30:31.056644: Epoch 774 
2023-10-26 16:30:31.056902: Current learning rate: 0.00262 
2023-10-26 16:30:35.264508: train_loss -0.8614 
2023-10-26 16:30:35.264869: val_loss -0.784 
2023-10-26 16:30:35.265145: Pseudo dice [0.8649, 0.9063, 0.9654, 0.6817, 0.9339] 
2023-10-26 16:30:35.265373: Epoch time: 4.21 s 
2023-10-26 16:30:36.398970:  
2023-10-26 16:30:36.399272: Epoch 775 
2023-10-26 16:30:36.399533: Current learning rate: 0.00261 
2023-10-26 16:30:40.425958: train_loss -0.8747 
2023-10-26 16:30:40.426347: val_loss -0.784 
2023-10-26 16:30:40.426616: Pseudo dice [0.8632, 0.9134, 0.9666, 0.6701, 0.9348] 
2023-10-26 16:30:40.426852: Epoch time: 4.03 s 
2023-10-26 16:30:41.566652:  
2023-10-26 16:30:41.566964: Epoch 776 
2023-10-26 16:30:41.567221: Current learning rate: 0.0026 
2023-10-26 16:30:45.627060: train_loss -0.8737 
2023-10-26 16:30:45.627443: val_loss -0.7948 
2023-10-26 16:30:45.627702: Pseudo dice [0.8691, 0.9118, 0.9656, 0.6493, 0.9357] 
2023-10-26 16:30:45.627928: Epoch time: 4.06 s 
2023-10-26 16:30:46.939163:  
2023-10-26 16:30:46.939579: Epoch 777 
2023-10-26 16:30:46.939917: Current learning rate: 0.00259 
2023-10-26 16:30:51.082868: train_loss -0.8676 
2023-10-26 16:30:51.083283: val_loss -0.8078 
2023-10-26 16:30:51.083596: Pseudo dice [0.8676, 0.9156, 0.9649, 0.737, 0.9397] 
2023-10-26 16:30:51.083837: Epoch time: 4.14 s 
2023-10-26 16:30:52.230255:  
2023-10-26 16:30:52.230562: Epoch 778 
2023-10-26 16:30:52.230817: Current learning rate: 0.00258 
2023-10-26 16:30:56.404211: train_loss -0.8586 
2023-10-26 16:30:56.404716: val_loss -0.7958 
2023-10-26 16:30:56.405087: Pseudo dice [0.8578, 0.9075, 0.9669, 0.4679, 0.9285] 
2023-10-26 16:30:56.405402: Epoch time: 4.17 s 
2023-10-26 16:30:57.554536:  
2023-10-26 16:30:57.554853: Epoch 779 
2023-10-26 16:30:57.555109: Current learning rate: 0.00257 
2023-10-26 16:31:01.722888: train_loss -0.8665 
2023-10-26 16:31:01.723491: val_loss -0.7747 
2023-10-26 16:31:01.723775: Pseudo dice [0.8683, 0.9147, 0.966, 0.4534, 0.9342] 
2023-10-26 16:31:01.724084: Epoch time: 4.17 s 
2023-10-26 16:31:02.876333:  
2023-10-26 16:31:02.876636: Epoch 780 
2023-10-26 16:31:02.876899: Current learning rate: 0.00256 
2023-10-26 16:31:06.969119: train_loss -0.8701 
2023-10-26 16:31:06.969527: val_loss -0.7698 
2023-10-26 16:31:06.969787: Pseudo dice [0.8587, 0.9147, 0.9654, 0.4233, 0.9323] 
2023-10-26 16:31:06.970035: Epoch time: 4.09 s 
2023-10-26 16:31:08.112634:  
2023-10-26 16:31:08.112956: Epoch 781 
2023-10-26 16:31:08.113222: Current learning rate: 0.00255 
2023-10-26 16:31:12.235383: train_loss -0.8658 
2023-10-26 16:31:12.235817: val_loss -0.7811 
2023-10-26 16:31:12.236096: Pseudo dice [0.8684, 0.9166, 0.9652, 0.485, 0.9351] 
2023-10-26 16:31:12.236324: Epoch time: 4.12 s 
2023-10-26 16:31:13.371052:  
2023-10-26 16:31:13.371376: Epoch 782 
2023-10-26 16:31:13.371621: Current learning rate: 0.00254 
2023-10-26 16:31:17.358551: train_loss -0.8716 
2023-10-26 16:31:17.358943: val_loss -0.7715 
2023-10-26 16:31:17.359213: Pseudo dice [0.8695, 0.9113, 0.9646, 0.6287, 0.9335] 
2023-10-26 16:31:17.359444: Epoch time: 3.99 s 
2023-10-26 16:31:18.670849:  
2023-10-26 16:31:18.671154: Epoch 783 
2023-10-26 16:31:18.671399: Current learning rate: 0.00253 
2023-10-26 16:31:22.845086: train_loss -0.865 
2023-10-26 16:31:22.845508: val_loss -0.7824 
2023-10-26 16:31:22.845784: Pseudo dice [0.87, 0.9096, 0.9674, 0.6328, 0.9364] 
2023-10-26 16:31:22.846037: Epoch time: 4.17 s 
2023-10-26 16:31:23.990039:  
2023-10-26 16:31:23.990353: Epoch 784 
2023-10-26 16:31:23.990605: Current learning rate: 0.00252 
2023-10-26 16:31:28.197598: train_loss -0.8696 
2023-10-26 16:31:28.197985: val_loss -0.7763 
2023-10-26 16:31:28.198252: Pseudo dice [0.8585, 0.9138, 0.9648, 0.5663, 0.9323] 
2023-10-26 16:31:28.198507: Epoch time: 4.21 s 
2023-10-26 16:31:29.337409:  
2023-10-26 16:31:29.337698: Epoch 785 
2023-10-26 16:31:29.337954: Current learning rate: 0.00251 
2023-10-26 16:31:33.424206: train_loss -0.8712 
2023-10-26 16:31:33.424633: val_loss -0.7886 
2023-10-26 16:31:33.424917: Pseudo dice [0.8676, 0.9183, 0.9656, 0.5483, 0.9374] 
2023-10-26 16:31:33.425163: Epoch time: 4.09 s 
2023-10-26 16:31:34.577591:  
2023-10-26 16:31:34.577912: Epoch 786 
2023-10-26 16:31:34.578157: Current learning rate: 0.0025 
2023-10-26 16:31:38.661764: train_loss -0.8608 
2023-10-26 16:31:38.662179: val_loss -0.7881 
2023-10-26 16:31:38.662449: Pseudo dice [0.859, 0.915, 0.9656, 0.5032, 0.9349] 
2023-10-26 16:31:38.662762: Epoch time: 4.08 s 
2023-10-26 16:31:39.816130:  
2023-10-26 16:31:39.816437: Epoch 787 
2023-10-26 16:31:39.816679: Current learning rate: 0.00249 
2023-10-26 16:31:43.852250: train_loss -0.8698 
2023-10-26 16:31:43.852865: val_loss -0.7751 
2023-10-26 16:31:43.853287: Pseudo dice [0.8656, 0.9144, 0.9657, 0.3991, 0.9335] 
2023-10-26 16:31:43.853660: Epoch time: 4.04 s 
2023-10-26 16:31:45.025730:  
2023-10-26 16:31:45.026099: Epoch 788 
2023-10-26 16:31:45.026414: Current learning rate: 0.00248 
2023-10-26 16:31:49.044743: train_loss -0.8651 
2023-10-26 16:31:49.045179: val_loss -0.789 
2023-10-26 16:31:49.045442: Pseudo dice [0.8595, 0.9161, 0.9665, 0.4479, 0.9346] 
2023-10-26 16:31:49.045682: Epoch time: 4.02 s 
2023-10-26 16:31:50.414416:  
2023-10-26 16:31:50.414779: Epoch 789 
2023-10-26 16:31:50.415051: Current learning rate: 0.00247 
2023-10-26 16:31:54.623104: train_loss -0.8696 
2023-10-26 16:31:54.623505: val_loss -0.8201 
2023-10-26 16:31:54.623887: Pseudo dice [0.862, 0.9117, 0.9669, 0.5, 0.9312] 
2023-10-26 16:31:54.624209: Epoch time: 4.21 s 
2023-10-26 16:31:55.777188:  
2023-10-26 16:31:55.777521: Epoch 790 
2023-10-26 16:31:55.777785: Current learning rate: 0.00245 
2023-10-26 16:31:59.841658: train_loss -0.8693 
2023-10-26 16:31:59.842080: val_loss -0.7749 
2023-10-26 16:31:59.842356: Pseudo dice [0.8623, 0.9172, 0.9661, 0.5564, 0.9295] 
2023-10-26 16:31:59.842600: Epoch time: 4.07 s 
2023-10-26 16:32:01.002706:  
2023-10-26 16:32:01.003012: Epoch 791 
2023-10-26 16:32:01.003278: Current learning rate: 0.00244 
2023-10-26 16:32:05.084101: train_loss -0.8631 
2023-10-26 16:32:05.084491: val_loss -0.7643 
2023-10-26 16:32:05.084750: Pseudo dice [0.8611, 0.9134, 0.9649, 0.5209, 0.9298] 
2023-10-26 16:32:05.084994: Epoch time: 4.08 s 
2023-10-26 16:32:06.246487:  
2023-10-26 16:32:06.246791: Epoch 792 
2023-10-26 16:32:06.247051: Current learning rate: 0.00243 
2023-10-26 16:32:10.286167: train_loss -0.8662 
2023-10-26 16:32:10.286590: val_loss -0.7751 
2023-10-26 16:32:10.286849: Pseudo dice [0.8553, 0.9092, 0.9651, 0.4946, 0.9358] 
2023-10-26 16:32:10.287116: Epoch time: 4.04 s 
2023-10-26 16:32:11.467700:  
2023-10-26 16:32:11.468020: Epoch 793 
2023-10-26 16:32:11.468278: Current learning rate: 0.00242 
2023-10-26 16:32:15.556613: train_loss -0.8658 
2023-10-26 16:32:15.557014: val_loss -0.7963 
2023-10-26 16:32:15.557317: Pseudo dice [0.8639, 0.9157, 0.9642, 0.4799, 0.9355] 
2023-10-26 16:32:15.557571: Epoch time: 4.09 s 
2023-10-26 16:32:16.723403:  
2023-10-26 16:32:16.723717: Epoch 794 
2023-10-26 16:32:16.723971: Current learning rate: 0.00241 
2023-10-26 16:32:20.711536: train_loss -0.8606 
2023-10-26 16:32:20.711967: val_loss -0.7542 
2023-10-26 16:32:20.712242: Pseudo dice [0.8564, 0.9095, 0.9658, 0.5127, 0.9301] 
2023-10-26 16:32:20.712485: Epoch time: 3.99 s 
2023-10-26 16:32:22.130054:  
2023-10-26 16:32:22.130347: Epoch 795 
2023-10-26 16:32:22.130672: Current learning rate: 0.0024 
2023-10-26 16:32:26.286856: train_loss -0.8686 
2023-10-26 16:32:26.287651: val_loss -0.7924 
2023-10-26 16:32:26.287923: Pseudo dice [0.8678, 0.9176, 0.9667, 0.5509, 0.9357] 
2023-10-26 16:32:26.288162: Epoch time: 4.16 s 
2023-10-26 16:32:27.444286:  
2023-10-26 16:32:27.444609: Epoch 796 
2023-10-26 16:32:27.444913: Current learning rate: 0.00239 
2023-10-26 16:32:31.590616: train_loss -0.8613 
2023-10-26 16:32:31.591008: val_loss -0.7809 
2023-10-26 16:32:31.591268: Pseudo dice [0.8623, 0.9126, 0.9657, 0.5665, 0.9328] 
2023-10-26 16:32:31.591504: Epoch time: 4.15 s 
2023-10-26 16:32:32.778073:  
2023-10-26 16:32:32.778404: Epoch 797 
2023-10-26 16:32:32.778664: Current learning rate: 0.00238 
2023-10-26 16:32:36.807028: train_loss -0.8718 
2023-10-26 16:32:36.807409: val_loss -0.7909 
2023-10-26 16:32:36.807670: Pseudo dice [0.8627, 0.9148, 0.9669, 0.4631, 0.935] 
2023-10-26 16:32:36.807919: Epoch time: 4.03 s 
2023-10-26 16:32:37.981561:  
2023-10-26 16:32:37.981856: Epoch 798 
2023-10-26 16:32:37.982109: Current learning rate: 0.00237 
2023-10-26 16:32:42.092855: train_loss -0.8643 
2023-10-26 16:32:42.093503: val_loss -0.7711 
2023-10-26 16:32:42.093822: Pseudo dice [0.8602, 0.9111, 0.9655, 0.5679, 0.9331] 
2023-10-26 16:32:42.094167: Epoch time: 4.11 s 
2023-10-26 16:32:43.253470:  
2023-10-26 16:32:43.253769: Epoch 799 
2023-10-26 16:32:43.254020: Current learning rate: 0.00236 
2023-10-26 16:32:47.241436: train_loss -0.8703 
2023-10-26 16:32:47.241866: val_loss -0.7686 
2023-10-26 16:32:47.242498: Pseudo dice [0.8654, 0.9125, 0.9658, 0.636, 0.9338] 
2023-10-26 16:32:47.242748: Epoch time: 3.99 s 
2023-10-26 16:32:48.502307:  
2023-10-26 16:32:48.502610: Epoch 800 
2023-10-26 16:32:48.502856: Current learning rate: 0.00235 
2023-10-26 16:32:52.634630: train_loss -0.8703 
2023-10-26 16:32:52.635010: val_loss -0.7517 
2023-10-26 16:32:52.635270: Pseudo dice [0.8552, 0.9093, 0.9659, 0.5398, 0.932] 
2023-10-26 16:32:52.635509: Epoch time: 4.13 s 
2023-10-26 16:32:53.964160:  
2023-10-26 16:32:53.964473: Epoch 801 
2023-10-26 16:32:53.964720: Current learning rate: 0.00234 
2023-10-26 16:32:57.976225: train_loss -0.8743 
2023-10-26 16:32:57.976635: val_loss -0.76 
2023-10-26 16:32:57.976915: Pseudo dice [0.8631, 0.9121, 0.9662, 0.5674, 0.935] 
2023-10-26 16:32:57.977154: Epoch time: 4.01 s 
2023-10-26 16:32:59.133098:  
2023-10-26 16:32:59.133404: Epoch 802 
2023-10-26 16:32:59.133653: Current learning rate: 0.00233 
2023-10-26 16:33:03.334212: train_loss -0.8635 
2023-10-26 16:33:03.334633: val_loss -0.7636 
2023-10-26 16:33:03.334909: Pseudo dice [0.8593, 0.9086, 0.9667, 0.5987, 0.9317] 
2023-10-26 16:33:03.335166: Epoch time: 4.2 s 
2023-10-26 16:33:04.502006:  
2023-10-26 16:33:04.502309: Epoch 803 
2023-10-26 16:33:04.502556: Current learning rate: 0.00232 
2023-10-26 16:33:08.714963: train_loss -0.8577 
2023-10-26 16:33:08.715346: val_loss -0.7761 
2023-10-26 16:33:08.715613: Pseudo dice [0.8664, 0.9072, 0.9663, 0.4801, 0.9322] 
2023-10-26 16:33:08.715885: Epoch time: 4.21 s 
2023-10-26 16:33:09.869244:  
2023-10-26 16:33:09.869550: Epoch 804 
2023-10-26 16:33:09.869797: Current learning rate: 0.00231 
2023-10-26 16:33:14.093782: train_loss -0.8696 
2023-10-26 16:33:14.094377: val_loss -0.7707 
2023-10-26 16:33:14.094680: Pseudo dice [0.8679, 0.9136, 0.9669, 0.5618, 0.9399] 
2023-10-26 16:33:14.094953: Epoch time: 4.23 s 
2023-10-26 16:33:15.276280:  
2023-10-26 16:33:15.276575: Epoch 805 
2023-10-26 16:33:15.276841: Current learning rate: 0.0023 
2023-10-26 16:33:19.302984: train_loss -0.8747 
2023-10-26 16:33:19.303469: val_loss -0.7838 
2023-10-26 16:33:19.304076: Pseudo dice [0.8657, 0.9106, 0.9653, 0.556, 0.9353] 
2023-10-26 16:33:19.304440: Epoch time: 4.03 s 
2023-10-26 16:33:20.494115:  
2023-10-26 16:33:20.494436: Epoch 806 
2023-10-26 16:33:20.494696: Current learning rate: 0.00229 
2023-10-26 16:33:24.658989: train_loss -0.8685 
2023-10-26 16:33:24.659428: val_loss -0.7659 
2023-10-26 16:33:24.659714: Pseudo dice [0.8596, 0.9162, 0.9646, 0.5147, 0.9321] 
2023-10-26 16:33:24.659976: Epoch time: 4.17 s 
2023-10-26 16:33:26.015503:  
2023-10-26 16:33:26.015909: Epoch 807 
2023-10-26 16:33:26.016161: Current learning rate: 0.00228 
2023-10-26 16:33:30.242904: train_loss -0.8715 
2023-10-26 16:33:30.243277: val_loss -0.7713 
2023-10-26 16:33:30.243543: Pseudo dice [0.8648, 0.9113, 0.9677, 0.4996, 0.9362] 
2023-10-26 16:33:30.243788: Epoch time: 4.23 s 
2023-10-26 16:33:31.423246:  
2023-10-26 16:33:31.423552: Epoch 808 
2023-10-26 16:33:31.423810: Current learning rate: 0.00226 
2023-10-26 16:33:35.489635: train_loss -0.8599 
2023-10-26 16:33:35.490076: val_loss -0.7732 
2023-10-26 16:33:35.490378: Pseudo dice [0.8617, 0.9125, 0.966, 0.5094, 0.9342] 
2023-10-26 16:33:35.490629: Epoch time: 4.07 s 
2023-10-26 16:33:36.655170:  
2023-10-26 16:33:36.655463: Epoch 809 
2023-10-26 16:33:36.655713: Current learning rate: 0.00225 
2023-10-26 16:33:40.842249: train_loss -0.8756 
2023-10-26 16:33:40.842685: val_loss -0.7824 
2023-10-26 16:33:40.843158: Pseudo dice [0.862, 0.9157, 0.9666, 0.3953, 0.9349] 
2023-10-26 16:33:40.843497: Epoch time: 4.19 s 
2023-10-26 16:33:42.006881:  
2023-10-26 16:33:42.007180: Epoch 810 
2023-10-26 16:33:42.007426: Current learning rate: 0.00224 
2023-10-26 16:33:46.033084: train_loss -0.8733 
2023-10-26 16:33:46.033496: val_loss -0.757 
2023-10-26 16:33:46.033757: Pseudo dice [0.8607, 0.9125, 0.9651, 0.4884, 0.931] 
2023-10-26 16:33:46.034015: Epoch time: 4.03 s 
2023-10-26 16:33:47.194419:  
2023-10-26 16:33:47.194720: Epoch 811 
2023-10-26 16:33:47.194976: Current learning rate: 0.00223 
2023-10-26 16:33:51.309816: train_loss -0.8657 
2023-10-26 16:33:51.310236: val_loss -0.7615 
2023-10-26 16:33:51.310547: Pseudo dice [0.8587, 0.9126, 0.9649, 0.5652, 0.9327] 
2023-10-26 16:33:51.310800: Epoch time: 4.12 s 
2023-10-26 16:33:52.489262:  
2023-10-26 16:33:52.489568: Epoch 812 
2023-10-26 16:33:52.489819: Current learning rate: 0.00222 
2023-10-26 16:33:56.566060: train_loss -0.863 
2023-10-26 16:33:56.566544: val_loss -0.7939 
2023-10-26 16:33:56.566921: Pseudo dice [0.8708, 0.9149, 0.964, 0.5849, 0.9392] 
2023-10-26 16:33:56.567266: Epoch time: 4.08 s 
2023-10-26 16:33:57.961327:  
2023-10-26 16:33:57.961649: Epoch 813 
2023-10-26 16:33:57.961899: Current learning rate: 0.00221 
2023-10-26 16:34:02.035967: train_loss -0.8628 
2023-10-26 16:34:02.036348: val_loss -0.7742 
2023-10-26 16:34:02.036621: Pseudo dice [0.8642, 0.9133, 0.9645, 0.6427, 0.9289] 
2023-10-26 16:34:02.036862: Epoch time: 4.08 s 
2023-10-26 16:34:03.194877:  
2023-10-26 16:34:03.195173: Epoch 814 
2023-10-26 16:34:03.195415: Current learning rate: 0.0022 
2023-10-26 16:34:07.354592: train_loss -0.8718 
2023-10-26 16:34:07.355151: val_loss -0.7799 
2023-10-26 16:34:07.355552: Pseudo dice [0.8635, 0.9167, 0.967, 0.6547, 0.9329] 
2023-10-26 16:34:07.355911: Epoch time: 4.16 s 
2023-10-26 16:34:08.525338:  
2023-10-26 16:34:08.525635: Epoch 815 
2023-10-26 16:34:08.525892: Current learning rate: 0.00219 
2023-10-26 16:34:12.598127: train_loss -0.8678 
2023-10-26 16:34:12.598512: val_loss -0.7712 
2023-10-26 16:34:12.598824: Pseudo dice [0.8638, 0.909, 0.9662, 0.6193, 0.9339] 
2023-10-26 16:34:12.599177: Epoch time: 4.07 s 
2023-10-26 16:34:13.763471:  
2023-10-26 16:34:13.763772: Epoch 816 
2023-10-26 16:34:13.764028: Current learning rate: 0.00218 
2023-10-26 16:34:17.794747: train_loss -0.8756 
2023-10-26 16:34:17.795175: val_loss -0.7772 
2023-10-26 16:34:17.795643: Pseudo dice [0.8665, 0.911, 0.9645, 0.6206, 0.9377] 
2023-10-26 16:34:17.795952: Epoch time: 4.03 s 
2023-10-26 16:34:18.951458:  
2023-10-26 16:34:18.951767: Epoch 817 
2023-10-26 16:34:18.952035: Current learning rate: 0.00217 
2023-10-26 16:34:23.183456: train_loss -0.8725 
2023-10-26 16:34:23.183823: val_loss -0.7721 
2023-10-26 16:34:23.184107: Pseudo dice [0.8607, 0.9125, 0.9643, 0.6222, 0.9351] 
2023-10-26 16:34:23.184335: Epoch time: 4.23 s 
2023-10-26 16:34:24.383848:  
2023-10-26 16:34:24.384224: Epoch 818 
2023-10-26 16:34:24.384547: Current learning rate: 0.00216 
2023-10-26 16:34:28.406781: train_loss -0.8678 
2023-10-26 16:34:28.407216: val_loss -0.7747 
2023-10-26 16:34:28.407870: Pseudo dice [0.8624, 0.9162, 0.9666, 0.5517, 0.9318] 
2023-10-26 16:34:28.408119: Epoch time: 4.02 s 
2023-10-26 16:34:29.740177:  
2023-10-26 16:34:29.740483: Epoch 819 
2023-10-26 16:34:29.740747: Current learning rate: 0.00215 
2023-10-26 16:34:33.869864: train_loss -0.8669 
2023-10-26 16:34:33.870285: val_loss -0.7801 
2023-10-26 16:34:33.870783: Pseudo dice [0.861, 0.9154, 0.9664, 0.5594, 0.9317] 
2023-10-26 16:34:33.871037: Epoch time: 4.13 s 
2023-10-26 16:34:35.001634:  
2023-10-26 16:34:35.001954: Epoch 820 
2023-10-26 16:34:35.002215: Current learning rate: 0.00214 
2023-10-26 16:34:39.003538: train_loss -0.8731 
2023-10-26 16:34:39.004013: val_loss -0.7801 
2023-10-26 16:34:39.004407: Pseudo dice [0.8593, 0.9096, 0.9649, 0.5372, 0.9326] 
2023-10-26 16:34:39.004748: Epoch time: 4.0 s 
2023-10-26 16:34:40.234896:  
2023-10-26 16:34:40.235201: Epoch 821 
2023-10-26 16:34:40.235464: Current learning rate: 0.00213 
2023-10-26 16:34:44.303087: train_loss -0.8714 
2023-10-26 16:34:44.303616: val_loss -0.7741 
2023-10-26 16:34:44.303908: Pseudo dice [0.8628, 0.9156, 0.9672, 0.5107, 0.934] 
2023-10-26 16:34:44.304189: Epoch time: 4.07 s 
2023-10-26 16:34:45.438554:  
2023-10-26 16:34:45.438865: Epoch 822 
2023-10-26 16:34:45.439133: Current learning rate: 0.00212 
2023-10-26 16:34:49.500373: train_loss -0.8716 
2023-10-26 16:34:49.500741: val_loss -0.7697 
2023-10-26 16:34:49.501037: Pseudo dice [0.8686, 0.9158, 0.9639, 0.6074, 0.9388] 
2023-10-26 16:34:49.501278: Epoch time: 4.06 s 
2023-10-26 16:34:50.611452:  
2023-10-26 16:34:50.611764: Epoch 823 
2023-10-26 16:34:50.612020: Current learning rate: 0.0021 
2023-10-26 16:34:54.597835: train_loss -0.8662 
2023-10-26 16:34:54.598248: val_loss -0.7846 
2023-10-26 16:34:54.598529: Pseudo dice [0.8624, 0.9117, 0.9646, 0.6234, 0.9297] 
2023-10-26 16:34:54.598761: Epoch time: 3.99 s 
2023-10-26 16:34:55.714094:  
2023-10-26 16:34:55.714402: Epoch 824 
2023-10-26 16:34:55.714662: Current learning rate: 0.00209 
2023-10-26 16:34:59.710797: train_loss -0.8814 
2023-10-26 16:34:59.711676: val_loss -0.787 
2023-10-26 16:34:59.712069: Pseudo dice [0.869, 0.9128, 0.965, 0.5491, 0.9397] 
2023-10-26 16:34:59.712656: Epoch time: 4.0 s 
2023-10-26 16:35:01.004510:  
2023-10-26 16:35:01.004831: Epoch 825 
2023-10-26 16:35:01.005081: Current learning rate: 0.00208 
2023-10-26 16:35:05.173270: train_loss -0.8715 
2023-10-26 16:35:05.173666: val_loss -0.7521 
2023-10-26 16:35:05.174037: Pseudo dice [0.8647, 0.913, 0.9653, 0.4862, 0.9314] 
2023-10-26 16:35:05.174353: Epoch time: 4.17 s 
2023-10-26 16:35:06.284073:  
2023-10-26 16:35:06.284398: Epoch 826 
2023-10-26 16:35:06.284652: Current learning rate: 0.00207 
2023-10-26 16:35:10.318493: train_loss -0.8682 
2023-10-26 16:35:10.318869: val_loss -0.7788 
2023-10-26 16:35:10.319156: Pseudo dice [0.8687, 0.9147, 0.9648, 0.537, 0.9346] 
2023-10-26 16:35:10.319392: Epoch time: 4.04 s 
2023-10-26 16:35:11.433481:  
2023-10-26 16:35:11.433763: Epoch 827 
2023-10-26 16:35:11.434010: Current learning rate: 0.00206 
2023-10-26 16:35:15.530753: train_loss -0.8716 
2023-10-26 16:35:15.531397: val_loss -0.7792 
2023-10-26 16:35:15.531797: Pseudo dice [0.8587, 0.9162, 0.9631, 0.5328, 0.928] 
2023-10-26 16:35:15.532131: Epoch time: 4.1 s 
2023-10-26 16:35:16.659057:  
2023-10-26 16:35:16.659349: Epoch 828 
2023-10-26 16:35:16.659599: Current learning rate: 0.00205 
2023-10-26 16:35:20.768029: train_loss -0.8637 
2023-10-26 16:35:20.768445: val_loss -0.7869 
2023-10-26 16:35:20.768815: Pseudo dice [0.8653, 0.9114, 0.9662, 0.5108, 0.9344] 
2023-10-26 16:35:20.769122: Epoch time: 4.11 s 
2023-10-26 16:35:21.875130:  
2023-10-26 16:35:21.875425: Epoch 829 
2023-10-26 16:35:21.875682: Current learning rate: 0.00204 
2023-10-26 16:35:26.117080: train_loss -0.8613 
2023-10-26 16:35:26.117444: val_loss -0.789 
2023-10-26 16:35:26.117706: Pseudo dice [0.8692, 0.9133, 0.9648, 0.5045, 0.9378] 
2023-10-26 16:35:26.117939: Epoch time: 4.24 s 
2023-10-26 16:35:27.230970:  
2023-10-26 16:35:27.231272: Epoch 830 
2023-10-26 16:35:27.231520: Current learning rate: 0.00203 
2023-10-26 16:35:31.310406: train_loss -0.8723 
2023-10-26 16:35:31.310896: val_loss -0.7771 
2023-10-26 16:35:31.311226: Pseudo dice [0.8642, 0.916, 0.9659, 0.5235, 0.9335] 
2023-10-26 16:35:31.311539: Epoch time: 4.08 s 
2023-10-26 16:35:32.493476:  
2023-10-26 16:35:32.493813: Epoch 831 
2023-10-26 16:35:32.494060: Current learning rate: 0.00202 
2023-10-26 16:35:36.679479: train_loss -0.8644 
2023-10-26 16:35:36.679901: val_loss -0.7695 
2023-10-26 16:35:36.680161: Pseudo dice [0.8627, 0.9109, 0.9633, 0.5691, 0.9326] 
2023-10-26 16:35:36.680400: Epoch time: 4.19 s 
2023-10-26 16:35:37.999959:  
2023-10-26 16:35:38.000275: Epoch 832 
2023-10-26 16:35:38.000534: Current learning rate: 0.00201 
2023-10-26 16:35:41.929981: train_loss -0.867 
2023-10-26 16:35:41.930376: val_loss -0.7732 
2023-10-26 16:35:41.930650: Pseudo dice [0.8557, 0.9119, 0.9632, 0.6476, 0.9277] 
2023-10-26 16:35:41.930892: Epoch time: 3.93 s 
2023-10-26 16:35:43.052718:  
2023-10-26 16:35:43.053029: Epoch 833 
2023-10-26 16:35:43.053287: Current learning rate: 0.002 
2023-10-26 16:35:47.034064: train_loss -0.8725 
2023-10-26 16:35:47.034517: val_loss -0.7905 
2023-10-26 16:35:47.034918: Pseudo dice [0.8599, 0.9156, 0.9619, 0.7379, 0.9299] 
2023-10-26 16:35:47.035211: Epoch time: 3.98 s 
2023-10-26 16:35:48.166426:  
2023-10-26 16:35:48.166733: Epoch 834 
2023-10-26 16:35:48.167142: Current learning rate: 0.00199 
2023-10-26 16:35:52.230620: train_loss -0.8699 
2023-10-26 16:35:52.231012: val_loss -0.7792 
2023-10-26 16:35:52.231275: Pseudo dice [0.8604, 0.9145, 0.9661, 0.6599, 0.9348] 
2023-10-26 16:35:52.231507: Epoch time: 4.06 s 
2023-10-26 16:35:53.341205:  
2023-10-26 16:35:53.341499: Epoch 835 
2023-10-26 16:35:53.341754: Current learning rate: 0.00198 
2023-10-26 16:35:57.364463: train_loss -0.8655 
2023-10-26 16:35:57.364898: val_loss -0.7829 
2023-10-26 16:35:57.365177: Pseudo dice [0.8568, 0.9117, 0.9647, 0.6929, 0.9283] 
2023-10-26 16:35:57.365426: Epoch time: 4.02 s 
2023-10-26 16:35:58.479244:  
2023-10-26 16:35:58.479561: Epoch 836 
2023-10-26 16:35:58.479818: Current learning rate: 0.00196 
2023-10-26 16:36:02.445263: train_loss -0.8641 
2023-10-26 16:36:02.445692: val_loss -0.7947 
2023-10-26 16:36:02.445972: Pseudo dice [0.8583, 0.9138, 0.9654, 0.6239, 0.9345] 
2023-10-26 16:36:02.446210: Epoch time: 3.97 s 
2023-10-26 16:36:03.557212:  
2023-10-26 16:36:03.557518: Epoch 837 
2023-10-26 16:36:03.557771: Current learning rate: 0.00195 
2023-10-26 16:36:07.550512: train_loss -0.8752 
2023-10-26 16:36:07.550915: val_loss -0.7994 
2023-10-26 16:36:07.551203: Pseudo dice [0.8648, 0.9111, 0.9647, 0.6544, 0.9345] 
2023-10-26 16:36:07.551450: Epoch time: 3.99 s 
2023-10-26 16:36:08.695013:  
2023-10-26 16:36:08.695322: Epoch 838 
2023-10-26 16:36:08.695587: Current learning rate: 0.00194 
2023-10-26 16:36:12.622580: train_loss -0.8703 
2023-10-26 16:36:12.622971: val_loss -0.7969 
2023-10-26 16:36:12.623225: Pseudo dice [0.8669, 0.9143, 0.9634, 0.6821, 0.9335] 
2023-10-26 16:36:12.623458: Epoch time: 3.93 s 
2023-10-26 16:36:13.950795:  
2023-10-26 16:36:13.951107: Epoch 839 
2023-10-26 16:36:13.951353: Current learning rate: 0.00193 
2023-10-26 16:36:17.858459: train_loss -0.8729 
2023-10-26 16:36:17.858838: val_loss -0.8177 
2023-10-26 16:36:17.859123: Pseudo dice [0.8618, 0.9141, 0.9664, 0.7168, 0.936] 
2023-10-26 16:36:17.859359: Epoch time: 3.91 s 
2023-10-26 16:36:19.089524:  
2023-10-26 16:36:19.089850: Epoch 840 
2023-10-26 16:36:19.090120: Current learning rate: 0.00192 
2023-10-26 16:36:23.166306: train_loss -0.869 
2023-10-26 16:36:23.166898: val_loss -0.7856 
2023-10-26 16:36:23.167207: Pseudo dice [0.8619, 0.9134, 0.9629, 0.634, 0.932] 
2023-10-26 16:36:23.167499: Epoch time: 4.08 s 
2023-10-26 16:36:24.285399:  
2023-10-26 16:36:24.285704: Epoch 841 
2023-10-26 16:36:24.285949: Current learning rate: 0.00191 
2023-10-26 16:36:28.338108: train_loss -0.878 
2023-10-26 16:36:28.338578: val_loss -0.7802 
2023-10-26 16:36:28.338964: Pseudo dice [0.8663, 0.9119, 0.9636, 0.6371, 0.9338] 
2023-10-26 16:36:28.339335: Epoch time: 4.05 s 
2023-10-26 16:36:29.480231:  
2023-10-26 16:36:29.480555: Epoch 842 
2023-10-26 16:36:29.480821: Current learning rate: 0.0019 
2023-10-26 16:36:33.468591: train_loss -0.8667 
2023-10-26 16:36:33.469013: val_loss -0.8197 
2023-10-26 16:36:33.469285: Pseudo dice [0.8654, 0.9131, 0.9668, 0.6836, 0.9351] 
2023-10-26 16:36:33.469658: Epoch time: 3.99 s 
2023-10-26 16:36:34.576559:  
2023-10-26 16:36:34.576868: Epoch 843 
2023-10-26 16:36:34.577120: Current learning rate: 0.00189 
2023-10-26 16:36:38.570843: train_loss -0.8665 
2023-10-26 16:36:38.571198: val_loss -0.809 
2023-10-26 16:36:38.571457: Pseudo dice [0.8656, 0.9161, 0.9643, 0.6784, 0.9378] 
2023-10-26 16:36:38.571708: Epoch time: 3.99 s 
2023-10-26 16:36:39.689836:  
2023-10-26 16:36:39.690148: Epoch 844 
2023-10-26 16:36:39.690403: Current learning rate: 0.00188 
2023-10-26 16:36:43.690141: train_loss -0.863 
2023-10-26 16:36:43.690610: val_loss -0.7721 
2023-10-26 16:36:43.690910: Pseudo dice [0.8659, 0.9113, 0.9668, 0.6582, 0.9354] 
2023-10-26 16:36:43.691165: Epoch time: 4.0 s 
2023-10-26 16:36:44.982449:  
2023-10-26 16:36:44.982772: Epoch 845 
2023-10-26 16:36:44.983031: Current learning rate: 0.00187 
2023-10-26 16:36:49.164545: train_loss -0.8615 
2023-10-26 16:36:49.164979: val_loss -0.777 
2023-10-26 16:36:49.165296: Pseudo dice [0.8639, 0.9091, 0.966, 0.6685, 0.9348] 
2023-10-26 16:36:49.165576: Epoch time: 4.18 s 
2023-10-26 16:36:50.293870:  
2023-10-26 16:36:50.294204: Epoch 846 
2023-10-26 16:36:50.294452: Current learning rate: 0.00186 
2023-10-26 16:36:54.375746: train_loss -0.8606 
2023-10-26 16:36:54.376139: val_loss -0.8069 
2023-10-26 16:36:54.376414: Pseudo dice [0.8666, 0.9113, 0.9649, 0.7084, 0.934] 
2023-10-26 16:36:54.376665: Epoch time: 4.08 s 
2023-10-26 16:36:55.484995:  
2023-10-26 16:36:55.485372: Epoch 847 
2023-10-26 16:36:55.485620: Current learning rate: 0.00185 
2023-10-26 16:36:59.542172: train_loss -0.8646 
2023-10-26 16:36:59.542576: val_loss -0.8128 
2023-10-26 16:36:59.542846: Pseudo dice [0.866, 0.9127, 0.9652, 0.715, 0.9362] 
2023-10-26 16:36:59.543094: Epoch time: 4.06 s 
2023-10-26 16:37:00.698255:  
2023-10-26 16:37:00.698573: Epoch 848 
2023-10-26 16:37:00.698828: Current learning rate: 0.00184 
2023-10-26 16:37:04.683178: train_loss -0.8639 
2023-10-26 16:37:04.683685: val_loss -0.7972 
2023-10-26 16:37:04.683977: Pseudo dice [0.8702, 0.9162, 0.9643, 0.6998, 0.9381] 
2023-10-26 16:37:04.684295: Epoch time: 3.99 s 
2023-10-26 16:37:05.856100:  
2023-10-26 16:37:05.856415: Epoch 849 
2023-10-26 16:37:05.856673: Current learning rate: 0.00182 
2023-10-26 16:37:09.857244: train_loss -0.8685 
2023-10-26 16:37:09.857682: val_loss -0.8041 
2023-10-26 16:37:09.857958: Pseudo dice [0.8661, 0.9134, 0.9658, 0.7141, 0.9366] 
2023-10-26 16:37:09.858203: Epoch time: 4.0 s 
2023-10-26 16:37:11.075913:  
2023-10-26 16:37:11.076229: Epoch 850 
2023-10-26 16:37:11.076487: Current learning rate: 0.00181 
2023-10-26 16:37:15.072111: train_loss -0.873 
2023-10-26 16:37:15.072484: val_loss -0.777 
2023-10-26 16:37:15.072908: Pseudo dice [0.8628, 0.9117, 0.9642, 0.6067, 0.9332] 
2023-10-26 16:37:15.073217: Epoch time: 4.0 s 
2023-10-26 16:37:16.206924:  
2023-10-26 16:37:16.207219: Epoch 851 
2023-10-26 16:37:16.207481: Current learning rate: 0.0018 
2023-10-26 16:37:20.204139: train_loss -0.8751 
2023-10-26 16:37:20.204709: val_loss -0.7747 
2023-10-26 16:37:20.205264: Pseudo dice [0.8617, 0.9113, 0.9637, 0.6178, 0.9329] 
2023-10-26 16:37:20.205590: Epoch time: 4.0 s 
2023-10-26 16:37:21.511860:  
2023-10-26 16:37:21.512191: Epoch 852 
2023-10-26 16:37:21.512438: Current learning rate: 0.00179 
2023-10-26 16:37:25.622034: train_loss -0.8729 
2023-10-26 16:37:25.622411: val_loss -0.7907 
2023-10-26 16:37:25.622671: Pseudo dice [0.8677, 0.9142, 0.9639, 0.647, 0.9391] 
2023-10-26 16:37:25.622913: Epoch time: 4.11 s 
2023-10-26 16:37:26.721981:  
2023-10-26 16:37:26.722277: Epoch 853 
2023-10-26 16:37:26.722526: Current learning rate: 0.00178 
2023-10-26 16:37:30.771044: train_loss -0.8717 
2023-10-26 16:37:30.771413: val_loss -0.7692 
2023-10-26 16:37:30.771686: Pseudo dice [0.8635, 0.911, 0.9642, 0.5262, 0.9288] 
2023-10-26 16:37:30.771929: Epoch time: 4.05 s 
2023-10-26 16:37:31.867640:  
2023-10-26 16:37:31.867954: Epoch 854 
2023-10-26 16:37:31.868198: Current learning rate: 0.00177 
2023-10-26 16:37:35.834778: train_loss -0.8728 
2023-10-26 16:37:35.835228: val_loss -0.7786 
2023-10-26 16:37:35.835504: Pseudo dice [0.8662, 0.9122, 0.965, 0.6761, 0.9339] 
2023-10-26 16:37:35.835751: Epoch time: 3.97 s 
2023-10-26 16:37:36.942611:  
2023-10-26 16:37:36.942913: Epoch 855 
2023-10-26 16:37:36.943175: Current learning rate: 0.00176 
2023-10-26 16:37:40.947785: train_loss -0.8792 
2023-10-26 16:37:40.948157: val_loss -0.777 
2023-10-26 16:37:40.948447: Pseudo dice [0.8646, 0.9126, 0.9642, 0.644, 0.9344] 
2023-10-26 16:37:40.948686: Epoch time: 4.01 s 
2023-10-26 16:37:42.084299:  
2023-10-26 16:37:42.084648: Epoch 856 
2023-10-26 16:37:42.084906: Current learning rate: 0.00175 
2023-10-26 16:37:46.095460: train_loss -0.8758 
2023-10-26 16:37:46.095827: val_loss -0.7803 
2023-10-26 16:37:46.096248: Pseudo dice [0.8694, 0.9134, 0.9667, 0.6905, 0.9389] 
2023-10-26 16:37:46.096708: Epoch time: 4.01 s 
2023-10-26 16:37:47.196716:  
2023-10-26 16:37:47.197025: Epoch 857 
2023-10-26 16:37:47.197275: Current learning rate: 0.00174 
2023-10-26 16:37:51.304514: train_loss -0.8711 
2023-10-26 16:37:51.304943: val_loss -0.7794 
2023-10-26 16:37:51.305293: Pseudo dice [0.8674, 0.908, 0.9639, 0.5224, 0.9339] 
2023-10-26 16:37:51.305627: Epoch time: 4.11 s 
2023-10-26 16:37:52.409678:  
2023-10-26 16:37:52.410040: Epoch 858 
2023-10-26 16:37:52.410365: Current learning rate: 0.00173 
2023-10-26 16:37:56.408643: train_loss -0.874 
2023-10-26 16:37:56.409000: val_loss -0.785 
2023-10-26 16:37:56.409304: Pseudo dice [0.8617, 0.9133, 0.9645, 0.5767, 0.9314] 
2023-10-26 16:37:56.409597: Epoch time: 4.0 s 
2023-10-26 16:37:57.690325:  
2023-10-26 16:37:57.690646: Epoch 859 
2023-10-26 16:37:57.690904: Current learning rate: 0.00172 
2023-10-26 16:38:01.660906: train_loss -0.8794 
2023-10-26 16:38:01.661285: val_loss -0.7933 
2023-10-26 16:38:01.661556: Pseudo dice [0.8681, 0.9136, 0.9644, 0.6284, 0.934] 
2023-10-26 16:38:01.661805: Epoch time: 3.97 s 
2023-10-26 16:38:02.753164:  
2023-10-26 16:38:02.753474: Epoch 860 
2023-10-26 16:38:02.753730: Current learning rate: 0.0017 
2023-10-26 16:38:06.721128: train_loss -0.8726 
2023-10-26 16:38:06.721527: val_loss -0.7801 
2023-10-26 16:38:06.721789: Pseudo dice [0.8654, 0.9119, 0.9649, 0.6586, 0.9376] 
2023-10-26 16:38:06.722037: Epoch time: 3.97 s 
2023-10-26 16:38:07.827136:  
2023-10-26 16:38:07.827458: Epoch 861 
2023-10-26 16:38:07.827721: Current learning rate: 0.00169 
2023-10-26 16:38:11.768559: train_loss -0.8753 
2023-10-26 16:38:11.769477: val_loss -0.7706 
2023-10-26 16:38:11.769917: Pseudo dice [0.8608, 0.9099, 0.9643, 0.6351, 0.9312] 
2023-10-26 16:38:11.770297: Epoch time: 3.94 s 
2023-10-26 16:38:12.913297:  
2023-10-26 16:38:12.913620: Epoch 862 
2023-10-26 16:38:12.913867: Current learning rate: 0.00168 
2023-10-26 16:38:16.964521: train_loss -0.8727 
2023-10-26 16:38:16.964897: val_loss -0.7673 
2023-10-26 16:38:16.965155: Pseudo dice [0.8618, 0.9109, 0.9642, 0.4806, 0.9304] 
2023-10-26 16:38:16.965381: Epoch time: 4.05 s 
2023-10-26 16:38:18.058795:  
2023-10-26 16:38:18.059109: Epoch 863 
2023-10-26 16:38:18.059360: Current learning rate: 0.00167 
2023-10-26 16:38:22.115767: train_loss -0.874 
2023-10-26 16:38:22.116230: val_loss -0.7833 
2023-10-26 16:38:22.116624: Pseudo dice [0.8668, 0.9119, 0.9646, 0.5833, 0.9319] 
2023-10-26 16:38:22.116970: Epoch time: 4.06 s 
2023-10-26 16:38:23.241132:  
2023-10-26 16:38:23.241449: Epoch 864 
2023-10-26 16:38:23.241704: Current learning rate: 0.00166 
2023-10-26 16:38:27.229029: train_loss -0.8771 
2023-10-26 16:38:27.229411: val_loss -0.7838 
2023-10-26 16:38:27.229680: Pseudo dice [0.8652, 0.91, 0.9626, 0.5201, 0.9344] 
2023-10-26 16:38:27.230020: Epoch time: 3.99 s 
2023-10-26 16:38:28.318770:  
2023-10-26 16:38:28.319065: Epoch 865 
2023-10-26 16:38:28.319328: Current learning rate: 0.00165 
2023-10-26 16:38:32.272089: train_loss -0.8721 
2023-10-26 16:38:32.272454: val_loss -0.7835 
2023-10-26 16:38:32.272734: Pseudo dice [0.8665, 0.9125, 0.9643, 0.5705, 0.9381] 
2023-10-26 16:38:32.272975: Epoch time: 3.95 s 
2023-10-26 16:38:33.524540:  
2023-10-26 16:38:33.524890: Epoch 866 
2023-10-26 16:38:33.525149: Current learning rate: 0.00164 
2023-10-26 16:38:37.617736: train_loss -0.8757 
2023-10-26 16:38:37.618114: val_loss -0.79 
2023-10-26 16:38:37.618400: Pseudo dice [0.8649, 0.9124, 0.9632, 0.5723, 0.9352] 
2023-10-26 16:38:37.618625: Epoch time: 4.09 s 
2023-10-26 16:38:38.732454:  
2023-10-26 16:38:38.732789: Epoch 867 
2023-10-26 16:38:38.733054: Current learning rate: 0.00163 
2023-10-26 16:38:42.838608: train_loss -0.8728 
2023-10-26 16:38:42.839285: val_loss -0.7591 
2023-10-26 16:38:42.839613: Pseudo dice [0.8621, 0.9093, 0.9629, 0.5352, 0.9319] 
2023-10-26 16:38:42.839910: Epoch time: 4.11 s 
2023-10-26 16:38:44.023132:  
2023-10-26 16:38:44.023455: Epoch 868 
2023-10-26 16:38:44.023718: Current learning rate: 0.00162 
2023-10-26 16:38:48.165846: train_loss -0.8664 
2023-10-26 16:38:48.166207: val_loss -0.7843 
2023-10-26 16:38:48.166492: Pseudo dice [0.8652, 0.9135, 0.9653, 0.5481, 0.9354] 
2023-10-26 16:38:48.166726: Epoch time: 4.14 s 
2023-10-26 16:38:49.265049:  
2023-10-26 16:38:49.265365: Epoch 869 
2023-10-26 16:38:49.265622: Current learning rate: 0.00161 
2023-10-26 16:38:53.469393: train_loss -0.8653 
2023-10-26 16:38:53.469768: val_loss -0.7778 
2023-10-26 16:38:53.470047: Pseudo dice [0.864, 0.916, 0.9649, 0.4207, 0.9361] 
2023-10-26 16:38:53.470309: Epoch time: 4.2 s 
2023-10-26 16:38:54.553687:  
2023-10-26 16:38:54.553990: Epoch 870 
2023-10-26 16:38:54.554241: Current learning rate: 0.00159 
2023-10-26 16:38:58.771471: train_loss -0.8657 
2023-10-26 16:38:58.771863: val_loss -0.794 
2023-10-26 16:38:58.772140: Pseudo dice [0.8681, 0.9121, 0.9632, 0.5352, 0.937] 
2023-10-26 16:38:58.772384: Epoch time: 4.22 s 
2023-10-26 16:38:59.858719:  
2023-10-26 16:38:59.859041: Epoch 871 
2023-10-26 16:38:59.859289: Current learning rate: 0.00158 
2023-10-26 16:39:03.993662: train_loss -0.8756 
2023-10-26 16:39:03.994041: val_loss -0.7829 
2023-10-26 16:39:03.994318: Pseudo dice [0.8614, 0.9135, 0.9651, 0.5005, 0.9328] 
2023-10-26 16:39:03.994552: Epoch time: 4.14 s 
2023-10-26 16:39:05.092701:  
2023-10-26 16:39:05.093016: Epoch 872 
2023-10-26 16:39:05.093263: Current learning rate: 0.00157 
2023-10-26 16:39:09.207369: train_loss -0.876 
2023-10-26 16:39:09.207757: val_loss -0.7659 
2023-10-26 16:39:09.208322: Pseudo dice [0.8638, 0.9109, 0.9663, 0.5041, 0.9325] 
2023-10-26 16:39:09.208620: Epoch time: 4.12 s 
2023-10-26 16:39:10.474611:  
2023-10-26 16:39:10.474933: Epoch 873 
2023-10-26 16:39:10.475198: Current learning rate: 0.00156 
2023-10-26 16:39:14.636788: train_loss -0.8792 
2023-10-26 16:39:14.637182: val_loss -0.7796 
2023-10-26 16:39:14.637454: Pseudo dice [0.8613, 0.9104, 0.9624, 0.5055, 0.9328] 
2023-10-26 16:39:14.637768: Epoch time: 4.16 s 
2023-10-26 16:39:15.820301:  
2023-10-26 16:39:15.820628: Epoch 874 
2023-10-26 16:39:15.820885: Current learning rate: 0.00155 
2023-10-26 16:39:19.936372: train_loss -0.8783 
2023-10-26 16:39:19.936800: val_loss -0.7863 
2023-10-26 16:39:19.937091: Pseudo dice [0.8651, 0.9079, 0.964, 0.5926, 0.9367] 
2023-10-26 16:39:19.937330: Epoch time: 4.12 s 
2023-10-26 16:39:21.087344:  
2023-10-26 16:39:21.087655: Epoch 875 
2023-10-26 16:39:21.087904: Current learning rate: 0.00154 
2023-10-26 16:39:25.067574: train_loss -0.8787 
2023-10-26 16:39:25.067928: val_loss -0.7748 
2023-10-26 16:39:25.068194: Pseudo dice [0.8575, 0.9057, 0.9632, 0.5589, 0.9329] 
2023-10-26 16:39:25.068421: Epoch time: 3.98 s 
2023-10-26 16:39:26.166332:  
2023-10-26 16:39:26.166635: Epoch 876 
2023-10-26 16:39:26.166884: Current learning rate: 0.00153 
2023-10-26 16:39:30.178718: train_loss -0.8728 
2023-10-26 16:39:30.179080: val_loss -0.7824 
2023-10-26 16:39:30.179348: Pseudo dice [0.8686, 0.9125, 0.9646, 0.5312, 0.9354] 
2023-10-26 16:39:30.179571: Epoch time: 4.01 s 
2023-10-26 16:39:31.399471:  
2023-10-26 16:39:31.399782: Epoch 877 
2023-10-26 16:39:31.400051: Current learning rate: 0.00152 
2023-10-26 16:39:35.448600: train_loss -0.8683 
2023-10-26 16:39:35.448954: val_loss -0.7713 
2023-10-26 16:39:35.449229: Pseudo dice [0.8612, 0.9112, 0.9655, 0.5247, 0.9334] 
2023-10-26 16:39:35.449462: Epoch time: 4.05 s 
2023-10-26 16:39:36.549802:  
2023-10-26 16:39:36.550119: Epoch 878 
2023-10-26 16:39:36.550370: Current learning rate: 0.00151 
2023-10-26 16:39:40.590643: train_loss -0.8693 
2023-10-26 16:39:40.591039: val_loss -0.7685 
2023-10-26 16:39:40.591302: Pseudo dice [0.8689, 0.9081, 0.9638, 0.5556, 0.9314] 
2023-10-26 16:39:40.591531: Epoch time: 4.04 s 
2023-10-26 16:39:41.878276:  
2023-10-26 16:39:41.878613: Epoch 879 
2023-10-26 16:39:41.878876: Current learning rate: 0.00149 
2023-10-26 16:39:45.959708: train_loss -0.8696 
2023-10-26 16:39:45.960115: val_loss -0.7692 
2023-10-26 16:39:45.960416: Pseudo dice [0.8629, 0.911, 0.9651, 0.5462, 0.9361] 
2023-10-26 16:39:45.960671: Epoch time: 4.08 s 
2023-10-26 16:39:47.061612:  
2023-10-26 16:39:47.061987: Epoch 880 
2023-10-26 16:39:47.062264: Current learning rate: 0.00148 
2023-10-26 16:39:51.187095: train_loss -0.8744 
2023-10-26 16:39:51.187512: val_loss -0.7831 
2023-10-26 16:39:51.187774: Pseudo dice [0.8665, 0.9141, 0.9654, 0.5678, 0.9364] 
2023-10-26 16:39:51.188025: Epoch time: 4.13 s 
2023-10-26 16:39:52.311002:  
2023-10-26 16:39:52.311307: Epoch 881 
2023-10-26 16:39:52.311593: Current learning rate: 0.00147 
2023-10-26 16:39:56.462409: train_loss -0.8688 
2023-10-26 16:39:56.462782: val_loss -0.7695 
2023-10-26 16:39:56.463064: Pseudo dice [0.8629, 0.913, 0.9638, 0.5559, 0.9372] 
2023-10-26 16:39:56.463309: Epoch time: 4.15 s 
2023-10-26 16:39:57.554438:  
2023-10-26 16:39:57.554757: Epoch 882 
2023-10-26 16:39:57.555035: Current learning rate: 0.00146 
2023-10-26 16:40:01.568740: train_loss -0.8862 
2023-10-26 16:40:01.569105: val_loss -0.7906 
2023-10-26 16:40:01.569367: Pseudo dice [0.8645, 0.912, 0.9635, 0.5039, 0.9334] 
2023-10-26 16:40:01.569615: Epoch time: 4.01 s 
2023-10-26 16:40:02.786186:  
2023-10-26 16:40:02.786498: Epoch 883 
2023-10-26 16:40:02.786756: Current learning rate: 0.00145 
2023-10-26 16:40:06.818864: train_loss -0.8721 
2023-10-26 16:40:06.819232: val_loss -0.7819 
2023-10-26 16:40:06.819493: Pseudo dice [0.8687, 0.9133, 0.9647, 0.5184, 0.9369] 
2023-10-26 16:40:06.819718: Epoch time: 4.03 s 
2023-10-26 16:40:07.913430:  
2023-10-26 16:40:07.913739: Epoch 884 
2023-10-26 16:40:07.913998: Current learning rate: 0.00144 
2023-10-26 16:40:11.933580: train_loss -0.8662 
2023-10-26 16:40:11.934109: val_loss -0.7899 
2023-10-26 16:40:11.934403: Pseudo dice [0.8637, 0.912, 0.9645, 0.4615, 0.9318] 
2023-10-26 16:40:11.934634: Epoch time: 4.02 s 
2023-10-26 16:40:13.060738:  
2023-10-26 16:40:13.061086: Epoch 885 
2023-10-26 16:40:13.061372: Current learning rate: 0.00143 
2023-10-26 16:40:17.079936: train_loss -0.8761 
2023-10-26 16:40:17.080333: val_loss -0.7967 
2023-10-26 16:40:17.080760: Pseudo dice [0.8667, 0.9136, 0.9639, 0.5088, 0.9375] 
2023-10-26 16:40:17.081046: Epoch time: 4.02 s 
2023-10-26 16:40:18.347001:  
2023-10-26 16:40:18.347479: Epoch 886 
2023-10-26 16:40:18.347727: Current learning rate: 0.00142 
2023-10-26 16:40:22.357035: train_loss -0.8814 
2023-10-26 16:40:22.357403: val_loss -0.757 
2023-10-26 16:40:22.357683: Pseudo dice [0.8627, 0.911, 0.9628, 0.4922, 0.9338] 
2023-10-26 16:40:22.357924: Epoch time: 4.01 s 
2023-10-26 16:40:23.453197:  
2023-10-26 16:40:23.453525: Epoch 887 
2023-10-26 16:40:23.453771: Current learning rate: 0.00141 
2023-10-26 16:40:27.476200: train_loss -0.8824 
2023-10-26 16:40:27.476615: val_loss -0.774 
2023-10-26 16:40:27.476927: Pseudo dice [0.8635, 0.9113, 0.9649, 0.4923, 0.9374] 
2023-10-26 16:40:27.477187: Epoch time: 4.02 s 
2023-10-26 16:40:28.670026:  
2023-10-26 16:40:28.670345: Epoch 888 
2023-10-26 16:40:28.670598: Current learning rate: 0.00139 
2023-10-26 16:40:32.748620: train_loss -0.8711 
2023-10-26 16:40:32.748991: val_loss -0.7798 
2023-10-26 16:40:32.749266: Pseudo dice [0.8625, 0.9093, 0.9631, 0.4743, 0.9333] 
2023-10-26 16:40:32.749506: Epoch time: 4.08 s 
2023-10-26 16:40:33.830171:  
2023-10-26 16:40:33.830480: Epoch 889 
2023-10-26 16:40:33.830725: Current learning rate: 0.00138 
2023-10-26 16:40:37.966934: train_loss -0.8702 
2023-10-26 16:40:37.967493: val_loss -0.7833 
2023-10-26 16:40:37.967761: Pseudo dice [0.864, 0.9136, 0.9627, 0.4884, 0.9336] 
2023-10-26 16:40:37.968037: Epoch time: 4.14 s 
2023-10-26 16:40:39.061386:  
2023-10-26 16:40:39.061683: Epoch 890 
2023-10-26 16:40:39.061939: Current learning rate: 0.00137 
2023-10-26 16:40:43.247120: train_loss -0.8718 
2023-10-26 16:40:43.247502: val_loss -0.773 
2023-10-26 16:40:43.247760: Pseudo dice [0.8606, 0.9118, 0.9645, 0.414, 0.9349] 
2023-10-26 16:40:43.248141: Epoch time: 4.19 s 
2023-10-26 16:40:44.370015:  
2023-10-26 16:40:44.370316: Epoch 891 
2023-10-26 16:40:44.370578: Current learning rate: 0.00136 
2023-10-26 16:40:48.411289: train_loss -0.8679 
2023-10-26 16:40:48.411707: val_loss -0.7906 
2023-10-26 16:40:48.412007: Pseudo dice [0.8651, 0.9123, 0.9648, 0.6581, 0.9337] 
2023-10-26 16:40:48.412260: Epoch time: 4.04 s 
2023-10-26 16:40:49.518920:  
2023-10-26 16:40:49.519228: Epoch 892 
2023-10-26 16:40:49.519482: Current learning rate: 0.00135 
2023-10-26 16:40:53.628410: train_loss -0.8752 
2023-10-26 16:40:53.628818: val_loss -0.792 
2023-10-26 16:40:53.629092: Pseudo dice [0.8613, 0.9105, 0.9645, 0.527, 0.9349] 
2023-10-26 16:40:53.629429: Epoch time: 4.11 s 
2023-10-26 16:40:54.873537:  
2023-10-26 16:40:54.873900: Epoch 893 
2023-10-26 16:40:54.874206: Current learning rate: 0.00134 
2023-10-26 16:40:58.940063: train_loss -0.8655 
2023-10-26 16:40:58.940429: val_loss -0.7852 
2023-10-26 16:40:58.940699: Pseudo dice [0.8652, 0.9078, 0.9637, 0.5082, 0.9338] 
2023-10-26 16:40:58.940955: Epoch time: 4.07 s 
2023-10-26 16:41:00.029081:  
2023-10-26 16:41:00.029377: Epoch 894 
2023-10-26 16:41:00.029638: Current learning rate: 0.00133 
2023-10-26 16:41:04.169377: train_loss -0.8741 
2023-10-26 16:41:04.169795: val_loss -0.7967 
2023-10-26 16:41:04.170079: Pseudo dice [0.8599, 0.9091, 0.9653, 0.4949, 0.9345] 
2023-10-26 16:41:04.170329: Epoch time: 4.14 s 
2023-10-26 16:41:05.264058:  
2023-10-26 16:41:05.264375: Epoch 895 
2023-10-26 16:41:05.264635: Current learning rate: 0.00132 
2023-10-26 16:41:09.420207: train_loss -0.8794 
2023-10-26 16:41:09.420613: val_loss -0.7574 
2023-10-26 16:41:09.420904: Pseudo dice [0.8608, 0.9097, 0.9649, 0.5138, 0.9336] 
2023-10-26 16:41:09.421147: Epoch time: 4.16 s 
2023-10-26 16:41:10.559355:  
2023-10-26 16:41:10.559688: Epoch 896 
2023-10-26 16:41:10.559983: Current learning rate: 0.0013 
2023-10-26 16:41:14.708457: train_loss -0.8714 
2023-10-26 16:41:14.708858: val_loss -0.7705 
2023-10-26 16:41:14.709132: Pseudo dice [0.8599, 0.9133, 0.9644, 0.5164, 0.9325] 
2023-10-26 16:41:14.709374: Epoch time: 4.15 s 
2023-10-26 16:41:15.858547:  
2023-10-26 16:41:15.858860: Epoch 897 
2023-10-26 16:41:15.859154: Current learning rate: 0.00129 
2023-10-26 16:41:19.952848: train_loss -0.8725 
2023-10-26 16:41:19.953220: val_loss -0.7641 
2023-10-26 16:41:19.953487: Pseudo dice [0.8625, 0.9124, 0.9643, 0.4539, 0.9333] 
2023-10-26 16:41:19.953721: Epoch time: 4.09 s 
2023-10-26 16:41:21.087377:  
2023-10-26 16:41:21.087684: Epoch 898 
2023-10-26 16:41:21.087953: Current learning rate: 0.00128 
2023-10-26 16:41:25.160102: train_loss -0.8689 
2023-10-26 16:41:25.160464: val_loss -0.7585 
2023-10-26 16:41:25.160734: Pseudo dice [0.8578, 0.9067, 0.9644, 0.5004, 0.9364] 
2023-10-26 16:41:25.161051: Epoch time: 4.07 s 
2023-10-26 16:41:26.319469:  
2023-10-26 16:41:26.319787: Epoch 899 
2023-10-26 16:41:26.320045: Current learning rate: 0.00127 
2023-10-26 16:41:30.333472: train_loss -0.874 
2023-10-26 16:41:30.333868: val_loss -0.786 
2023-10-26 16:41:30.334138: Pseudo dice [0.8706, 0.9111, 0.964, 0.5917, 0.9388] 
2023-10-26 16:41:30.334376: Epoch time: 4.01 s 
2023-10-26 16:41:31.801312:  
2023-10-26 16:41:31.801653: Epoch 900 
2023-10-26 16:41:31.801917: Current learning rate: 0.00126 
2023-10-26 16:41:35.988616: train_loss -0.879 
2023-10-26 16:41:35.989007: val_loss -0.7757 
2023-10-26 16:41:35.989282: Pseudo dice [0.8622, 0.9113, 0.9642, 0.5279, 0.9354] 
2023-10-26 16:41:35.989521: Epoch time: 4.19 s 
2023-10-26 16:41:37.112045:  
2023-10-26 16:41:37.112362: Epoch 901 
2023-10-26 16:41:37.112623: Current learning rate: 0.00125 
2023-10-26 16:41:41.344279: train_loss -0.8716 
2023-10-26 16:41:41.344640: val_loss -0.8041 
2023-10-26 16:41:41.344906: Pseudo dice [0.8601, 0.91, 0.964, 0.5446, 0.9362] 
2023-10-26 16:41:41.345138: Epoch time: 4.23 s 
2023-10-26 16:41:42.456740:  
2023-10-26 16:41:42.457071: Epoch 902 
2023-10-26 16:41:42.457336: Current learning rate: 0.00124 
2023-10-26 16:41:46.345010: train_loss -0.8883 
2023-10-26 16:41:46.345435: val_loss -0.7868 
2023-10-26 16:41:46.345713: Pseudo dice [0.8625, 0.9106, 0.9635, 0.5596, 0.9336] 
2023-10-26 16:41:46.345951: Epoch time: 3.89 s 
2023-10-26 16:41:47.477548:  
2023-10-26 16:41:47.477923: Epoch 903 
2023-10-26 16:41:47.478242: Current learning rate: 0.00122 
2023-10-26 16:41:51.658759: train_loss -0.8703 
2023-10-26 16:41:51.659167: val_loss -0.7985 
2023-10-26 16:41:51.659443: Pseudo dice [0.8613, 0.9108, 0.9628, 0.5312, 0.9388] 
2023-10-26 16:41:51.659688: Epoch time: 4.18 s 
2023-10-26 16:41:52.758218:  
2023-10-26 16:41:52.758528: Epoch 904 
2023-10-26 16:41:52.758789: Current learning rate: 0.00121 
2023-10-26 16:41:56.808615: train_loss -0.8711 
2023-10-26 16:41:56.808997: val_loss -0.755 
2023-10-26 16:41:56.809256: Pseudo dice [0.859, 0.9104, 0.9625, 0.4436, 0.9335] 
2023-10-26 16:41:56.809486: Epoch time: 4.05 s 
2023-10-26 16:41:57.982733:  
2023-10-26 16:41:57.983063: Epoch 905 
2023-10-26 16:41:57.983315: Current learning rate: 0.0012 
2023-10-26 16:42:02.136865: train_loss -0.8725 
2023-10-26 16:42:02.137257: val_loss -0.7854 
2023-10-26 16:42:02.137529: Pseudo dice [0.8589, 0.9105, 0.9643, 0.4796, 0.931] 
2023-10-26 16:42:02.137766: Epoch time: 4.15 s 
2023-10-26 16:42:03.442414:  
2023-10-26 16:42:03.442741: Epoch 906 
2023-10-26 16:42:03.443014: Current learning rate: 0.00119 
2023-10-26 16:42:07.650857: train_loss -0.8694 
2023-10-26 16:42:07.651222: val_loss -0.793 
2023-10-26 16:42:07.651486: Pseudo dice [0.8639, 0.9107, 0.9648, 0.4477, 0.9376] 
2023-10-26 16:42:07.651712: Epoch time: 4.21 s 
2023-10-26 16:42:08.747207:  
2023-10-26 16:42:08.747540: Epoch 907 
2023-10-26 16:42:08.747791: Current learning rate: 0.00118 
2023-10-26 16:42:13.019235: train_loss -0.8718 
2023-10-26 16:42:13.019757: val_loss -0.795 
2023-10-26 16:42:13.020096: Pseudo dice [0.8623, 0.9099, 0.964, 0.5219, 0.9366] 
2023-10-26 16:42:13.020392: Epoch time: 4.27 s 
2023-10-26 16:42:14.116724:  
2023-10-26 16:42:14.117052: Epoch 908 
2023-10-26 16:42:14.117322: Current learning rate: 0.00117 
2023-10-26 16:42:18.187850: train_loss -0.875 
2023-10-26 16:42:18.188246: val_loss -0.7583 
2023-10-26 16:42:18.188518: Pseudo dice [0.8573, 0.9071, 0.9646, 0.4523, 0.9341] 
2023-10-26 16:42:18.188761: Epoch time: 4.07 s 
2023-10-26 16:42:19.281210:  
2023-10-26 16:42:19.281544: Epoch 909 
2023-10-26 16:42:19.281798: Current learning rate: 0.00116 
2023-10-26 16:42:23.239449: train_loss -0.8721 
2023-10-26 16:42:23.239849: val_loss -0.7757 
2023-10-26 16:42:23.240119: Pseudo dice [0.8512, 0.9099, 0.9654, 0.4333, 0.9282] 
2023-10-26 16:42:23.240355: Epoch time: 3.96 s 
2023-10-26 16:42:24.328052:  
2023-10-26 16:42:24.328398: Epoch 910 
2023-10-26 16:42:24.328649: Current learning rate: 0.00115 
2023-10-26 16:42:28.453399: train_loss -0.8693 
2023-10-26 16:42:28.453819: val_loss -0.7804 
2023-10-26 16:42:28.454094: Pseudo dice [0.8651, 0.9097, 0.9643, 0.481, 0.9319] 
2023-10-26 16:42:28.454344: Epoch time: 4.13 s 
2023-10-26 16:42:29.547117:  
2023-10-26 16:42:29.547432: Epoch 911 
2023-10-26 16:42:29.547693: Current learning rate: 0.00113 
2023-10-26 16:42:33.750023: train_loss -0.8806 
2023-10-26 16:42:33.750437: val_loss -0.7718 
2023-10-26 16:42:33.750710: Pseudo dice [0.8649, 0.9095, 0.9647, 0.5286, 0.9333] 
2023-10-26 16:42:33.750947: Epoch time: 4.2 s 
2023-10-26 16:42:34.839882:  
2023-10-26 16:42:34.840196: Epoch 912 
2023-10-26 16:42:34.840448: Current learning rate: 0.00112 
2023-10-26 16:42:39.008384: train_loss -0.8736 
2023-10-26 16:42:39.008765: val_loss -0.7646 
2023-10-26 16:42:39.009570: Pseudo dice [0.863, 0.9097, 0.964, 0.6211, 0.93] 
2023-10-26 16:42:39.009953: Epoch time: 4.17 s 
2023-10-26 16:42:40.262388:  
2023-10-26 16:42:40.262702: Epoch 913 
2023-10-26 16:42:40.262947: Current learning rate: 0.00111 
2023-10-26 16:42:44.351207: train_loss -0.8686 
2023-10-26 16:42:44.351607: val_loss -0.7817 
2023-10-26 16:42:44.351866: Pseudo dice [0.8678, 0.9124, 0.9638, 0.6336, 0.9388] 
2023-10-26 16:42:44.352109: Epoch time: 4.09 s 
2023-10-26 16:42:45.438087:  
2023-10-26 16:42:45.438408: Epoch 914 
2023-10-26 16:42:45.438659: Current learning rate: 0.0011 
2023-10-26 16:42:49.622854: train_loss -0.8779 
2023-10-26 16:42:49.623261: val_loss -0.7679 
2023-10-26 16:42:49.623553: Pseudo dice [0.8596, 0.91, 0.9638, 0.6052, 0.9341] 
2023-10-26 16:42:49.623791: Epoch time: 4.19 s 
2023-10-26 16:42:50.759100:  
2023-10-26 16:42:50.762505: Epoch 915 
2023-10-26 16:42:50.762770: Current learning rate: 0.00109 
2023-10-26 16:42:54.907300: train_loss -0.8823 
2023-10-26 16:42:54.907677: val_loss -0.7733 
2023-10-26 16:42:54.907956: Pseudo dice [0.8624, 0.9114, 0.9645, 0.5482, 0.9324] 
2023-10-26 16:42:54.908198: Epoch time: 4.15 s 
2023-10-26 16:42:55.992412:  
2023-10-26 16:42:55.992737: Epoch 916 
2023-10-26 16:42:55.993007: Current learning rate: 0.00108 
2023-10-26 16:42:59.952006: train_loss -0.8802 
2023-10-26 16:42:59.952406: val_loss -0.7788 
2023-10-26 16:42:59.952693: Pseudo dice [0.8551, 0.9076, 0.9656, 0.5828, 0.9306] 
2023-10-26 16:42:59.952936: Epoch time: 3.96 s 
2023-10-26 16:43:01.132680:  
2023-10-26 16:43:01.132991: Epoch 917 
2023-10-26 16:43:01.133239: Current learning rate: 0.00106 
2023-10-26 16:43:05.139737: train_loss -0.8712 
2023-10-26 16:43:05.140120: val_loss -0.7747 
2023-10-26 16:43:05.140392: Pseudo dice [0.8642, 0.9141, 0.9649, 0.6265, 0.9337] 
2023-10-26 16:43:05.140624: Epoch time: 4.01 s 
2023-10-26 16:43:06.215946:  
2023-10-26 16:43:06.216252: Epoch 918 
2023-10-26 16:43:06.216505: Current learning rate: 0.00105 
2023-10-26 16:43:10.441999: train_loss -0.8736 
2023-10-26 16:43:10.442400: val_loss -0.7819 
2023-10-26 16:43:10.442660: Pseudo dice [0.8626, 0.9142, 0.9638, 0.5344, 0.9361] 
2023-10-26 16:43:10.442909: Epoch time: 4.23 s 
2023-10-26 16:43:11.527438:  
2023-10-26 16:43:11.527778: Epoch 919 
2023-10-26 16:43:11.528041: Current learning rate: 0.00104 
2023-10-26 16:43:15.694038: train_loss -0.8807 
2023-10-26 16:43:15.694449: val_loss -0.7869 
2023-10-26 16:43:15.694709: Pseudo dice [0.8654, 0.9108, 0.9653, 0.5822, 0.9371] 
2023-10-26 16:43:15.694945: Epoch time: 4.17 s 
2023-10-26 16:43:16.941750:  
2023-10-26 16:43:16.942068: Epoch 920 
2023-10-26 16:43:16.942319: Current learning rate: 0.00103 
2023-10-26 16:43:21.002965: train_loss -0.8756 
2023-10-26 16:43:21.003407: val_loss -0.7718 
2023-10-26 16:43:21.003702: Pseudo dice [0.8603, 0.9081, 0.9648, 0.5599, 0.9349] 
2023-10-26 16:43:21.003950: Epoch time: 4.06 s 
2023-10-26 16:43:22.085321:  
2023-10-26 16:43:22.085672: Epoch 921 
2023-10-26 16:43:22.085979: Current learning rate: 0.00102 
2023-10-26 16:43:26.345006: train_loss -0.8713 
2023-10-26 16:43:26.345501: val_loss -0.7598 
2023-10-26 16:43:26.345820: Pseudo dice [0.8607, 0.9113, 0.9641, 0.5182, 0.9329] 
2023-10-26 16:43:26.346065: Epoch time: 4.26 s 
2023-10-26 16:43:27.434239:  
2023-10-26 16:43:27.434541: Epoch 922 
2023-10-26 16:43:27.434796: Current learning rate: 0.00101 
2023-10-26 16:43:31.710821: train_loss -0.8667 
2023-10-26 16:43:31.711190: val_loss -0.7772 
2023-10-26 16:43:31.711463: Pseudo dice [0.8612, 0.9114, 0.9654, 0.5061, 0.9348] 
2023-10-26 16:43:31.711698: Epoch time: 4.28 s 
2023-10-26 16:43:32.796510:  
2023-10-26 16:43:32.796822: Epoch 923 
2023-10-26 16:43:32.797078: Current learning rate: 0.001 
2023-10-26 16:43:36.755737: train_loss -0.8658 
2023-10-26 16:43:36.756146: val_loss -0.7636 
2023-10-26 16:43:36.756407: Pseudo dice [0.861, 0.9088, 0.9645, 0.5211, 0.9333] 
2023-10-26 16:43:36.756660: Epoch time: 3.96 s 
2023-10-26 16:43:37.853936:  
2023-10-26 16:43:37.854229: Epoch 924 
2023-10-26 16:43:37.854485: Current learning rate: 0.00098 
2023-10-26 16:43:42.054625: train_loss -0.8808 
2023-10-26 16:43:42.054989: val_loss -0.7772 
2023-10-26 16:43:42.055254: Pseudo dice [0.8619, 0.9106, 0.9637, 0.4965, 0.9354] 
2023-10-26 16:43:42.055493: Epoch time: 4.2 s 
2023-10-26 16:43:43.149672:  
2023-10-26 16:43:43.149966: Epoch 925 
2023-10-26 16:43:43.150205: Current learning rate: 0.00097 
2023-10-26 16:43:47.350157: train_loss -0.8721 
2023-10-26 16:43:47.350516: val_loss -0.7723 
2023-10-26 16:43:47.350785: Pseudo dice [0.8571, 0.9085, 0.9651, 0.4974, 0.9335] 
2023-10-26 16:43:47.351026: Epoch time: 4.2 s 
2023-10-26 16:43:48.450144:  
2023-10-26 16:43:48.450442: Epoch 926 
2023-10-26 16:43:48.450693: Current learning rate: 0.00096 
2023-10-26 16:43:52.509758: train_loss -0.8804 
2023-10-26 16:43:52.510142: val_loss -0.7877 
2023-10-26 16:43:52.510400: Pseudo dice [0.8542, 0.9074, 0.9657, 0.4873, 0.9333] 
2023-10-26 16:43:52.510628: Epoch time: 4.06 s 
2023-10-26 16:43:53.774768:  
2023-10-26 16:43:53.775098: Epoch 927 
2023-10-26 16:43:53.775358: Current learning rate: 0.00095 
2023-10-26 16:43:57.902843: train_loss -0.8729 
2023-10-26 16:43:57.903208: val_loss -0.8133 
2023-10-26 16:43:57.903466: Pseudo dice [0.8592, 0.9114, 0.9659, 0.514, 0.9332] 
2023-10-26 16:43:57.903716: Epoch time: 4.13 s 
2023-10-26 16:43:59.091838:  
2023-10-26 16:43:59.092150: Epoch 928 
2023-10-26 16:43:59.092392: Current learning rate: 0.00094 
2023-10-26 16:44:03.354499: train_loss -0.8772 
2023-10-26 16:44:03.354913: val_loss -0.8048 
2023-10-26 16:44:03.355317: Pseudo dice [0.8663, 0.9087, 0.9654, 0.4845, 0.9374] 
2023-10-26 16:44:03.355634: Epoch time: 4.26 s 
2023-10-26 16:44:04.590622:  
2023-10-26 16:44:04.590935: Epoch 929 
2023-10-26 16:44:04.591205: Current learning rate: 0.00092 
2023-10-26 16:44:08.707395: train_loss -0.8705 
2023-10-26 16:44:08.707843: val_loss -0.7605 
2023-10-26 16:44:08.708236: Pseudo dice [0.8609, 0.9078, 0.9636, 0.5602, 0.9318] 
2023-10-26 16:44:08.708567: Epoch time: 4.12 s 
2023-10-26 16:44:09.814718:  
2023-10-26 16:44:09.815031: Epoch 930 
2023-10-26 16:44:09.815282: Current learning rate: 0.00091 
2023-10-26 16:44:13.974187: train_loss -0.8734 
2023-10-26 16:44:13.974560: val_loss -0.7701 
2023-10-26 16:44:13.974870: Pseudo dice [0.859, 0.9094, 0.965, 0.5404, 0.934] 
2023-10-26 16:44:13.975188: Epoch time: 4.16 s 
2023-10-26 16:44:15.090264:  
2023-10-26 16:44:15.090583: Epoch 931 
2023-10-26 16:44:15.090829: Current learning rate: 0.0009 
2023-10-26 16:44:19.272854: train_loss -0.8764 
2023-10-26 16:44:19.273233: val_loss -0.7684 
2023-10-26 16:44:19.273498: Pseudo dice [0.8637, 0.9094, 0.9646, 0.5375, 0.9331] 
2023-10-26 16:44:19.273729: Epoch time: 4.18 s 
2023-10-26 16:44:20.386950:  
2023-10-26 16:44:20.387247: Epoch 932 
2023-10-26 16:44:20.387492: Current learning rate: 0.00089 
2023-10-26 16:44:24.529695: train_loss -0.8822 
2023-10-26 16:44:24.530133: val_loss -0.7799 
2023-10-26 16:44:24.530411: Pseudo dice [0.863, 0.9086, 0.9636, 0.5831, 0.9316] 
2023-10-26 16:44:24.530677: Epoch time: 4.14 s 
2023-10-26 16:44:25.624458:  
2023-10-26 16:44:25.624762: Epoch 933 
2023-10-26 16:44:25.625022: Current learning rate: 0.00088 
2023-10-26 16:44:29.626916: train_loss -0.8784 
2023-10-26 16:44:29.627285: val_loss -0.7636 
2023-10-26 16:44:29.627563: Pseudo dice [0.8616, 0.9099, 0.9648, 0.5407, 0.9331] 
2023-10-26 16:44:29.627813: Epoch time: 4.0 s 
2023-10-26 16:44:30.885226:  
2023-10-26 16:44:30.885530: Epoch 934 
2023-10-26 16:44:30.885795: Current learning rate: 0.00087 
2023-10-26 16:44:35.034128: train_loss -0.8737 
2023-10-26 16:44:35.034626: val_loss -0.7611 
2023-10-26 16:44:35.034969: Pseudo dice [0.8563, 0.9108, 0.9638, 0.605, 0.9321] 
2023-10-26 16:44:35.035270: Epoch time: 4.15 s 
2023-10-26 16:44:36.139940:  
2023-10-26 16:44:36.140258: Epoch 935 
2023-10-26 16:44:36.140509: Current learning rate: 0.00085 
2023-10-26 16:44:40.439361: train_loss -0.8802 
2023-10-26 16:44:40.439744: val_loss -0.7723 
2023-10-26 16:44:40.440021: Pseudo dice [0.8634, 0.9124, 0.965, 0.4839, 0.9314] 
2023-10-26 16:44:40.440275: Epoch time: 4.3 s 
2023-10-26 16:44:41.555554:  
2023-10-26 16:44:41.555857: Epoch 936 
2023-10-26 16:44:41.556115: Current learning rate: 0.00084 
2023-10-26 16:44:45.677768: train_loss -0.882 
2023-10-26 16:44:45.678226: val_loss -0.7665 
2023-10-26 16:44:45.678710: Pseudo dice [0.8609, 0.9111, 0.9648, 0.5436, 0.9324] 
2023-10-26 16:44:45.679021: Epoch time: 4.12 s 
2023-10-26 16:44:46.818058:  
2023-10-26 16:44:46.818408: Epoch 937 
2023-10-26 16:44:46.818743: Current learning rate: 0.00083 
2023-10-26 16:44:50.953177: train_loss -0.8705 
2023-10-26 16:44:50.953746: val_loss -0.7918 
2023-10-26 16:44:50.954051: Pseudo dice [0.8655, 0.9074, 0.9656, 0.5901, 0.9374] 
2023-10-26 16:44:50.954344: Epoch time: 4.14 s 
2023-10-26 16:44:52.056263:  
2023-10-26 16:44:52.056611: Epoch 938 
2023-10-26 16:44:52.057066: Current learning rate: 0.00082 
2023-10-26 16:44:56.205283: train_loss -0.8698 
2023-10-26 16:44:56.205734: val_loss -0.7661 
2023-10-26 16:44:56.206007: Pseudo dice [0.8599, 0.9087, 0.9649, 0.5675, 0.932] 
2023-10-26 16:44:56.206373: Epoch time: 4.15 s 
2023-10-26 16:44:57.308444:  
2023-10-26 16:44:57.308744: Epoch 939 
2023-10-26 16:44:57.309002: Current learning rate: 0.00081 
2023-10-26 16:45:01.425111: train_loss -0.8706 
2023-10-26 16:45:01.425466: val_loss -0.7677 
2023-10-26 16:45:01.425717: Pseudo dice [0.8612, 0.9113, 0.9642, 0.555, 0.9335] 
2023-10-26 16:45:01.425946: Epoch time: 4.12 s 
2023-10-26 16:45:02.736128:  
2023-10-26 16:45:02.736445: Epoch 940 
2023-10-26 16:45:02.736691: Current learning rate: 0.00079 
2023-10-26 16:45:06.847218: train_loss -0.8817 
2023-10-26 16:45:06.847632: val_loss -0.7748 
2023-10-26 16:45:06.847936: Pseudo dice [0.8571, 0.908, 0.9637, 0.4958, 0.9291] 
2023-10-26 16:45:06.848273: Epoch time: 4.11 s 
2023-10-26 16:45:07.933990:  
2023-10-26 16:45:07.934291: Epoch 941 
2023-10-26 16:45:07.934548: Current learning rate: 0.00078 
2023-10-26 16:45:12.075498: train_loss -0.8794 
2023-10-26 16:45:12.075855: val_loss -0.7867 
2023-10-26 16:45:12.076116: Pseudo dice [0.8659, 0.9134, 0.9643, 0.5891, 0.9326] 
2023-10-26 16:45:12.076344: Epoch time: 4.14 s 
2023-10-26 16:45:13.165399:  
2023-10-26 16:45:13.165695: Epoch 942 
2023-10-26 16:45:13.165948: Current learning rate: 0.00077 
2023-10-26 16:45:17.002015: train_loss -0.8803 
2023-10-26 16:45:17.002383: val_loss -0.7712 
2023-10-26 16:45:17.002654: Pseudo dice [0.8612, 0.9103, 0.964, 0.546, 0.9325] 
2023-10-26 16:45:17.002891: Epoch time: 3.84 s 
2023-10-26 16:45:18.116078:  
2023-10-26 16:45:18.116428: Epoch 943 
2023-10-26 16:45:18.116731: Current learning rate: 0.00076 
2023-10-26 16:45:22.172356: train_loss -0.875 
2023-10-26 16:45:22.172794: val_loss -0.7816 
2023-10-26 16:45:22.173227: Pseudo dice [0.8657, 0.9085, 0.9641, 0.4484, 0.9361] 
2023-10-26 16:45:22.173582: Epoch time: 4.06 s 
2023-10-26 16:45:23.284235:  
2023-10-26 16:45:23.284540: Epoch 944 
2023-10-26 16:45:23.284784: Current learning rate: 0.00075 
2023-10-26 16:45:27.467999: train_loss -0.876 
2023-10-26 16:45:27.468374: val_loss -0.7775 
2023-10-26 16:45:27.468632: Pseudo dice [0.8603, 0.9111, 0.9649, 0.5693, 0.9342] 
2023-10-26 16:45:27.468861: Epoch time: 4.18 s 
2023-10-26 16:45:28.570423:  
2023-10-26 16:45:28.570722: Epoch 945 
2023-10-26 16:45:28.570973: Current learning rate: 0.00074 
2023-10-26 16:45:32.831725: train_loss -0.8742 
2023-10-26 16:45:32.832142: val_loss -0.7831 
2023-10-26 16:45:32.832423: Pseudo dice [0.8616, 0.9124, 0.9667, 0.4448, 0.9358] 
2023-10-26 16:45:32.832691: Epoch time: 4.26 s 
2023-10-26 16:45:33.926310:  
2023-10-26 16:45:33.926623: Epoch 946 
2023-10-26 16:45:33.926879: Current learning rate: 0.00072 
2023-10-26 16:45:37.891568: train_loss -0.8772 
2023-10-26 16:45:37.891938: val_loss -0.7731 
2023-10-26 16:45:37.892248: Pseudo dice [0.8581, 0.9064, 0.9635, 0.5726, 0.9343] 
2023-10-26 16:45:37.892510: Epoch time: 3.97 s 
2023-10-26 16:45:39.141708:  
2023-10-26 16:45:39.142021: Epoch 947 
2023-10-26 16:45:39.142262: Current learning rate: 0.00071 
2023-10-26 16:45:43.288285: train_loss -0.8735 
2023-10-26 16:45:43.288704: val_loss -0.7638 
2023-10-26 16:45:43.288968: Pseudo dice [0.8591, 0.9096, 0.9656, 0.4991, 0.9319] 
2023-10-26 16:45:43.289213: Epoch time: 4.15 s 
2023-10-26 16:45:44.391127:  
2023-10-26 16:45:44.391421: Epoch 948 
2023-10-26 16:45:44.391673: Current learning rate: 0.0007 
2023-10-26 16:45:48.438897: train_loss -0.878 
2023-10-26 16:45:48.439286: val_loss -0.7687 
2023-10-26 16:45:48.439553: Pseudo dice [0.8594, 0.9116, 0.9653, 0.5418, 0.9292] 
2023-10-26 16:45:48.439804: Epoch time: 4.05 s 
2023-10-26 16:45:49.558223:  
2023-10-26 16:45:49.558535: Epoch 949 
2023-10-26 16:45:49.558779: Current learning rate: 0.00069 
2023-10-26 16:45:53.652014: train_loss -0.8786 
2023-10-26 16:45:53.652392: val_loss -0.7754 
2023-10-26 16:45:53.652652: Pseudo dice [0.8602, 0.9098, 0.9645, 0.5889, 0.932] 
2023-10-26 16:45:53.652881: Epoch time: 4.09 s 
2023-10-26 16:45:54.852823:  
2023-10-26 16:45:54.853145: Epoch 950 
2023-10-26 16:45:54.853398: Current learning rate: 0.00067 
2023-10-26 16:45:58.960291: train_loss -0.8751 
2023-10-26 16:45:58.960725: val_loss -0.7793 
2023-10-26 16:45:58.960996: Pseudo dice [0.862, 0.9116, 0.9647, 0.5783, 0.9334] 
2023-10-26 16:45:58.961246: Epoch time: 4.11 s 
2023-10-26 16:46:00.041757:  
2023-10-26 16:46:00.042077: Epoch 951 
2023-10-26 16:46:00.042336: Current learning rate: 0.00066 
2023-10-26 16:46:04.270838: train_loss -0.881 
2023-10-26 16:46:04.271228: val_loss -0.7616 
2023-10-26 16:46:04.271512: Pseudo dice [0.8669, 0.9079, 0.962, 0.6825, 0.9318] 
2023-10-26 16:46:04.271745: Epoch time: 4.23 s 
2023-10-26 16:46:05.351021:  
2023-10-26 16:46:05.351317: Epoch 952 
2023-10-26 16:46:05.351565: Current learning rate: 0.00065 
2023-10-26 16:46:09.488957: train_loss -0.8829 
2023-10-26 16:46:09.489446: val_loss -0.7919 
2023-10-26 16:46:09.489811: Pseudo dice [0.8614, 0.9099, 0.9643, 0.5426, 0.9343] 
2023-10-26 16:46:09.490080: Epoch time: 4.14 s 
2023-10-26 16:46:10.679566:  
2023-10-26 16:46:10.679877: Epoch 953 
2023-10-26 16:46:10.680154: Current learning rate: 0.00064 
2023-10-26 16:46:14.788297: train_loss -0.8781 
2023-10-26 16:46:14.788674: val_loss -0.7795 
2023-10-26 16:46:14.788936: Pseudo dice [0.8674, 0.9134, 0.9637, 0.5998, 0.9349] 
2023-10-26 16:46:14.789161: Epoch time: 4.11 s 
2023-10-26 16:46:16.071540:  
2023-10-26 16:46:16.071834: Epoch 954 
2023-10-26 16:46:16.072078: Current learning rate: 0.00063 
2023-10-26 16:46:20.092015: train_loss -0.8713 
2023-10-26 16:46:20.092400: val_loss -0.7648 
2023-10-26 16:46:20.092675: Pseudo dice [0.8578, 0.913, 0.9644, 0.5496, 0.93] 
2023-10-26 16:46:20.092916: Epoch time: 4.02 s 
2023-10-26 16:46:21.206218:  
2023-10-26 16:46:21.206519: Epoch 955 
2023-10-26 16:46:21.206777: Current learning rate: 0.00061 
2023-10-26 16:46:25.190539: train_loss -0.8739 
2023-10-26 16:46:25.190985: val_loss -0.7719 
2023-10-26 16:46:25.191260: Pseudo dice [0.8615, 0.9099, 0.9666, 0.5628, 0.9332] 
2023-10-26 16:46:25.191497: Epoch time: 3.98 s 
2023-10-26 16:46:26.308259:  
2023-10-26 16:46:26.308573: Epoch 956 
2023-10-26 16:46:26.308824: Current learning rate: 0.0006 
2023-10-26 16:46:30.371708: train_loss -0.8761 
2023-10-26 16:46:30.372096: val_loss -0.7652 
2023-10-26 16:46:30.372355: Pseudo dice [0.8598, 0.909, 0.9633, 0.6072, 0.9344] 
2023-10-26 16:46:30.372594: Epoch time: 4.06 s 
2023-10-26 16:46:31.484452:  
2023-10-26 16:46:31.484748: Epoch 957 
2023-10-26 16:46:31.485003: Current learning rate: 0.00059 
2023-10-26 16:46:35.533733: train_loss -0.8741 
2023-10-26 16:46:35.534147: val_loss -0.7591 
2023-10-26 16:46:35.534421: Pseudo dice [0.8592, 0.9067, 0.9621, 0.5774, 0.9346] 
2023-10-26 16:46:35.534672: Epoch time: 4.05 s 
2023-10-26 16:46:36.699663:  
2023-10-26 16:46:36.699971: Epoch 958 
2023-10-26 16:46:36.700215: Current learning rate: 0.00058 
2023-10-26 16:46:40.812150: train_loss -0.875 
2023-10-26 16:46:40.812558: val_loss -0.7713 
2023-10-26 16:46:40.812845: Pseudo dice [0.8662, 0.9081, 0.9648, 0.4718, 0.9357] 
2023-10-26 16:46:40.813180: Epoch time: 4.11 s 
2023-10-26 16:46:41.930297:  
2023-10-26 16:46:41.930583: Epoch 959 
2023-10-26 16:46:41.930898: Current learning rate: 0.00056 
2023-10-26 16:46:46.041928: train_loss -0.8786 
2023-10-26 16:46:46.042482: val_loss -0.7867 
2023-10-26 16:46:46.042742: Pseudo dice [0.865, 0.9114, 0.9659, 0.4528, 0.9371] 
2023-10-26 16:46:46.042971: Epoch time: 4.11 s 
2023-10-26 16:46:47.334884:  
2023-10-26 16:46:47.335185: Epoch 960 
2023-10-26 16:46:47.335433: Current learning rate: 0.00055 
2023-10-26 16:46:51.511462: train_loss -0.879 
2023-10-26 16:46:51.511995: val_loss -0.7835 
2023-10-26 16:46:51.512345: Pseudo dice [0.8624, 0.9117, 0.9656, 0.4253, 0.9352] 
2023-10-26 16:46:51.512675: Epoch time: 4.18 s 
2023-10-26 16:46:52.647990:  
2023-10-26 16:46:52.648290: Epoch 961 
2023-10-26 16:46:52.648535: Current learning rate: 0.00054 
2023-10-26 16:46:56.837332: train_loss -0.8792 
2023-10-26 16:46:56.837815: val_loss -0.768 
2023-10-26 16:46:56.838141: Pseudo dice [0.8613, 0.9105, 0.965, 0.4741, 0.9321] 
2023-10-26 16:46:56.838498: Epoch time: 4.19 s 
2023-10-26 16:46:57.974442:  
2023-10-26 16:46:57.974750: Epoch 962 
2023-10-26 16:46:57.975016: Current learning rate: 0.00053 
2023-10-26 16:47:01.973349: train_loss -0.8801 
2023-10-26 16:47:01.973738: val_loss -0.7574 
2023-10-26 16:47:01.974014: Pseudo dice [0.863, 0.9047, 0.9655, 0.4484, 0.9333] 
2023-10-26 16:47:01.974244: Epoch time: 4.0 s 
2023-10-26 16:47:03.089159:  
2023-10-26 16:47:03.089460: Epoch 963 
2023-10-26 16:47:03.089703: Current learning rate: 0.00051 
2023-10-26 16:47:07.104337: train_loss -0.8762 
2023-10-26 16:47:07.104753: val_loss -0.766 
2023-10-26 16:47:07.105037: Pseudo dice [0.8586, 0.9091, 0.9638, 0.4291, 0.9324] 
2023-10-26 16:47:07.105279: Epoch time: 4.02 s 
2023-10-26 16:47:08.219234:  
2023-10-26 16:47:08.219548: Epoch 964 
2023-10-26 16:47:08.219793: Current learning rate: 0.0005 
2023-10-26 16:47:12.329386: train_loss -0.8786 
2023-10-26 16:47:12.330026: val_loss -0.7498 
2023-10-26 16:47:12.330424: Pseudo dice [0.8567, 0.9066, 0.9645, 0.4868, 0.9271] 
2023-10-26 16:47:12.330742: Epoch time: 4.11 s 
2023-10-26 16:47:13.475897:  
2023-10-26 16:47:13.476204: Epoch 965 
2023-10-26 16:47:13.476454: Current learning rate: 0.00049 
2023-10-26 16:47:17.554431: train_loss -0.8789 
2023-10-26 16:47:17.554801: val_loss -0.7657 
2023-10-26 16:47:17.555086: Pseudo dice [0.8623, 0.9085, 0.9648, 0.4688, 0.9338] 
2023-10-26 16:47:17.555319: Epoch time: 4.08 s 
2023-10-26 16:47:18.708490:  
2023-10-26 16:47:18.708785: Epoch 966 
2023-10-26 16:47:18.709048: Current learning rate: 0.00048 
2023-10-26 16:47:22.736310: train_loss -0.871 
2023-10-26 16:47:22.736740: val_loss -0.7737 
2023-10-26 16:47:22.737016: Pseudo dice [0.864, 0.91, 0.9647, 0.4622, 0.9337] 
2023-10-26 16:47:22.737264: Epoch time: 4.03 s 
2023-10-26 16:47:24.097556:  
2023-10-26 16:47:24.097870: Epoch 967 
2023-10-26 16:47:24.098347: Current learning rate: 0.00046 
2023-10-26 16:47:28.128211: train_loss -0.8728 
2023-10-26 16:47:28.128709: val_loss -0.7713 
2023-10-26 16:47:28.129054: Pseudo dice [0.8601, 0.9063, 0.9635, 0.4389, 0.9313] 
2023-10-26 16:47:28.129353: Epoch time: 4.03 s 
2023-10-26 16:47:29.270498:  
2023-10-26 16:47:29.270808: Epoch 968 
2023-10-26 16:47:29.271058: Current learning rate: 0.00045 
2023-10-26 16:47:33.256301: train_loss -0.8738 
2023-10-26 16:47:33.256703: val_loss -0.7828 
2023-10-26 16:47:33.256987: Pseudo dice [0.8644, 0.9093, 0.9651, 0.4175, 0.9334] 
2023-10-26 16:47:33.257221: Epoch time: 3.99 s 
2023-10-26 16:47:34.396395:  
2023-10-26 16:47:34.396717: Epoch 969 
2023-10-26 16:47:34.396974: Current learning rate: 0.00044 
2023-10-26 16:47:38.356995: train_loss -0.8871 
2023-10-26 16:47:38.357341: val_loss -0.7352 
2023-10-26 16:47:38.357589: Pseudo dice [0.8535, 0.9056, 0.9669, 0.4453, 0.9298] 
2023-10-26 16:47:38.357813: Epoch time: 3.96 s 
2023-10-26 16:47:39.474680:  
2023-10-26 16:47:39.474997: Epoch 970 
2023-10-26 16:47:39.475250: Current learning rate: 0.00043 
2023-10-26 16:47:43.437080: train_loss -0.8759 
2023-10-26 16:47:43.437469: val_loss -0.7596 
2023-10-26 16:47:43.437724: Pseudo dice [0.8616, 0.9061, 0.9636, 0.4618, 0.9299] 
2023-10-26 16:47:43.437966: Epoch time: 3.96 s 
2023-10-26 16:47:44.558073:  
2023-10-26 16:47:44.558384: Epoch 971 
2023-10-26 16:47:44.558643: Current learning rate: 0.00041 
2023-10-26 16:47:48.723263: train_loss -0.8759 
2023-10-26 16:47:48.723656: val_loss -0.7707 
2023-10-26 16:47:48.723921: Pseudo dice [0.8593, 0.9072, 0.9642, 0.4349, 0.9328] 
2023-10-26 16:47:48.724162: Epoch time: 4.17 s 
2023-10-26 16:47:49.877214:  
2023-10-26 16:47:49.877518: Epoch 972 
2023-10-26 16:47:49.877774: Current learning rate: 0.0004 
2023-10-26 16:47:53.951069: train_loss -0.8776 
2023-10-26 16:47:53.951421: val_loss -0.7634 
2023-10-26 16:47:53.951683: Pseudo dice [0.8568, 0.9108, 0.9654, 0.4899, 0.9338] 
2023-10-26 16:47:53.951927: Epoch time: 4.07 s 
2023-10-26 16:47:55.094145:  
2023-10-26 16:47:55.094450: Epoch 973 
2023-10-26 16:47:55.094744: Current learning rate: 0.00039 
2023-10-26 16:47:59.071314: train_loss -0.8784 
2023-10-26 16:47:59.071671: val_loss -0.7593 
2023-10-26 16:47:59.071948: Pseudo dice [0.8619, 0.9115, 0.9642, 0.4775, 0.9331] 
2023-10-26 16:47:59.072187: Epoch time: 3.98 s 
2023-10-26 16:48:00.379551:  
2023-10-26 16:48:00.379859: Epoch 974 
2023-10-26 16:48:00.380131: Current learning rate: 0.00037 
2023-10-26 16:48:04.348256: train_loss -0.8799 
2023-10-26 16:48:04.348665: val_loss -0.7669 
2023-10-26 16:48:04.348932: Pseudo dice [0.862, 0.9059, 0.9653, 0.3938, 0.9327] 
2023-10-26 16:48:04.349170: Epoch time: 3.97 s 
2023-10-26 16:48:05.491123:  
2023-10-26 16:48:05.491441: Epoch 975 
2023-10-26 16:48:05.491713: Current learning rate: 0.00036 
2023-10-26 16:48:09.389943: train_loss -0.8786 
2023-10-26 16:48:09.390428: val_loss -0.7536 
2023-10-26 16:48:09.390823: Pseudo dice [0.8572, 0.9067, 0.9655, 0.4084, 0.9331] 
2023-10-26 16:48:09.391173: Epoch time: 3.9 s 
2023-10-26 16:48:10.535200:  
2023-10-26 16:48:10.535495: Epoch 976 
2023-10-26 16:48:10.535736: Current learning rate: 0.00035 
2023-10-26 16:48:14.381997: train_loss -0.8742 
2023-10-26 16:48:14.382553: val_loss -0.764 
2023-10-26 16:48:14.382952: Pseudo dice [0.8638, 0.9092, 0.9645, 0.4718, 0.9341] 
2023-10-26 16:48:14.383272: Epoch time: 3.85 s 
2023-10-26 16:48:15.500616:  
2023-10-26 16:48:15.500913: Epoch 977 
2023-10-26 16:48:15.501164: Current learning rate: 0.00034 
2023-10-26 16:48:19.360118: train_loss -0.8825 
2023-10-26 16:48:19.360504: val_loss -0.7794 
2023-10-26 16:48:19.360775: Pseudo dice [0.8661, 0.9111, 0.965, 0.4911, 0.9344] 
2023-10-26 16:48:19.361016: Epoch time: 3.86 s 
2023-10-26 16:48:20.479671:  
2023-10-26 16:48:20.479987: Epoch 978 
2023-10-26 16:48:20.480239: Current learning rate: 0.00032 
2023-10-26 16:48:24.391846: train_loss -0.88 
2023-10-26 16:48:24.392211: val_loss -0.7655 
2023-10-26 16:48:24.392500: Pseudo dice [0.8661, 0.9103, 0.965, 0.4349, 0.9355] 
2023-10-26 16:48:24.392738: Epoch time: 3.91 s 
2023-10-26 16:48:25.500478:  
2023-10-26 16:48:25.500775: Epoch 979 
2023-10-26 16:48:25.501023: Current learning rate: 0.00031 
2023-10-26 16:48:29.478056: train_loss -0.8803 
2023-10-26 16:48:29.478434: val_loss -0.7662 
2023-10-26 16:48:29.478692: Pseudo dice [0.8593, 0.9085, 0.9652, 0.5073, 0.9358] 
2023-10-26 16:48:29.478931: Epoch time: 3.98 s 
2023-10-26 16:48:30.789451:  
2023-10-26 16:48:30.789766: Epoch 980 
2023-10-26 16:48:30.790025: Current learning rate: 0.0003 
2023-10-26 16:48:34.676165: train_loss -0.8834 
2023-10-26 16:48:34.676593: val_loss -0.7656 
2023-10-26 16:48:34.676876: Pseudo dice [0.8602, 0.9074, 0.9657, 0.4426, 0.9317] 
2023-10-26 16:48:34.677144: Epoch time: 3.89 s 
2023-10-26 16:48:35.785242:  
2023-10-26 16:48:35.785543: Epoch 981 
2023-10-26 16:48:35.785801: Current learning rate: 0.00028 
2023-10-26 16:48:39.675262: train_loss -0.8736 
2023-10-26 16:48:39.675647: val_loss -0.7715 
2023-10-26 16:48:39.675921: Pseudo dice [0.8612, 0.9093, 0.9655, 0.4513, 0.9336] 
2023-10-26 16:48:39.676175: Epoch time: 3.89 s 
2023-10-26 16:48:40.814516:  
2023-10-26 16:48:40.814828: Epoch 982 
2023-10-26 16:48:40.815083: Current learning rate: 0.00027 
2023-10-26 16:48:44.699252: train_loss -0.8696 
2023-10-26 16:48:44.699695: val_loss -0.7569 
2023-10-26 16:48:44.699974: Pseudo dice [0.8605, 0.9085, 0.9637, 0.479, 0.9343] 
2023-10-26 16:48:44.700233: Epoch time: 3.89 s 
2023-10-26 16:48:45.818513:  
2023-10-26 16:48:45.818829: Epoch 983 
2023-10-26 16:48:45.819077: Current learning rate: 0.00026 
2023-10-26 16:48:49.746064: train_loss -0.8806 
2023-10-26 16:48:49.746433: val_loss -0.777 
2023-10-26 16:48:49.746700: Pseudo dice [0.8617, 0.9075, 0.9645, 0.4882, 0.9335] 
2023-10-26 16:48:49.746937: Epoch time: 3.93 s 
2023-10-26 16:48:50.933237:  
2023-10-26 16:48:50.933540: Epoch 984 
2023-10-26 16:48:50.933788: Current learning rate: 0.00024 
2023-10-26 16:48:54.739044: train_loss -0.8785 
2023-10-26 16:48:54.739458: val_loss -0.7736 
2023-10-26 16:48:54.739925: Pseudo dice [0.8569, 0.906, 0.9643, 0.4464, 0.933] 
2023-10-26 16:48:54.740177: Epoch time: 3.81 s 
2023-10-26 16:48:55.879047:  
2023-10-26 16:48:55.879350: Epoch 985 
2023-10-26 16:48:55.879596: Current learning rate: 0.00023 
2023-10-26 16:48:59.754284: train_loss -0.8823 
2023-10-26 16:48:59.754636: val_loss -0.7744 
2023-10-26 16:48:59.755003: Pseudo dice [0.8643, 0.9082, 0.9638, 0.543, 0.9356] 
2023-10-26 16:48:59.755250: Epoch time: 3.88 s 
2023-10-26 16:49:00.879006:  
2023-10-26 16:49:00.879310: Epoch 986 
2023-10-26 16:49:00.879554: Current learning rate: 0.00021 
2023-10-26 16:49:04.815425: train_loss -0.8766 
2023-10-26 16:49:04.815830: val_loss -0.77 
2023-10-26 16:49:04.816108: Pseudo dice [0.8593, 0.9086, 0.9652, 0.4514, 0.9322] 
2023-10-26 16:49:04.816346: Epoch time: 3.94 s 
2023-10-26 16:49:06.100332:  
2023-10-26 16:49:06.100656: Epoch 987 
2023-10-26 16:49:06.100916: Current learning rate: 0.0002 
2023-10-26 16:49:09.960619: train_loss -0.8909 
2023-10-26 16:49:09.961044: val_loss -0.7775 
2023-10-26 16:49:09.961314: Pseudo dice [0.8615, 0.9061, 0.9648, 0.4914, 0.9321] 
2023-10-26 16:49:09.961555: Epoch time: 3.86 s 
2023-10-26 16:49:11.111598:  
2023-10-26 16:49:11.111934: Epoch 988 
2023-10-26 16:49:11.112181: Current learning rate: 0.00019 
2023-10-26 16:49:14.929309: train_loss -0.8824 
2023-10-26 16:49:14.929671: val_loss -0.7806 
2023-10-26 16:49:14.929947: Pseudo dice [0.8637, 0.9094, 0.9644, 0.514, 0.9336] 
2023-10-26 16:49:14.930181: Epoch time: 3.82 s 
2023-10-26 16:49:16.044352:  
2023-10-26 16:49:16.044669: Epoch 989 
2023-10-26 16:49:16.044953: Current learning rate: 0.00017 
2023-10-26 16:49:19.957669: train_loss -0.881 
2023-10-26 16:49:19.958040: val_loss -0.7626 
2023-10-26 16:49:19.958306: Pseudo dice [0.8583, 0.9072, 0.9649, 0.4799, 0.9313] 
2023-10-26 16:49:19.958562: Epoch time: 3.91 s 
2023-10-26 16:49:21.080464:  
2023-10-26 16:49:21.080777: Epoch 990 
2023-10-26 16:49:21.081027: Current learning rate: 0.00016 
2023-10-26 16:49:24.876971: train_loss -0.8778 
2023-10-26 16:49:24.877335: val_loss -0.7764 
2023-10-26 16:49:24.877603: Pseudo dice [0.8634, 0.9078, 0.9645, 0.5117, 0.9348] 
2023-10-26 16:49:24.877861: Epoch time: 3.8 s 
2023-10-26 16:49:25.986908:  
2023-10-26 16:49:25.987224: Epoch 991 
2023-10-26 16:49:25.987484: Current learning rate: 0.00014 
2023-10-26 16:49:29.985102: train_loss -0.8763 
2023-10-26 16:49:29.985465: val_loss -0.7768 
2023-10-26 16:49:29.985726: Pseudo dice [0.8617, 0.9112, 0.9628, 0.4434, 0.9335] 
2023-10-26 16:49:29.985962: Epoch time: 4.0 s 
2023-10-26 16:49:31.116692:  
2023-10-26 16:49:31.117016: Epoch 992 
2023-10-26 16:49:31.117262: Current learning rate: 0.00013 
2023-10-26 16:49:35.006203: train_loss -0.8824 
2023-10-26 16:49:35.006618: val_loss -0.7761 
2023-10-26 16:49:35.006882: Pseudo dice [0.8628, 0.9109, 0.9648, 0.4716, 0.9348] 
2023-10-26 16:49:35.007138: Epoch time: 3.89 s 
2023-10-26 16:49:36.332893:  
2023-10-26 16:49:36.333273: Epoch 993 
2023-10-26 16:49:36.333595: Current learning rate: 0.00011 
2023-10-26 16:49:40.142043: train_loss -0.876 
2023-10-26 16:49:40.142468: val_loss -0.7386 
2023-10-26 16:49:40.142749: Pseudo dice [0.8603, 0.9059, 0.9639, 0.4882, 0.9295] 
2023-10-26 16:49:40.143002: Epoch time: 3.81 s 
2023-10-26 16:49:41.254605:  
2023-10-26 16:49:41.254978: Epoch 994 
2023-10-26 16:49:41.255241: Current learning rate: 0.0001 
2023-10-26 16:49:45.147822: train_loss -0.8884 
2023-10-26 16:49:45.148193: val_loss -0.773 
2023-10-26 16:49:45.148458: Pseudo dice [0.8623, 0.9095, 0.9645, 0.5113, 0.9302] 
2023-10-26 16:49:45.148692: Epoch time: 3.89 s 
2023-10-26 16:49:46.279160:  
2023-10-26 16:49:46.279517: Epoch 995 
2023-10-26 16:49:46.279836: Current learning rate: 8e-05 
2023-10-26 16:49:50.203897: train_loss -0.8828 
2023-10-26 16:49:50.204276: val_loss -0.7642 
2023-10-26 16:49:50.204542: Pseudo dice [0.8599, 0.9083, 0.9638, 0.4826, 0.9327] 
2023-10-26 16:49:50.204780: Epoch time: 3.93 s 
2023-10-26 16:49:51.329279:  
2023-10-26 16:49:51.329597: Epoch 996 
2023-10-26 16:49:51.329848: Current learning rate: 7e-05 
2023-10-26 16:49:55.088166: train_loss -0.8848 
2023-10-26 16:49:55.088585: val_loss -0.7669 
2023-10-26 16:49:55.088944: Pseudo dice [0.8603, 0.9096, 0.9648, 0.478, 0.9341] 
2023-10-26 16:49:55.089177: Epoch time: 3.76 s 
2023-10-26 16:49:56.201371:  
2023-10-26 16:49:56.201722: Epoch 997 
2023-10-26 16:49:56.202038: Current learning rate: 5e-05 
2023-10-26 16:49:59.944606: train_loss -0.8773 
2023-10-26 16:49:59.944978: val_loss -0.7708 
2023-10-26 16:49:59.945244: Pseudo dice [0.8592, 0.9096, 0.9653, 0.4374, 0.9319] 
2023-10-26 16:49:59.945493: Epoch time: 3.74 s 
2023-10-26 16:50:01.074938:  
2023-10-26 16:50:01.075238: Epoch 998 
2023-10-26 16:50:01.075483: Current learning rate: 4e-05 
2023-10-26 16:50:04.740702: train_loss -0.8799 
2023-10-26 16:50:04.741069: val_loss -0.7647 
2023-10-26 16:50:04.741322: Pseudo dice [0.8635, 0.9097, 0.964, 0.4785, 0.9317] 
2023-10-26 16:50:04.741556: Epoch time: 3.67 s 
2023-10-26 16:50:05.854062:  
2023-10-26 16:50:05.854367: Epoch 999 
2023-10-26 16:50:05.854608: Current learning rate: 2e-05 
2023-10-26 16:50:09.546326: train_loss -0.8766 
2023-10-26 16:50:09.546742: val_loss -0.76 
2023-10-26 16:50:09.547190: Pseudo dice [0.8641, 0.9111, 0.9647, 0.5242, 0.9332] 
2023-10-26 16:50:09.547497: Epoch time: 3.69 s 
2023-10-26 16:50:11.002798: Training done. 
2023-10-26 16:50:11.014326: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-26 16:50:11.015062: The split file contains 5 splits. 
2023-10-26 16:50:11.015359: Desired fold for training: 3 
2023-10-26 16:50:11.015616: This split has 35 training and 9 validation cases. 
2023-10-26 16:50:11.015998: predicting t2_haste_tra_2_2mm_002 
2023-10-26 16:50:11.194058: predicting t2_haste_tra_2_2mm_004 
2023-10-26 16:50:11.218947: predicting t2_haste_tra_2_2mm_005 
2023-10-26 16:50:11.243315: predicting t2_haste_tra_2_2mm_012 
2023-10-26 16:50:11.267062: predicting t2_haste_tra_2_2mm_102 
2023-10-26 16:50:11.291021: predicting t2_haste_tra_2_2mm_104 
2023-10-26 16:50:11.315288: predicting t2_haste_tra_2_2mm_105 
2023-10-26 16:50:11.339067: predicting t2_haste_tra_2_2mm_112 
2023-10-26 16:50:11.362964: predicting t2_haste_tra_2_2mm_205 
2023-10-26 16:50:23.960601: Validation complete 
2023-10-26 16:50:23.960887: Mean Validation Dice:  0.780162633266824 
