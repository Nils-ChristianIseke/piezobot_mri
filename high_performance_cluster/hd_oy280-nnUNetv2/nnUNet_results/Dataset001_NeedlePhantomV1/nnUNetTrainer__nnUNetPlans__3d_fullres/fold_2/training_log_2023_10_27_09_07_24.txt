
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [24, 56, 40], 'median_image_size_in_voxels': [22.0, 56.0, 36.0], 'spacing': [2.419999837875366, 1.46875, 1.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [2, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_NeedlePhantomV1', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.419999837875366, 1.46875, 1.46875], 'original_median_shape_after_transp': [22, 56, 36], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1610.0, 'mean': 465.1097412109375, 'median': 478.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 1246.0, 'std': 241.4937744140625}}} 
 
2023-10-27 09:07:25.338184: unpacking dataset... 
2023-10-27 09:07:29.628184: unpacking done... 
2023-10-27 09:07:29.629142: do_dummy_2d_data_aug: False 
2023-10-27 09:07:29.630536: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-27 09:07:29.631532: The split file contains 5 splits. 
2023-10-27 09:07:29.632037: Desired fold for training: 2 
2023-10-27 09:07:29.632287: This split has 36 training and 8 validation cases. 
2023-10-27 09:07:30.580641: Unable to plot network architecture: 
2023-10-27 09:07:30.581134: 'torch._C.Node' object is not subscriptable 
2023-10-27 09:07:30.616011:  
2023-10-27 09:07:30.616539: Epoch 950 
2023-10-27 09:07:30.617072: Current learning rate: 0.00067 
2023-10-27 09:07:36.479557: train_loss -0.9141 
2023-10-27 09:07:36.480053: val_loss -0.6742 
2023-10-27 09:07:36.480365: Pseudo dice [0.8341, 0.9216, 0.9672, 0.3416, 0.673] 
2023-10-27 09:07:36.480686: Epoch time: 5.86 s 
2023-10-27 09:07:37.700794:  
2023-10-27 09:07:37.701223: Epoch 951 
2023-10-27 09:07:37.701523: Current learning rate: 0.00066 
2023-10-27 09:07:42.412765: train_loss -0.9169 
2023-10-27 09:07:42.413340: val_loss -0.6995 
2023-10-27 09:07:42.413633: Pseudo dice [0.8257, 0.9221, 0.9666, 0.2458, 0.8038] 
2023-10-27 09:07:42.413886: Epoch time: 4.71 s 
2023-10-27 09:07:43.632385:  
2023-10-27 09:07:43.632796: Epoch 952 
2023-10-27 09:07:43.633098: Current learning rate: 0.00065 
2023-10-27 09:07:48.585483: train_loss -0.9158 
2023-10-27 09:07:48.585963: val_loss -0.6905 
2023-10-27 09:07:48.586246: Pseudo dice [0.8351, 0.9228, 0.9659, 0.3501, 0.7457] 
2023-10-27 09:07:48.586533: Epoch time: 4.95 s 
2023-10-27 09:07:50.017006:  
2023-10-27 09:07:50.017583: Epoch 953 
2023-10-27 09:07:50.017879: Current learning rate: 0.00064 
2023-10-27 09:07:54.434256: train_loss -0.9174 
2023-10-27 09:07:54.434866: val_loss -0.7099 
2023-10-27 09:07:54.435198: Pseudo dice [0.8284, 0.9255, 0.9655, 0.323, 0.7403] 
2023-10-27 09:07:54.435518: Epoch time: 4.42 s 
2023-10-27 09:07:55.653186:  
2023-10-27 09:07:55.653654: Epoch 954 
2023-10-27 09:07:55.653919: Current learning rate: 0.00063 
2023-10-27 09:07:59.935136: train_loss -0.9152 
2023-10-27 09:07:59.935635: val_loss -0.6882 
2023-10-27 09:07:59.935924: Pseudo dice [0.8293, 0.9238, 0.9654, 0.3389, 0.7695] 
2023-10-27 09:07:59.936177: Epoch time: 4.28 s 
2023-10-27 09:08:01.158705:  
2023-10-27 09:08:01.159187: Epoch 955 
2023-10-27 09:08:01.159549: Current learning rate: 0.00061 
2023-10-27 09:08:05.486322: train_loss -0.9159 
2023-10-27 09:08:05.486831: val_loss -0.669 
2023-10-27 09:08:05.487130: Pseudo dice [0.8346, 0.9231, 0.9654, 0.25, 0.6707] 
2023-10-27 09:08:05.487481: Epoch time: 4.33 s 
2023-10-27 09:08:06.698198:  
2023-10-27 09:08:06.698574: Epoch 956 
2023-10-27 09:08:06.698843: Current learning rate: 0.0006 
2023-10-27 09:08:10.930284: train_loss -0.9205 
2023-10-27 09:08:10.930826: val_loss -0.6896 
2023-10-27 09:08:10.931124: Pseudo dice [0.8361, 0.925, 0.9662, 0.3958, 0.7332] 
2023-10-27 09:08:10.931398: Epoch time: 4.23 s 
2023-10-27 09:08:12.138166:  
2023-10-27 09:08:12.138552: Epoch 957 
2023-10-27 09:08:12.138821: Current learning rate: 0.00059 
2023-10-27 09:08:16.453895: train_loss -0.92 
2023-10-27 09:08:16.454391: val_loss -0.644 
2023-10-27 09:08:16.454728: Pseudo dice [0.8359, 0.919, 0.966, 0.2296, 0.7362] 
2023-10-27 09:08:16.454979: Epoch time: 4.32 s 
2023-10-27 09:08:17.661077:  
2023-10-27 09:08:17.661398: Epoch 958 
2023-10-27 09:08:17.661674: Current learning rate: 0.00058 
2023-10-27 09:08:21.905162: train_loss -0.923 
2023-10-27 09:08:21.909061: val_loss -0.6583 
2023-10-27 09:08:21.909433: Pseudo dice [0.8293, 0.9205, 0.9651, 0.179, 0.7336] 
2023-10-27 09:08:21.909782: Epoch time: 4.24 s 
2023-10-27 09:08:23.116487:  
2023-10-27 09:08:23.119985: Epoch 959 
2023-10-27 09:08:23.120656: Current learning rate: 0.00056 
2023-10-27 09:08:27.403369: train_loss -0.9139 
2023-10-27 09:08:27.407742: val_loss -0.6557 
2023-10-27 09:08:27.408547: Pseudo dice [0.8329, 0.9188, 0.9653, 0.2379, 0.748] 
2023-10-27 09:08:27.408836: Epoch time: 4.29 s 
2023-10-27 09:08:28.790842:  
2023-10-27 09:08:28.794573: Epoch 960 
2023-10-27 09:08:28.795022: Current learning rate: 0.00055 
2023-10-27 09:08:33.047484: train_loss -0.9153 
2023-10-27 09:08:33.048029: val_loss -0.6939 
2023-10-27 09:08:33.048329: Pseudo dice [0.8308, 0.9223, 0.967, 0.2493, 0.7751] 
2023-10-27 09:08:33.048918: Epoch time: 4.26 s 
2023-10-27 09:08:34.264484:  
2023-10-27 09:08:34.264866: Epoch 961 
2023-10-27 09:08:34.265149: Current learning rate: 0.00054 
2023-10-27 09:08:38.597355: train_loss -0.9165 
2023-10-27 09:08:38.597928: val_loss -0.6579 
2023-10-27 09:08:38.598235: Pseudo dice [0.8285, 0.9195, 0.9667, 0.1739, 0.7326] 
2023-10-27 09:08:38.598504: Epoch time: 4.33 s 
2023-10-27 09:08:39.807588:  
2023-10-27 09:08:39.807937: Epoch 962 
2023-10-27 09:08:39.808215: Current learning rate: 0.00053 
2023-10-27 09:08:44.112275: train_loss -0.9166 
2023-10-27 09:08:44.112763: val_loss -0.6827 
2023-10-27 09:08:44.113119: Pseudo dice [0.839, 0.9251, 0.9671, 0.2273, 0.7568] 
2023-10-27 09:08:44.113425: Epoch time: 4.31 s 
2023-10-27 09:08:45.348740:  
2023-10-27 09:08:45.349090: Epoch 963 
2023-10-27 09:08:45.349358: Current learning rate: 0.00051 
2023-10-27 09:08:49.607444: train_loss -0.9175 
2023-10-27 09:08:49.607894: val_loss -0.6688 
2023-10-27 09:08:49.608186: Pseudo dice [0.8342, 0.9208, 0.9663, 0.3074, 0.7209] 
2023-10-27 09:08:49.608478: Epoch time: 4.26 s 
2023-10-27 09:08:50.807516:  
2023-10-27 09:08:50.807856: Epoch 964 
2023-10-27 09:08:50.808234: Current learning rate: 0.0005 
2023-10-27 09:08:55.138661: train_loss -0.9187 
2023-10-27 09:08:55.139208: val_loss -0.6728 
2023-10-27 09:08:55.139504: Pseudo dice [0.8362, 0.9238, 0.965, 0.3487, 0.7252] 
2023-10-27 09:08:55.139758: Epoch time: 4.33 s 
2023-10-27 09:08:56.341935:  
2023-10-27 09:08:56.342262: Epoch 965 
2023-10-27 09:08:56.342533: Current learning rate: 0.00049 
2023-10-27 09:09:00.652813: train_loss -0.9146 
2023-10-27 09:09:00.653313: val_loss -0.6014 
2023-10-27 09:09:00.653606: Pseudo dice [0.8401, 0.9194, 0.9655, 0.2731, 0.6481] 
2023-10-27 09:09:00.653850: Epoch time: 4.31 s 
2023-10-27 09:09:02.033199:  
2023-10-27 09:09:02.033538: Epoch 966 
2023-10-27 09:09:02.033811: Current learning rate: 0.00048 
2023-10-27 09:09:06.385555: train_loss -0.9167 
2023-10-27 09:09:06.386074: val_loss -0.6359 
2023-10-27 09:09:06.386363: Pseudo dice [0.834, 0.9228, 0.9672, 0.2723, 0.6479] 
2023-10-27 09:09:06.386636: Epoch time: 4.35 s 
2023-10-27 09:09:07.585966:  
2023-10-27 09:09:07.586351: Epoch 967 
2023-10-27 09:09:07.586619: Current learning rate: 0.00046 
2023-10-27 09:09:11.931180: train_loss -0.9194 
2023-10-27 09:09:11.931607: val_loss -0.6985 
2023-10-27 09:09:11.931926: Pseudo dice [0.8382, 0.9272, 0.9657, 0.2441, 0.7552] 
2023-10-27 09:09:11.932230: Epoch time: 4.35 s 
2023-10-27 09:09:13.127946:  
2023-10-27 09:09:13.128293: Epoch 968 
2023-10-27 09:09:13.128563: Current learning rate: 0.00045 
2023-10-27 09:09:17.484069: train_loss -0.9205 
2023-10-27 09:09:17.484574: val_loss -0.6992 
2023-10-27 09:09:17.484877: Pseudo dice [0.8368, 0.9263, 0.9662, 0.2472, 0.7741] 
2023-10-27 09:09:17.485177: Epoch time: 4.36 s 
2023-10-27 09:09:18.692014:  
2023-10-27 09:09:18.692362: Epoch 969 
2023-10-27 09:09:18.692636: Current learning rate: 0.00044 
2023-10-27 09:09:23.038688: train_loss -0.9167 
2023-10-27 09:09:23.039141: val_loss -0.6805 
2023-10-27 09:09:23.039442: Pseudo dice [0.8328, 0.9205, 0.9655, 0.2341, 0.7826] 
2023-10-27 09:09:23.039695: Epoch time: 4.35 s 
2023-10-27 09:09:24.252079:  
2023-10-27 09:09:24.252531: Epoch 970 
2023-10-27 09:09:24.252810: Current learning rate: 0.00043 
2023-10-27 09:09:28.549161: train_loss -0.9186 
2023-10-27 09:09:28.549682: val_loss -0.653 
2023-10-27 09:09:28.549976: Pseudo dice [0.8379, 0.9253, 0.9664, 0.2592, 0.6943] 
2023-10-27 09:09:28.550247: Epoch time: 4.3 s 
2023-10-27 09:09:29.757670:  
2023-10-27 09:09:29.758005: Epoch 971 
2023-10-27 09:09:29.758284: Current learning rate: 0.00041 
2023-10-27 09:09:34.113602: train_loss -0.916 
2023-10-27 09:09:34.114195: val_loss -0.6558 
2023-10-27 09:09:34.114500: Pseudo dice [0.836, 0.9233, 0.9663, 0.235, 0.7114] 
2023-10-27 09:09:34.114771: Epoch time: 4.36 s 
2023-10-27 09:09:35.510702:  
2023-10-27 09:09:35.511030: Epoch 972 
2023-10-27 09:09:35.511323: Current learning rate: 0.0004 
2023-10-27 09:09:39.900177: train_loss -0.9192 
2023-10-27 09:09:39.900732: val_loss -0.672 
2023-10-27 09:09:39.901024: Pseudo dice [0.8377, 0.9243, 0.9675, 0.2983, 0.7184] 
2023-10-27 09:09:39.901333: Epoch time: 4.39 s 
2023-10-27 09:09:41.106925:  
2023-10-27 09:09:41.107249: Epoch 973 
2023-10-27 09:09:41.107516: Current learning rate: 0.00039 
2023-10-27 09:09:45.450590: train_loss -0.9181 
2023-10-27 09:09:45.451114: val_loss -0.5906 
2023-10-27 09:09:45.451437: Pseudo dice [0.8273, 0.9194, 0.9655, 0.2543, 0.62] 
2023-10-27 09:09:45.451702: Epoch time: 4.34 s 
2023-10-27 09:09:46.662849:  
2023-10-27 09:09:46.663355: Epoch 974 
2023-10-27 09:09:46.663642: Current learning rate: 0.00037 
2023-10-27 09:09:50.985120: train_loss -0.919 
2023-10-27 09:09:50.985567: val_loss -0.7011 
2023-10-27 09:09:50.985880: Pseudo dice [0.8352, 0.9236, 0.9653, 0.2956, 0.7665] 
2023-10-27 09:09:50.986135: Epoch time: 4.32 s 
2023-10-27 09:09:52.185213:  
2023-10-27 09:09:52.185568: Epoch 975 
2023-10-27 09:09:52.185840: Current learning rate: 0.00036 
2023-10-27 09:09:56.598004: train_loss -0.9213 
2023-10-27 09:09:56.598480: val_loss -0.6491 
2023-10-27 09:09:56.598758: Pseudo dice [0.8327, 0.9176, 0.9644, 0.2303, 0.7208] 
2023-10-27 09:09:56.599040: Epoch time: 4.41 s 
2023-10-27 09:09:57.804732:  
2023-10-27 09:09:57.805138: Epoch 976 
2023-10-27 09:09:57.805432: Current learning rate: 0.00035 
2023-10-27 09:10:02.146850: train_loss -0.9165 
2023-10-27 09:10:02.147352: val_loss -0.6785 
2023-10-27 09:10:02.147656: Pseudo dice [0.8376, 0.9257, 0.9658, 0.2751, 0.7215] 
2023-10-27 09:10:02.147913: Epoch time: 4.34 s 
2023-10-27 09:10:03.360507:  
2023-10-27 09:10:03.360843: Epoch 977 
2023-10-27 09:10:03.361145: Current learning rate: 0.00034 
2023-10-27 09:10:07.642398: train_loss -0.923 
2023-10-27 09:10:07.642859: val_loss -0.6369 
2023-10-27 09:10:07.643180: Pseudo dice [0.831, 0.9218, 0.9663, 0.2767, 0.6946] 
2023-10-27 09:10:07.643455: Epoch time: 4.28 s 
2023-10-27 09:10:08.835108:  
2023-10-27 09:10:08.835422: Epoch 978 
2023-10-27 09:10:08.835678: Current learning rate: 0.00032 
2023-10-27 09:10:13.116432: train_loss -0.9154 
2023-10-27 09:10:13.116940: val_loss -0.7021 
2023-10-27 09:10:13.117363: Pseudo dice [0.8392, 0.9248, 0.9642, 0.2964, 0.7779] 
2023-10-27 09:10:13.117687: Epoch time: 4.28 s 
2023-10-27 09:10:14.514952:  
2023-10-27 09:10:14.515401: Epoch 979 
2023-10-27 09:10:14.515685: Current learning rate: 0.00031 
2023-10-27 09:10:18.796002: train_loss -0.9214 
2023-10-27 09:10:18.796457: val_loss -0.6555 
2023-10-27 09:10:18.796751: Pseudo dice [0.8364, 0.9215, 0.9649, 0.3197, 0.7059] 
2023-10-27 09:10:18.796998: Epoch time: 4.28 s 
2023-10-27 09:10:19.997952:  
2023-10-27 09:10:19.998301: Epoch 980 
2023-10-27 09:10:19.998592: Current learning rate: 0.0003 
2023-10-27 09:10:24.261883: train_loss -0.9199 
2023-10-27 09:10:24.262368: val_loss -0.6925 
2023-10-27 09:10:24.262706: Pseudo dice [0.8363, 0.9227, 0.965, 0.2717, 0.7705] 
2023-10-27 09:10:24.262997: Epoch time: 4.26 s 
2023-10-27 09:10:25.463470:  
2023-10-27 09:10:25.463817: Epoch 981 
2023-10-27 09:10:25.464089: Current learning rate: 0.00028 
2023-10-27 09:10:29.766803: train_loss -0.9195 
2023-10-27 09:10:29.767273: val_loss -0.6755 
2023-10-27 09:10:29.767587: Pseudo dice [0.8327, 0.9195, 0.9643, 0.2792, 0.7525] 
2023-10-27 09:10:29.767864: Epoch time: 4.3 s 
2023-10-27 09:10:30.964759:  
2023-10-27 09:10:30.965110: Epoch 982 
2023-10-27 09:10:30.965385: Current learning rate: 0.00027 
2023-10-27 09:10:35.295200: train_loss -0.9159 
2023-10-27 09:10:35.295713: val_loss -0.6309 
2023-10-27 09:10:35.296207: Pseudo dice [0.8277, 0.9216, 0.9662, 0.2508, 0.6568] 
2023-10-27 09:10:35.296520: Epoch time: 4.33 s 
2023-10-27 09:10:36.499860:  
2023-10-27 09:10:36.500230: Epoch 983 
2023-10-27 09:10:36.500521: Current learning rate: 0.00026 
2023-10-27 09:10:40.802325: train_loss -0.9169 
2023-10-27 09:10:40.806709: val_loss -0.6275 
2023-10-27 09:10:40.806993: Pseudo dice [0.831, 0.9203, 0.965, 0.3202, 0.6076] 
2023-10-27 09:10:40.807283: Epoch time: 4.3 s 
2023-10-27 09:10:42.013287:  
2023-10-27 09:10:42.013613: Epoch 984 
2023-10-27 09:10:42.013919: Current learning rate: 0.00024 
2023-10-27 09:10:46.378745: train_loss -0.9215 
2023-10-27 09:10:46.382954: val_loss -0.7043 
2023-10-27 09:10:46.383750: Pseudo dice [0.8288, 0.9226, 0.9654, 0.2872, 0.8132] 
2023-10-27 09:10:46.384108: Epoch time: 4.37 s 
2023-10-27 09:10:47.754609:  
2023-10-27 09:10:47.755247: Epoch 985 
2023-10-27 09:10:47.755574: Current learning rate: 0.00023 
2023-10-27 09:10:52.074641: train_loss -0.9215 
2023-10-27 09:10:52.078243: val_loss -0.6258 
2023-10-27 09:10:52.078553: Pseudo dice [0.829, 0.9191, 0.9658, 0.2331, 0.7133] 
2023-10-27 09:10:52.078832: Epoch time: 4.32 s 
2023-10-27 09:10:53.299396:  
2023-10-27 09:10:53.299811: Epoch 986 
2023-10-27 09:10:53.300148: Current learning rate: 0.00021 
2023-10-27 09:10:57.689324: train_loss -0.9119 
2023-10-27 09:10:57.689769: val_loss -0.7107 
2023-10-27 09:10:57.690078: Pseudo dice [0.8268, 0.922, 0.9662, 0.2456, 0.8439] 
2023-10-27 09:10:57.690354: Epoch time: 4.39 s 
2023-10-27 09:10:58.901821:  
2023-10-27 09:10:58.902166: Epoch 987 
2023-10-27 09:10:58.902446: Current learning rate: 0.0002 
2023-10-27 09:11:03.248554: train_loss -0.9169 
2023-10-27 09:11:03.252345: val_loss -0.699 
2023-10-27 09:11:03.252883: Pseudo dice [0.8221, 0.9221, 0.9669, 0.3054, 0.7705] 
2023-10-27 09:11:03.253276: Epoch time: 4.35 s 
2023-10-27 09:11:04.477131:  
2023-10-27 09:11:04.477518: Epoch 988 
2023-10-27 09:11:04.477822: Current learning rate: 0.00019 
2023-10-27 09:11:08.829117: train_loss -0.9173 
2023-10-27 09:11:08.829574: val_loss -0.6557 
2023-10-27 09:11:08.829875: Pseudo dice [0.8286, 0.9206, 0.9661, 0.2949, 0.7259] 
2023-10-27 09:11:08.830185: Epoch time: 4.35 s 
2023-10-27 09:11:10.035646:  
2023-10-27 09:11:10.035969: Epoch 989 
2023-10-27 09:11:10.036246: Current learning rate: 0.00017 
2023-10-27 09:11:14.373733: train_loss -0.9196 
2023-10-27 09:11:14.374308: val_loss -0.6761 
2023-10-27 09:11:14.374604: Pseudo dice [0.8336, 0.9221, 0.9658, 0.2493, 0.7202] 
2023-10-27 09:11:14.374880: Epoch time: 4.34 s 
2023-10-27 09:11:15.586029:  
2023-10-27 09:11:15.586404: Epoch 990 
2023-10-27 09:11:15.586666: Current learning rate: 0.00016 
2023-10-27 09:11:19.963986: train_loss -0.9169 
2023-10-27 09:11:19.964421: val_loss -0.6793 
2023-10-27 09:11:19.964725: Pseudo dice [0.8319, 0.9222, 0.9658, 0.2446, 0.7685] 
2023-10-27 09:11:19.964988: Epoch time: 4.38 s 
2023-10-27 09:11:21.172540:  
2023-10-27 09:11:21.172864: Epoch 991 
2023-10-27 09:11:21.173138: Current learning rate: 0.00014 
2023-10-27 09:11:25.548911: train_loss -0.9166 
2023-10-27 09:11:25.552159: val_loss -0.6926 
2023-10-27 09:11:25.552488: Pseudo dice [0.8316, 0.922, 0.9654, 0.3417, 0.7496] 
2023-10-27 09:11:25.552752: Epoch time: 4.38 s 
2023-10-27 09:11:26.939527:  
2023-10-27 09:11:26.939866: Epoch 992 
2023-10-27 09:11:26.940145: Current learning rate: 0.00013 
2023-10-27 09:11:31.322225: train_loss -0.9213 
2023-10-27 09:11:31.322704: val_loss -0.6968 
2023-10-27 09:11:31.323035: Pseudo dice [0.8242, 0.9212, 0.9671, 0.2542, 0.8353] 
2023-10-27 09:11:31.323343: Epoch time: 4.38 s 
2023-10-27 09:11:32.544093:  
2023-10-27 09:11:32.544562: Epoch 993 
2023-10-27 09:11:32.544893: Current learning rate: 0.00011 
2023-10-27 09:11:36.956998: train_loss -0.9162 
2023-10-27 09:11:36.960341: val_loss -0.7032 
2023-10-27 09:11:36.960646: Pseudo dice [0.8333, 0.925, 0.9674, 0.249, 0.7887] 
2023-10-27 09:11:36.960927: Epoch time: 4.41 s 
2023-10-27 09:11:38.169814:  
2023-10-27 09:11:38.170160: Epoch 994 
2023-10-27 09:11:38.170430: Current learning rate: 0.0001 
2023-10-27 09:11:42.561158: train_loss -0.9182 
2023-10-27 09:11:42.561579: val_loss -0.6734 
2023-10-27 09:11:42.561878: Pseudo dice [0.8272, 0.9236, 0.9663, 0.1944, 0.738] 
2023-10-27 09:11:42.562165: Epoch time: 4.39 s 
2023-10-27 09:11:43.772482:  
2023-10-27 09:11:43.772808: Epoch 995 
2023-10-27 09:11:43.773075: Current learning rate: 8e-05 
2023-10-27 09:11:48.221288: train_loss -0.9161 
2023-10-27 09:11:48.221764: val_loss -0.6545 
2023-10-27 09:11:48.222075: Pseudo dice [0.8274, 0.9252, 0.968, 0.1932, 0.6593] 
2023-10-27 09:11:48.222345: Epoch time: 4.45 s 
2023-10-27 09:11:49.440694:  
2023-10-27 09:11:49.441024: Epoch 996 
2023-10-27 09:11:49.441306: Current learning rate: 7e-05 
2023-10-27 09:11:53.807270: train_loss -0.9159 
2023-10-27 09:11:53.807729: val_loss -0.6317 
2023-10-27 09:11:53.808030: Pseudo dice [0.8354, 0.9213, 0.9658, 0.1799, 0.6545] 
2023-10-27 09:11:53.808295: Epoch time: 4.37 s 
2023-10-27 09:11:55.026757:  
2023-10-27 09:11:55.027146: Epoch 997 
2023-10-27 09:11:55.027418: Current learning rate: 5e-05 
2023-10-27 09:11:59.371819: train_loss -0.9175 
2023-10-27 09:11:59.375778: val_loss -0.6653 
2023-10-27 09:11:59.376093: Pseudo dice [0.8401, 0.9229, 0.9654, 0.2876, 0.7437] 
2023-10-27 09:11:59.376536: Epoch time: 4.35 s 
2023-10-27 09:12:00.769276:  
2023-10-27 09:12:00.769620: Epoch 998 
2023-10-27 09:12:00.769878: Current learning rate: 4e-05 
2023-10-27 09:12:05.183712: train_loss -0.9182 
2023-10-27 09:12:05.184211: val_loss -0.6336 
2023-10-27 09:12:05.184526: Pseudo dice [0.8316, 0.9201, 0.9657, 0.253, 0.7066] 
2023-10-27 09:12:05.184801: Epoch time: 4.42 s 
2023-10-27 09:12:06.392489:  
2023-10-27 09:12:06.392894: Epoch 999 
2023-10-27 09:12:06.393450: Current learning rate: 2e-05 
2023-10-27 09:12:10.746897: train_loss -0.9188 
2023-10-27 09:12:10.750414: val_loss -0.6443 
2023-10-27 09:12:10.750700: Pseudo dice [0.8374, 0.9226, 0.9651, 0.2527, 0.6617] 
2023-10-27 09:12:10.750956: Epoch time: 4.36 s 
2023-10-27 09:12:12.122544: Training done. 
2023-10-27 09:12:12.131491: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-27 09:12:12.132047: The split file contains 5 splits. 
2023-10-27 09:12:12.132261: Desired fold for training: 2 
2023-10-27 09:12:12.132476: This split has 36 training and 8 validation cases. 
2023-10-27 09:12:12.132827: predicting t2_haste_tra_2_2mm_001 
2023-10-27 09:12:12.433819: predicting t2_haste_tra_2_2mm_015 
2023-10-27 09:12:12.481711: predicting t2_haste_tra_2_2mm_018 
2023-10-27 09:12:12.513209: predicting t2_haste_tra_2_2mm_019 
2023-10-27 09:12:12.544703: predicting t2_haste_tra_2_2mm_101 
2023-10-27 09:12:12.576014: predicting t2_haste_tra_2_2mm_115 
2023-10-27 09:12:12.607980: predicting t2_haste_tra_2_2mm_118 
2023-10-27 09:12:12.640351: predicting t2_haste_tra_2_2mm_119 
2023-10-27 09:12:22.007902: Validation complete 
2023-10-27 09:12:22.011343: Mean Validation Dice:  0.6708371630138255 
