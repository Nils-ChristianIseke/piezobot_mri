
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [24, 56, 40], 'median_image_size_in_voxels': [22.0, 56.0, 36.0], 'spacing': [2.419999837875366, 1.46875, 1.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [2, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_NeedlePhantomV1', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.419999837875366, 1.46875, 1.46875], 'original_median_shape_after_transp': [22, 56, 36], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1610.0, 'mean': 465.1097412109375, 'median': 478.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 1246.0, 'std': 241.4937744140625}}} 
 
2023-10-27 09:35:29.041608: unpacking dataset... 
2023-10-27 09:35:33.958508: unpacking done... 
2023-10-27 09:35:34.039587: do_dummy_2d_data_aug: False 
2023-10-27 09:35:34.042244: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-27 09:35:34.042927: The split file contains 5 splits. 
2023-10-27 09:35:34.043129: Desired fold for training: 4 
2023-10-27 09:35:34.043324: This split has 36 training and 8 validation cases. 
2023-10-27 09:35:44.931472: Unable to plot network architecture: 
2023-10-27 09:35:44.932119: 'torch._C.Node' object is not subscriptable 
2023-10-27 09:35:44.957539:  
2023-10-27 09:35:44.958605: Epoch 0 
2023-10-27 09:35:44.959062: Current learning rate: 0.01 
2023-10-27 09:35:51.444641: train_loss -0.1156 
2023-10-27 09:35:51.445326: val_loss -0.5276 
2023-10-27 09:35:51.445807: Pseudo dice [0.5192, 0.8487, 0.9414, 0.0, 0.8516] 
2023-10-27 09:35:51.446102: Epoch time: 6.47 s 
2023-10-27 09:35:51.446414: Yayy! New best EMA pseudo Dice: 0.6322 
2023-10-27 09:35:53.000965:  
2023-10-27 09:35:53.001259: Epoch 1 
2023-10-27 09:35:53.001518: Current learning rate: 0.00999 
2023-10-27 09:35:56.565077: train_loss -0.4775 
2023-10-27 09:35:56.565451: val_loss -0.6354 
2023-10-27 09:35:56.565709: Pseudo dice [0.8, 0.8635, 0.9612, 0.0, 0.9132] 
2023-10-27 09:35:56.565939: Epoch time: 3.56 s 
2023-10-27 09:35:56.566154: Yayy! New best EMA pseudo Dice: 0.6397 
2023-10-27 09:35:57.637175:  
2023-10-27 09:35:57.637538: Epoch 2 
2023-10-27 09:35:57.637783: Current learning rate: 0.00998 
2023-10-27 09:36:01.242895: train_loss -0.5184 
2023-10-27 09:36:01.243320: val_loss -0.6497 
2023-10-27 09:36:01.243593: Pseudo dice [0.8347, 0.8708, 0.9619, 0.0, 0.91] 
2023-10-27 09:36:01.243825: Epoch time: 3.61 s 
2023-10-27 09:36:01.244061: Yayy! New best EMA pseudo Dice: 0.6473 
2023-10-27 09:36:02.514272:  
2023-10-27 09:36:02.514588: Epoch 3 
2023-10-27 09:36:02.514840: Current learning rate: 0.00997 
2023-10-27 09:36:06.170109: train_loss -0.5527 
2023-10-27 09:36:06.170525: val_loss -0.6632 
2023-10-27 09:36:06.170789: Pseudo dice [0.8514, 0.8788, 0.9647, 0.0, 0.9277] 
2023-10-27 09:36:06.171026: Epoch time: 3.66 s 
2023-10-27 09:36:06.171242: Yayy! New best EMA pseudo Dice: 0.655 
2023-10-27 09:36:07.243999:  
2023-10-27 09:36:07.244405: Epoch 4 
2023-10-27 09:36:07.244691: Current learning rate: 0.00996 
2023-10-27 09:36:11.018994: train_loss -0.558 
2023-10-27 09:36:11.022807: val_loss -0.6679 
2023-10-27 09:36:11.023098: Pseudo dice [0.8607, 0.8741, 0.9676, 0.0, 0.9267] 
2023-10-27 09:36:11.023401: Epoch time: 3.78 s 
2023-10-27 09:36:11.023629: Yayy! New best EMA pseudo Dice: 0.6621 
2023-10-27 09:36:12.131534:  
2023-10-27 09:36:12.131918: Epoch 5 
2023-10-27 09:36:12.132271: Current learning rate: 0.00995 
2023-10-27 09:36:15.790015: train_loss -0.5618 
2023-10-27 09:36:15.790454: val_loss -0.6803 
2023-10-27 09:36:15.790725: Pseudo dice [0.8634, 0.891, 0.9671, 0.0988, 0.9276] 
2023-10-27 09:36:15.790964: Epoch time: 3.66 s 
2023-10-27 09:36:15.791182: Yayy! New best EMA pseudo Dice: 0.6708 
2023-10-27 09:36:16.853978:  
2023-10-27 09:36:16.854295: Epoch 6 
2023-10-27 09:36:16.854565: Current learning rate: 0.00995 
2023-10-27 09:36:20.388366: train_loss -0.5714 
2023-10-27 09:36:20.388780: val_loss -0.6992 
2023-10-27 09:36:20.389040: Pseudo dice [0.8585, 0.8777, 0.9632, 0.7597, 0.9301] 
2023-10-27 09:36:20.389293: Epoch time: 3.53 s 
2023-10-27 09:36:20.389520: Yayy! New best EMA pseudo Dice: 0.6915 
2023-10-27 09:36:21.450705:  
2023-10-27 09:36:21.451100: Epoch 7 
2023-10-27 09:36:21.451357: Current learning rate: 0.00994 
2023-10-27 09:36:25.013959: train_loss -0.5909 
2023-10-27 09:36:25.014387: val_loss -0.7053 
2023-10-27 09:36:25.014654: Pseudo dice [0.8578, 0.8881, 0.9672, 0.8323, 0.9145] 
2023-10-27 09:36:25.014894: Epoch time: 3.56 s 
2023-10-27 09:36:25.015105: Yayy! New best EMA pseudo Dice: 0.7116 
2023-10-27 09:36:26.103243:  
2023-10-27 09:36:26.103620: Epoch 8 
2023-10-27 09:36:26.103902: Current learning rate: 0.00993 
2023-10-27 09:36:29.681462: train_loss -0.6055 
2023-10-27 09:36:29.681905: val_loss -0.7121 
2023-10-27 09:36:29.682281: Pseudo dice [0.8612, 0.893, 0.9676, 0.7309, 0.9338] 
2023-10-27 09:36:29.682577: Epoch time: 3.58 s 
2023-10-27 09:36:29.682889: Yayy! New best EMA pseudo Dice: 0.7282 
2023-10-27 09:36:30.911473:  
2023-10-27 09:36:30.911818: Epoch 9 
2023-10-27 09:36:30.912072: Current learning rate: 0.00992 
2023-10-27 09:36:34.580514: train_loss -0.6102 
2023-10-27 09:36:34.583974: val_loss -0.7159 
2023-10-27 09:36:34.584404: Pseudo dice [0.8686, 0.8712, 0.967, 0.7936, 0.9358] 
2023-10-27 09:36:34.584807: Epoch time: 3.67 s 
2023-10-27 09:36:34.585092: Yayy! New best EMA pseudo Dice: 0.7441 
2023-10-27 09:36:35.641102:  
2023-10-27 09:36:35.644333: Epoch 10 
2023-10-27 09:36:35.644590: Current learning rate: 0.00991 
2023-10-27 09:36:39.191802: train_loss -0.6019 
2023-10-27 09:36:39.192367: val_loss -0.7255 
2023-10-27 09:36:39.192678: Pseudo dice [0.864, 0.8985, 0.9693, 0.7461, 0.932] 
2023-10-27 09:36:39.192923: Epoch time: 3.55 s 
2023-10-27 09:36:39.193253: Yayy! New best EMA pseudo Dice: 0.7579 
2023-10-27 09:36:40.253919:  
2023-10-27 09:36:40.254290: Epoch 11 
2023-10-27 09:36:40.254541: Current learning rate: 0.0099 
2023-10-27 09:36:43.928979: train_loss -0.6101 
2023-10-27 09:36:43.929439: val_loss -0.7263 
2023-10-27 09:36:43.929710: Pseudo dice [0.8738, 0.8988, 0.9667, 0.795, 0.919] 
2023-10-27 09:36:43.929946: Epoch time: 3.68 s 
2023-10-27 09:36:43.930169: Yayy! New best EMA pseudo Dice: 0.7711 
2023-10-27 09:36:44.991744:  
2023-10-27 09:36:44.992118: Epoch 12 
2023-10-27 09:36:44.992369: Current learning rate: 0.00989 
2023-10-27 09:36:48.630742: train_loss -0.6113 
2023-10-27 09:36:48.631192: val_loss -0.7272 
2023-10-27 09:36:48.631483: Pseudo dice [0.8796, 0.8978, 0.9656, 0.8432, 0.9364] 
2023-10-27 09:36:48.631770: Epoch time: 3.64 s 
2023-10-27 09:36:48.631995: Yayy! New best EMA pseudo Dice: 0.7845 
2023-10-27 09:36:49.709128:  
2023-10-27 09:36:49.709457: Epoch 13 
2023-10-27 09:36:49.709714: Current learning rate: 0.00988 
2023-10-27 09:36:53.333395: train_loss -0.6069 
2023-10-27 09:36:53.333758: val_loss -0.7257 
2023-10-27 09:36:53.334012: Pseudo dice [0.8606, 0.8978, 0.9679, 0.8656, 0.9292] 
2023-10-27 09:36:53.334239: Epoch time: 3.63 s 
2023-10-27 09:36:53.334446: Yayy! New best EMA pseudo Dice: 0.7965 
2023-10-27 09:36:54.405952:  
2023-10-27 09:36:54.406362: Epoch 14 
2023-10-27 09:36:54.406626: Current learning rate: 0.00987 
2023-10-27 09:36:57.950261: train_loss -0.6171 
2023-10-27 09:36:57.950640: val_loss -0.7313 
2023-10-27 09:36:57.950917: Pseudo dice [0.8739, 0.8968, 0.97, 0.8288, 0.9348] 
2023-10-27 09:36:57.951146: Epoch time: 3.55 s 
2023-10-27 09:36:57.951360: Yayy! New best EMA pseudo Dice: 0.8069 
2023-10-27 09:36:59.034996:  
2023-10-27 09:36:59.035297: Epoch 15 
2023-10-27 09:36:59.035553: Current learning rate: 0.00986 
2023-10-27 09:37:02.832132: train_loss -0.6185 
2023-10-27 09:37:02.832520: val_loss -0.7333 
2023-10-27 09:37:02.832791: Pseudo dice [0.8666, 0.9035, 0.9688, 0.8204, 0.9294] 
2023-10-27 09:37:02.833024: Epoch time: 3.8 s 
2023-10-27 09:37:02.833243: Yayy! New best EMA pseudo Dice: 0.816 
2023-10-27 09:37:03.921016:  
2023-10-27 09:37:03.921447: Epoch 16 
2023-10-27 09:37:03.921736: Current learning rate: 0.00986 
2023-10-27 09:37:07.541416: train_loss -0.6154 
2023-10-27 09:37:07.541835: val_loss -0.7201 
2023-10-27 09:37:07.542119: Pseudo dice [0.8739, 0.9046, 0.9682, 0.9003, 0.9324] 
2023-10-27 09:37:07.542358: Epoch time: 3.62 s 
2023-10-27 09:37:07.542567: Yayy! New best EMA pseudo Dice: 0.826 
2023-10-27 09:37:08.647320:  
2023-10-27 09:37:08.647666: Epoch 17 
2023-10-27 09:37:08.647937: Current learning rate: 0.00985 
2023-10-27 09:37:12.208170: train_loss -0.6262 
2023-10-27 09:37:12.208546: val_loss -0.7386 
2023-10-27 09:37:12.208820: Pseudo dice [0.8669, 0.9023, 0.968, 0.8823, 0.9276] 
2023-10-27 09:37:12.209060: Epoch time: 3.56 s 
2023-10-27 09:37:12.209272: Yayy! New best EMA pseudo Dice: 0.8343 
2023-10-27 09:37:13.304229:  
2023-10-27 09:37:13.304628: Epoch 18 
2023-10-27 09:37:13.304878: Current learning rate: 0.00984 
2023-10-27 09:37:16.858176: train_loss -0.6252 
2023-10-27 09:37:16.858551: val_loss -0.7159 
2023-10-27 09:37:16.858815: Pseudo dice [0.8633, 0.8958, 0.9664, 0.8085, 0.9399] 
2023-10-27 09:37:16.859046: Epoch time: 3.55 s 
2023-10-27 09:37:16.859257: Yayy! New best EMA pseudo Dice: 0.8404 
2023-10-27 09:37:17.951228:  
2023-10-27 09:37:17.951622: Epoch 19 
2023-10-27 09:37:17.951880: Current learning rate: 0.00983 
2023-10-27 09:37:21.475792: train_loss -0.6251 
2023-10-27 09:37:21.476212: val_loss -0.7267 
2023-10-27 09:37:21.476514: Pseudo dice [0.8778, 0.9008, 0.9683, 0.8574, 0.9398] 
2023-10-27 09:37:21.476775: Epoch time: 3.53 s 
2023-10-27 09:37:21.477020: Yayy! New best EMA pseudo Dice: 0.8472 
2023-10-27 09:37:22.566989:  
2023-10-27 09:37:22.567333: Epoch 20 
2023-10-27 09:37:22.567598: Current learning rate: 0.00982 
2023-10-27 09:37:26.110352: train_loss -0.6207 
2023-10-27 09:37:26.110750: val_loss -0.744 
2023-10-27 09:37:26.111018: Pseudo dice [0.8834, 0.9049, 0.9711, 0.8945, 0.9374] 
2023-10-27 09:37:26.111254: Epoch time: 3.54 s 
2023-10-27 09:37:26.111486: Yayy! New best EMA pseudo Dice: 0.8543 
2023-10-27 09:37:27.350633:  
2023-10-27 09:37:27.350974: Epoch 21 
2023-10-27 09:37:27.351247: Current learning rate: 0.00981 
2023-10-27 09:37:30.963140: train_loss -0.6289 
2023-10-27 09:37:30.966089: val_loss -0.7181 
2023-10-27 09:37:30.966379: Pseudo dice [0.8683, 0.9021, 0.9695, 0.9095, 0.9337] 
2023-10-27 09:37:30.966651: Epoch time: 3.61 s 
2023-10-27 09:37:30.966878: Yayy! New best EMA pseudo Dice: 0.8605 
2023-10-27 09:37:32.024220:  
2023-10-27 09:37:32.024626: Epoch 22 
2023-10-27 09:37:32.024888: Current learning rate: 0.0098 
2023-10-27 09:37:35.687252: train_loss -0.6301 
2023-10-27 09:37:35.687653: val_loss -0.7298 
2023-10-27 09:37:35.687925: Pseudo dice [0.8748, 0.9057, 0.9696, 0.9152, 0.9316] 
2023-10-27 09:37:35.688162: Epoch time: 3.66 s 
2023-10-27 09:37:35.688373: Yayy! New best EMA pseudo Dice: 0.8664 
2023-10-27 09:37:36.748446:  
2023-10-27 09:37:36.748852: Epoch 23 
2023-10-27 09:37:36.749145: Current learning rate: 0.00979 
2023-10-27 09:37:40.379091: train_loss -0.6313 
2023-10-27 09:37:40.379480: val_loss -0.7449 
2023-10-27 09:37:40.379756: Pseudo dice [0.8743, 0.9094, 0.9699, 0.9287, 0.937] 
2023-10-27 09:37:40.379991: Epoch time: 3.63 s 
2023-10-27 09:37:40.380202: Yayy! New best EMA pseudo Dice: 0.8722 
2023-10-27 09:37:41.423745:  
2023-10-27 09:37:41.424068: Epoch 24 
2023-10-27 09:37:41.424336: Current learning rate: 0.00978 
2023-10-27 09:37:44.993422: train_loss -0.6242 
2023-10-27 09:37:44.993827: val_loss -0.7472 
2023-10-27 09:37:44.994086: Pseudo dice [0.8773, 0.9024, 0.9697, 0.8917, 0.9399] 
2023-10-27 09:37:44.994325: Epoch time: 3.57 s 
2023-10-27 09:37:44.994565: Yayy! New best EMA pseudo Dice: 0.8766 
2023-10-27 09:37:46.051161:  
2023-10-27 09:37:46.051548: Epoch 25 
2023-10-27 09:37:46.051796: Current learning rate: 0.00977 
2023-10-27 09:37:49.611844: train_loss -0.6304 
2023-10-27 09:37:49.612568: val_loss -0.7663 
2023-10-27 09:37:49.612852: Pseudo dice [0.8708, 0.9088, 0.9688, 0.8262, 0.9365] 
2023-10-27 09:37:49.613091: Epoch time: 3.56 s 
2023-10-27 09:37:49.613318: Yayy! New best EMA pseudo Dice: 0.8791 
2023-10-27 09:37:50.666599:  
2023-10-27 09:37:50.666990: Epoch 26 
2023-10-27 09:37:50.667259: Current learning rate: 0.00977 
2023-10-27 09:37:54.205715: train_loss -0.6937 
2023-10-27 09:37:54.206393: val_loss -0.7909 
2023-10-27 09:37:54.206777: Pseudo dice [0.825, 0.883, 0.9648, 0.0124, 0.8936] 
2023-10-27 09:37:54.207049: Epoch time: 3.54 s 
2023-10-27 09:37:55.206353:  
2023-10-27 09:37:55.206694: Epoch 27 
2023-10-27 09:37:55.206957: Current learning rate: 0.00976 
2023-10-27 09:37:58.814986: train_loss -0.7649 
2023-10-27 09:37:58.815658: val_loss -0.8281 
2023-10-27 09:37:58.815930: Pseudo dice [0.8566, 0.884, 0.9662, 0.7951, 0.8801] 
2023-10-27 09:37:58.816167: Epoch time: 3.61 s 
2023-10-27 09:37:59.964715:  
2023-10-27 09:37:59.965090: Epoch 28 
2023-10-27 09:37:59.965371: Current learning rate: 0.00975 
2023-10-27 09:38:03.553626: train_loss -0.7831 
2023-10-27 09:38:03.554438: val_loss -0.8635 
2023-10-27 09:38:03.554737: Pseudo dice [0.8544, 0.8953, 0.9683, 0.8465, 0.9282] 
2023-10-27 09:38:03.554982: Epoch time: 3.59 s 
2023-10-27 09:38:04.555308:  
2023-10-27 09:38:04.555621: Epoch 29 
2023-10-27 09:38:04.555912: Current learning rate: 0.00974 
2023-10-27 09:38:08.066949: train_loss -0.8125 
2023-10-27 09:38:08.067627: val_loss -0.8743 
2023-10-27 09:38:08.067914: Pseudo dice [0.8636, 0.9014, 0.9686, 0.8785, 0.9211] 
2023-10-27 09:38:08.068143: Epoch time: 3.51 s 
2023-10-27 09:38:09.079236:  
2023-10-27 09:38:09.079579: Epoch 30 
2023-10-27 09:38:09.079835: Current learning rate: 0.00973 
2023-10-27 09:38:12.628177: train_loss -0.8011 
2023-10-27 09:38:12.628883: val_loss -0.8417 
2023-10-27 09:38:12.629197: Pseudo dice [0.8605, 0.8821, 0.9651, 0.8317, 0.9335] 
2023-10-27 09:38:12.629450: Epoch time: 3.55 s 
2023-10-27 09:38:13.645725:  
2023-10-27 09:38:13.646109: Epoch 31 
2023-10-27 09:38:13.646383: Current learning rate: 0.00972 
2023-10-27 09:38:17.204289: train_loss -0.7996 
2023-10-27 09:38:17.205017: val_loss -0.8662 
2023-10-27 09:38:17.205334: Pseudo dice [0.8533, 0.8917, 0.9674, 0.8194, 0.9258] 
2023-10-27 09:38:17.205599: Epoch time: 3.56 s 
2023-10-27 09:38:18.225150:  
2023-10-27 09:38:18.225544: Epoch 32 
2023-10-27 09:38:18.225800: Current learning rate: 0.00971 
2023-10-27 09:38:21.733968: train_loss -0.8192 
2023-10-27 09:38:21.734695: val_loss -0.8343 
2023-10-27 09:38:21.734997: Pseudo dice [0.8514, 0.8909, 0.9669, 0.2365, 0.9248] 
2023-10-27 09:38:21.735241: Epoch time: 3.51 s 
2023-10-27 09:38:22.749362:  
2023-10-27 09:38:22.749699: Epoch 33 
2023-10-27 09:38:22.749956: Current learning rate: 0.0097 
2023-10-27 09:38:26.236335: train_loss -0.8127 
2023-10-27 09:38:26.237040: val_loss -0.867 
2023-10-27 09:38:26.237307: Pseudo dice [0.8604, 0.8984, 0.9658, 0.7625, 0.9289] 
2023-10-27 09:38:26.237547: Epoch time: 3.49 s 
2023-10-27 09:38:27.392145:  
2023-10-27 09:38:27.392474: Epoch 34 
2023-10-27 09:38:27.392731: Current learning rate: 0.00969 
2023-10-27 09:38:30.914740: train_loss -0.8162 
2023-10-27 09:38:30.915602: val_loss -0.8828 
2023-10-27 09:38:30.915892: Pseudo dice [0.8767, 0.8998, 0.9657, 0.8233, 0.9362] 
2023-10-27 09:38:30.916124: Epoch time: 3.52 s 
2023-10-27 09:38:31.944681:  
2023-10-27 09:38:31.945057: Epoch 35 
2023-10-27 09:38:31.945309: Current learning rate: 0.00968 
2023-10-27 09:38:35.485672: train_loss -0.8273 
2023-10-27 09:38:35.486408: val_loss -0.8605 
2023-10-27 09:38:35.486695: Pseudo dice [0.8794, 0.9013, 0.9655, 0.5963, 0.9358] 
2023-10-27 09:38:35.486958: Epoch time: 3.54 s 
2023-10-27 09:38:36.516822:  
2023-10-27 09:38:36.517150: Epoch 36 
2023-10-27 09:38:36.517435: Current learning rate: 0.00968 
2023-10-27 09:38:40.055271: train_loss -0.8313 
2023-10-27 09:38:40.055959: val_loss -0.8661 
2023-10-27 09:38:40.056234: Pseudo dice [0.8784, 0.8977, 0.9665, 0.6364, 0.9385] 
2023-10-27 09:38:40.056469: Epoch time: 3.54 s 
2023-10-27 09:38:41.090255:  
2023-10-27 09:38:41.090658: Epoch 37 
2023-10-27 09:38:41.090930: Current learning rate: 0.00967 
2023-10-27 09:38:44.617949: train_loss -0.837 
2023-10-27 09:38:44.618660: val_loss -0.8835 
2023-10-27 09:38:44.618934: Pseudo dice [0.8791, 0.9019, 0.968, 0.8121, 0.9415] 
2023-10-27 09:38:44.619163: Epoch time: 3.53 s 
2023-10-27 09:38:45.652478:  
2023-10-27 09:38:45.652794: Epoch 38 
2023-10-27 09:38:45.653052: Current learning rate: 0.00966 
2023-10-27 09:38:49.173652: train_loss -0.8291 
2023-10-27 09:38:49.174320: val_loss -0.8794 
2023-10-27 09:38:49.174627: Pseudo dice [0.8803, 0.9102, 0.9687, 0.8773, 0.9402] 
2023-10-27 09:38:49.174861: Epoch time: 3.52 s 
2023-10-27 09:38:50.211967:  
2023-10-27 09:38:50.212358: Epoch 39 
2023-10-27 09:38:50.212637: Current learning rate: 0.00965 
2023-10-27 09:38:53.795121: train_loss -0.831 
2023-10-27 09:38:53.795775: val_loss -0.8448 
2023-10-27 09:38:53.796037: Pseudo dice [0.872, 0.9097, 0.9689, 0.3616, 0.9364] 
2023-10-27 09:38:53.796263: Epoch time: 3.58 s 
2023-10-27 09:38:54.975535:  
2023-10-27 09:38:54.975856: Epoch 40 
2023-10-27 09:38:54.976157: Current learning rate: 0.00964 
2023-10-27 09:38:58.501618: train_loss -0.8221 
2023-10-27 09:38:58.502340: val_loss -0.8775 
2023-10-27 09:38:58.502617: Pseudo dice [0.875, 0.9069, 0.9695, 0.8663, 0.9314] 
2023-10-27 09:38:58.502861: Epoch time: 3.53 s 
2023-10-27 09:38:59.550299:  
2023-10-27 09:38:59.550654: Epoch 41 
2023-10-27 09:38:59.550927: Current learning rate: 0.00963 
2023-10-27 09:39:03.109267: train_loss -0.83 
2023-10-27 09:39:03.109959: val_loss -0.8849 
2023-10-27 09:39:03.110233: Pseudo dice [0.8729, 0.9129, 0.9706, 0.8863, 0.9389] 
2023-10-27 09:39:03.110469: Epoch time: 3.56 s 
2023-10-27 09:39:04.110381:  
2023-10-27 09:39:04.110792: Epoch 42 
2023-10-27 09:39:04.111063: Current learning rate: 0.00962 
2023-10-27 09:39:07.666950: train_loss -0.8319 
2023-10-27 09:39:07.667607: val_loss -0.88 
2023-10-27 09:39:07.667898: Pseudo dice [0.8691, 0.9096, 0.9691, 0.8457, 0.9344] 
2023-10-27 09:39:07.668152: Epoch time: 3.56 s 
2023-10-27 09:39:07.668492: Yayy! New best EMA pseudo Dice: 0.8805 
2023-10-27 09:39:08.734206:  
2023-10-27 09:39:08.734535: Epoch 43 
2023-10-27 09:39:08.734788: Current learning rate: 0.00961 
2023-10-27 09:39:12.285231: train_loss -0.8302 
2023-10-27 09:39:12.286333: val_loss -0.8551 
2023-10-27 09:39:12.286811: Pseudo dice [0.8709, 0.8977, 0.9693, 0.4344, 0.9313] 
2023-10-27 09:39:12.287157: Epoch time: 3.55 s 
2023-10-27 09:39:13.292152:  
2023-10-27 09:39:13.292477: Epoch 44 
2023-10-27 09:39:13.292733: Current learning rate: 0.0096 
2023-10-27 09:39:16.861492: train_loss -0.8303 
2023-10-27 09:39:16.862166: val_loss -0.8819 
2023-10-27 09:39:16.862468: Pseudo dice [0.8807, 0.9089, 0.9697, 0.8061, 0.9408] 
2023-10-27 09:39:16.862737: Epoch time: 3.57 s 
2023-10-27 09:39:17.866601:  
2023-10-27 09:39:17.866905: Epoch 45 
2023-10-27 09:39:17.867157: Current learning rate: 0.00959 
2023-10-27 09:39:21.411249: train_loss -0.839 
2023-10-27 09:39:21.411934: val_loss -0.8788 
2023-10-27 09:39:21.412235: Pseudo dice [0.8789, 0.9113, 0.9683, 0.8471, 0.9367] 
2023-10-27 09:39:21.412488: Epoch time: 3.55 s 
2023-10-27 09:39:22.562779:  
2023-10-27 09:39:22.563095: Epoch 46 
2023-10-27 09:39:22.563351: Current learning rate: 0.00959 
2023-10-27 09:39:26.102963: train_loss -0.8309 
2023-10-27 09:39:26.103657: val_loss -0.8842 
2023-10-27 09:39:26.103937: Pseudo dice [0.8817, 0.9092, 0.9665, 0.8483, 0.9393] 
2023-10-27 09:39:26.104185: Epoch time: 3.54 s 
2023-10-27 09:39:26.104421: Yayy! New best EMA pseudo Dice: 0.8832 
2023-10-27 09:39:27.156954:  
2023-10-27 09:39:27.157289: Epoch 47 
2023-10-27 09:39:27.157556: Current learning rate: 0.00958 
2023-10-27 09:39:30.716155: train_loss -0.8285 
2023-10-27 09:39:30.716835: val_loss -0.8806 
2023-10-27 09:39:30.717123: Pseudo dice [0.8707, 0.8983, 0.9681, 0.8541, 0.9321] 
2023-10-27 09:39:30.717382: Epoch time: 3.56 s 
2023-10-27 09:39:30.717610: Yayy! New best EMA pseudo Dice: 0.8854 
2023-10-27 09:39:31.770842:  
2023-10-27 09:39:31.771168: Epoch 48 
2023-10-27 09:39:31.771428: Current learning rate: 0.00957 
2023-10-27 09:39:35.344152: train_loss -0.8348 
2023-10-27 09:39:35.345106: val_loss -0.8752 
2023-10-27 09:39:35.345563: Pseudo dice [0.8796, 0.9051, 0.9691, 0.8117, 0.9372] 
2023-10-27 09:39:35.345976: Epoch time: 3.57 s 
2023-10-27 09:39:35.346472: Yayy! New best EMA pseudo Dice: 0.8869 
2023-10-27 09:39:36.409645:  
2023-10-27 09:39:36.410010: Epoch 49 
2023-10-27 09:39:36.410266: Current learning rate: 0.00956 
2023-10-27 09:39:39.971156: train_loss -0.8362 
2023-10-27 09:39:39.971944: val_loss -0.88 
2023-10-27 09:39:39.972380: Pseudo dice [0.8712, 0.908, 0.9697, 0.8864, 0.9296] 
2023-10-27 09:39:39.972660: Epoch time: 3.56 s 
2023-10-27 09:39:40.031112: Yayy! New best EMA pseudo Dice: 0.8895 
2023-10-27 09:39:41.093380:  
2023-10-27 09:39:41.093715: Epoch 50 
2023-10-27 09:39:41.093979: Current learning rate: 0.00955 
2023-10-27 09:39:44.579866: train_loss -0.8428 
2023-10-27 09:39:44.580527: val_loss -0.8734 
2023-10-27 09:39:44.580824: Pseudo dice [0.8796, 0.9113, 0.9706, 0.6106, 0.9406] 
2023-10-27 09:39:44.581068: Epoch time: 3.49 s 
2023-10-27 09:39:45.577453:  
2023-10-27 09:39:45.577839: Epoch 51 
2023-10-27 09:39:45.578126: Current learning rate: 0.00954 
2023-10-27 09:39:49.087995: train_loss -0.8275 
2023-10-27 09:39:49.088785: val_loss -0.8875 
2023-10-27 09:39:49.089246: Pseudo dice [0.877, 0.9162, 0.9708, 0.8854, 0.9354] 
2023-10-27 09:39:49.089694: Epoch time: 3.51 s 
2023-10-27 09:39:49.089997: Yayy! New best EMA pseudo Dice: 0.8898 
2023-10-27 09:39:50.294764:  
2023-10-27 09:39:50.295110: Epoch 52 
2023-10-27 09:39:50.295371: Current learning rate: 0.00953 
2023-10-27 09:39:53.850705: train_loss -0.8357 
2023-10-27 09:39:53.851431: val_loss -0.8863 
2023-10-27 09:39:53.851704: Pseudo dice [0.8819, 0.907, 0.9701, 0.8172, 0.9414] 
2023-10-27 09:39:53.851940: Epoch time: 3.56 s 
2023-10-27 09:39:53.852151: Yayy! New best EMA pseudo Dice: 0.8912 
2023-10-27 09:39:54.921633:  
2023-10-27 09:39:54.921992: Epoch 53 
2023-10-27 09:39:54.922244: Current learning rate: 0.00952 
2023-10-27 09:39:58.464027: train_loss -0.8382 
2023-10-27 09:39:58.464707: val_loss -0.883 
2023-10-27 09:39:58.464972: Pseudo dice [0.8673, 0.9042, 0.9692, 0.8791, 0.9409] 
2023-10-27 09:39:58.465201: Epoch time: 3.54 s 
2023-10-27 09:39:58.465431: Yayy! New best EMA pseudo Dice: 0.8933 
2023-10-27 09:39:59.539684:  
2023-10-27 09:39:59.539999: Epoch 54 
2023-10-27 09:39:59.540271: Current learning rate: 0.00951 
2023-10-27 09:40:03.076672: train_loss -0.841 
2023-10-27 09:40:03.077419: val_loss -0.8848 
2023-10-27 09:40:03.077696: Pseudo dice [0.8722, 0.9027, 0.9681, 0.8703, 0.94] 
2023-10-27 09:40:03.077921: Epoch time: 3.54 s 
2023-10-27 09:40:03.078129: Yayy! New best EMA pseudo Dice: 0.895 
2023-10-27 09:40:04.159922:  
2023-10-27 09:40:04.160287: Epoch 55 
2023-10-27 09:40:04.160580: Current learning rate: 0.0095 
2023-10-27 09:40:07.699641: train_loss -0.8404 
2023-10-27 09:40:07.700321: val_loss -0.8836 
2023-10-27 09:40:07.700601: Pseudo dice [0.8716, 0.9043, 0.9705, 0.8894, 0.9404] 
2023-10-27 09:40:07.700831: Epoch time: 3.54 s 
2023-10-27 09:40:07.701044: Yayy! New best EMA pseudo Dice: 0.897 
2023-10-27 09:40:08.776811:  
2023-10-27 09:40:08.777223: Epoch 56 
2023-10-27 09:40:08.777474: Current learning rate: 0.00949 
2023-10-27 09:40:12.365621: train_loss -0.8468 
2023-10-27 09:40:12.366289: val_loss -0.8845 
2023-10-27 09:40:12.366569: Pseudo dice [0.8741, 0.9062, 0.967, 0.8871, 0.9389] 
2023-10-27 09:40:12.366811: Epoch time: 3.59 s 
2023-10-27 09:40:12.367027: Yayy! New best EMA pseudo Dice: 0.8988 
2023-10-27 09:40:13.444152:  
2023-10-27 09:40:13.444508: Epoch 57 
2023-10-27 09:40:13.444764: Current learning rate: 0.00949 
2023-10-27 09:40:16.952279: train_loss -0.841 
2023-10-27 09:40:16.953007: val_loss -0.8743 
2023-10-27 09:40:16.953280: Pseudo dice [0.8793, 0.9101, 0.9662, 0.5789, 0.9419] 
2023-10-27 09:40:16.953512: Epoch time: 3.51 s 
2023-10-27 09:40:17.956689:  
2023-10-27 09:40:17.957009: Epoch 58 
2023-10-27 09:40:17.957274: Current learning rate: 0.00948 
2023-10-27 09:40:21.407997: train_loss -0.8457 
2023-10-27 09:40:21.408714: val_loss -0.8809 
2023-10-27 09:40:21.408986: Pseudo dice [0.8681, 0.9031, 0.9681, 0.8268, 0.9357] 
2023-10-27 09:40:21.409223: Epoch time: 3.45 s 
2023-10-27 09:40:22.575103:  
2023-10-27 09:40:22.575441: Epoch 59 
2023-10-27 09:40:22.575704: Current learning rate: 0.00947 
2023-10-27 09:40:26.076502: train_loss -0.8403 
2023-10-27 09:40:26.077136: val_loss -0.8844 
2023-10-27 09:40:26.077415: Pseudo dice [0.8729, 0.9071, 0.9694, 0.8424, 0.9387] 
2023-10-27 09:40:26.077642: Epoch time: 3.5 s 
2023-10-27 09:40:27.091331:  
2023-10-27 09:40:27.091673: Epoch 60 
2023-10-27 09:40:27.091941: Current learning rate: 0.00946 
2023-10-27 09:40:30.629030: train_loss -0.8405 
2023-10-27 09:40:30.629674: val_loss -0.8547 
2023-10-27 09:40:30.629945: Pseudo dice [0.8744, 0.907, 0.9698, 0.4156, 0.9378] 
2023-10-27 09:40:30.630179: Epoch time: 3.54 s 
2023-10-27 09:40:31.644456:  
2023-10-27 09:40:31.644777: Epoch 61 
2023-10-27 09:40:31.645022: Current learning rate: 0.00945 
2023-10-27 09:40:35.238748: train_loss -0.8307 
2023-10-27 09:40:35.239557: val_loss -0.8794 
2023-10-27 09:40:35.239846: Pseudo dice [0.8721, 0.8944, 0.9663, 0.8457, 0.9354] 
2023-10-27 09:40:35.240118: Epoch time: 3.59 s 
2023-10-27 09:40:36.256179:  
2023-10-27 09:40:36.256597: Epoch 62 
2023-10-27 09:40:36.256879: Current learning rate: 0.00944 
2023-10-27 09:40:39.741556: train_loss -0.8362 
2023-10-27 09:40:39.742254: val_loss -0.8728 
2023-10-27 09:40:39.742531: Pseudo dice [0.8734, 0.8928, 0.967, 0.8163, 0.9397] 
2023-10-27 09:40:39.742768: Epoch time: 3.49 s 
2023-10-27 09:40:40.762871:  
2023-10-27 09:40:40.763251: Epoch 63 
2023-10-27 09:40:40.763572: Current learning rate: 0.00943 
2023-10-27 09:40:44.323954: train_loss -0.8383 
2023-10-27 09:40:44.324626: val_loss -0.8817 
2023-10-27 09:40:44.324910: Pseudo dice [0.8778, 0.9105, 0.9694, 0.7738, 0.9425] 
2023-10-27 09:40:44.325146: Epoch time: 3.56 s 
2023-10-27 09:40:45.344702:  
2023-10-27 09:40:45.345103: Epoch 64 
2023-10-27 09:40:45.345377: Current learning rate: 0.00942 
2023-10-27 09:40:48.883403: train_loss -0.8419 
2023-10-27 09:40:48.884098: val_loss -0.8846 
2023-10-27 09:40:48.884569: Pseudo dice [0.8802, 0.9153, 0.9682, 0.8226, 0.9443] 
2023-10-27 09:40:48.884830: Epoch time: 3.54 s 
2023-10-27 09:40:50.048632:  
2023-10-27 09:40:50.049060: Epoch 65 
2023-10-27 09:40:50.049331: Current learning rate: 0.00941 
2023-10-27 09:40:53.648253: train_loss -0.8391 
2023-10-27 09:40:53.649000: val_loss -0.8814 
2023-10-27 09:40:53.649283: Pseudo dice [0.8776, 0.9063, 0.9703, 0.8018, 0.9346] 
2023-10-27 09:40:53.649537: Epoch time: 3.6 s 
2023-10-27 09:40:54.683836:  
2023-10-27 09:40:54.684171: Epoch 66 
2023-10-27 09:40:54.684502: Current learning rate: 0.0094 
2023-10-27 09:40:58.226570: train_loss -0.8346 
2023-10-27 09:40:58.227256: val_loss -0.8839 
2023-10-27 09:40:58.227573: Pseudo dice [0.8707, 0.9108, 0.9686, 0.8301, 0.9418] 
2023-10-27 09:40:58.227815: Epoch time: 3.54 s 
2023-10-27 09:40:59.252924:  
2023-10-27 09:40:59.253259: Epoch 67 
2023-10-27 09:40:59.253538: Current learning rate: 0.00939 
2023-10-27 09:41:02.783363: train_loss -0.8453 
2023-10-27 09:41:02.784045: val_loss -0.8808 
2023-10-27 09:41:02.784334: Pseudo dice [0.884, 0.9084, 0.9694, 0.8916, 0.9283] 
2023-10-27 09:41:02.784606: Epoch time: 3.53 s 
2023-10-27 09:41:03.815713:  
2023-10-27 09:41:03.816089: Epoch 68 
2023-10-27 09:41:03.816368: Current learning rate: 0.00939 
2023-10-27 09:41:07.379989: train_loss -0.8405 
2023-10-27 09:41:07.381153: val_loss -0.881 
2023-10-27 09:41:07.381451: Pseudo dice [0.8815, 0.9139, 0.968, 0.8516, 0.941] 
2023-10-27 09:41:07.381698: Epoch time: 3.56 s 
2023-10-27 09:41:08.416454:  
2023-10-27 09:41:08.416786: Epoch 69 
2023-10-27 09:41:08.417062: Current learning rate: 0.00938 
2023-10-27 09:41:12.121161: train_loss -0.827 
2023-10-27 09:41:12.121863: val_loss -0.8787 
2023-10-27 09:41:12.122142: Pseudo dice [0.8768, 0.9077, 0.9709, 0.7655, 0.9427] 
2023-10-27 09:41:12.122380: Epoch time: 3.71 s 
2023-10-27 09:41:13.167161:  
2023-10-27 09:41:13.167506: Epoch 70 
2023-10-27 09:41:13.167775: Current learning rate: 0.00937 
2023-10-27 09:41:16.835083: train_loss -0.8419 
2023-10-27 09:41:16.835814: val_loss -0.889 
2023-10-27 09:41:16.836086: Pseudo dice [0.8772, 0.9119, 0.9699, 0.9029, 0.9407] 
2023-10-27 09:41:16.836321: Epoch time: 3.67 s 
2023-10-27 09:41:16.836545: Yayy! New best EMA pseudo Dice: 0.8998 
2023-10-27 09:41:18.083592:  
2023-10-27 09:41:18.083916: Epoch 71 
2023-10-27 09:41:18.084168: Current learning rate: 0.00936 
2023-10-27 09:41:21.597065: train_loss -0.8479 
2023-10-27 09:41:21.597769: val_loss -0.8882 
2023-10-27 09:41:21.598043: Pseudo dice [0.8776, 0.9158, 0.9706, 0.8785, 0.9455] 
2023-10-27 09:41:21.598292: Epoch time: 3.51 s 
2023-10-27 09:41:21.598520: Yayy! New best EMA pseudo Dice: 0.9016 
2023-10-27 09:41:22.796733:  
2023-10-27 09:41:22.797112: Epoch 72 
2023-10-27 09:41:22.797384: Current learning rate: 0.00935 
2023-10-27 09:41:26.304089: train_loss -0.8425 
2023-10-27 09:41:26.304762: val_loss -0.8736 
2023-10-27 09:41:26.305032: Pseudo dice [0.873, 0.898, 0.9684, 0.698, 0.9331] 
2023-10-27 09:41:26.305259: Epoch time: 3.51 s 
2023-10-27 09:41:27.336099:  
2023-10-27 09:41:27.336478: Epoch 73 
2023-10-27 09:41:27.336754: Current learning rate: 0.00934 
2023-10-27 09:41:30.820833: train_loss -0.8538 
2023-10-27 09:41:30.821477: val_loss -0.8913 
2023-10-27 09:41:30.821748: Pseudo dice [0.8807, 0.9055, 0.97, 0.9147, 0.9469] 
2023-10-27 09:41:30.821982: Epoch time: 3.49 s 
2023-10-27 09:41:31.853707:  
2023-10-27 09:41:31.854065: Epoch 74 
2023-10-27 09:41:31.854361: Current learning rate: 0.00933 
2023-10-27 09:41:35.370564: train_loss -0.8506 
2023-10-27 09:41:35.371490: val_loss -0.8907 
2023-10-27 09:41:35.371928: Pseudo dice [0.8853, 0.9122, 0.9686, 0.8725, 0.9389] 
2023-10-27 09:41:35.372225: Epoch time: 3.52 s 
2023-10-27 09:41:35.372514: Yayy! New best EMA pseudo Dice: 0.9027 
2023-10-27 09:41:36.481676:  
2023-10-27 09:41:36.481997: Epoch 75 
2023-10-27 09:41:36.482262: Current learning rate: 0.00932 
2023-10-27 09:41:40.003134: train_loss -0.8461 
2023-10-27 09:41:40.003795: val_loss -0.8928 
2023-10-27 09:41:40.004105: Pseudo dice [0.8815, 0.9082, 0.9697, 0.9216, 0.9413] 
2023-10-27 09:41:40.004334: Epoch time: 3.52 s 
2023-10-27 09:41:40.004569: Yayy! New best EMA pseudo Dice: 0.9049 
2023-10-27 09:41:41.248454:  
2023-10-27 09:41:41.248782: Epoch 76 
2023-10-27 09:41:41.249040: Current learning rate: 0.00931 
2023-10-27 09:41:44.810631: train_loss -0.8535 
2023-10-27 09:41:44.811313: val_loss -0.8903 
2023-10-27 09:41:44.811596: Pseudo dice [0.8805, 0.9099, 0.9706, 0.8446, 0.9454] 
2023-10-27 09:41:44.811843: Epoch time: 3.56 s 
2023-10-27 09:41:44.812053: Yayy! New best EMA pseudo Dice: 0.9054 
2023-10-27 09:41:45.917866:  
2023-10-27 09:41:45.918228: Epoch 77 
2023-10-27 09:41:45.918508: Current learning rate: 0.0093 
2023-10-27 09:41:49.456986: train_loss -0.8487 
2023-10-27 09:41:49.457646: val_loss -0.8927 
2023-10-27 09:41:49.457926: Pseudo dice [0.8802, 0.9144, 0.9692, 0.895, 0.947] 
2023-10-27 09:41:49.458159: Epoch time: 3.54 s 
2023-10-27 09:41:49.458373: Yayy! New best EMA pseudo Dice: 0.907 
2023-10-27 09:41:50.574699:  
2023-10-27 09:41:50.575068: Epoch 78 
2023-10-27 09:41:50.575351: Current learning rate: 0.0093 
2023-10-27 09:41:54.123712: train_loss -0.848 
2023-10-27 09:41:54.124402: val_loss -0.887 
2023-10-27 09:41:54.124670: Pseudo dice [0.8819, 0.9166, 0.9676, 0.8458, 0.9441] 
2023-10-27 09:41:54.124906: Epoch time: 3.55 s 
2023-10-27 09:41:54.125118: Yayy! New best EMA pseudo Dice: 0.9074 
2023-10-27 09:41:55.240605:  
2023-10-27 09:41:55.240935: Epoch 79 
2023-10-27 09:41:55.241210: Current learning rate: 0.00929 
2023-10-27 09:41:58.747588: train_loss -0.8482 
2023-10-27 09:41:58.748333: val_loss -0.8869 
2023-10-27 09:41:58.748695: Pseudo dice [0.8806, 0.8974, 0.9689, 0.8739, 0.9472] 
2023-10-27 09:41:58.748999: Epoch time: 3.51 s 
2023-10-27 09:41:58.749421: Yayy! New best EMA pseudo Dice: 0.908 
2023-10-27 09:41:59.859504:  
2023-10-27 09:41:59.859907: Epoch 80 
2023-10-27 09:41:59.860184: Current learning rate: 0.00928 
2023-10-27 09:42:03.363831: train_loss -0.8481 
2023-10-27 09:42:03.364477: val_loss -0.8953 
2023-10-27 09:42:03.364749: Pseudo dice [0.8878, 0.9134, 0.9706, 0.9218, 0.9472] 
2023-10-27 09:42:03.364980: Epoch time: 3.5 s 
2023-10-27 09:42:03.365195: Yayy! New best EMA pseudo Dice: 0.91 
2023-10-27 09:42:04.478642:  
2023-10-27 09:42:04.479021: Epoch 81 
2023-10-27 09:42:04.479308: Current learning rate: 0.00927 
2023-10-27 09:42:07.963671: train_loss -0.847 
2023-10-27 09:42:07.964420: val_loss -0.8821 
2023-10-27 09:42:07.964684: Pseudo dice [0.8759, 0.9109, 0.9677, 0.7143, 0.9467] 
2023-10-27 09:42:07.964907: Epoch time: 3.49 s 
2023-10-27 09:42:09.153864:  
2023-10-27 09:42:09.154259: Epoch 82 
2023-10-27 09:42:09.154557: Current learning rate: 0.00926 
2023-10-27 09:42:12.682862: train_loss -0.848 
2023-10-27 09:42:12.683538: val_loss -0.8851 
2023-10-27 09:42:12.683799: Pseudo dice [0.869, 0.9116, 0.9677, 0.8873, 0.946] 
2023-10-27 09:42:12.684027: Epoch time: 3.53 s 
2023-10-27 09:42:13.684867:  
2023-10-27 09:42:13.685218: Epoch 83 
2023-10-27 09:42:13.685484: Current learning rate: 0.00925 
2023-10-27 09:42:17.197254: train_loss -0.8502 
2023-10-27 09:42:17.197941: val_loss -0.8865 
2023-10-27 09:42:17.198203: Pseudo dice [0.8683, 0.8995, 0.9684, 0.8982, 0.9437] 
2023-10-27 09:42:17.198436: Epoch time: 3.51 s 
2023-10-27 09:42:18.194592:  
2023-10-27 09:42:18.194905: Epoch 84 
2023-10-27 09:42:18.195152: Current learning rate: 0.00924 
2023-10-27 09:42:21.705177: train_loss -0.858 
2023-10-27 09:42:21.705985: val_loss -0.8882 
2023-10-27 09:42:21.706264: Pseudo dice [0.8821, 0.9077, 0.9697, 0.8559, 0.941] 
2023-10-27 09:42:21.706527: Epoch time: 3.51 s 
2023-10-27 09:42:22.711042:  
2023-10-27 09:42:22.711365: Epoch 85 
2023-10-27 09:42:22.711616: Current learning rate: 0.00923 
2023-10-27 09:42:26.200371: train_loss -0.8466 
2023-10-27 09:42:26.201023: val_loss -0.8772 
2023-10-27 09:42:26.201329: Pseudo dice [0.883, 0.9111, 0.9694, 0.6461, 0.9467] 
2023-10-27 09:42:26.201566: Epoch time: 3.49 s 
2023-10-27 09:42:27.206968:  
2023-10-27 09:42:27.207302: Epoch 86 
2023-10-27 09:42:27.207574: Current learning rate: 0.00922 
2023-10-27 09:42:30.694018: train_loss -0.8563 
2023-10-27 09:42:30.694728: val_loss -0.8946 
2023-10-27 09:42:30.695083: Pseudo dice [0.8746, 0.9068, 0.9708, 0.8302, 0.9467] 
2023-10-27 09:42:30.695321: Epoch time: 3.49 s 
2023-10-27 09:42:31.703944:  
2023-10-27 09:42:31.704265: Epoch 87 
2023-10-27 09:42:31.704528: Current learning rate: 0.00921 
2023-10-27 09:42:35.281199: train_loss -0.8497 
2023-10-27 09:42:35.281910: val_loss -0.8918 
2023-10-27 09:42:35.282218: Pseudo dice [0.874, 0.91, 0.9672, 0.8816, 0.9467] 
2023-10-27 09:42:35.282554: Epoch time: 3.58 s 
2023-10-27 09:42:36.292460:  
2023-10-27 09:42:36.292787: Epoch 88 
2023-10-27 09:42:36.293042: Current learning rate: 0.0092 
2023-10-27 09:42:39.989218: train_loss -0.845 
2023-10-27 09:42:39.989900: val_loss -0.889 
2023-10-27 09:42:39.990171: Pseudo dice [0.8793, 0.9126, 0.9691, 0.8983, 0.9436] 
2023-10-27 09:42:39.990421: Epoch time: 3.7 s 
2023-10-27 09:42:40.999484:  
2023-10-27 09:42:40.999805: Epoch 89 
2023-10-27 09:42:41.000059: Current learning rate: 0.0092 
2023-10-27 09:42:44.542722: train_loss -0.8463 
2023-10-27 09:42:44.543658: val_loss -0.8913 
2023-10-27 09:42:44.543961: Pseudo dice [0.8796, 0.9121, 0.9703, 0.8458, 0.9371] 
2023-10-27 09:42:44.544288: Epoch time: 3.54 s 
2023-10-27 09:42:45.544769:  
2023-10-27 09:42:45.545187: Epoch 90 
2023-10-27 09:42:45.545492: Current learning rate: 0.00919 
2023-10-27 09:42:49.085185: train_loss -0.8387 
2023-10-27 09:42:49.086005: val_loss -0.8889 
2023-10-27 09:42:49.086413: Pseudo dice [0.8769, 0.9072, 0.9688, 0.8887, 0.9401] 
2023-10-27 09:42:49.086746: Epoch time: 3.54 s 
2023-10-27 09:42:50.090105:  
2023-10-27 09:42:50.090503: Epoch 91 
2023-10-27 09:42:50.090761: Current learning rate: 0.00918 
2023-10-27 09:42:53.733364: train_loss -0.8484 
2023-10-27 09:42:53.734058: val_loss -0.8899 
2023-10-27 09:42:53.734340: Pseudo dice [0.8761, 0.9082, 0.9664, 0.8663, 0.9421] 
2023-10-27 09:42:53.734594: Epoch time: 3.64 s 
2023-10-27 09:42:54.730639:  
2023-10-27 09:42:54.730967: Epoch 92 
2023-10-27 09:42:54.731222: Current learning rate: 0.00917 
2023-10-27 09:42:58.269763: train_loss -0.8478 
2023-10-27 09:42:58.270484: val_loss -0.8894 
2023-10-27 09:42:58.270787: Pseudo dice [0.8797, 0.9161, 0.971, 0.8437, 0.9447] 
2023-10-27 09:42:58.271019: Epoch time: 3.54 s 
2023-10-27 09:42:59.263679:  
2023-10-27 09:42:59.264041: Epoch 93 
2023-10-27 09:42:59.264300: Current learning rate: 0.00916 
2023-10-27 09:43:02.805194: train_loss -0.8424 
2023-10-27 09:43:02.805860: val_loss -0.8891 
2023-10-27 09:43:02.806136: Pseudo dice [0.879, 0.9101, 0.9699, 0.8781, 0.9387] 
2023-10-27 09:43:02.806364: Epoch time: 3.54 s 
2023-10-27 09:43:03.805491:  
2023-10-27 09:43:03.805828: Epoch 94 
2023-10-27 09:43:03.806082: Current learning rate: 0.00915 
2023-10-27 09:43:07.371022: train_loss -0.833 
2023-10-27 09:43:07.371785: val_loss -0.8867 
2023-10-27 09:43:07.372070: Pseudo dice [0.872, 0.9073, 0.9683, 0.8971, 0.9381] 
2023-10-27 09:43:07.372307: Epoch time: 3.57 s 
2023-10-27 09:43:07.372525: Yayy! New best EMA pseudo Dice: 0.9106 
2023-10-27 09:43:08.584909:  
2023-10-27 09:43:08.585310: Epoch 95 
2023-10-27 09:43:08.585570: Current learning rate: 0.00914 
2023-10-27 09:43:12.152792: train_loss -0.8415 
2023-10-27 09:43:12.153497: val_loss -0.8822 
2023-10-27 09:43:12.153800: Pseudo dice [0.8655, 0.9093, 0.9668, 0.9094, 0.9391] 
2023-10-27 09:43:12.154045: Epoch time: 3.57 s 
2023-10-27 09:43:12.154258: Yayy! New best EMA pseudo Dice: 0.9113 
2023-10-27 09:43:13.218011:  
2023-10-27 09:43:13.218375: Epoch 96 
2023-10-27 09:43:13.218662: Current learning rate: 0.00913 
2023-10-27 09:43:16.955407: train_loss -0.8502 
2023-10-27 09:43:16.956155: val_loss -0.8474 
2023-10-27 09:43:16.956425: Pseudo dice [0.8697, 0.9009, 0.9695, 0.0057, 0.9403] 
2023-10-27 09:43:16.956686: Epoch time: 3.74 s 
2023-10-27 09:43:17.970343:  
2023-10-27 09:43:17.970686: Epoch 97 
2023-10-27 09:43:17.970970: Current learning rate: 0.00912 
2023-10-27 09:43:21.650815: train_loss -0.8416 
2023-10-27 09:43:21.651608: val_loss -0.8892 
2023-10-27 09:43:21.651876: Pseudo dice [0.885, 0.9094, 0.9683, 0.8979, 0.946] 
2023-10-27 09:43:21.652111: Epoch time: 3.68 s 
2023-10-27 09:43:22.665369:  
2023-10-27 09:43:22.665721: Epoch 98 
2023-10-27 09:43:22.665985: Current learning rate: 0.00911 
2023-10-27 09:43:26.278502: train_loss -0.8417 
2023-10-27 09:43:26.279234: val_loss -0.8877 
2023-10-27 09:43:26.279532: Pseudo dice [0.8807, 0.9145, 0.9639, 0.9148, 0.9433] 
2023-10-27 09:43:26.279784: Epoch time: 3.61 s 
2023-10-27 09:43:27.292833:  
2023-10-27 09:43:27.293231: Epoch 99 
2023-10-27 09:43:27.293529: Current learning rate: 0.0091 
2023-10-27 09:43:30.822614: train_loss -0.8454 
2023-10-27 09:43:30.823320: val_loss -0.8909 
2023-10-27 09:43:30.823610: Pseudo dice [0.8804, 0.9132, 0.971, 0.8679, 0.9454] 
2023-10-27 09:43:30.823848: Epoch time: 3.53 s 
2023-10-27 09:43:31.900952:  
2023-10-27 09:43:31.901359: Epoch 100 
2023-10-27 09:43:31.901625: Current learning rate: 0.0091 
2023-10-27 09:43:35.460935: train_loss -0.841 
2023-10-27 09:43:35.461612: val_loss -0.8855 
2023-10-27 09:43:35.461904: Pseudo dice [0.8842, 0.9099, 0.9678, 0.9011, 0.9456] 
2023-10-27 09:43:35.462141: Epoch time: 3.56 s 
2023-10-27 09:43:36.611285:  
2023-10-27 09:43:36.611627: Epoch 101 
2023-10-27 09:43:36.611883: Current learning rate: 0.00909 
2023-10-27 09:43:40.166816: train_loss -0.8505 
2023-10-27 09:43:40.167604: val_loss -0.8908 
2023-10-27 09:43:40.167884: Pseudo dice [0.8777, 0.9076, 0.9714, 0.8728, 0.9456] 
2023-10-27 09:43:40.168125: Epoch time: 3.56 s 
2023-10-27 09:43:41.189433:  
2023-10-27 09:43:41.189828: Epoch 102 
2023-10-27 09:43:41.190082: Current learning rate: 0.00908 
2023-10-27 09:43:44.773939: train_loss -0.8524 
2023-10-27 09:43:44.774621: val_loss -0.8885 
2023-10-27 09:43:44.774917: Pseudo dice [0.8817, 0.9124, 0.9704, 0.8999, 0.9423] 
2023-10-27 09:43:44.775168: Epoch time: 3.59 s 
2023-10-27 09:43:45.780634:  
2023-10-27 09:43:45.780996: Epoch 103 
2023-10-27 09:43:45.781268: Current learning rate: 0.00907 
2023-10-27 09:43:49.319768: train_loss -0.8581 
2023-10-27 09:43:49.320476: val_loss -0.891 
2023-10-27 09:43:49.320747: Pseudo dice [0.8847, 0.9105, 0.9666, 0.9024, 0.9489] 
2023-10-27 09:43:49.320982: Epoch time: 3.54 s 
2023-10-27 09:43:50.326693:  
2023-10-27 09:43:50.327016: Epoch 104 
2023-10-27 09:43:50.327286: Current learning rate: 0.00906 
2023-10-27 09:43:53.899960: train_loss -0.8514 
2023-10-27 09:43:53.900671: val_loss -0.8806 
2023-10-27 09:43:53.900936: Pseudo dice [0.8784, 0.9075, 0.9663, 0.7203, 0.9438] 
2023-10-27 09:43:53.901168: Epoch time: 3.57 s 
2023-10-27 09:43:54.907135:  
2023-10-27 09:43:54.907443: Epoch 105 
2023-10-27 09:43:54.907699: Current learning rate: 0.00905 
2023-10-27 09:43:58.419665: train_loss -0.8407 
2023-10-27 09:43:58.420290: val_loss -0.8895 
2023-10-27 09:43:58.420568: Pseudo dice [0.8731, 0.9087, 0.9678, 0.8943, 0.9441] 
2023-10-27 09:43:58.420796: Epoch time: 3.51 s 
2023-10-27 09:43:59.421498:  
2023-10-27 09:43:59.421814: Epoch 106 
2023-10-27 09:43:59.422079: Current learning rate: 0.00904 
2023-10-27 09:44:02.954944: train_loss -0.8514 
2023-10-27 09:44:02.955613: val_loss -0.8889 
2023-10-27 09:44:02.955894: Pseudo dice [0.8772, 0.9118, 0.9715, 0.8393, 0.9333] 
2023-10-27 09:44:02.956129: Epoch time: 3.53 s 
2023-10-27 09:44:03.960897:  
2023-10-27 09:44:03.961217: Epoch 107 
2023-10-27 09:44:03.961502: Current learning rate: 0.00903 
2023-10-27 09:44:07.606021: train_loss -0.8533 
2023-10-27 09:44:07.606668: val_loss -0.8956 
2023-10-27 09:44:07.606933: Pseudo dice [0.8849, 0.9149, 0.9719, 0.9054, 0.9448] 
2023-10-27 09:44:07.607150: Epoch time: 3.65 s 
2023-10-27 09:44:08.614811:  
2023-10-27 09:44:08.615155: Epoch 108 
2023-10-27 09:44:08.615428: Current learning rate: 0.00902 
2023-10-27 09:44:12.159075: train_loss -0.853 
2023-10-27 09:44:12.159830: val_loss -0.8874 
2023-10-27 09:44:12.160117: Pseudo dice [0.8774, 0.9073, 0.9682, 0.8689, 0.9456] 
2023-10-27 09:44:12.160364: Epoch time: 3.55 s 
2023-10-27 09:44:13.170548:  
2023-10-27 09:44:13.170961: Epoch 109 
2023-10-27 09:44:13.171290: Current learning rate: 0.00901 
2023-10-27 09:44:16.715645: train_loss -0.8523 
2023-10-27 09:44:16.716322: val_loss -0.8884 
2023-10-27 09:44:16.716606: Pseudo dice [0.8795, 0.9152, 0.9711, 0.8581, 0.9483] 
2023-10-27 09:44:16.716838: Epoch time: 3.55 s 
2023-10-27 09:44:17.722792:  
2023-10-27 09:44:17.723196: Epoch 110 
2023-10-27 09:44:17.723493: Current learning rate: 0.009 
2023-10-27 09:44:21.226780: train_loss -0.8597 
2023-10-27 09:44:21.227469: val_loss -0.8946 
2023-10-27 09:44:21.227759: Pseudo dice [0.8816, 0.9125, 0.9699, 0.9107, 0.9404] 
2023-10-27 09:44:21.228034: Epoch time: 3.5 s 
2023-10-27 09:44:22.236724:  
2023-10-27 09:44:22.237067: Epoch 111 
2023-10-27 09:44:22.237336: Current learning rate: 0.009 
2023-10-27 09:44:25.794177: train_loss -0.8611 
2023-10-27 09:44:25.794846: val_loss -0.8937 
2023-10-27 09:44:25.795120: Pseudo dice [0.8817, 0.9148, 0.9688, 0.841, 0.9472] 
2023-10-27 09:44:25.795353: Epoch time: 3.56 s 
2023-10-27 09:44:26.801197:  
2023-10-27 09:44:26.801534: Epoch 112 
2023-10-27 09:44:26.801815: Current learning rate: 0.00899 
2023-10-27 09:44:30.325710: train_loss -0.8563 
2023-10-27 09:44:30.326341: val_loss -0.8967 
2023-10-27 09:44:30.326640: Pseudo dice [0.8854, 0.9122, 0.9684, 0.885, 0.9506] 
2023-10-27 09:44:30.326878: Epoch time: 3.53 s 
2023-10-27 09:44:30.327088: Yayy! New best EMA pseudo Dice: 0.9117 
2023-10-27 09:44:31.408069:  
2023-10-27 09:44:31.408533: Epoch 113 
2023-10-27 09:44:31.408787: Current learning rate: 0.00898 
2023-10-27 09:44:34.955547: train_loss -0.8602 
2023-10-27 09:44:34.955994: val_loss -0.8857 
2023-10-27 09:44:34.956292: Pseudo dice [0.8813, 0.9084, 0.9699, 0.8575, 0.9439] 
2023-10-27 09:44:34.956540: Epoch time: 3.55 s 
2023-10-27 09:44:34.956759: Yayy! New best EMA pseudo Dice: 0.9117 
2023-10-27 09:44:36.180159:  
2023-10-27 09:44:36.180557: Epoch 114 
2023-10-27 09:44:36.180823: Current learning rate: 0.00897 
2023-10-27 09:44:39.725371: train_loss -0.8554 
2023-10-27 09:44:39.726353: val_loss -0.8885 
2023-10-27 09:44:39.726665: Pseudo dice [0.8856, 0.913, 0.9697, 0.8469, 0.9488] 
2023-10-27 09:44:39.726929: Epoch time: 3.55 s 
2023-10-27 09:44:39.727142: Yayy! New best EMA pseudo Dice: 0.9118 
2023-10-27 09:44:40.802620:  
2023-10-27 09:44:40.803019: Epoch 115 
2023-10-27 09:44:40.803266: Current learning rate: 0.00896 
2023-10-27 09:44:44.350616: train_loss -0.863 
2023-10-27 09:44:44.351387: val_loss -0.898 
2023-10-27 09:44:44.351670: Pseudo dice [0.8881, 0.9171, 0.972, 0.8853, 0.9475] 
2023-10-27 09:44:44.351910: Epoch time: 3.55 s 
2023-10-27 09:44:44.352124: Yayy! New best EMA pseudo Dice: 0.9128 
2023-10-27 09:44:45.440478:  
2023-10-27 09:44:45.440788: Epoch 116 
2023-10-27 09:44:45.441043: Current learning rate: 0.00895 
2023-10-27 09:44:48.957104: train_loss -0.8576 
2023-10-27 09:44:48.957744: val_loss -0.8882 
2023-10-27 09:44:48.958028: Pseudo dice [0.8844, 0.9151, 0.9691, 0.8381, 0.9457] 
2023-10-27 09:44:48.958273: Epoch time: 3.52 s 
2023-10-27 09:44:49.974991:  
2023-10-27 09:44:49.975376: Epoch 117 
2023-10-27 09:44:49.975653: Current learning rate: 0.00894 
2023-10-27 09:44:53.478907: train_loss -0.8578 
2023-10-27 09:44:53.479573: val_loss -0.8897 
2023-10-27 09:44:53.479835: Pseudo dice [0.8754, 0.9065, 0.9725, 0.8542, 0.9456] 
2023-10-27 09:44:53.480054: Epoch time: 3.5 s 
2023-10-27 09:44:54.498698:  
2023-10-27 09:44:54.499008: Epoch 118 
2023-10-27 09:44:54.499261: Current learning rate: 0.00893 
2023-10-27 09:44:58.026428: train_loss -0.8575 
2023-10-27 09:44:58.027042: val_loss -0.8968 
2023-10-27 09:44:58.027323: Pseudo dice [0.881, 0.9136, 0.9718, 0.9124, 0.9463] 
2023-10-27 09:44:58.027562: Epoch time: 3.53 s 
2023-10-27 09:44:58.027777: Yayy! New best EMA pseudo Dice: 0.9137 
2023-10-27 09:44:59.251158:  
2023-10-27 09:44:59.251465: Epoch 119 
2023-10-27 09:44:59.251713: Current learning rate: 0.00892 
2023-10-27 09:45:02.821404: train_loss -0.8534 
2023-10-27 09:45:02.821811: val_loss -0.8826 
2023-10-27 09:45:02.822075: Pseudo dice [0.8697, 0.9022, 0.9672, 0.8136, 0.9468] 
2023-10-27 09:45:02.822313: Epoch time: 3.57 s 
2023-10-27 09:45:03.843472:  
2023-10-27 09:45:03.843823: Epoch 120 
2023-10-27 09:45:03.844080: Current learning rate: 0.00891 
2023-10-27 09:45:07.360734: train_loss -0.8369 
2023-10-27 09:45:07.361209: val_loss -0.8868 
2023-10-27 09:45:07.361626: Pseudo dice [0.8728, 0.9112, 0.9689, 0.8744, 0.9404] 
2023-10-27 09:45:07.361968: Epoch time: 3.52 s 
2023-10-27 09:45:08.382735:  
2023-10-27 09:45:08.383127: Epoch 121 
2023-10-27 09:45:08.383382: Current learning rate: 0.0089 
2023-10-27 09:45:11.920202: train_loss -0.8431 
2023-10-27 09:45:11.920708: val_loss -0.8869 
2023-10-27 09:45:11.921013: Pseudo dice [0.88, 0.9103, 0.9681, 0.8801, 0.9431] 
2023-10-27 09:45:11.921236: Epoch time: 3.54 s 
2023-10-27 09:45:12.942050:  
2023-10-27 09:45:12.942363: Epoch 122 
2023-10-27 09:45:12.942619: Current learning rate: 0.00889 
2023-10-27 09:45:16.420695: train_loss -0.8427 
2023-10-27 09:45:16.421083: val_loss -0.8778 
2023-10-27 09:45:16.421346: Pseudo dice [0.8779, 0.9066, 0.9682, 0.748, 0.9358] 
2023-10-27 09:45:16.421601: Epoch time: 3.48 s 
2023-10-27 09:45:17.437556:  
2023-10-27 09:45:17.437935: Epoch 123 
2023-10-27 09:45:17.438187: Current learning rate: 0.00889 
2023-10-27 09:45:20.957826: train_loss -0.8551 
2023-10-27 09:45:20.958225: val_loss -0.86 
2023-10-27 09:45:20.958504: Pseudo dice [0.8805, 0.9044, 0.9656, 0.4201, 0.9406] 
2023-10-27 09:45:20.958741: Epoch time: 3.52 s 
2023-10-27 09:45:21.976402:  
2023-10-27 09:45:21.976727: Epoch 124 
2023-10-27 09:45:21.976985: Current learning rate: 0.00888 
2023-10-27 09:45:25.522367: train_loss -0.8526 
2023-10-27 09:45:25.522769: val_loss -0.8903 
2023-10-27 09:45:25.523034: Pseudo dice [0.8765, 0.9078, 0.9686, 0.8979, 0.9473] 
2023-10-27 09:45:25.523271: Epoch time: 3.55 s 
2023-10-27 09:45:26.562200:  
2023-10-27 09:45:26.562533: Epoch 125 
2023-10-27 09:45:26.562831: Current learning rate: 0.00887 
2023-10-27 09:45:30.091067: train_loss -0.8411 
2023-10-27 09:45:30.091471: val_loss -0.8301 
2023-10-27 09:45:30.091764: Pseudo dice [0.8628, 0.889, 0.9671, 0.0, 0.9396] 
2023-10-27 09:45:30.092014: Epoch time: 3.53 s 
2023-10-27 09:45:31.251946:  
2023-10-27 09:45:31.252341: Epoch 126 
2023-10-27 09:45:31.252598: Current learning rate: 0.00886 
2023-10-27 09:45:34.798483: train_loss -0.8368 
2023-10-27 09:45:34.799033: val_loss -0.8866 
2023-10-27 09:45:34.799314: Pseudo dice [0.8835, 0.9115, 0.9671, 0.9028, 0.9289] 
2023-10-27 09:45:34.799563: Epoch time: 3.55 s 
2023-10-27 09:45:35.818724:  
2023-10-27 09:45:35.819072: Epoch 127 
2023-10-27 09:45:35.819320: Current learning rate: 0.00885 
2023-10-27 09:45:39.337121: train_loss -0.8278 
2023-10-27 09:45:39.337512: val_loss -0.8751 
2023-10-27 09:45:39.337802: Pseudo dice [0.8706, 0.9002, 0.9685, 0.8449, 0.9451] 
2023-10-27 09:45:39.338059: Epoch time: 3.52 s 
2023-10-27 09:45:40.364214:  
2023-10-27 09:45:40.364577: Epoch 128 
2023-10-27 09:45:40.364830: Current learning rate: 0.00884 
2023-10-27 09:45:43.891040: train_loss -0.8338 
2023-10-27 09:45:43.891525: val_loss -0.8861 
2023-10-27 09:45:43.891816: Pseudo dice [0.8843, 0.9036, 0.9713, 0.9008, 0.9437] 
2023-10-27 09:45:43.892056: Epoch time: 3.53 s 
2023-10-27 09:45:44.916653:  
2023-10-27 09:45:44.917006: Epoch 129 
2023-10-27 09:45:44.917254: Current learning rate: 0.00883 
2023-10-27 09:45:48.462230: train_loss -0.8346 
2023-10-27 09:45:48.462627: val_loss -0.8832 
2023-10-27 09:45:48.462894: Pseudo dice [0.8719, 0.9026, 0.9693, 0.9029, 0.9404] 
2023-10-27 09:45:48.463133: Epoch time: 3.55 s 
2023-10-27 09:45:49.488322:  
2023-10-27 09:45:49.488711: Epoch 130 
2023-10-27 09:45:49.488970: Current learning rate: 0.00882 
2023-10-27 09:45:53.003708: train_loss -0.8493 
2023-10-27 09:45:53.004226: val_loss -0.8811 
2023-10-27 09:45:53.004518: Pseudo dice [0.881, 0.912, 0.9701, 0.7294, 0.9336] 
2023-10-27 09:45:53.004805: Epoch time: 3.52 s 
2023-10-27 09:45:54.026697:  
2023-10-27 09:45:54.027022: Epoch 131 
2023-10-27 09:45:54.027297: Current learning rate: 0.00881 
2023-10-27 09:45:57.507300: train_loss -0.8548 
2023-10-27 09:45:57.507697: val_loss -0.8809 
2023-10-27 09:45:57.507952: Pseudo dice [0.8762, 0.9045, 0.9706, 0.8084, 0.9431] 
2023-10-27 09:45:57.508179: Epoch time: 3.48 s 
2023-10-27 09:45:58.664584:  
2023-10-27 09:45:58.664934: Epoch 132 
2023-10-27 09:45:58.665202: Current learning rate: 0.0088 
2023-10-27 09:46:02.174948: train_loss -0.8539 
2023-10-27 09:46:02.175388: val_loss -0.8939 
2023-10-27 09:46:02.175658: Pseudo dice [0.8853, 0.9178, 0.97, 0.8675, 0.9447] 
2023-10-27 09:46:02.175978: Epoch time: 3.51 s 
2023-10-27 09:46:03.200979:  
2023-10-27 09:46:03.201376: Epoch 133 
2023-10-27 09:46:03.201667: Current learning rate: 0.00879 
2023-10-27 09:46:06.696539: train_loss -0.8515 
2023-10-27 09:46:06.697067: val_loss -0.8893 
2023-10-27 09:46:06.697342: Pseudo dice [0.8732, 0.9126, 0.9697, 0.8987, 0.9448] 
2023-10-27 09:46:06.697569: Epoch time: 3.5 s 
2023-10-27 09:46:07.720869:  
2023-10-27 09:46:07.721253: Epoch 134 
2023-10-27 09:46:07.721535: Current learning rate: 0.00879 
2023-10-27 09:46:11.252027: train_loss -0.8532 
2023-10-27 09:46:11.252422: val_loss -0.883 
2023-10-27 09:46:11.252690: Pseudo dice [0.8721, 0.9033, 0.9701, 0.8003, 0.9378] 
2023-10-27 09:46:11.252921: Epoch time: 3.53 s 
2023-10-27 09:46:12.286585:  
2023-10-27 09:46:12.286952: Epoch 135 
2023-10-27 09:46:12.287223: Current learning rate: 0.00878 
2023-10-27 09:46:15.807361: train_loss -0.8481 
2023-10-27 09:46:15.808112: val_loss -0.8909 
2023-10-27 09:46:15.808466: Pseudo dice [0.8833, 0.9164, 0.968, 0.8663, 0.9466] 
2023-10-27 09:46:15.808750: Epoch time: 3.52 s 
2023-10-27 09:46:16.840805:  
2023-10-27 09:46:16.841163: Epoch 136 
2023-10-27 09:46:16.841437: Current learning rate: 0.00877 
2023-10-27 09:46:20.482001: train_loss -0.8593 
2023-10-27 09:46:20.482647: val_loss -0.8922 
2023-10-27 09:46:20.482947: Pseudo dice [0.8759, 0.9129, 0.9696, 0.8987, 0.9367] 
2023-10-27 09:46:20.483177: Epoch time: 3.64 s 
2023-10-27 09:46:21.517698:  
2023-10-27 09:46:21.518072: Epoch 137 
2023-10-27 09:46:21.518341: Current learning rate: 0.00876 
2023-10-27 09:46:25.064502: train_loss -0.8515 
2023-10-27 09:46:25.064994: val_loss -0.8885 
2023-10-27 09:46:25.065291: Pseudo dice [0.8834, 0.9106, 0.9691, 0.8431, 0.9463] 
2023-10-27 09:46:25.065558: Epoch time: 3.55 s 
2023-10-27 09:46:26.229023:  
2023-10-27 09:46:26.229347: Epoch 138 
2023-10-27 09:46:26.229604: Current learning rate: 0.00875 
2023-10-27 09:46:29.777862: train_loss -0.8517 
2023-10-27 09:46:29.778379: val_loss -0.8936 
2023-10-27 09:46:29.778661: Pseudo dice [0.8885, 0.9168, 0.9697, 0.856, 0.9483] 
2023-10-27 09:46:29.778900: Epoch time: 3.55 s 
2023-10-27 09:46:30.816788:  
2023-10-27 09:46:30.817177: Epoch 139 
2023-10-27 09:46:30.817491: Current learning rate: 0.00874 
2023-10-27 09:46:34.375483: train_loss -0.848 
2023-10-27 09:46:34.375871: val_loss -0.8894 
2023-10-27 09:46:34.376139: Pseudo dice [0.8796, 0.9118, 0.9678, 0.8675, 0.944] 
2023-10-27 09:46:34.376371: Epoch time: 3.56 s 
2023-10-27 09:46:35.413514:  
2023-10-27 09:46:35.413887: Epoch 140 
2023-10-27 09:46:35.414142: Current learning rate: 0.00873 
2023-10-27 09:46:38.945701: train_loss -0.8471 
2023-10-27 09:46:38.946087: val_loss -0.8861 
2023-10-27 09:46:38.946363: Pseudo dice [0.8809, 0.9111, 0.9693, 0.8978, 0.9449] 
2023-10-27 09:46:38.946627: Epoch time: 3.53 s 
2023-10-27 09:46:39.983847:  
2023-10-27 09:46:39.984241: Epoch 141 
2023-10-27 09:46:39.984530: Current learning rate: 0.00872 
2023-10-27 09:46:43.537369: train_loss -0.8502 
2023-10-27 09:46:43.537772: val_loss -0.8888 
2023-10-27 09:46:43.538039: Pseudo dice [0.8854, 0.9162, 0.9675, 0.8555, 0.9367] 
2023-10-27 09:46:43.538270: Epoch time: 3.55 s 
2023-10-27 09:46:44.575952:  
2023-10-27 09:46:44.576276: Epoch 142 
2023-10-27 09:46:44.576574: Current learning rate: 0.00871 
2023-10-27 09:46:48.101702: train_loss -0.8516 
2023-10-27 09:46:48.102220: val_loss -0.8914 
2023-10-27 09:46:48.102489: Pseudo dice [0.8838, 0.9081, 0.9695, 0.8889, 0.9467] 
2023-10-27 09:46:48.102721: Epoch time: 3.53 s 
2023-10-27 09:46:49.134464:  
2023-10-27 09:46:49.134820: Epoch 143 
2023-10-27 09:46:49.135076: Current learning rate: 0.0087 
2023-10-27 09:46:52.676775: train_loss -0.8484 
2023-10-27 09:46:52.677170: val_loss -0.891 
2023-10-27 09:46:52.677465: Pseudo dice [0.8738, 0.9112, 0.9672, 0.8763, 0.9453] 
2023-10-27 09:46:52.677716: Epoch time: 3.54 s 
2023-10-27 09:46:53.853855:  
2023-10-27 09:46:53.854208: Epoch 144 
2023-10-27 09:46:53.854486: Current learning rate: 0.00869 
2023-10-27 09:46:57.379550: train_loss -0.8501 
2023-10-27 09:46:57.379982: val_loss -0.8625 
2023-10-27 09:46:57.380248: Pseudo dice [0.8796, 0.9104, 0.9666, 0.518, 0.9398] 
2023-10-27 09:46:57.380511: Epoch time: 3.53 s 
2023-10-27 09:46:58.421431:  
2023-10-27 09:46:58.421833: Epoch 145 
2023-10-27 09:46:58.422079: Current learning rate: 0.00868 
2023-10-27 09:47:02.021833: train_loss -0.8546 
2023-10-27 09:47:02.022255: val_loss -0.8948 
2023-10-27 09:47:02.022542: Pseudo dice [0.8886, 0.9159, 0.9709, 0.8852, 0.9472] 
2023-10-27 09:47:02.022788: Epoch time: 3.6 s 
2023-10-27 09:47:03.063036:  
2023-10-27 09:47:03.063449: Epoch 146 
2023-10-27 09:47:03.063707: Current learning rate: 0.00868 
2023-10-27 09:47:06.577258: train_loss -0.8565 
2023-10-27 09:47:06.577654: val_loss -0.8943 
2023-10-27 09:47:06.577931: Pseudo dice [0.8857, 0.9112, 0.9705, 0.9057, 0.9442] 
2023-10-27 09:47:06.578155: Epoch time: 3.51 s 
2023-10-27 09:47:07.618140:  
2023-10-27 09:47:07.618533: Epoch 147 
2023-10-27 09:47:07.618800: Current learning rate: 0.00867 
2023-10-27 09:47:11.124229: train_loss -0.8537 
2023-10-27 09:47:11.124629: val_loss -0.892 
2023-10-27 09:47:11.124897: Pseudo dice [0.8852, 0.9089, 0.9708, 0.8376, 0.9433] 
2023-10-27 09:47:11.125129: Epoch time: 3.51 s 
2023-10-27 09:47:12.164470:  
2023-10-27 09:47:12.164863: Epoch 148 
2023-10-27 09:47:12.165153: Current learning rate: 0.00866 
2023-10-27 09:47:15.711870: train_loss -0.8504 
2023-10-27 09:47:15.712238: val_loss -0.8879 
2023-10-27 09:47:15.712496: Pseudo dice [0.8796, 0.9136, 0.9685, 0.8139, 0.9471] 
2023-10-27 09:47:15.712717: Epoch time: 3.55 s 
2023-10-27 09:47:16.756552:  
2023-10-27 09:47:16.756948: Epoch 149 
2023-10-27 09:47:16.757215: Current learning rate: 0.00865 
2023-10-27 09:47:20.255308: train_loss -0.8597 
2023-10-27 09:47:20.255694: val_loss -0.8819 
2023-10-27 09:47:20.255957: Pseudo dice [0.8824, 0.91, 0.9683, 0.8723, 0.9387] 
2023-10-27 09:47:20.256192: Epoch time: 3.5 s 
2023-10-27 09:47:21.505528:  
2023-10-27 09:47:21.505838: Epoch 150 
2023-10-27 09:47:21.506089: Current learning rate: 0.00864 
2023-10-27 09:47:25.081879: train_loss -0.8588 
2023-10-27 09:47:25.082325: val_loss -0.8884 
2023-10-27 09:47:25.082597: Pseudo dice [0.8783, 0.9104, 0.9693, 0.8955, 0.928] 
2023-10-27 09:47:25.082825: Epoch time: 3.58 s 
2023-10-27 09:47:26.122421:  
2023-10-27 09:47:26.122812: Epoch 151 
2023-10-27 09:47:26.123063: Current learning rate: 0.00863 
2023-10-27 09:47:29.655026: train_loss -0.8579 
2023-10-27 09:47:29.655391: val_loss -0.8922 
2023-10-27 09:47:29.655667: Pseudo dice [0.8893, 0.9134, 0.9703, 0.8741, 0.9457] 
2023-10-27 09:47:29.655900: Epoch time: 3.53 s 
2023-10-27 09:47:30.692302:  
2023-10-27 09:47:30.692691: Epoch 152 
2023-10-27 09:47:30.692966: Current learning rate: 0.00862 
2023-10-27 09:47:34.221347: train_loss -0.8609 
2023-10-27 09:47:34.221805: val_loss -0.8897 
2023-10-27 09:47:34.222126: Pseudo dice [0.8825, 0.9131, 0.9708, 0.8763, 0.9467] 
2023-10-27 09:47:34.222372: Epoch time: 3.53 s 
2023-10-27 09:47:35.255999:  
2023-10-27 09:47:35.256327: Epoch 153 
2023-10-27 09:47:35.256606: Current learning rate: 0.00861 
2023-10-27 09:47:38.788191: train_loss -0.8586 
2023-10-27 09:47:38.788572: val_loss -0.8894 
2023-10-27 09:47:38.788833: Pseudo dice [0.8858, 0.9126, 0.9703, 0.8427, 0.9483] 
2023-10-27 09:47:38.789130: Epoch time: 3.53 s 
2023-10-27 09:47:39.834873:  
2023-10-27 09:47:39.835199: Epoch 154 
2023-10-27 09:47:39.835489: Current learning rate: 0.0086 
2023-10-27 09:47:43.362535: train_loss -0.8593 
2023-10-27 09:47:43.362896: val_loss -0.8946 
2023-10-27 09:47:43.363158: Pseudo dice [0.8909, 0.9151, 0.9708, 0.8681, 0.946] 
2023-10-27 09:47:43.363379: Epoch time: 3.53 s 
2023-10-27 09:47:44.411279:  
2023-10-27 09:47:44.411694: Epoch 155 
2023-10-27 09:47:44.411969: Current learning rate: 0.00859 
2023-10-27 09:47:47.921271: train_loss -0.866 
2023-10-27 09:47:47.921720: val_loss -0.8992 
2023-10-27 09:47:47.921993: Pseudo dice [0.885, 0.9181, 0.971, 0.8821, 0.9515] 
2023-10-27 09:47:47.922224: Epoch time: 3.51 s 
2023-10-27 09:47:49.105139:  
2023-10-27 09:47:49.105553: Epoch 156 
2023-10-27 09:47:49.105802: Current learning rate: 0.00858 
2023-10-27 09:47:52.656009: train_loss -0.8505 
2023-10-27 09:47:52.656553: val_loss -0.8831 
2023-10-27 09:47:52.656838: Pseudo dice [0.8831, 0.9118, 0.9654, 0.7665, 0.9307] 
2023-10-27 09:47:52.657075: Epoch time: 3.55 s 
2023-10-27 09:47:53.704289:  
2023-10-27 09:47:53.704638: Epoch 157 
2023-10-27 09:47:53.704895: Current learning rate: 0.00858 
2023-10-27 09:47:57.234682: train_loss -0.8626 
2023-10-27 09:47:57.235107: val_loss -0.8884 
2023-10-27 09:47:57.235380: Pseudo dice [0.8796, 0.9127, 0.9679, 0.8699, 0.944] 
2023-10-27 09:47:57.235613: Epoch time: 3.53 s 
2023-10-27 09:47:58.281241:  
2023-10-27 09:47:58.281657: Epoch 158 
2023-10-27 09:47:58.281918: Current learning rate: 0.00857 
2023-10-27 09:48:01.767030: train_loss -0.8567 
2023-10-27 09:48:01.767414: val_loss -0.8909 
2023-10-27 09:48:01.767685: Pseudo dice [0.8793, 0.912, 0.9712, 0.8609, 0.942] 
2023-10-27 09:48:01.767921: Epoch time: 3.49 s 
2023-10-27 09:48:02.814639:  
2023-10-27 09:48:02.815058: Epoch 159 
2023-10-27 09:48:02.815315: Current learning rate: 0.00856 
2023-10-27 09:48:06.575758: train_loss -0.8601 
2023-10-27 09:48:06.576232: val_loss -0.8804 
2023-10-27 09:48:06.576704: Pseudo dice [0.884, 0.9142, 0.9705, 0.8626, 0.9316] 
2023-10-27 09:48:06.576981: Epoch time: 3.76 s 
2023-10-27 09:48:07.623993:  
2023-10-27 09:48:07.624331: Epoch 160 
2023-10-27 09:48:07.624608: Current learning rate: 0.00855 
2023-10-27 09:48:11.167574: train_loss -0.8545 
2023-10-27 09:48:11.168215: val_loss -0.8768 
2023-10-27 09:48:11.168596: Pseudo dice [0.8845, 0.9081, 0.9695, 0.7248, 0.9476] 
2023-10-27 09:48:11.168891: Epoch time: 3.54 s 
2023-10-27 09:48:12.225642:  
2023-10-27 09:48:12.225986: Epoch 161 
2023-10-27 09:48:12.226265: Current learning rate: 0.00854 
2023-10-27 09:48:15.859720: train_loss -0.8615 
2023-10-27 09:48:15.860189: val_loss -0.8831 
2023-10-27 09:48:15.860543: Pseudo dice [0.8857, 0.9078, 0.9696, 0.8477, 0.9489] 
2023-10-27 09:48:15.860885: Epoch time: 3.64 s 
2023-10-27 09:48:17.044816:  
2023-10-27 09:48:17.045203: Epoch 162 
2023-10-27 09:48:17.045465: Current learning rate: 0.00853 
2023-10-27 09:48:20.646857: train_loss -0.8657 
2023-10-27 09:48:20.647282: val_loss -0.8856 
2023-10-27 09:48:20.647569: Pseudo dice [0.8868, 0.9147, 0.9697, 0.7969, 0.9458] 
2023-10-27 09:48:20.647803: Epoch time: 3.6 s 
2023-10-27 09:48:21.696835:  
2023-10-27 09:48:21.697232: Epoch 163 
2023-10-27 09:48:21.697499: Current learning rate: 0.00852 
2023-10-27 09:48:25.413004: train_loss -0.8617 
2023-10-27 09:48:25.413446: val_loss -0.8937 
2023-10-27 09:48:25.413711: Pseudo dice [0.8793, 0.9116, 0.9699, 0.847, 0.9476] 
2023-10-27 09:48:25.413949: Epoch time: 3.72 s 
2023-10-27 09:48:26.464221:  
2023-10-27 09:48:26.464627: Epoch 164 
2023-10-27 09:48:26.464876: Current learning rate: 0.00851 
2023-10-27 09:48:30.004166: train_loss -0.8585 
2023-10-27 09:48:30.004586: val_loss -0.8708 
2023-10-27 09:48:30.004855: Pseudo dice [0.8793, 0.9104, 0.9694, 0.4617, 0.9464] 
2023-10-27 09:48:30.005117: Epoch time: 3.54 s 
2023-10-27 09:48:31.031511:  
2023-10-27 09:48:31.031845: Epoch 165 
2023-10-27 09:48:31.032111: Current learning rate: 0.0085 
2023-10-27 09:48:34.571351: train_loss -0.8576 
2023-10-27 09:48:34.571740: val_loss -0.8987 
2023-10-27 09:48:34.572008: Pseudo dice [0.8919, 0.9182, 0.9715, 0.9009, 0.9471] 
2023-10-27 09:48:34.572241: Epoch time: 3.54 s 
2023-10-27 09:48:35.600719:  
2023-10-27 09:48:35.601089: Epoch 166 
2023-10-27 09:48:35.601356: Current learning rate: 0.00849 
2023-10-27 09:48:39.321197: train_loss -0.8539 
2023-10-27 09:48:39.321636: val_loss -0.8907 
2023-10-27 09:48:39.321889: Pseudo dice [0.8819, 0.9033, 0.9691, 0.8817, 0.9482] 
2023-10-27 09:48:39.322116: Epoch time: 3.72 s 
2023-10-27 09:48:40.365082:  
2023-10-27 09:48:40.365387: Epoch 167 
2023-10-27 09:48:40.365653: Current learning rate: 0.00848 
2023-10-27 09:48:44.024608: train_loss -0.8563 
2023-10-27 09:48:44.025032: val_loss -0.8924 
2023-10-27 09:48:44.025302: Pseudo dice [0.8892, 0.9166, 0.9715, 0.8618, 0.9487] 
2023-10-27 09:48:44.025559: Epoch time: 3.66 s 
2023-10-27 09:48:45.216303:  
2023-10-27 09:48:45.216712: Epoch 168 
2023-10-27 09:48:45.216992: Current learning rate: 0.00847 
2023-10-27 09:48:48.751477: train_loss -0.857 
2023-10-27 09:48:48.752026: val_loss -0.8874 
2023-10-27 09:48:48.752301: Pseudo dice [0.8879, 0.9174, 0.9719, 0.8099, 0.9435] 
2023-10-27 09:48:48.752624: Epoch time: 3.54 s 
2023-10-27 09:48:49.795837:  
2023-10-27 09:48:49.796275: Epoch 169 
2023-10-27 09:48:49.796561: Current learning rate: 0.00847 
2023-10-27 09:48:53.400929: train_loss -0.8547 
2023-10-27 09:48:53.401337: val_loss -0.8866 
2023-10-27 09:48:53.401607: Pseudo dice [0.8846, 0.9158, 0.9668, 0.8829, 0.9395] 
2023-10-27 09:48:53.401850: Epoch time: 3.61 s 
2023-10-27 09:48:54.462088:  
2023-10-27 09:48:54.462485: Epoch 170 
2023-10-27 09:48:54.462762: Current learning rate: 0.00846 
2023-10-27 09:48:58.050143: train_loss -0.8441 
2023-10-27 09:48:58.050563: val_loss -0.8892 
2023-10-27 09:48:58.050827: Pseudo dice [0.8863, 0.9066, 0.9683, 0.8589, 0.9447] 
2023-10-27 09:48:58.051063: Epoch time: 3.59 s 
2023-10-27 09:48:59.100877:  
2023-10-27 09:48:59.101215: Epoch 171 
2023-10-27 09:48:59.101517: Current learning rate: 0.00845 
2023-10-27 09:49:02.761770: train_loss -0.8471 
2023-10-27 09:49:02.762357: val_loss -0.8601 
2023-10-27 09:49:02.762627: Pseudo dice [0.881, 0.9064, 0.9681, 0.4925, 0.927] 
2023-10-27 09:49:02.762868: Epoch time: 3.66 s 
2023-10-27 09:49:03.812667:  
2023-10-27 09:49:03.813016: Epoch 172 
2023-10-27 09:49:03.813286: Current learning rate: 0.00844 
2023-10-27 09:49:07.339332: train_loss -0.8393 
2023-10-27 09:49:07.339701: val_loss -0.8818 
2023-10-27 09:49:07.339986: Pseudo dice [0.881, 0.9072, 0.9668, 0.8642, 0.9359] 
2023-10-27 09:49:07.340217: Epoch time: 3.53 s 
2023-10-27 09:49:08.379578:  
2023-10-27 09:49:08.379976: Epoch 173 
2023-10-27 09:49:08.380251: Current learning rate: 0.00843 
2023-10-27 09:49:11.860252: train_loss -0.8355 
2023-10-27 09:49:11.860945: val_loss -0.8766 
2023-10-27 09:49:11.861328: Pseudo dice [0.8795, 0.9036, 0.9673, 0.8255, 0.9439] 
2023-10-27 09:49:11.861711: Epoch time: 3.48 s 
2023-10-27 09:49:13.040493:  
2023-10-27 09:49:13.040884: Epoch 174 
2023-10-27 09:49:13.041163: Current learning rate: 0.00842 
2023-10-27 09:49:16.532897: train_loss -0.8449 
2023-10-27 09:49:16.533303: val_loss -0.8845 
2023-10-27 09:49:16.533573: Pseudo dice [0.88, 0.9081, 0.969, 0.8587, 0.9432] 
2023-10-27 09:49:16.533817: Epoch time: 3.49 s 
2023-10-27 09:49:17.576662:  
2023-10-27 09:49:17.577004: Epoch 175 
2023-10-27 09:49:17.577255: Current learning rate: 0.00841 
2023-10-27 09:49:21.060956: train_loss -0.8557 
2023-10-27 09:49:21.061342: val_loss -0.8911 
2023-10-27 09:49:21.061608: Pseudo dice [0.8784, 0.9115, 0.9702, 0.8836, 0.9441] 
2023-10-27 09:49:21.061855: Epoch time: 3.48 s 
2023-10-27 09:49:22.097573:  
2023-10-27 09:49:22.097897: Epoch 176 
2023-10-27 09:49:22.098181: Current learning rate: 0.0084 
2023-10-27 09:49:25.596529: train_loss -0.8509 
2023-10-27 09:49:25.596978: val_loss -0.8781 
2023-10-27 09:49:25.597239: Pseudo dice [0.878, 0.9001, 0.9654, 0.9025, 0.9268] 
2023-10-27 09:49:25.597473: Epoch time: 3.5 s 
2023-10-27 09:49:26.640429:  
2023-10-27 09:49:26.640877: Epoch 177 
2023-10-27 09:49:26.641143: Current learning rate: 0.00839 
2023-10-27 09:49:30.121110: train_loss -0.8522 
2023-10-27 09:49:30.121527: val_loss -0.8864 
2023-10-27 09:49:30.121789: Pseudo dice [0.8833, 0.9129, 0.9713, 0.8084, 0.9459] 
2023-10-27 09:49:30.122023: Epoch time: 3.48 s 
2023-10-27 09:49:31.167488:  
2023-10-27 09:49:31.167809: Epoch 178 
2023-10-27 09:49:31.168066: Current learning rate: 0.00838 
2023-10-27 09:49:34.675814: train_loss -0.8508 
2023-10-27 09:49:34.676194: val_loss -0.8532 
2023-10-27 09:49:34.676454: Pseudo dice [0.8897, 0.8914, 0.9699, 0.0488, 0.9481] 
2023-10-27 09:49:34.676689: Epoch time: 3.51 s 
2023-10-27 09:49:35.719499:  
2023-10-27 09:49:35.719830: Epoch 179 
2023-10-27 09:49:35.720102: Current learning rate: 0.00837 
2023-10-27 09:49:39.210151: train_loss -0.8412 
2023-10-27 09:49:39.210541: val_loss -0.8823 
2023-10-27 09:49:39.210821: Pseudo dice [0.8742, 0.9002, 0.97, 0.7714, 0.938] 
2023-10-27 09:49:39.211056: Epoch time: 3.49 s 
2023-10-27 09:49:40.385464:  
2023-10-27 09:49:40.385791: Epoch 180 
2023-10-27 09:49:40.386096: Current learning rate: 0.00836 
2023-10-27 09:49:43.945143: train_loss -0.8505 
2023-10-27 09:49:43.945540: val_loss -0.8905 
2023-10-27 09:49:43.945806: Pseudo dice [0.8789, 0.907, 0.9697, 0.8855, 0.9419] 
2023-10-27 09:49:43.946033: Epoch time: 3.56 s 
2023-10-27 09:49:44.986336:  
2023-10-27 09:49:44.986728: Epoch 181 
2023-10-27 09:49:44.986999: Current learning rate: 0.00836 
2023-10-27 09:49:48.544581: train_loss -0.8517 
2023-10-27 09:49:48.545003: val_loss -0.8848 
2023-10-27 09:49:48.545261: Pseudo dice [0.8857, 0.9036, 0.9666, 0.9149, 0.9456] 
2023-10-27 09:49:48.545506: Epoch time: 3.56 s 
2023-10-27 09:49:49.583698:  
2023-10-27 09:49:49.584045: Epoch 182 
2023-10-27 09:49:49.584300: Current learning rate: 0.00835 
2023-10-27 09:49:53.099853: train_loss -0.8549 
2023-10-27 09:49:53.100241: val_loss -0.8774 
2023-10-27 09:49:53.100543: Pseudo dice [0.8727, 0.9085, 0.9674, 0.8929, 0.9384] 
2023-10-27 09:49:53.100791: Epoch time: 3.52 s 
2023-10-27 09:49:54.140427:  
2023-10-27 09:49:54.140807: Epoch 183 
2023-10-27 09:49:54.141050: Current learning rate: 0.00834 
2023-10-27 09:49:57.656743: train_loss -0.8561 
2023-10-27 09:49:57.657119: val_loss -0.8905 
2023-10-27 09:49:57.657377: Pseudo dice [0.8695, 0.9098, 0.9712, 0.8089, 0.9463] 
2023-10-27 09:49:57.657607: Epoch time: 3.52 s 
2023-10-27 09:49:58.692588:  
2023-10-27 09:49:58.692906: Epoch 184 
2023-10-27 09:49:58.693153: Current learning rate: 0.00833 
2023-10-27 09:50:02.291331: train_loss -0.8553 
2023-10-27 09:50:02.291748: val_loss -0.8919 
2023-10-27 09:50:02.292059: Pseudo dice [0.885, 0.9127, 0.9675, 0.8969, 0.947] 
2023-10-27 09:50:02.292285: Epoch time: 3.6 s 
2023-10-27 09:50:03.368386:  
2023-10-27 09:50:03.368956: Epoch 185 
2023-10-27 09:50:03.369202: Current learning rate: 0.00832 
2023-10-27 09:50:06.952729: train_loss -0.8445 
2023-10-27 09:50:06.953156: val_loss -0.8624 
2023-10-27 09:50:06.953429: Pseudo dice [0.8868, 0.9059, 0.9708, 0.6058, 0.9402] 
2023-10-27 09:50:06.953670: Epoch time: 3.59 s 
2023-10-27 09:50:08.153473:  
2023-10-27 09:50:08.153800: Epoch 186 
2023-10-27 09:50:08.154057: Current learning rate: 0.00831 
2023-10-27 09:50:11.761446: train_loss -0.8563 
2023-10-27 09:50:11.762029: val_loss -0.893 
2023-10-27 09:50:11.762310: Pseudo dice [0.8875, 0.9191, 0.9704, 0.8944, 0.9441] 
2023-10-27 09:50:11.762581: Epoch time: 3.61 s 
2023-10-27 09:50:12.819810:  
2023-10-27 09:50:12.820193: Epoch 187 
2023-10-27 09:50:12.820526: Current learning rate: 0.0083 
2023-10-27 09:50:16.371448: train_loss -0.8515 
2023-10-27 09:50:16.371865: val_loss -0.8925 
2023-10-27 09:50:16.372118: Pseudo dice [0.8902, 0.9089, 0.9698, 0.8675, 0.9476] 
2023-10-27 09:50:16.372344: Epoch time: 3.55 s 
2023-10-27 09:50:17.435606:  
2023-10-27 09:50:17.435909: Epoch 188 
2023-10-27 09:50:17.436160: Current learning rate: 0.00829 
2023-10-27 09:50:21.149597: train_loss -0.8626 
2023-10-27 09:50:21.150310: val_loss -0.8952 
2023-10-27 09:50:21.150612: Pseudo dice [0.8928, 0.9078, 0.9681, 0.8769, 0.9498] 
2023-10-27 09:50:21.150855: Epoch time: 3.71 s 
2023-10-27 09:50:22.198640:  
2023-10-27 09:50:22.198969: Epoch 189 
2023-10-27 09:50:22.199214: Current learning rate: 0.00828 
2023-10-27 09:50:25.813220: train_loss -0.8612 
2023-10-27 09:50:25.813640: val_loss -0.8757 
2023-10-27 09:50:25.813917: Pseudo dice [0.8863, 0.9129, 0.9714, 0.6471, 0.9446] 
2023-10-27 09:50:25.814154: Epoch time: 3.62 s 
2023-10-27 09:50:26.865872:  
2023-10-27 09:50:26.866254: Epoch 190 
2023-10-27 09:50:26.866543: Current learning rate: 0.00827 
2023-10-27 09:50:30.424551: train_loss -0.8614 
2023-10-27 09:50:30.424965: val_loss -0.8864 
2023-10-27 09:50:30.425234: Pseudo dice [0.8818, 0.9159, 0.9692, 0.8564, 0.9455] 
2023-10-27 09:50:30.425471: Epoch time: 3.56 s 
2023-10-27 09:50:31.473272:  
2023-10-27 09:50:31.473593: Epoch 191 
2023-10-27 09:50:31.473845: Current learning rate: 0.00826 
2023-10-27 09:50:35.009950: train_loss -0.8628 
2023-10-27 09:50:35.010353: val_loss -0.8789 
2023-10-27 09:50:35.010620: Pseudo dice [0.8812, 0.9112, 0.9702, 0.6536, 0.945] 
2023-10-27 09:50:35.010860: Epoch time: 3.54 s 
2023-10-27 09:50:36.206309:  
2023-10-27 09:50:36.206654: Epoch 192 
2023-10-27 09:50:36.206910: Current learning rate: 0.00825 
2023-10-27 09:50:39.754244: train_loss -0.8615 
2023-10-27 09:50:39.754670: val_loss -0.8709 
2023-10-27 09:50:39.754938: Pseudo dice [0.8898, 0.9152, 0.9691, 0.5559, 0.948] 
2023-10-27 09:50:39.755168: Epoch time: 3.55 s 
2023-10-27 09:50:40.815152:  
2023-10-27 09:50:40.815478: Epoch 193 
2023-10-27 09:50:40.815760: Current learning rate: 0.00824 
2023-10-27 09:50:44.462996: train_loss -0.8525 
2023-10-27 09:50:44.463375: val_loss -0.8952 
2023-10-27 09:50:44.463657: Pseudo dice [0.8877, 0.9139, 0.9694, 0.886, 0.9461] 
2023-10-27 09:50:44.463892: Epoch time: 3.65 s 
2023-10-27 09:50:45.512754:  
2023-10-27 09:50:45.513113: Epoch 194 
2023-10-27 09:50:45.513380: Current learning rate: 0.00824 
2023-10-27 09:50:49.132555: train_loss -0.8528 
2023-10-27 09:50:49.132953: val_loss -0.8812 
2023-10-27 09:50:49.133217: Pseudo dice [0.8793, 0.9125, 0.9673, 0.7752, 0.9445] 
2023-10-27 09:50:49.133461: Epoch time: 3.62 s 
2023-10-27 09:50:50.186845:  
2023-10-27 09:50:50.187193: Epoch 195 
2023-10-27 09:50:50.187465: Current learning rate: 0.00823 
2023-10-27 09:50:53.735188: train_loss -0.8649 
2023-10-27 09:50:53.735575: val_loss -0.8946 
2023-10-27 09:50:53.735857: Pseudo dice [0.8836, 0.9151, 0.9713, 0.8688, 0.9475] 
2023-10-27 09:50:53.736084: Epoch time: 3.55 s 
2023-10-27 09:50:54.788645:  
2023-10-27 09:50:54.789008: Epoch 196 
2023-10-27 09:50:54.789253: Current learning rate: 0.00822 
2023-10-27 09:50:58.354219: train_loss -0.8606 
2023-10-27 09:50:58.354619: val_loss -0.8984 
2023-10-27 09:50:58.354882: Pseudo dice [0.886, 0.9162, 0.9702, 0.9064, 0.9466] 
2023-10-27 09:50:58.355113: Epoch time: 3.57 s 
2023-10-27 09:50:59.418803:  
2023-10-27 09:50:59.419124: Epoch 197 
2023-10-27 09:50:59.419369: Current learning rate: 0.00821 
2023-10-27 09:51:02.930251: train_loss -0.8589 
2023-10-27 09:51:02.930639: val_loss -0.8955 
2023-10-27 09:51:02.930904: Pseudo dice [0.891, 0.912, 0.9704, 0.8983, 0.946] 
2023-10-27 09:51:02.931138: Epoch time: 3.51 s 
2023-10-27 09:51:04.116189:  
2023-10-27 09:51:04.116579: Epoch 198 
2023-10-27 09:51:04.116840: Current learning rate: 0.0082 
2023-10-27 09:51:07.691643: train_loss -0.8674 
2023-10-27 09:51:07.692134: val_loss -0.8912 
2023-10-27 09:51:07.692410: Pseudo dice [0.8877, 0.9177, 0.9691, 0.8753, 0.9433] 
2023-10-27 09:51:07.692643: Epoch time: 3.58 s 
2023-10-27 09:51:08.748047:  
2023-10-27 09:51:08.748458: Epoch 199 
2023-10-27 09:51:08.748739: Current learning rate: 0.00819 
2023-10-27 09:51:12.281613: train_loss -0.862 
2023-10-27 09:51:12.281988: val_loss -0.8712 
2023-10-27 09:51:12.282250: Pseudo dice [0.8903, 0.9142, 0.9689, 0.6584, 0.948] 
2023-10-27 09:51:12.282497: Epoch time: 3.53 s 
2023-10-27 09:51:13.407566:  
2023-10-27 09:51:13.407887: Epoch 200 
2023-10-27 09:51:13.408138: Current learning rate: 0.00818 
2023-10-27 09:51:17.004975: train_loss -0.8618 
2023-10-27 09:51:17.005341: val_loss -0.8878 
2023-10-27 09:51:17.005629: Pseudo dice [0.8853, 0.9162, 0.9707, 0.8624, 0.9461] 
2023-10-27 09:51:17.005883: Epoch time: 3.6 s 
2023-10-27 09:51:18.062088:  
2023-10-27 09:51:18.062461: Epoch 201 
2023-10-27 09:51:18.062745: Current learning rate: 0.00817 
2023-10-27 09:51:21.641945: train_loss -0.8643 
2023-10-27 09:51:21.642484: val_loss -0.8949 
2023-10-27 09:51:21.642765: Pseudo dice [0.8852, 0.9154, 0.9711, 0.8942, 0.9461] 
2023-10-27 09:51:21.643001: Epoch time: 3.58 s 
2023-10-27 09:51:22.702699:  
2023-10-27 09:51:22.703088: Epoch 202 
2023-10-27 09:51:22.703376: Current learning rate: 0.00816 
2023-10-27 09:51:26.243061: train_loss -0.8646 
2023-10-27 09:51:26.243585: val_loss -0.8806 
2023-10-27 09:51:26.243864: Pseudo dice [0.8774, 0.9094, 0.969, 0.8616, 0.9292] 
2023-10-27 09:51:26.244099: Epoch time: 3.54 s 
2023-10-27 09:51:27.440175:  
2023-10-27 09:51:27.440553: Epoch 203 
2023-10-27 09:51:27.440826: Current learning rate: 0.00815 
2023-10-27 09:51:30.992747: train_loss -0.8628 
2023-10-27 09:51:30.993179: val_loss -0.8581 
2023-10-27 09:51:30.993465: Pseudo dice [0.8737, 0.9073, 0.9691, 0.3489, 0.9419] 
2023-10-27 09:51:30.993718: Epoch time: 3.55 s 
2023-10-27 09:51:32.046790:  
2023-10-27 09:51:32.047186: Epoch 204 
2023-10-27 09:51:32.047456: Current learning rate: 0.00814 
2023-10-27 09:51:35.583739: train_loss -0.8672 
2023-10-27 09:51:35.584217: val_loss -0.8927 
2023-10-27 09:51:35.584589: Pseudo dice [0.8798, 0.9153, 0.9686, 0.8938, 0.9454] 
2023-10-27 09:51:35.584939: Epoch time: 3.54 s 
2023-10-27 09:51:36.639470:  
2023-10-27 09:51:36.639867: Epoch 205 
2023-10-27 09:51:36.640144: Current learning rate: 0.00813 
2023-10-27 09:51:40.182521: train_loss -0.863 
2023-10-27 09:51:40.182915: val_loss -0.8907 
2023-10-27 09:51:40.183192: Pseudo dice [0.8817, 0.9065, 0.9707, 0.8611, 0.9435] 
2023-10-27 09:51:40.183432: Epoch time: 3.54 s 
2023-10-27 09:51:41.188574:  
2023-10-27 09:51:41.188972: Epoch 206 
2023-10-27 09:51:41.189225: Current learning rate: 0.00813 
2023-10-27 09:51:44.722533: train_loss -0.8439 
2023-10-27 09:51:44.723016: val_loss -0.8828 
2023-10-27 09:51:44.723287: Pseudo dice [0.8755, 0.9083, 0.9705, 0.8569, 0.9426] 
2023-10-27 09:51:44.723523: Epoch time: 3.53 s 
2023-10-27 09:51:45.733676:  
2023-10-27 09:51:45.734026: Epoch 207 
2023-10-27 09:51:45.734293: Current learning rate: 0.00812 
2023-10-27 09:51:49.265121: train_loss -0.8432 
2023-10-27 09:51:49.265481: val_loss -0.8394 
2023-10-27 09:51:49.265751: Pseudo dice [0.8804, 0.9053, 0.9673, 0.0504, 0.9279] 
2023-10-27 09:51:49.265988: Epoch time: 3.53 s 
2023-10-27 09:51:50.274691:  
2023-10-27 09:51:50.275093: Epoch 208 
2023-10-27 09:51:50.275338: Current learning rate: 0.00811 
2023-10-27 09:51:53.795584: train_loss -0.8533 
2023-10-27 09:51:53.796011: val_loss -0.8855 
2023-10-27 09:51:53.796255: Pseudo dice [0.8901, 0.9133, 0.9692, 0.7914, 0.9426] 
2023-10-27 09:51:53.796476: Epoch time: 3.52 s 
2023-10-27 09:51:54.819113:  
2023-10-27 09:51:54.819501: Epoch 209 
2023-10-27 09:51:54.819810: Current learning rate: 0.0081 
2023-10-27 09:51:58.431666: train_loss -0.859 
2023-10-27 09:51:58.432544: val_loss -0.8861 
2023-10-27 09:51:58.432819: Pseudo dice [0.8852, 0.9158, 0.9716, 0.7789, 0.9457] 
2023-10-27 09:51:58.433053: Epoch time: 3.61 s 
2023-10-27 09:51:59.600688:  
2023-10-27 09:51:59.601092: Epoch 210 
2023-10-27 09:51:59.601355: Current learning rate: 0.00809 
2023-10-27 09:52:03.360429: train_loss -0.8614 
2023-10-27 09:52:03.360797: val_loss -0.8932 
2023-10-27 09:52:03.361059: Pseudo dice [0.8909, 0.9145, 0.9728, 0.8314, 0.9457] 
2023-10-27 09:52:03.361300: Epoch time: 3.76 s 
2023-10-27 09:52:04.373496:  
2023-10-27 09:52:04.373943: Epoch 211 
2023-10-27 09:52:04.374375: Current learning rate: 0.00808 
2023-10-27 09:52:07.999439: train_loss -0.8701 
2023-10-27 09:52:07.999854: val_loss -0.8922 
2023-10-27 09:52:08.000117: Pseudo dice [0.888, 0.9142, 0.9726, 0.8648, 0.9505] 
2023-10-27 09:52:08.000351: Epoch time: 3.63 s 
2023-10-27 09:52:09.024212:  
2023-10-27 09:52:09.024560: Epoch 212 
2023-10-27 09:52:09.024806: Current learning rate: 0.00807 
2023-10-27 09:52:12.615352: train_loss -0.8686 
2023-10-27 09:52:12.615780: val_loss -0.8493 
2023-10-27 09:52:12.616076: Pseudo dice [0.8826, 0.9025, 0.969, 0.0383, 0.9482] 
2023-10-27 09:52:12.616338: Epoch time: 3.59 s 
2023-10-27 09:52:13.634182:  
2023-10-27 09:52:13.634578: Epoch 213 
2023-10-27 09:52:13.634822: Current learning rate: 0.00806 
2023-10-27 09:52:17.232953: train_loss -0.8597 
2023-10-27 09:52:17.236677: val_loss -0.8974 
2023-10-27 09:52:17.236932: Pseudo dice [0.8881, 0.9141, 0.9714, 0.8817, 0.9455] 
2023-10-27 09:52:17.237163: Epoch time: 3.6 s 
2023-10-27 09:52:18.255242:  
2023-10-27 09:52:18.255641: Epoch 214 
2023-10-27 09:52:18.255884: Current learning rate: 0.00805 
2023-10-27 09:52:21.864249: train_loss -0.872 
2023-10-27 09:52:21.864684: val_loss -0.8257 
2023-10-27 09:52:21.864965: Pseudo dice [0.8905, 0.9024, 0.9668, 0.0, 0.9485] 
2023-10-27 09:52:21.865209: Epoch time: 3.61 s 
2023-10-27 09:52:22.889871:  
2023-10-27 09:52:22.890282: Epoch 215 
2023-10-27 09:52:22.890550: Current learning rate: 0.00804 
2023-10-27 09:52:26.517049: train_loss -0.8548 
2023-10-27 09:52:26.517578: val_loss -0.8891 
2023-10-27 09:52:26.517834: Pseudo dice [0.8776, 0.9115, 0.9673, 0.8894, 0.942] 
2023-10-27 09:52:26.518056: Epoch time: 3.63 s 
2023-10-27 09:52:27.724590:  
2023-10-27 09:52:27.725204: Epoch 216 
2023-10-27 09:52:27.725726: Current learning rate: 0.00803 
2023-10-27 09:52:31.333408: train_loss -0.8606 
2023-10-27 09:52:31.333829: val_loss -0.8747 
2023-10-27 09:52:31.334089: Pseudo dice [0.8783, 0.919, 0.9717, 0.5164, 0.9493] 
2023-10-27 09:52:31.334322: Epoch time: 3.61 s 
2023-10-27 09:52:32.353405:  
2023-10-27 09:52:32.353790: Epoch 217 
2023-10-27 09:52:32.354037: Current learning rate: 0.00802 
2023-10-27 09:52:35.894610: train_loss -0.8617 
2023-10-27 09:52:35.895360: val_loss -0.8885 
2023-10-27 09:52:35.895636: Pseudo dice [0.8795, 0.9081, 0.97, 0.9054, 0.9481] 
2023-10-27 09:52:35.895988: Epoch time: 3.54 s 
2023-10-27 09:52:36.918154:  
2023-10-27 09:52:36.918499: Epoch 218 
2023-10-27 09:52:36.918775: Current learning rate: 0.00801 
2023-10-27 09:52:40.436422: train_loss -0.8529 
2023-10-27 09:52:40.436877: val_loss -0.8884 
2023-10-27 09:52:40.437136: Pseudo dice [0.8816, 0.9128, 0.9704, 0.7521, 0.9482] 
2023-10-27 09:52:40.437374: Epoch time: 3.52 s 
2023-10-27 09:52:41.459103:  
2023-10-27 09:52:41.459468: Epoch 219 
2023-10-27 09:52:41.459762: Current learning rate: 0.00801 
2023-10-27 09:52:44.990313: train_loss -0.8563 
2023-10-27 09:52:44.990739: val_loss -0.8953 
2023-10-27 09:52:44.991025: Pseudo dice [0.885, 0.9096, 0.9711, 0.8966, 0.9489] 
2023-10-27 09:52:44.991282: Epoch time: 3.53 s 
2023-10-27 09:52:46.006088:  
2023-10-27 09:52:46.006408: Epoch 220 
2023-10-27 09:52:46.006687: Current learning rate: 0.008 
2023-10-27 09:52:49.537178: train_loss -0.8614 
2023-10-27 09:52:49.537567: val_loss -0.8963 
2023-10-27 09:52:49.537843: Pseudo dice [0.8852, 0.9102, 0.9706, 0.9239, 0.944] 
2023-10-27 09:52:49.538070: Epoch time: 3.53 s 
2023-10-27 09:52:50.548816:  
2023-10-27 09:52:50.549152: Epoch 221 
2023-10-27 09:52:50.549432: Current learning rate: 0.00799 
2023-10-27 09:52:54.107051: train_loss -0.8625 
2023-10-27 09:52:54.107483: val_loss -0.89 
2023-10-27 09:52:54.107768: Pseudo dice [0.8917, 0.9166, 0.9679, 0.8425, 0.9429] 
2023-10-27 09:52:54.108007: Epoch time: 3.56 s 
2023-10-27 09:52:55.264325:  
2023-10-27 09:52:55.264720: Epoch 222 
2023-10-27 09:52:55.264957: Current learning rate: 0.00798 
2023-10-27 09:52:58.963344: train_loss -0.8642 
2023-10-27 09:52:58.963927: val_loss -0.8916 
2023-10-27 09:52:58.964260: Pseudo dice [0.8847, 0.9111, 0.9696, 0.8279, 0.9475] 
2023-10-27 09:52:58.964497: Epoch time: 3.7 s 
2023-10-27 09:52:59.979640:  
2023-10-27 09:52:59.979985: Epoch 223 
2023-10-27 09:52:59.980250: Current learning rate: 0.00797 
2023-10-27 09:53:03.572392: train_loss -0.8658 
2023-10-27 09:53:03.572844: val_loss -0.8941 
2023-10-27 09:53:03.573106: Pseudo dice [0.8881, 0.9109, 0.97, 0.8895, 0.9469] 
2023-10-27 09:53:03.573331: Epoch time: 3.59 s 
2023-10-27 09:53:04.588374:  
2023-10-27 09:53:04.588695: Epoch 224 
2023-10-27 09:53:04.588938: Current learning rate: 0.00796 
2023-10-27 09:53:08.200463: train_loss -0.864 
2023-10-27 09:53:08.200960: val_loss -0.8929 
2023-10-27 09:53:08.201221: Pseudo dice [0.8816, 0.9092, 0.9708, 0.8722, 0.9452] 
2023-10-27 09:53:08.201462: Epoch time: 3.61 s 
2023-10-27 09:53:09.932234:  
2023-10-27 09:53:10.019267: Epoch 225 
2023-10-27 09:53:10.032022: Current learning rate: 0.00795 
2023-10-27 09:53:14.636708: train_loss -0.8617 
2023-10-27 09:53:14.637112: val_loss -0.8829 
2023-10-27 09:53:14.637394: Pseudo dice [0.8832, 0.9119, 0.9699, 0.8152, 0.9413] 
2023-10-27 09:53:14.637655: Epoch time: 4.71 s 
2023-10-27 09:53:15.818564:  
2023-10-27 09:53:15.818960: Epoch 226 
2023-10-27 09:53:15.819226: Current learning rate: 0.00794 
2023-10-27 09:53:19.322851: train_loss -0.8674 
2023-10-27 09:53:19.323225: val_loss -0.8931 
2023-10-27 09:53:19.323492: Pseudo dice [0.892, 0.9127, 0.9726, 0.8675, 0.9467] 
2023-10-27 09:53:19.323724: Epoch time: 3.5 s 
2023-10-27 09:53:20.333935:  
2023-10-27 09:53:20.334328: Epoch 227 
2023-10-27 09:53:20.334602: Current learning rate: 0.00793 
2023-10-27 09:53:23.924034: train_loss -0.8713 
2023-10-27 09:53:23.924461: val_loss -0.8958 
2023-10-27 09:53:23.924823: Pseudo dice [0.89, 0.9155, 0.9725, 0.884, 0.9467] 
2023-10-27 09:53:23.925075: Epoch time: 3.59 s 
2023-10-27 09:53:24.931837:  
2023-10-27 09:53:24.932244: Epoch 228 
2023-10-27 09:53:24.932497: Current learning rate: 0.00792 
2023-10-27 09:53:28.477874: train_loss -0.8526 
2023-10-27 09:53:28.478307: val_loss -0.8903 
2023-10-27 09:53:28.478577: Pseudo dice [0.8886, 0.908, 0.9701, 0.8379, 0.9481] 
2023-10-27 09:53:28.478806: Epoch time: 3.55 s 
2023-10-27 09:53:30.004797:  
2023-10-27 09:53:30.005187: Epoch 229 
2023-10-27 09:53:30.005461: Current learning rate: 0.00791 
2023-10-27 09:53:33.573390: train_loss -0.8556 
2023-10-27 09:53:33.573822: val_loss -0.889 
2023-10-27 09:53:33.574080: Pseudo dice [0.8797, 0.9068, 0.9696, 0.8266, 0.9454] 
2023-10-27 09:53:33.574320: Epoch time: 3.57 s 
2023-10-27 09:53:34.579395:  
2023-10-27 09:53:34.579813: Epoch 230 
2023-10-27 09:53:34.580069: Current learning rate: 0.0079 
2023-10-27 09:53:38.144921: train_loss -0.8651 
2023-10-27 09:53:38.145317: val_loss -0.8943 
2023-10-27 09:53:38.145639: Pseudo dice [0.8944, 0.915, 0.9696, 0.8013, 0.9486] 
2023-10-27 09:53:38.145957: Epoch time: 3.57 s 
2023-10-27 09:53:39.148377:  
2023-10-27 09:53:39.148715: Epoch 231 
2023-10-27 09:53:39.148981: Current learning rate: 0.00789 
2023-10-27 09:53:42.682630: train_loss -0.8712 
2023-10-27 09:53:42.683008: val_loss -0.8923 
2023-10-27 09:53:42.683273: Pseudo dice [0.8837, 0.9142, 0.9695, 0.8886, 0.9448] 
2023-10-27 09:53:42.683509: Epoch time: 3.53 s 
2023-10-27 09:53:43.688345:  
2023-10-27 09:53:43.688708: Epoch 232 
2023-10-27 09:53:43.688991: Current learning rate: 0.00789 
2023-10-27 09:53:47.286384: train_loss -0.8566 
2023-10-27 09:53:47.286748: val_loss -0.8948 
2023-10-27 09:53:47.287015: Pseudo dice [0.8837, 0.9119, 0.9696, 0.9116, 0.9457] 
2023-10-27 09:53:47.287275: Epoch time: 3.6 s 
2023-10-27 09:53:48.288858:  
2023-10-27 09:53:48.289234: Epoch 233 
2023-10-27 09:53:48.289486: Current learning rate: 0.00788 
2023-10-27 09:53:51.923844: train_loss -0.8664 
2023-10-27 09:53:51.924233: val_loss -0.8905 
2023-10-27 09:53:51.924509: Pseudo dice [0.8851, 0.9154, 0.9702, 0.8425, 0.9414] 
2023-10-27 09:53:51.924743: Epoch time: 3.64 s 
2023-10-27 09:53:52.925603:  
2023-10-27 09:53:52.925982: Epoch 234 
2023-10-27 09:53:52.926255: Current learning rate: 0.00787 
2023-10-27 09:53:56.458694: train_loss -0.8671 
2023-10-27 09:53:56.459119: val_loss -0.8801 
2023-10-27 09:53:56.459437: Pseudo dice [0.8786, 0.9072, 0.9693, 0.3362, 0.9469] 
2023-10-27 09:53:56.459692: Epoch time: 3.53 s 
2023-10-27 09:53:57.602553:  
2023-10-27 09:53:57.602949: Epoch 235 
2023-10-27 09:53:57.603221: Current learning rate: 0.00786 
2023-10-27 09:54:01.256094: train_loss -0.8571 
2023-10-27 09:54:01.256538: val_loss -0.8365 
2023-10-27 09:54:01.256806: Pseudo dice [0.8555, 0.8881, 0.9686, 0.0, 0.9443] 
2023-10-27 09:54:01.257049: Epoch time: 3.65 s 
2023-10-27 09:54:02.264908:  
2023-10-27 09:54:02.265241: Epoch 236 
2023-10-27 09:54:02.265532: Current learning rate: 0.00785 
2023-10-27 09:54:05.866553: train_loss -0.8396 
2023-10-27 09:54:05.867040: val_loss -0.8933 
2023-10-27 09:54:05.867337: Pseudo dice [0.8969, 0.9129, 0.9685, 0.8805, 0.9481] 
2023-10-27 09:54:05.867581: Epoch time: 3.6 s 
2023-10-27 09:54:06.882126:  
2023-10-27 09:54:06.882476: Epoch 237 
2023-10-27 09:54:06.882751: Current learning rate: 0.00784 
2023-10-27 09:54:10.464507: train_loss -0.8565 
2023-10-27 09:54:10.464890: val_loss -0.8837 
2023-10-27 09:54:10.465166: Pseudo dice [0.8793, 0.905, 0.9674, 0.8425, 0.9461] 
2023-10-27 09:54:10.465422: Epoch time: 3.58 s 
2023-10-27 09:54:11.477771:  
2023-10-27 09:54:11.478172: Epoch 238 
2023-10-27 09:54:11.478459: Current learning rate: 0.00783 
2023-10-27 09:54:15.079942: train_loss -0.8487 
2023-10-27 09:54:15.080352: val_loss -0.8873 
2023-10-27 09:54:15.080650: Pseudo dice [0.882, 0.9108, 0.9698, 0.8668, 0.941] 
2023-10-27 09:54:15.080908: Epoch time: 3.6 s 
2023-10-27 09:54:16.092098:  
2023-10-27 09:54:16.092452: Epoch 239 
2023-10-27 09:54:16.092733: Current learning rate: 0.00782 
2023-10-27 09:54:19.687266: train_loss -0.859 
2023-10-27 09:54:19.687808: val_loss -0.8806 
2023-10-27 09:54:19.688134: Pseudo dice [0.8805, 0.913, 0.9715, 0.823, 0.9473] 
2023-10-27 09:54:19.688512: Epoch time: 3.6 s 
2023-10-27 09:54:20.709843:  
2023-10-27 09:54:20.710193: Epoch 240 
2023-10-27 09:54:20.710454: Current learning rate: 0.00781 
2023-10-27 09:54:24.281758: train_loss -0.8505 
2023-10-27 09:54:24.282123: val_loss -0.8862 
2023-10-27 09:54:24.282421: Pseudo dice [0.8829, 0.9073, 0.9702, 0.8814, 0.9421] 
2023-10-27 09:54:24.282661: Epoch time: 3.57 s 
2023-10-27 09:54:25.304818:  
2023-10-27 09:54:25.305202: Epoch 241 
2023-10-27 09:54:25.305489: Current learning rate: 0.0078 
2023-10-27 09:54:28.863747: train_loss -0.8565 
2023-10-27 09:54:28.867177: val_loss -0.8931 
2023-10-27 09:54:28.867438: Pseudo dice [0.8818, 0.9176, 0.9701, 0.881, 0.9438] 
2023-10-27 09:54:28.867674: Epoch time: 3.56 s 
2023-10-27 09:54:30.040431:  
2023-10-27 09:54:30.040815: Epoch 242 
2023-10-27 09:54:30.041068: Current learning rate: 0.00779 
2023-10-27 09:54:33.638718: train_loss -0.8505 
2023-10-27 09:54:33.639148: val_loss -0.888 
2023-10-27 09:54:33.639423: Pseudo dice [0.8773, 0.9121, 0.9709, 0.8554, 0.9431] 
2023-10-27 09:54:33.639666: Epoch time: 3.6 s 
2023-10-27 09:54:34.666690:  
2023-10-27 09:54:34.667088: Epoch 243 
2023-10-27 09:54:34.667367: Current learning rate: 0.00778 
2023-10-27 09:54:38.235270: train_loss -0.8601 
2023-10-27 09:54:38.235779: val_loss -0.8892 
2023-10-27 09:54:38.236142: Pseudo dice [0.8898, 0.9118, 0.9721, 0.8505, 0.9474] 
2023-10-27 09:54:38.236639: Epoch time: 3.57 s 
2023-10-27 09:54:39.263647:  
2023-10-27 09:54:39.263960: Epoch 244 
2023-10-27 09:54:39.264243: Current learning rate: 0.00777 
2023-10-27 09:54:42.827753: train_loss -0.8595 
2023-10-27 09:54:42.828147: val_loss -0.8988 
2023-10-27 09:54:42.828416: Pseudo dice [0.8915, 0.9223, 0.972, 0.9022, 0.9457] 
2023-10-27 09:54:42.828653: Epoch time: 3.56 s 
2023-10-27 09:54:43.847939:  
2023-10-27 09:54:43.848351: Epoch 245 
2023-10-27 09:54:43.848613: Current learning rate: 0.00777 
2023-10-27 09:54:47.373050: train_loss -0.8688 
2023-10-27 09:54:47.373450: val_loss -0.8624 
2023-10-27 09:54:47.373717: Pseudo dice [0.8821, 0.9139, 0.9715, 0.3766, 0.9482] 
2023-10-27 09:54:47.373957: Epoch time: 3.53 s 
2023-10-27 09:54:48.393804:  
2023-10-27 09:54:48.394200: Epoch 246 
2023-10-27 09:54:48.394450: Current learning rate: 0.00776 
2023-10-27 09:54:51.962823: train_loss -0.8678 
2023-10-27 09:54:51.963228: val_loss -0.8849 
2023-10-27 09:54:51.963492: Pseudo dice [0.8835, 0.9074, 0.9708, 0.8063, 0.9491] 
2023-10-27 09:54:51.963734: Epoch time: 3.57 s 
2023-10-27 09:54:52.983850:  
2023-10-27 09:54:52.984245: Epoch 247 
2023-10-27 09:54:52.984504: Current learning rate: 0.00775 
2023-10-27 09:54:56.543633: train_loss -0.867 
2023-10-27 09:54:56.544005: val_loss -0.8962 
2023-10-27 09:54:56.544276: Pseudo dice [0.8784, 0.9156, 0.9718, 0.8806, 0.9491] 
2023-10-27 09:54:56.544531: Epoch time: 3.56 s 
2023-10-27 09:54:57.704503:  
2023-10-27 09:54:57.704907: Epoch 248 
2023-10-27 09:54:57.705162: Current learning rate: 0.00774 
2023-10-27 09:55:01.250381: train_loss -0.8646 
2023-10-27 09:55:01.250915: val_loss -0.893 
2023-10-27 09:55:01.251198: Pseudo dice [0.8855, 0.9173, 0.9714, 0.8864, 0.9477] 
2023-10-27 09:55:01.251433: Epoch time: 3.55 s 
2023-10-27 09:55:02.271716:  
2023-10-27 09:55:02.272043: Epoch 249 
2023-10-27 09:55:02.272320: Current learning rate: 0.00773 
2023-10-27 09:55:05.831421: train_loss -0.8709 
2023-10-27 09:55:05.831855: val_loss -0.8883 
2023-10-27 09:55:05.832121: Pseudo dice [0.8872, 0.9104, 0.9695, 0.8719, 0.9421] 
2023-10-27 09:55:05.832367: Epoch time: 3.56 s 
2023-10-27 09:55:06.984645:  
2023-10-27 09:55:06.985044: Epoch 250 
2023-10-27 09:55:06.985304: Current learning rate: 0.00772 
2023-10-27 09:55:10.535028: train_loss -0.8637 
2023-10-27 09:55:10.535513: val_loss -0.8957 
2023-10-27 09:55:10.535787: Pseudo dice [0.8838, 0.9178, 0.9694, 0.8644, 0.9443] 
2023-10-27 09:55:10.536016: Epoch time: 3.55 s 
2023-10-27 09:55:11.558865:  
2023-10-27 09:55:11.559254: Epoch 251 
2023-10-27 09:55:11.559517: Current learning rate: 0.00771 
2023-10-27 09:55:15.148172: train_loss -0.8754 
2023-10-27 09:55:15.148568: val_loss -0.8925 
2023-10-27 09:55:15.148838: Pseudo dice [0.8903, 0.9158, 0.9698, 0.8896, 0.9405] 
2023-10-27 09:55:15.149069: Epoch time: 3.59 s 
2023-10-27 09:55:16.174942:  
2023-10-27 09:55:16.175275: Epoch 252 
2023-10-27 09:55:16.175539: Current learning rate: 0.0077 
2023-10-27 09:55:19.697126: train_loss -0.8687 
2023-10-27 09:55:19.697674: val_loss -0.8947 
2023-10-27 09:55:19.697940: Pseudo dice [0.8917, 0.9124, 0.9713, 0.8743, 0.9494] 
2023-10-27 09:55:19.698160: Epoch time: 3.52 s 
2023-10-27 09:55:20.822537:  
2023-10-27 09:55:20.822863: Epoch 253 
2023-10-27 09:55:20.823110: Current learning rate: 0.00769 
2023-10-27 09:55:24.381267: train_loss -0.8544 
2023-10-27 09:55:24.381641: val_loss -0.8929 
2023-10-27 09:55:24.381907: Pseudo dice [0.8847, 0.9065, 0.9684, 0.873, 0.945] 
2023-10-27 09:55:24.382133: Epoch time: 3.56 s 
2023-10-27 09:55:25.402265:  
2023-10-27 09:55:25.402566: Epoch 254 
2023-10-27 09:55:25.402824: Current learning rate: 0.00768 
2023-10-27 09:55:29.067030: train_loss -0.848 
2023-10-27 09:55:29.067454: val_loss -0.8776 
2023-10-27 09:55:29.067721: Pseudo dice [0.8845, 0.9081, 0.9699, 0.6313, 0.9459] 
2023-10-27 09:55:29.067955: Epoch time: 3.67 s 
2023-10-27 09:55:30.091334:  
2023-10-27 09:55:30.091677: Epoch 255 
2023-10-27 09:55:30.091982: Current learning rate: 0.00767 
2023-10-27 09:55:33.661040: train_loss -0.863 
2023-10-27 09:55:33.661418: val_loss -0.8934 
2023-10-27 09:55:33.661690: Pseudo dice [0.8824, 0.917, 0.9716, 0.8901, 0.9462] 
2023-10-27 09:55:33.661933: Epoch time: 3.57 s 
2023-10-27 09:55:34.675341:  
2023-10-27 09:55:34.675671: Epoch 256 
2023-10-27 09:55:34.675913: Current learning rate: 0.00766 
2023-10-27 09:55:38.200094: train_loss -0.8616 
2023-10-27 09:55:38.200488: val_loss -0.895 
2023-10-27 09:55:38.200803: Pseudo dice [0.8861, 0.915, 0.9713, 0.8934, 0.9425] 
2023-10-27 09:55:38.201028: Epoch time: 3.53 s 
2023-10-27 09:55:39.218493:  
2023-10-27 09:55:39.218869: Epoch 257 
2023-10-27 09:55:39.219119: Current learning rate: 0.00765 
2023-10-27 09:55:42.868508: train_loss -0.8688 
2023-10-27 09:55:42.868903: val_loss -0.8979 
2023-10-27 09:55:42.869164: Pseudo dice [0.8863, 0.9141, 0.9695, 0.8942, 0.9496] 
2023-10-27 09:55:42.869417: Epoch time: 3.65 s 
2023-10-27 09:55:43.891462:  
2023-10-27 09:55:43.891791: Epoch 258 
2023-10-27 09:55:43.892053: Current learning rate: 0.00764 
2023-10-27 09:55:47.461040: train_loss -0.8683 
2023-10-27 09:55:47.461431: val_loss -0.8813 
2023-10-27 09:55:47.461688: Pseudo dice [0.8896, 0.9166, 0.9706, 0.8013, 0.944] 
2023-10-27 09:55:47.461910: Epoch time: 3.57 s 
2023-10-27 09:55:48.481693:  
2023-10-27 09:55:48.482003: Epoch 259 
2023-10-27 09:55:48.482254: Current learning rate: 0.00764 
2023-10-27 09:55:51.999067: train_loss -0.8621 
2023-10-27 09:55:51.999459: val_loss -0.8962 
2023-10-27 09:55:51.999718: Pseudo dice [0.8918, 0.9203, 0.9716, 0.9, 0.9491] 
2023-10-27 09:55:51.999944: Epoch time: 3.52 s 
2023-10-27 09:55:53.024586:  
2023-10-27 09:55:53.024985: Epoch 260 
2023-10-27 09:55:53.025265: Current learning rate: 0.00763 
2023-10-27 09:55:56.539778: train_loss -0.8638 
2023-10-27 09:55:56.540193: val_loss -0.899 
2023-10-27 09:55:56.540490: Pseudo dice [0.8944, 0.9138, 0.971, 0.9003, 0.9478] 
2023-10-27 09:55:56.540729: Epoch time: 3.52 s 
2023-10-27 09:55:57.703341:  
2023-10-27 09:55:57.703723: Epoch 261 
2023-10-27 09:55:57.703977: Current learning rate: 0.00762 
2023-10-27 09:56:01.282355: train_loss -0.8659 
2023-10-27 09:56:01.282770: val_loss -0.8909 
2023-10-27 09:56:01.283042: Pseudo dice [0.8845, 0.9136, 0.9683, 0.9002, 0.9458] 
2023-10-27 09:56:01.283273: Epoch time: 3.58 s 
2023-10-27 09:56:02.304106:  
2023-10-27 09:56:02.304465: Epoch 262 
2023-10-27 09:56:02.304713: Current learning rate: 0.00761 
2023-10-27 09:56:05.913856: train_loss -0.8567 
2023-10-27 09:56:05.914425: val_loss -0.8862 
2023-10-27 09:56:05.915026: Pseudo dice [0.885, 0.9096, 0.9711, 0.8869, 0.9461] 
2023-10-27 09:56:05.915622: Epoch time: 3.61 s 
2023-10-27 09:56:06.937307:  
2023-10-27 09:56:06.937656: Epoch 263 
2023-10-27 09:56:06.937926: Current learning rate: 0.0076 
2023-10-27 09:56:10.463872: train_loss -0.8605 
2023-10-27 09:56:10.464336: val_loss -0.8915 
2023-10-27 09:56:10.465013: Pseudo dice [0.8781, 0.912, 0.9709, 0.9081, 0.9471] 
2023-10-27 09:56:10.465338: Epoch time: 3.53 s 
2023-10-27 09:56:10.465575: Yayy! New best EMA pseudo Dice: 0.9142 
2023-10-27 09:56:11.568536:  
2023-10-27 09:56:11.568869: Epoch 264 
2023-10-27 09:56:11.569143: Current learning rate: 0.00759 
2023-10-27 09:56:15.105483: train_loss -0.8623 
2023-10-27 09:56:15.105835: val_loss -0.8975 
2023-10-27 09:56:15.106104: Pseudo dice [0.8867, 0.9121, 0.9694, 0.9194, 0.949] 
2023-10-27 09:56:15.106332: Epoch time: 3.54 s 
2023-10-27 09:56:15.106553: Yayy! New best EMA pseudo Dice: 0.9155 
2023-10-27 09:56:16.215816:  
2023-10-27 09:56:16.216155: Epoch 265 
2023-10-27 09:56:16.216450: Current learning rate: 0.00758 
2023-10-27 09:56:19.752476: train_loss -0.8675 
2023-10-27 09:56:19.753003: val_loss -0.8919 
2023-10-27 09:56:19.753261: Pseudo dice [0.8849, 0.9117, 0.9705, 0.8697, 0.9463] 
2023-10-27 09:56:19.753491: Epoch time: 3.54 s 
2023-10-27 09:56:19.753698: Yayy! New best EMA pseudo Dice: 0.9156 
2023-10-27 09:56:20.851980:  
2023-10-27 09:56:20.852303: Epoch 266 
2023-10-27 09:56:20.852571: Current learning rate: 0.00757 
2023-10-27 09:56:24.371250: train_loss -0.8565 
2023-10-27 09:56:24.371632: val_loss -0.8915 
2023-10-27 09:56:24.371881: Pseudo dice [0.8856, 0.9161, 0.9708, 0.8943, 0.9395] 
2023-10-27 09:56:24.372106: Epoch time: 3.52 s 
2023-10-27 09:56:24.372322: Yayy! New best EMA pseudo Dice: 0.9162 
2023-10-27 09:56:25.618658:  
2023-10-27 09:56:25.618992: Epoch 267 
2023-10-27 09:56:25.619260: Current learning rate: 0.00756 
2023-10-27 09:56:29.184219: train_loss -0.8654 
2023-10-27 09:56:29.184775: val_loss -0.8834 
2023-10-27 09:56:29.185042: Pseudo dice [0.8807, 0.9118, 0.9708, 0.6713, 0.9419] 
2023-10-27 09:56:29.185282: Epoch time: 3.57 s 
2023-10-27 09:56:30.211731:  
2023-10-27 09:56:30.212157: Epoch 268 
2023-10-27 09:56:30.212430: Current learning rate: 0.00755 
2023-10-27 09:56:33.736359: train_loss -0.8576 
2023-10-27 09:56:33.736748: val_loss -0.8916 
2023-10-27 09:56:33.737010: Pseudo dice [0.8859, 0.9104, 0.9723, 0.8679, 0.9383] 
2023-10-27 09:56:33.737243: Epoch time: 3.53 s 
2023-10-27 09:56:34.758910:  
2023-10-27 09:56:34.759299: Epoch 269 
2023-10-27 09:56:34.759598: Current learning rate: 0.00754 
2023-10-27 09:56:38.288368: train_loss -0.8666 
2023-10-27 09:56:38.288764: val_loss -0.8985 
2023-10-27 09:56:38.289021: Pseudo dice [0.8953, 0.9181, 0.9696, 0.8921, 0.9506] 
2023-10-27 09:56:38.289248: Epoch time: 3.53 s 
2023-10-27 09:56:39.306936:  
2023-10-27 09:56:39.307261: Epoch 270 
2023-10-27 09:56:39.307532: Current learning rate: 0.00753 
2023-10-27 09:56:42.824422: train_loss -0.8703 
2023-10-27 09:56:42.824829: val_loss -0.897 
2023-10-27 09:56:42.825088: Pseudo dice [0.8905, 0.919, 0.9711, 0.8891, 0.9493] 
2023-10-27 09:56:42.825314: Epoch time: 3.52 s 
2023-10-27 09:56:43.849976:  
2023-10-27 09:56:43.850284: Epoch 271 
2023-10-27 09:56:43.850533: Current learning rate: 0.00752 
2023-10-27 09:56:47.387635: train_loss -0.8651 
2023-10-27 09:56:47.388039: val_loss -0.8965 
2023-10-27 09:56:47.388294: Pseudo dice [0.8919, 0.9149, 0.9707, 0.8814, 0.9459] 
2023-10-27 09:56:47.388534: Epoch time: 3.54 s 
2023-10-27 09:56:48.413366:  
2023-10-27 09:56:48.413734: Epoch 272 
2023-10-27 09:56:48.414006: Current learning rate: 0.00751 
2023-10-27 09:56:51.934918: train_loss -0.8688 
2023-10-27 09:56:51.935317: val_loss -0.8928 
2023-10-27 09:56:51.935608: Pseudo dice [0.8888, 0.9105, 0.9711, 0.8571, 0.9493] 
2023-10-27 09:56:51.935833: Epoch time: 3.52 s 
2023-10-27 09:56:52.964188:  
2023-10-27 09:56:52.964572: Epoch 273 
2023-10-27 09:56:52.964829: Current learning rate: 0.00751 
2023-10-27 09:56:56.487742: train_loss -0.8701 
2023-10-27 09:56:56.488258: val_loss -0.8929 
2023-10-27 09:56:56.488628: Pseudo dice [0.8826, 0.9154, 0.9675, 0.89, 0.9456] 
2023-10-27 09:56:56.488948: Epoch time: 3.52 s 
2023-10-27 09:56:57.655742:  
2023-10-27 09:56:57.656122: Epoch 274 
2023-10-27 09:56:57.656374: Current learning rate: 0.0075 
2023-10-27 09:57:01.208204: train_loss -0.8721 
2023-10-27 09:57:01.208645: val_loss -0.895 
2023-10-27 09:57:01.208918: Pseudo dice [0.8862, 0.9149, 0.9704, 0.8876, 0.9419] 
2023-10-27 09:57:01.209151: Epoch time: 3.55 s 
2023-10-27 09:57:01.209381: Yayy! New best EMA pseudo Dice: 0.9162 
2023-10-27 09:57:02.311729:  
2023-10-27 09:57:02.312095: Epoch 275 
2023-10-27 09:57:02.312376: Current learning rate: 0.00749 
2023-10-27 09:57:05.917055: train_loss -0.8733 
2023-10-27 09:57:05.917426: val_loss -0.8991 
2023-10-27 09:57:05.917684: Pseudo dice [0.8889, 0.9167, 0.9724, 0.8591, 0.9444] 
2023-10-27 09:57:05.917911: Epoch time: 3.61 s 
2023-10-27 09:57:05.918122: Yayy! New best EMA pseudo Dice: 0.9162 
2023-10-27 09:57:07.017618:  
2023-10-27 09:57:07.017985: Epoch 276 
2023-10-27 09:57:07.018254: Current learning rate: 0.00748 
2023-10-27 09:57:10.573638: train_loss -0.8722 
2023-10-27 09:57:10.574081: val_loss -0.8958 
2023-10-27 09:57:10.574361: Pseudo dice [0.8876, 0.9158, 0.9706, 0.8813, 0.9459] 
2023-10-27 09:57:10.574614: Epoch time: 3.56 s 
2023-10-27 09:57:10.574840: Yayy! New best EMA pseudo Dice: 0.9166 
2023-10-27 09:57:11.675902:  
2023-10-27 09:57:11.676240: Epoch 277 
2023-10-27 09:57:11.676520: Current learning rate: 0.00747 
2023-10-27 09:57:15.197119: train_loss -0.863 
2023-10-27 09:57:15.197535: val_loss -0.8876 
2023-10-27 09:57:15.197789: Pseudo dice [0.873, 0.9083, 0.9718, 0.8561, 0.9412] 
2023-10-27 09:57:15.198025: Epoch time: 3.52 s 
2023-10-27 09:57:16.218980:  
2023-10-27 09:57:16.219369: Epoch 278 
2023-10-27 09:57:16.219615: Current learning rate: 0.00746 
2023-10-27 09:57:19.760950: train_loss -0.8433 
2023-10-27 09:57:19.761328: val_loss -0.877 
2023-10-27 09:57:19.761591: Pseudo dice [0.874, 0.8996, 0.967, 0.8445, 0.9432] 
2023-10-27 09:57:19.761817: Epoch time: 3.54 s 
2023-10-27 09:57:20.789510:  
2023-10-27 09:57:20.789822: Epoch 279 
2023-10-27 09:57:20.790081: Current learning rate: 0.00745 
2023-10-27 09:57:24.399534: train_loss -0.8537 
2023-10-27 09:57:24.399921: val_loss -0.8862 
2023-10-27 09:57:24.400181: Pseudo dice [0.8777, 0.9064, 0.9702, 0.858, 0.9425] 
2023-10-27 09:57:24.400416: Epoch time: 3.61 s 
2023-10-27 09:57:25.560917:  
2023-10-27 09:57:25.561236: Epoch 280 
2023-10-27 09:57:25.561491: Current learning rate: 0.00744 
2023-10-27 09:57:29.121958: train_loss -0.857 
2023-10-27 09:57:29.122406: val_loss -0.8953 
2023-10-27 09:57:29.122678: Pseudo dice [0.8907, 0.9169, 0.9717, 0.8706, 0.9466] 
2023-10-27 09:57:29.122918: Epoch time: 3.56 s 
2023-10-27 09:57:30.146991:  
2023-10-27 09:57:30.147315: Epoch 281 
2023-10-27 09:57:30.147575: Current learning rate: 0.00743 
2023-10-27 09:57:33.713893: train_loss -0.8308 
2023-10-27 09:57:33.714371: val_loss -0.8308 
2023-10-27 09:57:33.714803: Pseudo dice [0.8773, 0.9092, 0.9681, 0.0, 0.9326] 
2023-10-27 09:57:33.715077: Epoch time: 3.57 s 
2023-10-27 09:57:34.739061:  
2023-10-27 09:57:34.739452: Epoch 282 
2023-10-27 09:57:34.739907: Current learning rate: 0.00742 
2023-10-27 09:57:38.295954: train_loss -0.8016 
2023-10-27 09:57:38.296337: val_loss -0.8384 
2023-10-27 09:57:38.296596: Pseudo dice [0.8885, 0.91, 0.9696, 0.0, 0.9441] 
2023-10-27 09:57:38.296825: Epoch time: 3.56 s 
2023-10-27 09:57:39.318504:  
2023-10-27 09:57:39.318915: Epoch 283 
2023-10-27 09:57:39.319178: Current learning rate: 0.00741 
2023-10-27 09:57:42.928710: train_loss -0.8137 
2023-10-27 09:57:42.929068: val_loss -0.8315 
2023-10-27 09:57:42.929323: Pseudo dice [0.8893, 0.9104, 0.9697, 0.0, 0.942] 
2023-10-27 09:57:42.929554: Epoch time: 3.61 s 
2023-10-27 09:57:43.951390:  
2023-10-27 09:57:43.951701: Epoch 284 
2023-10-27 09:57:43.951944: Current learning rate: 0.0074 
2023-10-27 09:57:47.618196: train_loss -0.8276 
2023-10-27 09:57:47.618706: val_loss -0.8886 
2023-10-27 09:57:47.618957: Pseudo dice [0.8874, 0.9135, 0.9701, 0.841, 0.946] 
2023-10-27 09:57:47.619182: Epoch time: 3.67 s 
2023-10-27 09:57:48.647337:  
2023-10-27 09:57:48.647735: Epoch 285 
2023-10-27 09:57:48.647983: Current learning rate: 0.00739 
2023-10-27 09:57:52.206177: train_loss -0.854 
2023-10-27 09:57:52.206743: val_loss -0.8778 
2023-10-27 09:57:52.207024: Pseudo dice [0.8876, 0.9165, 0.9677, 0.6014, 0.9458] 
2023-10-27 09:57:52.207257: Epoch time: 3.56 s 
2023-10-27 09:57:53.375710:  
2023-10-27 09:57:53.376062: Epoch 286 
2023-10-27 09:57:53.376333: Current learning rate: 0.00738 
2023-10-27 09:57:56.900519: train_loss -0.8567 
2023-10-27 09:57:56.900880: val_loss -0.8984 
2023-10-27 09:57:56.901141: Pseudo dice [0.8906, 0.9165, 0.9725, 0.8895, 0.9463] 
2023-10-27 09:57:56.901376: Epoch time: 3.53 s 
2023-10-27 09:57:57.934911:  
2023-10-27 09:57:57.935252: Epoch 287 
2023-10-27 09:57:57.935530: Current learning rate: 0.00738 
2023-10-27 09:58:01.534251: train_loss -0.8562 
2023-10-27 09:58:01.534659: val_loss -0.8861 
2023-10-27 09:58:01.534917: Pseudo dice [0.8845, 0.907, 0.9693, 0.867, 0.9498] 
2023-10-27 09:58:01.535136: Epoch time: 3.6 s 
2023-10-27 09:58:02.572322:  
2023-10-27 09:58:02.572818: Epoch 288 
2023-10-27 09:58:02.573132: Current learning rate: 0.00737 
2023-10-27 09:58:06.140539: train_loss -0.8626 
2023-10-27 09:58:06.140971: val_loss -0.8867 
2023-10-27 09:58:06.141240: Pseudo dice [0.8877, 0.9144, 0.9693, 0.8462, 0.9445] 
2023-10-27 09:58:06.141488: Epoch time: 3.57 s 
2023-10-27 09:58:07.186880:  
2023-10-27 09:58:07.187210: Epoch 289 
2023-10-27 09:58:07.187482: Current learning rate: 0.00736 
2023-10-27 09:58:10.729957: train_loss -0.8516 
2023-10-27 09:58:10.730477: val_loss -0.8816 
2023-10-27 09:58:10.730833: Pseudo dice [0.8714, 0.9074, 0.9699, 0.8486, 0.9419] 
2023-10-27 09:58:10.731097: Epoch time: 3.54 s 
2023-10-27 09:58:11.770869:  
2023-10-27 09:58:11.771208: Epoch 290 
2023-10-27 09:58:11.771464: Current learning rate: 0.00735 
2023-10-27 09:58:15.348680: train_loss -0.8463 
2023-10-27 09:58:15.349086: val_loss -0.8831 
2023-10-27 09:58:15.349352: Pseudo dice [0.882, 0.903, 0.9695, 0.8444, 0.9376] 
2023-10-27 09:58:15.349590: Epoch time: 3.58 s 
2023-10-27 09:58:16.394434:  
2023-10-27 09:58:16.394753: Epoch 291 
2023-10-27 09:58:16.395005: Current learning rate: 0.00734 
2023-10-27 09:58:19.983080: train_loss -0.8581 
2023-10-27 09:58:19.983472: val_loss -0.8957 
2023-10-27 09:58:19.983739: Pseudo dice [0.8879, 0.9138, 0.9712, 0.8423, 0.9472] 
2023-10-27 09:58:19.983976: Epoch time: 3.59 s 
2023-10-27 09:58:21.165085:  
2023-10-27 09:58:21.165430: Epoch 292 
2023-10-27 09:58:21.165690: Current learning rate: 0.00733 
2023-10-27 09:58:24.703321: train_loss -0.86 
2023-10-27 09:58:24.703710: val_loss -0.8899 
2023-10-27 09:58:24.703970: Pseudo dice [0.888, 0.9094, 0.9725, 0.7831, 0.9478] 
2023-10-27 09:58:24.704203: Epoch time: 3.54 s 
2023-10-27 09:58:25.745544:  
2023-10-27 09:58:25.745965: Epoch 293 
2023-10-27 09:58:25.746218: Current learning rate: 0.00732 
2023-10-27 09:58:29.449384: train_loss -0.8616 
2023-10-27 09:58:29.449986: val_loss -0.8919 
2023-10-27 09:58:29.450292: Pseudo dice [0.8901, 0.9135, 0.9698, 0.8665, 0.949] 
2023-10-27 09:58:29.450588: Epoch time: 3.7 s 
2023-10-27 09:58:30.490512:  
2023-10-27 09:58:30.490921: Epoch 294 
2023-10-27 09:58:30.491181: Current learning rate: 0.00731 
2023-10-27 09:58:34.099985: train_loss -0.862 
2023-10-27 09:58:34.100402: val_loss -0.8944 
2023-10-27 09:58:34.100672: Pseudo dice [0.8873, 0.9137, 0.9709, 0.8974, 0.9495] 
2023-10-27 09:58:34.100925: Epoch time: 3.61 s 
2023-10-27 09:58:35.150620:  
2023-10-27 09:58:35.150992: Epoch 295 
2023-10-27 09:58:35.151319: Current learning rate: 0.0073 
2023-10-27 09:58:38.671375: train_loss -0.8626 
2023-10-27 09:58:38.671852: val_loss -0.8898 
2023-10-27 09:58:38.672107: Pseudo dice [0.8855, 0.9083, 0.9697, 0.8549, 0.9427] 
2023-10-27 09:58:38.672339: Epoch time: 3.52 s 
2023-10-27 09:58:39.713493:  
2023-10-27 09:58:39.713839: Epoch 296 
2023-10-27 09:58:39.714093: Current learning rate: 0.00729 
2023-10-27 09:58:43.259608: train_loss -0.859 
2023-10-27 09:58:43.260083: val_loss -0.891 
2023-10-27 09:58:43.260362: Pseudo dice [0.8884, 0.9124, 0.9712, 0.918, 0.94] 
2023-10-27 09:58:43.260630: Epoch time: 3.55 s 
2023-10-27 09:58:44.299541:  
2023-10-27 09:58:44.299883: Epoch 297 
2023-10-27 09:58:44.300158: Current learning rate: 0.00728 
2023-10-27 09:58:47.806947: train_loss -0.8621 
2023-10-27 09:58:47.807403: val_loss -0.872 
2023-10-27 09:58:47.808004: Pseudo dice [0.8831, 0.9101, 0.9696, 0.6082, 0.9332] 
2023-10-27 09:58:47.808422: Epoch time: 3.51 s 
2023-10-27 09:58:48.986834:  
2023-10-27 09:58:48.987205: Epoch 298 
2023-10-27 09:58:48.987485: Current learning rate: 0.00727 
2023-10-27 09:58:52.523372: train_loss -0.8568 
2023-10-27 09:58:52.523802: val_loss -0.8969 
2023-10-27 09:58:52.524062: Pseudo dice [0.8879, 0.9115, 0.9718, 0.9193, 0.9479] 
2023-10-27 09:58:52.524290: Epoch time: 3.54 s 
2023-10-27 09:58:53.566132:  
2023-10-27 09:58:53.566495: Epoch 299 
2023-10-27 09:58:53.566768: Current learning rate: 0.00726 
2023-10-27 09:58:57.136876: train_loss -0.8543 
2023-10-27 09:58:57.137271: val_loss -0.8916 
2023-10-27 09:58:57.137548: Pseudo dice [0.892, 0.9155, 0.973, 0.8873, 0.9442] 
2023-10-27 09:58:57.137781: Epoch time: 3.57 s 
2023-10-27 09:58:58.254371:  
2023-10-27 09:58:58.254785: Epoch 300 
2023-10-27 09:58:58.255056: Current learning rate: 0.00725 
2023-10-27 09:59:01.815928: train_loss -0.8448 
2023-10-27 09:59:01.816430: val_loss -0.8869 
2023-10-27 09:59:01.816722: Pseudo dice [0.8824, 0.9106, 0.968, 0.8706, 0.9342] 
2023-10-27 09:59:01.816996: Epoch time: 3.56 s 
2023-10-27 09:59:02.858599:  
2023-10-27 09:59:02.858999: Epoch 301 
2023-10-27 09:59:02.859438: Current learning rate: 0.00724 
2023-10-27 09:59:06.399971: train_loss -0.8581 
2023-10-27 09:59:06.400481: val_loss -0.8988 
2023-10-27 09:59:06.400811: Pseudo dice [0.8873, 0.9138, 0.9714, 0.8864, 0.945] 
2023-10-27 09:59:06.401045: Epoch time: 3.54 s 
2023-10-27 09:59:07.444065:  
2023-10-27 09:59:07.444383: Epoch 302 
2023-10-27 09:59:07.444653: Current learning rate: 0.00724 
2023-10-27 09:59:11.004224: train_loss -0.864 
2023-10-27 09:59:11.004618: val_loss -0.8949 
2023-10-27 09:59:11.004890: Pseudo dice [0.8909, 0.9147, 0.9708, 0.8897, 0.9494] 
2023-10-27 09:59:11.005137: Epoch time: 3.56 s 
2023-10-27 09:59:12.063789:  
2023-10-27 09:59:12.064109: Epoch 303 
2023-10-27 09:59:12.064375: Current learning rate: 0.00723 
2023-10-27 09:59:15.633868: train_loss -0.8628 
2023-10-27 09:59:15.634329: val_loss -0.8936 
2023-10-27 09:59:15.634665: Pseudo dice [0.8924, 0.9123, 0.9677, 0.8892, 0.9489] 
2023-10-27 09:59:15.634908: Epoch time: 3.57 s 
2023-10-27 09:59:16.675804:  
2023-10-27 09:59:16.676120: Epoch 304 
2023-10-27 09:59:16.676374: Current learning rate: 0.00722 
2023-10-27 09:59:20.238720: train_loss -0.8634 
2023-10-27 09:59:20.239233: val_loss -0.8953 
2023-10-27 09:59:20.239517: Pseudo dice [0.8928, 0.9129, 0.9684, 0.8825, 0.9478] 
2023-10-27 09:59:20.239779: Epoch time: 3.56 s 
2023-10-27 09:59:21.423338:  
2023-10-27 09:59:21.423662: Epoch 305 
2023-10-27 09:59:21.423908: Current learning rate: 0.00721 
2023-10-27 09:59:25.118228: train_loss -0.8683 
2023-10-27 09:59:25.118746: val_loss -0.8925 
2023-10-27 09:59:25.119006: Pseudo dice [0.8838, 0.9092, 0.9681, 0.8847, 0.9444] 
2023-10-27 09:59:25.119240: Epoch time: 3.7 s 
2023-10-27 09:59:26.158940:  
2023-10-27 09:59:26.159282: Epoch 306 
2023-10-27 09:59:26.159541: Current learning rate: 0.0072 
2023-10-27 09:59:29.777879: train_loss -0.869 
2023-10-27 09:59:29.778367: val_loss -0.8955 
2023-10-27 09:59:29.778678: Pseudo dice [0.8861, 0.9155, 0.9708, 0.8914, 0.948] 
2023-10-27 09:59:29.778926: Epoch time: 3.62 s 
2023-10-27 09:59:30.816586:  
2023-10-27 09:59:30.816953: Epoch 307 
2023-10-27 09:59:30.817222: Current learning rate: 0.00719 
2023-10-27 09:59:34.455279: train_loss -0.8586 
2023-10-27 09:59:34.455834: val_loss -0.8982 
2023-10-27 09:59:34.456103: Pseudo dice [0.8907, 0.915, 0.972, 0.8472, 0.9483] 
2023-10-27 09:59:34.456348: Epoch time: 3.64 s 
2023-10-27 09:59:35.495059:  
2023-10-27 09:59:35.495385: Epoch 308 
2023-10-27 09:59:35.495643: Current learning rate: 0.00718 
2023-10-27 09:59:39.052001: train_loss -0.8421 
2023-10-27 09:59:39.052368: val_loss -0.8772 
2023-10-27 09:59:39.052637: Pseudo dice [0.8742, 0.8835, 0.9684, 0.8854, 0.9454] 
2023-10-27 09:59:39.052867: Epoch time: 3.56 s 
2023-10-27 09:59:40.098078:  
2023-10-27 09:59:40.098388: Epoch 309 
2023-10-27 09:59:40.098651: Current learning rate: 0.00717 
2023-10-27 09:59:43.651868: train_loss -0.8619 
2023-10-27 09:59:43.652330: val_loss -0.8863 
2023-10-27 09:59:43.652595: Pseudo dice [0.8885, 0.9152, 0.9695, 0.8711, 0.9467] 
2023-10-27 09:59:43.652819: Epoch time: 3.55 s 
2023-10-27 09:59:44.698807:  
2023-10-27 09:59:44.699224: Epoch 310 
2023-10-27 09:59:44.699506: Current learning rate: 0.00716 
2023-10-27 09:59:48.316326: train_loss -0.8581 
2023-10-27 09:59:48.316708: val_loss -0.8775 
2023-10-27 09:59:48.316972: Pseudo dice [0.8779, 0.9005, 0.9697, 0.8677, 0.9439] 
2023-10-27 09:59:48.317201: Epoch time: 3.62 s 
2023-10-27 09:59:49.497610:  
2023-10-27 09:59:49.497962: Epoch 311 
2023-10-27 09:59:49.498255: Current learning rate: 0.00715 
2023-10-27 09:59:53.073871: train_loss -0.8723 
2023-10-27 09:59:53.074265: val_loss -0.8934 
2023-10-27 09:59:53.074550: Pseudo dice [0.889, 0.9148, 0.9678, 0.881, 0.9506] 
2023-10-27 09:59:53.074789: Epoch time: 3.58 s 
2023-10-27 09:59:54.113739:  
2023-10-27 09:59:54.114161: Epoch 312 
2023-10-27 09:59:54.114421: Current learning rate: 0.00714 
2023-10-27 09:59:57.663793: train_loss -0.8556 
2023-10-27 09:59:57.664239: val_loss -0.8671 
2023-10-27 09:59:57.664515: Pseudo dice [0.884, 0.9116, 0.9673, 0.6019, 0.9455] 
2023-10-27 09:59:57.664765: Epoch time: 3.55 s 
2023-10-27 09:59:58.713495:  
2023-10-27 09:59:58.713840: Epoch 313 
2023-10-27 09:59:58.714088: Current learning rate: 0.00713 
2023-10-27 10:00:02.271776: train_loss -0.865 
2023-10-27 10:00:02.275355: val_loss -0.8922 
2023-10-27 10:00:02.275645: Pseudo dice [0.8759, 0.903, 0.9694, 0.9013, 0.9446] 
2023-10-27 10:00:02.275874: Epoch time: 3.56 s 
2023-10-27 10:00:03.312382:  
2023-10-27 10:00:03.312798: Epoch 314 
2023-10-27 10:00:03.313049: Current learning rate: 0.00712 
2023-10-27 10:00:06.842010: train_loss -0.8544 
2023-10-27 10:00:06.842387: val_loss -0.888 
2023-10-27 10:00:06.842659: Pseudo dice [0.8871, 0.9098, 0.9712, 0.8169, 0.9472] 
2023-10-27 10:00:06.842881: Epoch time: 3.53 s 
2023-10-27 10:00:07.887048:  
2023-10-27 10:00:07.887465: Epoch 315 
2023-10-27 10:00:07.887722: Current learning rate: 0.00711 
2023-10-27 10:00:11.413597: train_loss -0.8575 
2023-10-27 10:00:11.413985: val_loss -0.8953 
2023-10-27 10:00:11.414263: Pseudo dice [0.8916, 0.914, 0.9718, 0.8994, 0.9469] 
2023-10-27 10:00:11.414540: Epoch time: 3.53 s 
2023-10-27 10:00:12.462371:  
2023-10-27 10:00:12.462794: Epoch 316 
2023-10-27 10:00:12.463064: Current learning rate: 0.0071 
2023-10-27 10:00:16.008554: train_loss -0.8683 
2023-10-27 10:00:16.008978: val_loss -0.885 
2023-10-27 10:00:16.009236: Pseudo dice [0.8885, 0.908, 0.9706, 0.8972, 0.9487] 
2023-10-27 10:00:16.009480: Epoch time: 3.55 s 
2023-10-27 10:00:17.201670:  
2023-10-27 10:00:17.202065: Epoch 317 
2023-10-27 10:00:17.202348: Current learning rate: 0.0071 
2023-10-27 10:00:20.769609: train_loss -0.8669 
2023-10-27 10:00:20.770066: val_loss -0.8926 
2023-10-27 10:00:20.770333: Pseudo dice [0.88, 0.911, 0.9701, 0.9021, 0.941] 
2023-10-27 10:00:20.770568: Epoch time: 3.57 s 
2023-10-27 10:00:21.817356:  
2023-10-27 10:00:21.817744: Epoch 318 
2023-10-27 10:00:21.818014: Current learning rate: 0.00709 
2023-10-27 10:00:25.412793: train_loss -0.8655 
2023-10-27 10:00:25.413298: val_loss -0.8854 
2023-10-27 10:00:25.413740: Pseudo dice [0.8911, 0.9082, 0.9691, 0.7747, 0.9418] 
2023-10-27 10:00:25.414263: Epoch time: 3.6 s 
2023-10-27 10:00:26.465432:  
2023-10-27 10:00:26.465812: Epoch 319 
2023-10-27 10:00:26.466102: Current learning rate: 0.00708 
2023-10-27 10:00:30.020247: train_loss -0.8699 
2023-10-27 10:00:30.020661: val_loss -0.8965 
2023-10-27 10:00:30.020937: Pseudo dice [0.8881, 0.9173, 0.9702, 0.921, 0.9494] 
2023-10-27 10:00:30.021181: Epoch time: 3.56 s 
2023-10-27 10:00:31.060175:  
2023-10-27 10:00:31.060666: Epoch 320 
2023-10-27 10:00:31.060934: Current learning rate: 0.00707 
2023-10-27 10:00:34.594471: train_loss -0.8687 
2023-10-27 10:00:34.594843: val_loss -0.8941 
2023-10-27 10:00:34.595109: Pseudo dice [0.8878, 0.9098, 0.9708, 0.8725, 0.9471] 
2023-10-27 10:00:34.595337: Epoch time: 3.53 s 
2023-10-27 10:00:35.643292:  
2023-10-27 10:00:35.643667: Epoch 321 
2023-10-27 10:00:35.643939: Current learning rate: 0.00706 
2023-10-27 10:00:39.242818: train_loss -0.8699 
2023-10-27 10:00:39.243244: val_loss -0.89 
2023-10-27 10:00:39.243529: Pseudo dice [0.8794, 0.9085, 0.9698, 0.8204, 0.9449] 
2023-10-27 10:00:39.243763: Epoch time: 3.6 s 
2023-10-27 10:00:40.299993:  
2023-10-27 10:00:40.300403: Epoch 322 
2023-10-27 10:00:40.300676: Current learning rate: 0.00705 
2023-10-27 10:00:43.859105: train_loss -0.87 
2023-10-27 10:00:43.859550: val_loss -0.8881 
2023-10-27 10:00:43.859811: Pseudo dice [0.888, 0.9121, 0.9704, 0.812, 0.9416] 
2023-10-27 10:00:43.860044: Epoch time: 3.56 s 
2023-10-27 10:00:45.048338:  
2023-10-27 10:00:45.048766: Epoch 323 
2023-10-27 10:00:45.049014: Current learning rate: 0.00704 
2023-10-27 10:00:48.596864: train_loss -0.8711 
2023-10-27 10:00:48.597245: val_loss -0.8948 
2023-10-27 10:00:48.597518: Pseudo dice [0.8924, 0.915, 0.9706, 0.7833, 0.9483] 
2023-10-27 10:00:48.597748: Epoch time: 3.55 s 
2023-10-27 10:00:49.642898:  
2023-10-27 10:00:49.643274: Epoch 324 
2023-10-27 10:00:49.643560: Current learning rate: 0.00703 
2023-10-27 10:00:53.192873: train_loss -0.8708 
2023-10-27 10:00:53.193268: val_loss -0.8881 
2023-10-27 10:00:53.193560: Pseudo dice [0.8849, 0.9074, 0.969, 0.8101, 0.9469] 
2023-10-27 10:00:53.193800: Epoch time: 3.55 s 
2023-10-27 10:00:54.236640:  
2023-10-27 10:00:54.237032: Epoch 325 
2023-10-27 10:00:54.237296: Current learning rate: 0.00702 
2023-10-27 10:00:57.808866: train_loss -0.8782 
2023-10-27 10:00:57.809235: val_loss -0.8921 
2023-10-27 10:00:57.809493: Pseudo dice [0.8932, 0.9154, 0.9693, 0.8757, 0.9496] 
2023-10-27 10:00:57.809724: Epoch time: 3.57 s 
2023-10-27 10:00:58.851693:  
2023-10-27 10:00:58.852036: Epoch 326 
2023-10-27 10:00:58.852311: Current learning rate: 0.00701 
2023-10-27 10:01:02.400802: train_loss -0.8734 
2023-10-27 10:01:02.401213: val_loss -0.8938 
2023-10-27 10:01:02.401471: Pseudo dice [0.8898, 0.9122, 0.9705, 0.8559, 0.9472] 
2023-10-27 10:01:02.401701: Epoch time: 3.55 s 
2023-10-27 10:01:03.444618:  
2023-10-27 10:01:03.445003: Epoch 327 
2023-10-27 10:01:03.445266: Current learning rate: 0.007 
2023-10-27 10:01:06.999807: train_loss -0.8635 
2023-10-27 10:01:07.000160: val_loss -0.8933 
2023-10-27 10:01:07.000432: Pseudo dice [0.8906, 0.9157, 0.9713, 0.9013, 0.9485] 
2023-10-27 10:01:07.000684: Epoch time: 3.56 s 
2023-10-27 10:01:08.037589:  
2023-10-27 10:01:08.037898: Epoch 328 
2023-10-27 10:01:08.038150: Current learning rate: 0.00699 
2023-10-27 10:01:11.585370: train_loss -0.8375 
2023-10-27 10:01:11.585768: val_loss -0.831 
2023-10-27 10:01:11.586042: Pseudo dice [0.8678, 0.8933, 0.9641, 0.0, 0.9334] 
2023-10-27 10:01:11.586274: Epoch time: 3.55 s 
2023-10-27 10:01:12.767174:  
2023-10-27 10:01:12.767517: Epoch 329 
2023-10-27 10:01:12.767797: Current learning rate: 0.00698 
2023-10-27 10:01:16.370034: train_loss -0.8266 
2023-10-27 10:01:16.370425: val_loss -0.8682 
2023-10-27 10:01:16.370709: Pseudo dice [0.8744, 0.9026, 0.9693, 0.5375, 0.9423] 
2023-10-27 10:01:16.370945: Epoch time: 3.6 s 
2023-10-27 10:01:17.408247:  
2023-10-27 10:01:17.408602: Epoch 330 
2023-10-27 10:01:17.408868: Current learning rate: 0.00697 
2023-10-27 10:01:21.126610: train_loss -0.8467 
2023-10-27 10:01:21.127065: val_loss -0.8879 
2023-10-27 10:01:21.127338: Pseudo dice [0.8816, 0.9169, 0.9664, 0.7387, 0.9433] 
2023-10-27 10:01:21.127579: Epoch time: 3.72 s 
2023-10-27 10:01:22.214250:  
2023-10-27 10:01:22.214787: Epoch 331 
2023-10-27 10:01:22.215067: Current learning rate: 0.00696 
2023-10-27 10:01:25.972606: train_loss -0.8548 
2023-10-27 10:01:25.973335: val_loss -0.8882 
2023-10-27 10:01:25.973942: Pseudo dice [0.8761, 0.907, 0.9677, 0.8788, 0.9449] 
2023-10-27 10:01:25.974419: Epoch time: 3.76 s 
2023-10-27 10:01:27.059118:  
2023-10-27 10:01:27.059590: Epoch 332 
2023-10-27 10:01:27.059849: Current learning rate: 0.00696 
2023-10-27 10:01:30.688201: train_loss -0.8527 
2023-10-27 10:01:30.688688: val_loss -0.8875 
2023-10-27 10:01:30.688987: Pseudo dice [0.8863, 0.9129, 0.971, 0.8403, 0.945] 
2023-10-27 10:01:30.689243: Epoch time: 3.63 s 
2023-10-27 10:01:31.746598:  
2023-10-27 10:01:31.747088: Epoch 333 
2023-10-27 10:01:31.747376: Current learning rate: 0.00695 
2023-10-27 10:01:35.396622: train_loss -0.8636 
2023-10-27 10:01:35.397066: val_loss -0.8934 
2023-10-27 10:01:35.397328: Pseudo dice [0.892, 0.919, 0.9677, 0.8932, 0.9475] 
2023-10-27 10:01:35.397568: Epoch time: 3.65 s 
2023-10-27 10:01:36.459559:  
2023-10-27 10:01:36.459918: Epoch 334 
2023-10-27 10:01:36.460168: Current learning rate: 0.00694 
2023-10-27 10:01:40.193304: train_loss -0.8688 
2023-10-27 10:01:40.194311: val_loss -0.8947 
2023-10-27 10:01:40.194829: Pseudo dice [0.889, 0.9197, 0.9704, 0.8989, 0.9477] 
2023-10-27 10:01:40.195243: Epoch time: 3.73 s 
2023-10-27 10:01:41.281110:  
2023-10-27 10:01:41.281452: Epoch 335 
2023-10-27 10:01:41.281723: Current learning rate: 0.00693 
2023-10-27 10:01:45.037461: train_loss -0.8645 
2023-10-27 10:01:45.037957: val_loss -0.8902 
2023-10-27 10:01:45.038321: Pseudo dice [0.8818, 0.9099, 0.969, 0.8059, 0.9436] 
2023-10-27 10:01:45.038599: Epoch time: 3.76 s 
2023-10-27 10:01:46.259516:  
2023-10-27 10:01:46.259867: Epoch 336 
2023-10-27 10:01:46.260117: Current learning rate: 0.00692 
2023-10-27 10:01:49.966291: train_loss -0.8647 
2023-10-27 10:01:49.966867: val_loss -0.8922 
2023-10-27 10:01:49.967134: Pseudo dice [0.8911, 0.9155, 0.9712, 0.8297, 0.9483] 
2023-10-27 10:01:49.967373: Epoch time: 3.71 s 
2023-10-27 10:01:51.034410:  
2023-10-27 10:01:51.034752: Epoch 337 
2023-10-27 10:01:51.034992: Current learning rate: 0.00691 
2023-10-27 10:01:54.717684: train_loss -0.8665 
2023-10-27 10:01:54.718145: val_loss -0.8925 
2023-10-27 10:01:54.718426: Pseudo dice [0.8897, 0.9196, 0.9713, 0.8898, 0.9471] 
2023-10-27 10:01:54.718671: Epoch time: 3.68 s 
2023-10-27 10:01:55.793411:  
2023-10-27 10:01:55.794570: Epoch 338 
2023-10-27 10:01:55.795188: Current learning rate: 0.0069 
2023-10-27 10:01:59.413229: train_loss -0.864 
2023-10-27 10:01:59.413752: val_loss -0.8845 
2023-10-27 10:01:59.414013: Pseudo dice [0.8885, 0.9136, 0.9692, 0.7352, 0.9476] 
2023-10-27 10:01:59.414251: Epoch time: 3.62 s 
2023-10-27 10:02:00.475351:  
2023-10-27 10:02:00.475849: Epoch 339 
2023-10-27 10:02:00.476088: Current learning rate: 0.00689 
2023-10-27 10:02:04.037391: train_loss -0.8716 
2023-10-27 10:02:04.037904: val_loss -0.8956 
2023-10-27 10:02:04.038154: Pseudo dice [0.8911, 0.9134, 0.9693, 0.8971, 0.9481] 
2023-10-27 10:02:04.038393: Epoch time: 3.56 s 
2023-10-27 10:02:05.106495:  
2023-10-27 10:02:05.107377: Epoch 340 
2023-10-27 10:02:05.107974: Current learning rate: 0.00688 
2023-10-27 10:02:08.572848: train_loss -0.8705 
2023-10-27 10:02:08.573360: val_loss -0.8912 
2023-10-27 10:02:08.573624: Pseudo dice [0.8919, 0.9137, 0.9694, 0.8031, 0.9461] 
2023-10-27 10:02:08.573867: Epoch time: 3.47 s 
2023-10-27 10:02:09.632892:  
2023-10-27 10:02:09.633222: Epoch 341 
2023-10-27 10:02:09.633489: Current learning rate: 0.00687 
2023-10-27 10:02:13.265996: train_loss -0.8617 
2023-10-27 10:02:13.266379: val_loss -0.8914 
2023-10-27 10:02:13.266649: Pseudo dice [0.8842, 0.916, 0.9694, 0.8337, 0.9451] 
2023-10-27 10:02:13.266885: Epoch time: 3.63 s 
2023-10-27 10:02:14.319797:  
2023-10-27 10:02:14.320147: Epoch 342 
2023-10-27 10:02:14.320402: Current learning rate: 0.00686 
2023-10-27 10:02:18.029068: train_loss -0.8612 
2023-10-27 10:02:18.029443: val_loss -0.8941 
2023-10-27 10:02:18.029706: Pseudo dice [0.8867, 0.915, 0.9703, 0.8837, 0.9464] 
2023-10-27 10:02:18.029932: Epoch time: 3.71 s 
2023-10-27 10:02:19.093107:  
2023-10-27 10:02:19.093428: Epoch 343 
2023-10-27 10:02:19.093671: Current learning rate: 0.00685 
2023-10-27 10:02:22.579791: train_loss -0.8668 
2023-10-27 10:02:22.580160: val_loss -0.8977 
2023-10-27 10:02:22.580422: Pseudo dice [0.8997, 0.9109, 0.9695, 0.8992, 0.9501] 
2023-10-27 10:02:22.580652: Epoch time: 3.49 s 
2023-10-27 10:02:23.639751:  
2023-10-27 10:02:23.640083: Epoch 344 
2023-10-27 10:02:23.640315: Current learning rate: 0.00684 
2023-10-27 10:02:27.076951: train_loss -0.8679 
2023-10-27 10:02:27.077414: val_loss -0.8887 
2023-10-27 10:02:27.077676: Pseudo dice [0.8899, 0.9191, 0.9696, 0.8876, 0.9372] 
2023-10-27 10:02:27.077915: Epoch time: 3.44 s 
2023-10-27 10:02:28.137716:  
2023-10-27 10:02:28.138006: Epoch 345 
2023-10-27 10:02:28.138248: Current learning rate: 0.00683 
2023-10-27 10:02:31.523545: train_loss -0.8494 
2023-10-27 10:02:31.523907: val_loss -0.8855 
2023-10-27 10:02:31.524163: Pseudo dice [0.8844, 0.9059, 0.9684, 0.8702, 0.9426] 
2023-10-27 10:02:31.524403: Epoch time: 3.39 s 
2023-10-27 10:02:32.579004:  
2023-10-27 10:02:32.579319: Epoch 346 
2023-10-27 10:02:32.579563: Current learning rate: 0.00682 
2023-10-27 10:02:35.931191: train_loss -0.8564 
2023-10-27 10:02:35.931630: val_loss -0.855 
2023-10-27 10:02:35.931880: Pseudo dice [0.882, 0.8951, 0.9677, 0.5287, 0.9302] 
2023-10-27 10:02:35.932106: Epoch time: 3.35 s 
2023-10-27 10:02:36.992259:  
2023-10-27 10:02:36.992562: Epoch 347 
2023-10-27 10:02:36.992805: Current learning rate: 0.00681 
2023-10-27 10:02:40.529978: train_loss -0.847 
2023-10-27 10:02:40.530675: val_loss -0.8663 
2023-10-27 10:02:40.530957: Pseudo dice [0.88, 0.9118, 0.9651, 0.5471, 0.9411] 
2023-10-27 10:02:40.531180: Epoch time: 3.54 s 
2023-10-27 10:02:41.582832:  
2023-10-27 10:02:41.583159: Epoch 348 
2023-10-27 10:02:41.583391: Current learning rate: 0.0068 
2023-10-27 10:02:45.105241: train_loss -0.8542 
2023-10-27 10:02:45.105650: val_loss -0.8768 
2023-10-27 10:02:45.105905: Pseudo dice [0.8802, 0.9069, 0.9678, 0.7114, 0.9459] 
2023-10-27 10:02:45.106130: Epoch time: 3.52 s 
2023-10-27 10:02:46.169843:  
2023-10-27 10:02:46.170273: Epoch 349 
2023-10-27 10:02:46.170600: Current learning rate: 0.0068 
2023-10-27 10:02:49.576961: train_loss -0.8593 
2023-10-27 10:02:49.577338: val_loss -0.8949 
2023-10-27 10:02:49.577598: Pseudo dice [0.8814, 0.9126, 0.9693, 0.8663, 0.9419] 
2023-10-27 10:02:49.577822: Epoch time: 3.41 s 
2023-10-27 10:02:50.720671:  
2023-10-27 10:02:50.721007: Epoch 350 
2023-10-27 10:02:50.721242: Current learning rate: 0.00679 
2023-10-27 10:02:54.120370: train_loss -0.857 
2023-10-27 10:02:54.120733: val_loss -0.8665 
2023-10-27 10:02:54.120994: Pseudo dice [0.874, 0.9094, 0.9695, 0.5513, 0.9431] 
2023-10-27 10:02:54.121225: Epoch time: 3.4 s 
2023-10-27 10:02:55.173024:  
2023-10-27 10:02:55.173364: Epoch 351 
2023-10-27 10:02:55.173609: Current learning rate: 0.00678 
2023-10-27 10:02:58.631034: train_loss -0.8555 
2023-10-27 10:02:58.631385: val_loss -0.8853 
2023-10-27 10:02:58.631637: Pseudo dice [0.8787, 0.9125, 0.967, 0.8986, 0.9309] 
2023-10-27 10:02:58.631853: Epoch time: 3.46 s 
2023-10-27 10:02:59.689446:  
2023-10-27 10:02:59.689738: Epoch 352 
2023-10-27 10:02:59.689976: Current learning rate: 0.00677 
2023-10-27 10:03:03.129960: train_loss -0.8611 
2023-10-27 10:03:03.130354: val_loss -0.8889 
2023-10-27 10:03:03.130621: Pseudo dice [0.8792, 0.9105, 0.9682, 0.8824, 0.9402] 
2023-10-27 10:03:03.130846: Epoch time: 3.44 s 
2023-10-27 10:03:04.188886:  
2023-10-27 10:03:04.189287: Epoch 353 
2023-10-27 10:03:04.189539: Current learning rate: 0.00676 
2023-10-27 10:03:07.569342: train_loss -0.8649 
2023-10-27 10:03:07.569764: val_loss -0.8886 
2023-10-27 10:03:07.570019: Pseudo dice [0.8841, 0.9088, 0.9706, 0.8347, 0.9476] 
2023-10-27 10:03:07.570250: Epoch time: 3.38 s 
2023-10-27 10:03:08.775921:  
2023-10-27 10:03:08.776209: Epoch 354 
2023-10-27 10:03:08.776464: Current learning rate: 0.00675 
2023-10-27 10:03:12.179050: train_loss -0.8628 
2023-10-27 10:03:12.179482: val_loss -0.8942 
2023-10-27 10:03:12.179819: Pseudo dice [0.8887, 0.9138, 0.97, 0.8507, 0.948] 
2023-10-27 10:03:12.180393: Epoch time: 3.4 s 
2023-10-27 10:03:13.242967:  
2023-10-27 10:03:13.243263: Epoch 355 
2023-10-27 10:03:13.243520: Current learning rate: 0.00674 
2023-10-27 10:03:16.675796: train_loss -0.849 
2023-10-27 10:03:16.676167: val_loss -0.8695 
2023-10-27 10:03:16.676433: Pseudo dice [0.8851, 0.9071, 0.9715, 0.6398, 0.9453] 
2023-10-27 10:03:16.676654: Epoch time: 3.43 s 
2023-10-27 10:03:17.736060:  
2023-10-27 10:03:17.736365: Epoch 356 
2023-10-27 10:03:17.736612: Current learning rate: 0.00673 
2023-10-27 10:03:21.133047: train_loss -0.8554 
2023-10-27 10:03:21.133433: val_loss -0.8948 
2023-10-27 10:03:21.133690: Pseudo dice [0.8792, 0.9125, 0.9713, 0.8376, 0.9445] 
2023-10-27 10:03:21.133905: Epoch time: 3.4 s 
2023-10-27 10:03:22.191478:  
2023-10-27 10:03:22.191766: Epoch 357 
2023-10-27 10:03:22.191995: Current learning rate: 0.00672 
2023-10-27 10:03:25.633879: train_loss -0.8515 
2023-10-27 10:03:25.634282: val_loss -0.8755 
2023-10-27 10:03:25.634546: Pseudo dice [0.8916, 0.9135, 0.9712, 0.7756, 0.9417] 
2023-10-27 10:03:25.634786: Epoch time: 3.44 s 
2023-10-27 10:03:26.693632:  
2023-10-27 10:03:26.693958: Epoch 358 
2023-10-27 10:03:26.694193: Current learning rate: 0.00671 
2023-10-27 10:03:30.077816: train_loss -0.8723 
2023-10-27 10:03:30.078292: val_loss -0.8925 
2023-10-27 10:03:30.078629: Pseudo dice [0.8859, 0.9119, 0.9673, 0.8948, 0.9442] 
2023-10-27 10:03:30.079050: Epoch time: 3.38 s 
2023-10-27 10:03:31.137319:  
2023-10-27 10:03:31.137664: Epoch 359 
2023-10-27 10:03:31.137913: Current learning rate: 0.0067 
2023-10-27 10:03:34.508250: train_loss -0.8653 
2023-10-27 10:03:34.508618: val_loss -0.8941 
2023-10-27 10:03:34.508873: Pseudo dice [0.8866, 0.9108, 0.9698, 0.8909, 0.9431] 
2023-10-27 10:03:34.509100: Epoch time: 3.37 s 
2023-10-27 10:03:35.702511:  
2023-10-27 10:03:35.702829: Epoch 360 
2023-10-27 10:03:35.703069: Current learning rate: 0.00669 
2023-10-27 10:03:39.109377: train_loss -0.8634 
2023-10-27 10:03:39.109989: val_loss -0.8963 
2023-10-27 10:03:39.110227: Pseudo dice [0.8941, 0.9123, 0.9711, 0.9016, 0.9443] 
2023-10-27 10:03:39.110444: Epoch time: 3.41 s 
2023-10-27 10:03:40.165432:  
2023-10-27 10:03:40.165723: Epoch 361 
2023-10-27 10:03:40.165954: Current learning rate: 0.00668 
2023-10-27 10:03:43.655964: train_loss -0.8658 
2023-10-27 10:03:43.656514: val_loss -0.8919 
2023-10-27 10:03:43.656801: Pseudo dice [0.8924, 0.9124, 0.9708, 0.8395, 0.9447] 
2023-10-27 10:03:43.657037: Epoch time: 3.49 s 
2023-10-27 10:03:44.716252:  
2023-10-27 10:03:44.716575: Epoch 362 
2023-10-27 10:03:44.716813: Current learning rate: 0.00667 
2023-10-27 10:03:48.111202: train_loss -0.86 
2023-10-27 10:03:48.111586: val_loss -0.8917 
2023-10-27 10:03:48.111848: Pseudo dice [0.8927, 0.9108, 0.9704, 0.8877, 0.947] 
2023-10-27 10:03:48.112069: Epoch time: 3.4 s 
2023-10-27 10:03:49.169372:  
2023-10-27 10:03:49.169663: Epoch 363 
2023-10-27 10:03:49.169899: Current learning rate: 0.00666 
2023-10-27 10:03:52.563570: train_loss -0.8527 
2023-10-27 10:03:52.564222: val_loss -0.8872 
2023-10-27 10:03:52.564527: Pseudo dice [0.8901, 0.9117, 0.9709, 0.8553, 0.9448] 
2023-10-27 10:03:52.564770: Epoch time: 3.39 s 
2023-10-27 10:03:53.640728:  
2023-10-27 10:03:53.641096: Epoch 364 
2023-10-27 10:03:53.641348: Current learning rate: 0.00665 
2023-10-27 10:03:57.005752: train_loss -0.8643 
2023-10-27 10:03:57.006179: val_loss -0.8938 
2023-10-27 10:03:57.006485: Pseudo dice [0.8903, 0.9134, 0.9692, 0.8693, 0.9427] 
2023-10-27 10:03:57.006738: Epoch time: 3.37 s 
2023-10-27 10:03:58.069664:  
2023-10-27 10:03:58.069969: Epoch 365 
2023-10-27 10:03:58.070219: Current learning rate: 0.00665 
2023-10-27 10:04:01.412319: train_loss -0.8679 
2023-10-27 10:04:01.412676: val_loss -0.8958 
2023-10-27 10:04:01.412928: Pseudo dice [0.8866, 0.915, 0.9704, 0.8781, 0.9486] 
2023-10-27 10:04:01.413167: Epoch time: 3.34 s 
2023-10-27 10:04:02.651222:  
2023-10-27 10:04:02.651606: Epoch 366 
2023-10-27 10:04:02.651851: Current learning rate: 0.00664 
2023-10-27 10:04:06.000795: train_loss -0.8695 
2023-10-27 10:04:06.001160: val_loss -0.8815 
2023-10-27 10:04:06.001417: Pseudo dice [0.895, 0.9051, 0.9709, 0.5838, 0.9455] 
2023-10-27 10:04:06.001638: Epoch time: 3.35 s 
2023-10-27 10:04:07.060137:  
2023-10-27 10:04:07.060430: Epoch 367 
2023-10-27 10:04:07.060671: Current learning rate: 0.00663 
2023-10-27 10:04:10.422903: train_loss -0.8699 
2023-10-27 10:04:10.423294: val_loss -0.8903 
2023-10-27 10:04:10.423569: Pseudo dice [0.8861, 0.9169, 0.9686, 0.812, 0.9497] 
2023-10-27 10:04:10.423801: Epoch time: 3.36 s 
2023-10-27 10:04:11.494737:  
2023-10-27 10:04:11.495116: Epoch 368 
2023-10-27 10:04:11.495365: Current learning rate: 0.00662 
2023-10-27 10:04:14.904215: train_loss -0.8663 
2023-10-27 10:04:14.904630: val_loss -0.8903 
2023-10-27 10:04:14.904927: Pseudo dice [0.882, 0.91, 0.9699, 0.8763, 0.9212] 
2023-10-27 10:04:14.905248: Epoch time: 3.41 s 
2023-10-27 10:04:15.975891:  
2023-10-27 10:04:15.976214: Epoch 369 
2023-10-27 10:04:15.976456: Current learning rate: 0.00661 
2023-10-27 10:04:19.357068: train_loss -0.8546 
2023-10-27 10:04:19.357445: val_loss -0.8829 
2023-10-27 10:04:19.357922: Pseudo dice [0.8828, 0.9114, 0.9716, 0.8007, 0.9416] 
2023-10-27 10:04:19.358172: Epoch time: 3.38 s 
2023-10-27 10:04:20.422967:  
2023-10-27 10:04:20.423297: Epoch 370 
2023-10-27 10:04:20.423550: Current learning rate: 0.0066 
2023-10-27 10:04:23.894998: train_loss -0.8612 
2023-10-27 10:04:23.895361: val_loss -0.8906 
2023-10-27 10:04:23.895640: Pseudo dice [0.8803, 0.9093, 0.9669, 0.9029, 0.9459] 
2023-10-27 10:04:23.895862: Epoch time: 3.47 s 
2023-10-27 10:04:24.956047:  
2023-10-27 10:04:24.956339: Epoch 371 
2023-10-27 10:04:24.956581: Current learning rate: 0.00659 
2023-10-27 10:04:28.309158: train_loss -0.8672 
2023-10-27 10:04:28.309544: val_loss -0.8677 
2023-10-27 10:04:28.309798: Pseudo dice [0.884, 0.9167, 0.9695, 0.8943, 0.9298] 
2023-10-27 10:04:28.310018: Epoch time: 3.35 s 
2023-10-27 10:04:29.509179:  
2023-10-27 10:04:29.509525: Epoch 372 
2023-10-27 10:04:29.509764: Current learning rate: 0.00658 
2023-10-27 10:04:32.963775: train_loss -0.8337 
2023-10-27 10:04:32.964288: val_loss -0.8813 
2023-10-27 10:04:32.964641: Pseudo dice [0.8834, 0.9044, 0.9701, 0.8592, 0.9423] 
2023-10-27 10:04:32.964876: Epoch time: 3.46 s 
2023-10-27 10:04:34.032336:  
2023-10-27 10:04:34.032655: Epoch 373 
2023-10-27 10:04:34.032907: Current learning rate: 0.00657 
2023-10-27 10:04:37.744416: train_loss -0.8582 
2023-10-27 10:04:37.744901: val_loss -0.8889 
2023-10-27 10:04:37.745159: Pseudo dice [0.8815, 0.9115, 0.9695, 0.9046, 0.9381] 
2023-10-27 10:04:37.745380: Epoch time: 3.71 s 
2023-10-27 10:04:38.806707:  
2023-10-27 10:04:38.806992: Epoch 374 
2023-10-27 10:04:38.807291: Current learning rate: 0.00656 
2023-10-27 10:04:42.202910: train_loss -0.853 
2023-10-27 10:04:42.203293: val_loss -0.894 
2023-10-27 10:04:42.203551: Pseudo dice [0.8889, 0.9157, 0.9708, 0.8709, 0.9466] 
2023-10-27 10:04:42.203773: Epoch time: 3.4 s 
2023-10-27 10:04:43.275141:  
2023-10-27 10:04:43.275676: Epoch 375 
2023-10-27 10:04:43.276001: Current learning rate: 0.00655 
2023-10-27 10:04:46.710677: train_loss -0.8612 
2023-10-27 10:04:46.711138: val_loss -0.8905 
2023-10-27 10:04:46.711404: Pseudo dice [0.8848, 0.9106, 0.9698, 0.8571, 0.9455] 
2023-10-27 10:04:46.711637: Epoch time: 3.44 s 
2023-10-27 10:04:47.786128:  
2023-10-27 10:04:47.786448: Epoch 376 
2023-10-27 10:04:47.786709: Current learning rate: 0.00654 
2023-10-27 10:04:51.200533: train_loss -0.8624 
2023-10-27 10:04:51.201008: val_loss -0.8963 
2023-10-27 10:04:51.201266: Pseudo dice [0.8878, 0.9132, 0.9691, 0.914, 0.9482] 
2023-10-27 10:04:51.201501: Epoch time: 3.41 s 
2023-10-27 10:04:52.268527:  
2023-10-27 10:04:52.268845: Epoch 377 
2023-10-27 10:04:52.269085: Current learning rate: 0.00653 
2023-10-27 10:04:55.722118: train_loss -0.864 
2023-10-27 10:04:55.722516: val_loss -0.8898 
2023-10-27 10:04:55.722773: Pseudo dice [0.8872, 0.9064, 0.9716, 0.8622, 0.9378] 
2023-10-27 10:04:55.722996: Epoch time: 3.45 s 
2023-10-27 10:04:56.922738:  
2023-10-27 10:04:56.923178: Epoch 378 
2023-10-27 10:04:56.923424: Current learning rate: 0.00652 
2023-10-27 10:05:00.298303: train_loss -0.8594 
2023-10-27 10:05:00.298715: val_loss -0.8989 
2023-10-27 10:05:00.298965: Pseudo dice [0.8882, 0.9182, 0.9716, 0.867, 0.9471] 
2023-10-27 10:05:00.299192: Epoch time: 3.38 s 
2023-10-27 10:05:01.363696:  
2023-10-27 10:05:01.363993: Epoch 379 
2023-10-27 10:05:01.364233: Current learning rate: 0.00651 
2023-10-27 10:05:04.750691: train_loss -0.8679 
2023-10-27 10:05:04.751152: val_loss -0.894 
2023-10-27 10:05:04.751473: Pseudo dice [0.8911, 0.9159, 0.9716, 0.8806, 0.9475] 
2023-10-27 10:05:04.751760: Epoch time: 3.39 s 
2023-10-27 10:05:05.812451:  
2023-10-27 10:05:05.812787: Epoch 380 
2023-10-27 10:05:05.813027: Current learning rate: 0.0065 
2023-10-27 10:05:09.205582: train_loss -0.8713 
2023-10-27 10:05:09.205962: val_loss -0.8901 
2023-10-27 10:05:09.206228: Pseudo dice [0.8868, 0.909, 0.9689, 0.8465, 0.9474] 
2023-10-27 10:05:09.206464: Epoch time: 3.39 s 
2023-10-27 10:05:10.270696:  
2023-10-27 10:05:10.271019: Epoch 381 
2023-10-27 10:05:10.271268: Current learning rate: 0.00649 
2023-10-27 10:05:13.631956: train_loss -0.8619 
2023-10-27 10:05:13.632358: val_loss -0.8862 
2023-10-27 10:05:13.632632: Pseudo dice [0.8861, 0.9125, 0.9685, 0.8343, 0.949] 
2023-10-27 10:05:13.632865: Epoch time: 3.36 s 
2023-10-27 10:05:14.703964:  
2023-10-27 10:05:14.704306: Epoch 382 
2023-10-27 10:05:14.704579: Current learning rate: 0.00648 
2023-10-27 10:05:18.062039: train_loss -0.8743 
2023-10-27 10:05:18.062403: val_loss -0.8939 
2023-10-27 10:05:18.062641: Pseudo dice [0.885, 0.9127, 0.9675, 0.9023, 0.9478] 
2023-10-27 10:05:18.062861: Epoch time: 3.36 s 
2023-10-27 10:05:19.132224:  
2023-10-27 10:05:19.132532: Epoch 383 
2023-10-27 10:05:19.132775: Current learning rate: 0.00648 
2023-10-27 10:05:22.495819: train_loss -0.8711 
2023-10-27 10:05:22.496182: val_loss -0.8929 
2023-10-27 10:05:22.496457: Pseudo dice [0.8829, 0.911, 0.9697, 0.8534, 0.9484] 
2023-10-27 10:05:22.496687: Epoch time: 3.36 s 
2023-10-27 10:05:23.713695:  
2023-10-27 10:05:23.714045: Epoch 384 
2023-10-27 10:05:23.714289: Current learning rate: 0.00647 
2023-10-27 10:05:27.169699: train_loss -0.8625 
2023-10-27 10:05:27.170094: val_loss -0.8935 
2023-10-27 10:05:27.170342: Pseudo dice [0.8881, 0.9122, 0.9697, 0.8822, 0.9459] 
2023-10-27 10:05:27.170576: Epoch time: 3.46 s 
2023-10-27 10:05:28.260806:  
2023-10-27 10:05:28.261186: Epoch 385 
2023-10-27 10:05:28.261431: Current learning rate: 0.00646 
2023-10-27 10:05:31.613218: train_loss -0.8749 
2023-10-27 10:05:31.613578: val_loss -0.8964 
2023-10-27 10:05:31.613832: Pseudo dice [0.8918, 0.9133, 0.9705, 0.8937, 0.9448] 
2023-10-27 10:05:31.614046: Epoch time: 3.35 s 
2023-10-27 10:05:32.687933:  
2023-10-27 10:05:32.688254: Epoch 386 
2023-10-27 10:05:32.688498: Current learning rate: 0.00645 
2023-10-27 10:05:36.017500: train_loss -0.876 
2023-10-27 10:05:36.017863: val_loss -0.8934 
2023-10-27 10:05:36.018118: Pseudo dice [0.8946, 0.9097, 0.9685, 0.8789, 0.9469] 
2023-10-27 10:05:36.018335: Epoch time: 3.33 s 
2023-10-27 10:05:37.094191:  
2023-10-27 10:05:37.094531: Epoch 387 
2023-10-27 10:05:37.094776: Current learning rate: 0.00644 
2023-10-27 10:05:40.440290: train_loss -0.8598 
2023-10-27 10:05:40.440770: val_loss -0.8841 
2023-10-27 10:05:40.441017: Pseudo dice [0.8841, 0.9083, 0.9678, 0.8193, 0.9404] 
2023-10-27 10:05:40.441247: Epoch time: 3.35 s 
2023-10-27 10:05:41.520425:  
2023-10-27 10:05:41.520897: Epoch 388 
2023-10-27 10:05:41.521150: Current learning rate: 0.00643 
2023-10-27 10:05:44.965029: train_loss -0.8637 
2023-10-27 10:05:44.965427: val_loss -0.897 
2023-10-27 10:05:44.965693: Pseudo dice [0.8911, 0.9132, 0.9705, 0.8953, 0.9457] 
2023-10-27 10:05:44.965911: Epoch time: 3.45 s 
2023-10-27 10:05:46.179451:  
2023-10-27 10:05:46.179801: Epoch 389 
2023-10-27 10:05:46.180080: Current learning rate: 0.00642 
2023-10-27 10:05:49.569534: train_loss -0.8724 
2023-10-27 10:05:49.569903: val_loss -0.8987 
2023-10-27 10:05:49.570158: Pseudo dice [0.8895, 0.9111, 0.9709, 0.9194, 0.9495] 
2023-10-27 10:05:49.570408: Epoch time: 3.39 s 
2023-10-27 10:05:49.570616: Yayy! New best EMA pseudo Dice: 0.9169 
2023-10-27 10:05:50.717819:  
2023-10-27 10:05:50.718144: Epoch 390 
2023-10-27 10:05:50.718385: Current learning rate: 0.00641 
2023-10-27 10:05:54.144493: train_loss -0.8752 
2023-10-27 10:05:54.144886: val_loss -0.8955 
2023-10-27 10:05:54.145146: Pseudo dice [0.8888, 0.9116, 0.9725, 0.8852, 0.9493] 
2023-10-27 10:05:54.145375: Epoch time: 3.43 s 
2023-10-27 10:05:54.145590: Yayy! New best EMA pseudo Dice: 0.9173 
2023-10-27 10:05:55.308566:  
2023-10-27 10:05:55.308928: Epoch 391 
2023-10-27 10:05:55.309166: Current learning rate: 0.0064 
2023-10-27 10:05:58.690611: train_loss -0.8674 
2023-10-27 10:05:58.690979: val_loss -0.8831 
2023-10-27 10:05:58.691240: Pseudo dice [0.8837, 0.9101, 0.9692, 0.8516, 0.9318] 
2023-10-27 10:05:58.691472: Epoch time: 3.38 s 
2023-10-27 10:05:59.765047:  
2023-10-27 10:05:59.765335: Epoch 392 
2023-10-27 10:05:59.765593: Current learning rate: 0.00639 
2023-10-27 10:06:03.177216: train_loss -0.8649 
2023-10-27 10:06:03.177613: val_loss -0.8943 
2023-10-27 10:06:03.177858: Pseudo dice [0.8846, 0.9128, 0.9689, 0.8744, 0.95] 
2023-10-27 10:06:03.178077: Epoch time: 3.41 s 
2023-10-27 10:06:04.254915:  
2023-10-27 10:06:04.255280: Epoch 393 
2023-10-27 10:06:04.255528: Current learning rate: 0.00638 
2023-10-27 10:06:07.616830: train_loss -0.8719 
2023-10-27 10:06:07.617361: val_loss -0.8955 
2023-10-27 10:06:07.617619: Pseudo dice [0.8873, 0.9169, 0.9696, 0.8958, 0.9471] 
2023-10-27 10:06:07.617849: Epoch time: 3.36 s 
2023-10-27 10:06:07.618050: Yayy! New best EMA pseudo Dice: 0.9173 
2023-10-27 10:06:08.779288:  
2023-10-27 10:06:08.779577: Epoch 394 
2023-10-27 10:06:08.779819: Current learning rate: 0.00637 
2023-10-27 10:06:12.209516: train_loss -0.8709 
2023-10-27 10:06:12.210299: val_loss -0.8936 
2023-10-27 10:06:12.210635: Pseudo dice [0.8889, 0.9119, 0.9711, 0.8901, 0.9484] 
2023-10-27 10:06:12.210952: Epoch time: 3.43 s 
2023-10-27 10:06:12.211211: Yayy! New best EMA pseudo Dice: 0.9178 
2023-10-27 10:06:13.521309:  
2023-10-27 10:06:13.521606: Epoch 395 
2023-10-27 10:06:13.521836: Current learning rate: 0.00636 
2023-10-27 10:06:17.042068: train_loss -0.8625 
2023-10-27 10:06:17.042578: val_loss -0.7762 
2023-10-27 10:06:17.042868: Pseudo dice [0.8832, 0.9071, 0.969, 0.069, 0.879] 
2023-10-27 10:06:17.043279: Epoch time: 3.52 s 
2023-10-27 10:06:18.111118:  
2023-10-27 10:06:18.111443: Epoch 396 
2023-10-27 10:06:18.111685: Current learning rate: 0.00635 
2023-10-27 10:06:21.555674: train_loss -0.8545 
2023-10-27 10:06:21.556182: val_loss -0.884 
2023-10-27 10:06:21.556466: Pseudo dice [0.8798, 0.909, 0.9657, 0.8606, 0.9452] 
2023-10-27 10:06:21.556699: Epoch time: 3.45 s 
2023-10-27 10:06:22.633237:  
2023-10-27 10:06:22.633604: Epoch 397 
2023-10-27 10:06:22.633914: Current learning rate: 0.00634 
2023-10-27 10:06:26.081442: train_loss -0.8652 
2023-10-27 10:06:26.081846: val_loss -0.8888 
2023-10-27 10:06:26.082093: Pseudo dice [0.8828, 0.9158, 0.9706, 0.8568, 0.9435] 
2023-10-27 10:06:26.082314: Epoch time: 3.45 s 
2023-10-27 10:06:27.165614:  
2023-10-27 10:06:27.165968: Epoch 398 
2023-10-27 10:06:27.166201: Current learning rate: 0.00633 
2023-10-27 10:06:30.794386: train_loss -0.8712 
2023-10-27 10:06:30.794761: val_loss -0.8955 
2023-10-27 10:06:30.795012: Pseudo dice [0.8931, 0.9163, 0.9713, 0.8664, 0.9489] 
2023-10-27 10:06:30.795244: Epoch time: 3.63 s 
2023-10-27 10:06:31.875034:  
2023-10-27 10:06:31.875349: Epoch 399 
2023-10-27 10:06:31.875606: Current learning rate: 0.00632 
2023-10-27 10:06:35.274826: train_loss -0.8723 
2023-10-27 10:06:35.275181: val_loss -0.8904 
2023-10-27 10:06:35.275443: Pseudo dice [0.8933, 0.9146, 0.9708, 0.8509, 0.9498] 
2023-10-27 10:06:35.275668: Epoch time: 3.4 s 
2023-10-27 10:06:36.440920:  
2023-10-27 10:06:36.441237: Epoch 400 
2023-10-27 10:06:36.441468: Current learning rate: 0.00631 
2023-10-27 10:06:39.887883: train_loss -0.8646 
2023-10-27 10:06:39.888236: val_loss -0.8948 
2023-10-27 10:06:39.888496: Pseudo dice [0.8921, 0.9112, 0.9713, 0.8664, 0.9484] 
2023-10-27 10:06:39.888713: Epoch time: 3.45 s 
2023-10-27 10:06:41.102037:  
2023-10-27 10:06:41.102359: Epoch 401 
2023-10-27 10:06:41.102584: Current learning rate: 0.0063 
2023-10-27 10:06:44.627982: train_loss -0.8677 
2023-10-27 10:06:44.628365: val_loss -0.8954 
2023-10-27 10:06:44.628620: Pseudo dice [0.892, 0.9155, 0.9707, 0.8668, 0.9488] 
2023-10-27 10:06:44.628851: Epoch time: 3.53 s 
2023-10-27 10:06:45.703600:  
2023-10-27 10:06:45.703944: Epoch 402 
2023-10-27 10:06:45.704183: Current learning rate: 0.0063 
2023-10-27 10:06:49.163496: train_loss -0.8738 
2023-10-27 10:06:49.164021: val_loss -0.8938 
2023-10-27 10:06:49.164279: Pseudo dice [0.8923, 0.9085, 0.9718, 0.8598, 0.9474] 
2023-10-27 10:06:49.164510: Epoch time: 3.46 s 
2023-10-27 10:06:50.243278:  
2023-10-27 10:06:50.243595: Epoch 403 
2023-10-27 10:06:50.243841: Current learning rate: 0.00629 
2023-10-27 10:06:53.631611: train_loss -0.8738 
2023-10-27 10:06:53.631975: val_loss -0.8952 
2023-10-27 10:06:53.632222: Pseudo dice [0.8834, 0.9084, 0.9702, 0.8901, 0.9465] 
2023-10-27 10:06:53.632457: Epoch time: 3.39 s 
2023-10-27 10:06:54.707235:  
2023-10-27 10:06:54.707558: Epoch 404 
2023-10-27 10:06:54.707816: Current learning rate: 0.00628 
2023-10-27 10:06:58.090891: train_loss -0.875 
2023-10-27 10:06:58.091251: val_loss -0.8971 
2023-10-27 10:06:58.091689: Pseudo dice [0.8894, 0.9182, 0.9716, 0.8833, 0.9449] 
2023-10-27 10:06:58.091946: Epoch time: 3.38 s 
2023-10-27 10:06:59.170065:  
2023-10-27 10:06:59.170377: Epoch 405 
2023-10-27 10:06:59.170634: Current learning rate: 0.00627 
2023-10-27 10:07:02.570908: train_loss -0.8762 
2023-10-27 10:07:02.571282: val_loss -0.8431 
2023-10-27 10:07:02.571558: Pseudo dice [0.8897, 0.9095, 0.9688, 0.1082, 0.9403] 
2023-10-27 10:07:02.571824: Epoch time: 3.4 s 
2023-10-27 10:07:03.643219:  
2023-10-27 10:07:03.643551: Epoch 406 
2023-10-27 10:07:03.643786: Current learning rate: 0.00626 
2023-10-27 10:07:07.066531: train_loss -0.8774 
2023-10-27 10:07:07.066921: val_loss -0.8957 
2023-10-27 10:07:07.067167: Pseudo dice [0.8974, 0.9111, 0.9702, 0.8986, 0.9447] 
2023-10-27 10:07:07.067390: Epoch time: 3.42 s 
2023-10-27 10:07:08.281181:  
2023-10-27 10:07:08.281544: Epoch 407 
2023-10-27 10:07:08.281786: Current learning rate: 0.00625 
2023-10-27 10:07:11.742596: train_loss -0.873 
2023-10-27 10:07:11.742991: val_loss -0.8671 
2023-10-27 10:07:11.743239: Pseudo dice [0.8616, 0.907, 0.9698, 0.262, 0.9425] 
2023-10-27 10:07:11.743475: Epoch time: 3.46 s 
2023-10-27 10:07:12.829083:  
2023-10-27 10:07:12.829392: Epoch 408 
2023-10-27 10:07:12.829636: Current learning rate: 0.00624 
2023-10-27 10:07:16.316442: train_loss -0.8674 
2023-10-27 10:07:16.316927: val_loss -0.8899 
2023-10-27 10:07:16.317277: Pseudo dice [0.8892, 0.9131, 0.9706, 0.8807, 0.9465] 
2023-10-27 10:07:16.317549: Epoch time: 3.49 s 
2023-10-27 10:07:17.398783:  
2023-10-27 10:07:17.399153: Epoch 409 
2023-10-27 10:07:17.399402: Current learning rate: 0.00623 
2023-10-27 10:07:20.764620: train_loss -0.8701 
2023-10-27 10:07:20.764983: val_loss -0.8934 
2023-10-27 10:07:20.765233: Pseudo dice [0.8853, 0.9053, 0.972, 0.8971, 0.9467] 
2023-10-27 10:07:20.765448: Epoch time: 3.37 s 
2023-10-27 10:07:21.846160:  
2023-10-27 10:07:21.846440: Epoch 410 
2023-10-27 10:07:21.846674: Current learning rate: 0.00622 
2023-10-27 10:07:25.190711: train_loss -0.8616 
2023-10-27 10:07:25.191068: val_loss -0.8859 
2023-10-27 10:07:25.191316: Pseudo dice [0.8882, 0.9143, 0.9727, 0.8247, 0.942] 
2023-10-27 10:07:25.191535: Epoch time: 3.35 s 
2023-10-27 10:07:26.214268:  
2023-10-27 10:07:26.214588: Epoch 411 
2023-10-27 10:07:26.214817: Current learning rate: 0.00621 
2023-10-27 10:07:29.544045: train_loss -0.8662 
2023-10-27 10:07:29.544451: val_loss -0.8916 
2023-10-27 10:07:29.544708: Pseudo dice [0.8857, 0.9057, 0.9708, 0.9022, 0.9475] 
2023-10-27 10:07:29.544944: Epoch time: 3.33 s 
2023-10-27 10:07:30.578293:  
2023-10-27 10:07:30.578579: Epoch 412 
2023-10-27 10:07:30.578823: Current learning rate: 0.0062 
2023-10-27 10:07:33.939311: train_loss -0.866 
2023-10-27 10:07:33.939675: val_loss -0.8954 
2023-10-27 10:07:33.939937: Pseudo dice [0.8833, 0.9096, 0.9717, 0.8769, 0.9424] 
2023-10-27 10:07:33.940176: Epoch time: 3.36 s 
2023-10-27 10:07:35.116722:  
2023-10-27 10:07:35.117013: Epoch 413 
2023-10-27 10:07:35.117247: Current learning rate: 0.00619 
2023-10-27 10:07:38.540519: train_loss -0.8715 
2023-10-27 10:07:38.540981: val_loss -0.8941 
2023-10-27 10:07:38.541233: Pseudo dice [0.894, 0.9152, 0.97, 0.8876, 0.9488] 
2023-10-27 10:07:38.541462: Epoch time: 3.42 s 
2023-10-27 10:07:39.572258:  
2023-10-27 10:07:39.572599: Epoch 414 
2023-10-27 10:07:39.572831: Current learning rate: 0.00618 
2023-10-27 10:07:43.027290: train_loss -0.8793 
2023-10-27 10:07:43.027657: val_loss -0.8968 
2023-10-27 10:07:43.027926: Pseudo dice [0.8863, 0.9134, 0.9708, 0.8684, 0.9473] 
2023-10-27 10:07:43.028147: Epoch time: 3.46 s 
2023-10-27 10:07:44.057110:  
2023-10-27 10:07:44.057416: Epoch 415 
2023-10-27 10:07:44.057658: Current learning rate: 0.00617 
2023-10-27 10:07:47.483800: train_loss -0.8721 
2023-10-27 10:07:47.484284: val_loss -0.8848 
2023-10-27 10:07:47.484594: Pseudo dice [0.8906, 0.9075, 0.9711, 0.6327, 0.951] 
2023-10-27 10:07:47.484823: Epoch time: 3.43 s 
2023-10-27 10:07:48.521963:  
2023-10-27 10:07:48.522417: Epoch 416 
2023-10-27 10:07:48.522648: Current learning rate: 0.00616 
2023-10-27 10:07:51.929654: train_loss -0.8668 
2023-10-27 10:07:51.930054: val_loss -0.8953 
2023-10-27 10:07:51.930310: Pseudo dice [0.8895, 0.9122, 0.9688, 0.8859, 0.9493] 
2023-10-27 10:07:51.930550: Epoch time: 3.41 s 
2023-10-27 10:07:52.968502:  
2023-10-27 10:07:52.968816: Epoch 417 
2023-10-27 10:07:52.969062: Current learning rate: 0.00615 
2023-10-27 10:07:56.314149: train_loss -0.8657 
2023-10-27 10:07:56.314508: val_loss -0.8871 
2023-10-27 10:07:56.314763: Pseudo dice [0.8808, 0.9121, 0.9672, 0.8734, 0.9424] 
2023-10-27 10:07:56.314981: Epoch time: 3.35 s 
2023-10-27 10:07:57.349976:  
2023-10-27 10:07:57.350329: Epoch 418 
2023-10-27 10:07:57.350576: Current learning rate: 0.00614 
2023-10-27 10:08:00.684604: train_loss -0.8605 
2023-10-27 10:08:00.684966: val_loss -0.8787 
2023-10-27 10:08:00.685210: Pseudo dice [0.8767, 0.9097, 0.9708, 0.7843, 0.9242] 
2023-10-27 10:08:00.685427: Epoch time: 3.34 s 
2023-10-27 10:08:01.856042:  
2023-10-27 10:08:01.856414: Epoch 419 
2023-10-27 10:08:01.856633: Current learning rate: 0.00613 
2023-10-27 10:08:05.279042: train_loss -0.8586 
2023-10-27 10:08:05.279561: val_loss -0.8548 
2023-10-27 10:08:05.279926: Pseudo dice [0.8809, 0.9099, 0.9708, 0.4085, 0.9469] 
2023-10-27 10:08:05.280373: Epoch time: 3.42 s 
2023-10-27 10:08:06.306002:  
2023-10-27 10:08:06.306344: Epoch 420 
2023-10-27 10:08:06.306593: Current learning rate: 0.00612 
2023-10-27 10:08:09.704096: train_loss -0.8645 
2023-10-27 10:08:09.704639: val_loss -0.8867 
2023-10-27 10:08:09.705008: Pseudo dice [0.8817, 0.9115, 0.9687, 0.873, 0.9403] 
2023-10-27 10:08:09.705319: Epoch time: 3.4 s 
2023-10-27 10:08:10.734523:  
2023-10-27 10:08:10.734849: Epoch 421 
2023-10-27 10:08:10.735093: Current learning rate: 0.00612 
2023-10-27 10:08:14.108670: train_loss -0.872 
2023-10-27 10:08:14.109067: val_loss -0.8958 
2023-10-27 10:08:14.109326: Pseudo dice [0.8881, 0.9123, 0.9718, 0.8791, 0.9438] 
2023-10-27 10:08:14.109565: Epoch time: 3.37 s 
2023-10-27 10:08:15.140818:  
2023-10-27 10:08:15.141113: Epoch 422 
2023-10-27 10:08:15.141350: Current learning rate: 0.00611 
2023-10-27 10:08:18.490492: train_loss -0.8745 
2023-10-27 10:08:18.490841: val_loss -0.8714 
2023-10-27 10:08:18.491078: Pseudo dice [0.8721, 0.908, 0.9698, 0.5723, 0.949] 
2023-10-27 10:08:18.491306: Epoch time: 3.35 s 
2023-10-27 10:08:19.523245:  
2023-10-27 10:08:19.523545: Epoch 423 
2023-10-27 10:08:19.523822: Current learning rate: 0.0061 
2023-10-27 10:08:22.981298: train_loss -0.8648 
2023-10-27 10:08:22.981889: val_loss -0.8933 
2023-10-27 10:08:22.982135: Pseudo dice [0.8917, 0.9107, 0.9707, 0.8471, 0.9462] 
2023-10-27 10:08:22.982360: Epoch time: 3.46 s 
2023-10-27 10:08:24.021468:  
2023-10-27 10:08:24.021840: Epoch 424 
2023-10-27 10:08:24.022076: Current learning rate: 0.00609 
2023-10-27 10:08:27.493250: train_loss -0.871 
2023-10-27 10:08:27.493696: val_loss -0.8938 
2023-10-27 10:08:27.493952: Pseudo dice [0.8895, 0.9099, 0.9703, 0.8465, 0.9462] 
2023-10-27 10:08:27.494174: Epoch time: 3.47 s 
2023-10-27 10:08:28.540985:  
2023-10-27 10:08:28.541383: Epoch 425 
2023-10-27 10:08:28.541634: Current learning rate: 0.00608 
2023-10-27 10:08:31.933568: train_loss -0.8712 
2023-10-27 10:08:31.934042: val_loss -0.8995 
2023-10-27 10:08:31.934390: Pseudo dice [0.892, 0.9128, 0.9717, 0.8826, 0.9495] 
2023-10-27 10:08:31.934633: Epoch time: 3.39 s 
2023-10-27 10:08:33.110245:  
2023-10-27 10:08:33.110666: Epoch 426 
2023-10-27 10:08:33.110909: Current learning rate: 0.00607 
2023-10-27 10:08:36.514176: train_loss -0.8742 
2023-10-27 10:08:36.514622: val_loss -0.8957 
2023-10-27 10:08:36.514872: Pseudo dice [0.8875, 0.9128, 0.9712, 0.8778, 0.9467] 
2023-10-27 10:08:36.515101: Epoch time: 3.4 s 
2023-10-27 10:08:37.562521:  
2023-10-27 10:08:37.562821: Epoch 427 
2023-10-27 10:08:37.563057: Current learning rate: 0.00606 
2023-10-27 10:08:40.921542: train_loss -0.8779 
2023-10-27 10:08:40.922002: val_loss -0.8946 
2023-10-27 10:08:40.922257: Pseudo dice [0.8917, 0.9104, 0.9709, 0.8704, 0.9453] 
2023-10-27 10:08:40.922498: Epoch time: 3.36 s 
2023-10-27 10:08:41.959366:  
2023-10-27 10:08:41.959724: Epoch 428 
2023-10-27 10:08:41.959968: Current learning rate: 0.00605 
2023-10-27 10:08:45.490460: train_loss -0.8734 
2023-10-27 10:08:45.490829: val_loss -0.8944 
2023-10-27 10:08:45.491070: Pseudo dice [0.8854, 0.9111, 0.9702, 0.8838, 0.9384] 
2023-10-27 10:08:45.491300: Epoch time: 3.53 s 
2023-10-27 10:08:46.535050:  
2023-10-27 10:08:46.535386: Epoch 429 
2023-10-27 10:08:46.535629: Current learning rate: 0.00604 
2023-10-27 10:08:49.927271: train_loss -0.8771 
2023-10-27 10:08:49.927641: val_loss -0.8963 
2023-10-27 10:08:49.927894: Pseudo dice [0.8908, 0.9097, 0.9721, 0.8642, 0.9472] 
2023-10-27 10:08:49.928136: Epoch time: 3.39 s 
2023-10-27 10:08:50.962409:  
2023-10-27 10:08:50.962712: Epoch 430 
2023-10-27 10:08:50.962942: Current learning rate: 0.00603 
2023-10-27 10:08:54.374190: train_loss -0.872 
2023-10-27 10:08:54.374736: val_loss -0.8895 
2023-10-27 10:08:54.374977: Pseudo dice [0.8919, 0.9016, 0.9712, 0.8793, 0.9443] 
2023-10-27 10:08:54.375205: Epoch time: 3.41 s 
2023-10-27 10:08:55.406487:  
2023-10-27 10:08:55.406778: Epoch 431 
2023-10-27 10:08:55.407020: Current learning rate: 0.00602 
2023-10-27 10:08:58.797323: train_loss -0.8765 
2023-10-27 10:08:58.797685: val_loss -0.8911 
2023-10-27 10:08:58.797940: Pseudo dice [0.8959, 0.9122, 0.9715, 0.8876, 0.9466] 
2023-10-27 10:08:58.798161: Epoch time: 3.39 s 
2023-10-27 10:08:59.965114:  
2023-10-27 10:08:59.965420: Epoch 432 
2023-10-27 10:08:59.965667: Current learning rate: 0.00601 
2023-10-27 10:09:03.395204: train_loss -0.8709 
2023-10-27 10:09:03.395627: val_loss -0.8929 
2023-10-27 10:09:03.395884: Pseudo dice [0.8803, 0.9075, 0.9684, 0.8818, 0.9467] 
2023-10-27 10:09:03.396117: Epoch time: 3.43 s 
2023-10-27 10:09:04.431546:  
2023-10-27 10:09:04.431903: Epoch 433 
2023-10-27 10:09:04.432148: Current learning rate: 0.006 
2023-10-27 10:09:08.018621: train_loss -0.8743 
2023-10-27 10:09:08.019075: val_loss -0.8944 
2023-10-27 10:09:08.019465: Pseudo dice [0.893, 0.9103, 0.971, 0.884, 0.9475] 
2023-10-27 10:09:08.019740: Epoch time: 3.59 s 
2023-10-27 10:09:09.053007:  
2023-10-27 10:09:09.053299: Epoch 434 
2023-10-27 10:09:09.053549: Current learning rate: 0.00599 
2023-10-27 10:09:12.417895: train_loss -0.8814 
2023-10-27 10:09:12.418282: val_loss -0.8966 
2023-10-27 10:09:12.418530: Pseudo dice [0.8882, 0.9147, 0.9694, 0.8468, 0.9475] 
2023-10-27 10:09:12.418753: Epoch time: 3.37 s 
2023-10-27 10:09:13.481858:  
2023-10-27 10:09:13.482149: Epoch 435 
2023-10-27 10:09:13.482387: Current learning rate: 0.00598 
2023-10-27 10:09:16.944439: train_loss -0.8692 
2023-10-27 10:09:16.944959: val_loss -0.8908 
2023-10-27 10:09:16.945222: Pseudo dice [0.8874, 0.9083, 0.9676, 0.8774, 0.9441] 
2023-10-27 10:09:16.945454: Epoch time: 3.46 s 
2023-10-27 10:09:17.981072:  
2023-10-27 10:09:17.981377: Epoch 436 
2023-10-27 10:09:17.981622: Current learning rate: 0.00597 
2023-10-27 10:09:21.309179: train_loss -0.8693 
2023-10-27 10:09:21.309538: val_loss -0.8892 
2023-10-27 10:09:21.309778: Pseudo dice [0.8878, 0.9088, 0.972, 0.845, 0.9446] 
2023-10-27 10:09:21.310001: Epoch time: 3.33 s 
2023-10-27 10:09:22.343673:  
2023-10-27 10:09:22.343960: Epoch 437 
2023-10-27 10:09:22.344190: Current learning rate: 0.00596 
2023-10-27 10:09:25.701270: train_loss -0.8693 
2023-10-27 10:09:25.701636: val_loss -0.8814 
2023-10-27 10:09:25.701869: Pseudo dice [0.874, 0.9067, 0.968, 0.6807, 0.9482] 
2023-10-27 10:09:25.702082: Epoch time: 3.36 s 
2023-10-27 10:09:26.872286:  
2023-10-27 10:09:26.872573: Epoch 438 
2023-10-27 10:09:26.872801: Current learning rate: 0.00595 
2023-10-27 10:09:30.391546: train_loss -0.8602 
2023-10-27 10:09:30.391947: val_loss -0.8932 
2023-10-27 10:09:30.392209: Pseudo dice [0.8927, 0.9204, 0.9713, 0.8493, 0.9499] 
2023-10-27 10:09:30.392462: Epoch time: 3.52 s 
2023-10-27 10:09:31.425762:  
2023-10-27 10:09:31.426168: Epoch 439 
2023-10-27 10:09:31.426410: Current learning rate: 0.00594 
2023-10-27 10:09:34.771303: train_loss -0.8688 
2023-10-27 10:09:34.771670: val_loss -0.8911 
2023-10-27 10:09:34.771914: Pseudo dice [0.881, 0.9098, 0.971, 0.8475, 0.9454] 
2023-10-27 10:09:34.772219: Epoch time: 3.35 s 
2023-10-27 10:09:35.807714:  
2023-10-27 10:09:35.808041: Epoch 440 
2023-10-27 10:09:35.808279: Current learning rate: 0.00593 
2023-10-27 10:09:39.173768: train_loss -0.8794 
2023-10-27 10:09:39.174175: val_loss -0.8906 
2023-10-27 10:09:39.174432: Pseudo dice [0.8914, 0.9118, 0.9709, 0.8513, 0.9489] 
2023-10-27 10:09:39.174687: Epoch time: 3.37 s 
2023-10-27 10:09:40.210795:  
2023-10-27 10:09:40.211118: Epoch 441 
2023-10-27 10:09:40.211352: Current learning rate: 0.00592 
2023-10-27 10:09:43.613413: train_loss -0.8741 
2023-10-27 10:09:43.613768: val_loss -0.893 
2023-10-27 10:09:43.614028: Pseudo dice [0.8897, 0.9146, 0.9716, 0.848, 0.9468] 
2023-10-27 10:09:43.614245: Epoch time: 3.4 s 
2023-10-27 10:09:44.648972:  
2023-10-27 10:09:44.649288: Epoch 442 
2023-10-27 10:09:44.649536: Current learning rate: 0.00592 
2023-10-27 10:09:48.009623: train_loss -0.8719 
2023-10-27 10:09:48.010013: val_loss -0.8838 
2023-10-27 10:09:48.010266: Pseudo dice [0.8831, 0.9095, 0.9689, 0.8933, 0.9245] 
2023-10-27 10:09:48.010514: Epoch time: 3.36 s 
2023-10-27 10:09:49.042438:  
2023-10-27 10:09:49.042737: Epoch 443 
2023-10-27 10:09:49.042984: Current learning rate: 0.00591 
2023-10-27 10:09:52.395015: train_loss -0.8631 
2023-10-27 10:09:52.395411: val_loss -0.8851 
2023-10-27 10:09:52.395655: Pseudo dice [0.8757, 0.9049, 0.9711, 0.8848, 0.9458] 
2023-10-27 10:09:52.395878: Epoch time: 3.35 s 
2023-10-27 10:09:53.421573:  
2023-10-27 10:09:53.421918: Epoch 444 
2023-10-27 10:09:53.422154: Current learning rate: 0.0059 
2023-10-27 10:09:56.758376: train_loss -0.8597 
2023-10-27 10:09:56.758753: val_loss -0.8921 
2023-10-27 10:09:56.758998: Pseudo dice [0.8883, 0.9158, 0.9707, 0.8944, 0.9503] 
2023-10-27 10:09:56.759215: Epoch time: 3.34 s 
2023-10-27 10:09:57.923505:  
2023-10-27 10:09:57.923831: Epoch 445 
2023-10-27 10:09:57.924073: Current learning rate: 0.00589 
2023-10-27 10:10:01.292714: train_loss -0.8675 
2023-10-27 10:10:01.293219: val_loss -0.8928 
2023-10-27 10:10:01.293480: Pseudo dice [0.8854, 0.9136, 0.9689, 0.8816, 0.9464] 
2023-10-27 10:10:01.293699: Epoch time: 3.37 s 
2023-10-27 10:10:02.314347:  
2023-10-27 10:10:02.314636: Epoch 446 
2023-10-27 10:10:02.314878: Current learning rate: 0.00588 
2023-10-27 10:10:05.705222: train_loss -0.8739 
2023-10-27 10:10:05.705591: val_loss -0.8907 
2023-10-27 10:10:05.705842: Pseudo dice [0.8895, 0.918, 0.9719, 0.8846, 0.9475] 
2023-10-27 10:10:05.706061: Epoch time: 3.39 s 
2023-10-27 10:10:06.729443:  
2023-10-27 10:10:06.729763: Epoch 447 
2023-10-27 10:10:06.729997: Current learning rate: 0.00587 
2023-10-27 10:10:10.094684: train_loss -0.8707 
2023-10-27 10:10:10.095135: val_loss -0.8838 
2023-10-27 10:10:10.095401: Pseudo dice [0.8817, 0.9119, 0.967, 0.7398, 0.9462] 
2023-10-27 10:10:10.095636: Epoch time: 3.37 s 
2023-10-27 10:10:11.123967:  
2023-10-27 10:10:11.124262: Epoch 448 
2023-10-27 10:10:11.124521: Current learning rate: 0.00586 
2023-10-27 10:10:14.545503: train_loss -0.8655 
2023-10-27 10:10:14.545888: val_loss -0.8826 
2023-10-27 10:10:14.546145: Pseudo dice [0.8725, 0.9043, 0.9681, 0.8938, 0.935] 
2023-10-27 10:10:14.546369: Epoch time: 3.42 s 
2023-10-27 10:10:15.570768:  
2023-10-27 10:10:15.571109: Epoch 449 
2023-10-27 10:10:15.571348: Current learning rate: 0.00585 
2023-10-27 10:10:18.872247: train_loss -0.8648 
2023-10-27 10:10:18.872663: val_loss -0.8955 
2023-10-27 10:10:18.872923: Pseudo dice [0.8882, 0.9115, 0.9713, 0.8633, 0.9469] 
2023-10-27 10:10:18.873155: Epoch time: 3.3 s 
2023-10-27 10:10:19.977970:  
2023-10-27 10:10:19.978335: Epoch 450 
2023-10-27 10:10:19.978585: Current learning rate: 0.00584 
2023-10-27 10:10:23.306019: train_loss -0.8685 
2023-10-27 10:10:23.306386: val_loss -0.8936 
2023-10-27 10:10:23.306654: Pseudo dice [0.8855, 0.9061, 0.971, 0.8934, 0.9467] 
2023-10-27 10:10:23.306881: Epoch time: 3.33 s 
2023-10-27 10:10:24.462733:  
2023-10-27 10:10:24.463015: Epoch 451 
2023-10-27 10:10:24.463241: Current learning rate: 0.00583 
2023-10-27 10:10:27.801370: train_loss -0.8719 
2023-10-27 10:10:27.801753: val_loss -0.8966 
2023-10-27 10:10:27.801999: Pseudo dice [0.8891, 0.9103, 0.9708, 0.8395, 0.9473] 
2023-10-27 10:10:27.802212: Epoch time: 3.34 s 
2023-10-27 10:10:28.820725:  
2023-10-27 10:10:28.821112: Epoch 452 
2023-10-27 10:10:28.821345: Current learning rate: 0.00582 
2023-10-27 10:10:32.159523: train_loss -0.8646 
2023-10-27 10:10:32.159940: val_loss -0.892 
2023-10-27 10:10:32.160190: Pseudo dice [0.894, 0.911, 0.9701, 0.8634, 0.9453] 
2023-10-27 10:10:32.160429: Epoch time: 3.34 s 
2023-10-27 10:10:33.191101:  
2023-10-27 10:10:33.191434: Epoch 453 
2023-10-27 10:10:33.191688: Current learning rate: 0.00581 
2023-10-27 10:10:36.540350: train_loss -0.8697 
2023-10-27 10:10:36.540717: val_loss -0.8873 
2023-10-27 10:10:36.540976: Pseudo dice [0.8893, 0.9063, 0.9698, 0.7872, 0.944] 
2023-10-27 10:10:36.541203: Epoch time: 3.35 s 
2023-10-27 10:10:37.567641:  
2023-10-27 10:10:37.567941: Epoch 454 
2023-10-27 10:10:37.568184: Current learning rate: 0.0058 
2023-10-27 10:10:40.901747: train_loss -0.871 
2023-10-27 10:10:40.902122: val_loss -0.8966 
2023-10-27 10:10:40.902384: Pseudo dice [0.8926, 0.9137, 0.9705, 0.8739, 0.9497] 
2023-10-27 10:10:40.902609: Epoch time: 3.33 s 
2023-10-27 10:10:41.943748:  
2023-10-27 10:10:41.944068: Epoch 455 
2023-10-27 10:10:41.944302: Current learning rate: 0.00579 
2023-10-27 10:10:45.330316: train_loss -0.8737 
2023-10-27 10:10:45.330673: val_loss -0.8886 
2023-10-27 10:10:45.330920: Pseudo dice [0.8896, 0.9148, 0.9712, 0.8292, 0.9466] 
2023-10-27 10:10:45.331163: Epoch time: 3.39 s 
2023-10-27 10:10:46.359650:  
2023-10-27 10:10:46.359976: Epoch 456 
2023-10-27 10:10:46.360215: Current learning rate: 0.00578 
2023-10-27 10:10:49.716664: train_loss -0.8697 
2023-10-27 10:10:49.717069: val_loss -0.89 
2023-10-27 10:10:49.717317: Pseudo dice [0.8841, 0.9063, 0.9703, 0.8498, 0.9436] 
2023-10-27 10:10:49.717659: Epoch time: 3.36 s 
2023-10-27 10:10:50.742965:  
2023-10-27 10:10:50.743318: Epoch 457 
2023-10-27 10:10:50.743567: Current learning rate: 0.00577 
2023-10-27 10:10:54.080195: train_loss -0.867 
2023-10-27 10:10:54.080584: val_loss -0.8901 
2023-10-27 10:10:54.080836: Pseudo dice [0.8796, 0.9134, 0.9682, 0.8635, 0.9457] 
2023-10-27 10:10:54.081047: Epoch time: 3.34 s 
2023-10-27 10:10:55.243406:  
2023-10-27 10:10:55.243737: Epoch 458 
2023-10-27 10:10:55.243973: Current learning rate: 0.00576 
2023-10-27 10:10:58.585867: train_loss -0.875 
2023-10-27 10:10:58.586268: val_loss -0.8904 
2023-10-27 10:10:58.586514: Pseudo dice [0.8896, 0.9027, 0.9699, 0.9151, 0.943] 
2023-10-27 10:10:58.586756: Epoch time: 3.34 s 
2023-10-27 10:10:59.610236:  
2023-10-27 10:10:59.610569: Epoch 459 
2023-10-27 10:10:59.610808: Current learning rate: 0.00575 
2023-10-27 10:11:02.964052: train_loss -0.8669 
2023-10-27 10:11:02.964409: val_loss -0.8934 
2023-10-27 10:11:02.964670: Pseudo dice [0.8882, 0.9172, 0.9707, 0.874, 0.9428] 
2023-10-27 10:11:02.964894: Epoch time: 3.35 s 
2023-10-27 10:11:03.991669:  
2023-10-27 10:11:03.992036: Epoch 460 
2023-10-27 10:11:03.992289: Current learning rate: 0.00574 
2023-10-27 10:11:07.361977: train_loss -0.8641 
2023-10-27 10:11:07.362329: val_loss -0.8969 
2023-10-27 10:11:07.362585: Pseudo dice [0.8896, 0.9185, 0.9713, 0.8695, 0.9479] 
2023-10-27 10:11:07.362802: Epoch time: 3.37 s 
2023-10-27 10:11:08.384158:  
2023-10-27 10:11:08.384490: Epoch 461 
2023-10-27 10:11:08.384732: Current learning rate: 0.00573 
2023-10-27 10:11:11.752668: train_loss -0.8661 
2023-10-27 10:11:11.753022: val_loss -0.8726 
2023-10-27 10:11:11.753268: Pseudo dice [0.8771, 0.9, 0.9706, 0.792, 0.9357] 
2023-10-27 10:11:11.753497: Epoch time: 3.37 s 
2023-10-27 10:11:12.779123:  
2023-10-27 10:11:12.779446: Epoch 462 
2023-10-27 10:11:12.779690: Current learning rate: 0.00572 
2023-10-27 10:11:16.141268: train_loss -0.8619 
2023-10-27 10:11:16.141634: val_loss -0.8936 
2023-10-27 10:11:16.141887: Pseudo dice [0.8873, 0.9186, 0.9692, 0.8491, 0.9448] 
2023-10-27 10:11:16.142431: Epoch time: 3.36 s 
2023-10-27 10:11:17.165784:  
2023-10-27 10:11:17.166062: Epoch 463 
2023-10-27 10:11:17.166296: Current learning rate: 0.00571 
2023-10-27 10:11:20.541011: train_loss -0.8777 
2023-10-27 10:11:20.541424: val_loss -0.9012 
2023-10-27 10:11:20.541698: Pseudo dice [0.8939, 0.9176, 0.9727, 0.9015, 0.9475] 
2023-10-27 10:11:20.541939: Epoch time: 3.38 s 
2023-10-27 10:11:21.699976:  
2023-10-27 10:11:21.700356: Epoch 464 
2023-10-27 10:11:21.700588: Current learning rate: 0.0057 
2023-10-27 10:11:25.136818: train_loss -0.8815 
2023-10-27 10:11:25.137161: val_loss -0.8953 
2023-10-27 10:11:25.137416: Pseudo dice [0.8911, 0.9103, 0.971, 0.8791, 0.9486] 
2023-10-27 10:11:25.137631: Epoch time: 3.44 s 
2023-10-27 10:11:26.157936:  
2023-10-27 10:11:26.158280: Epoch 465 
2023-10-27 10:11:26.158527: Current learning rate: 0.0057 
2023-10-27 10:11:29.559203: train_loss -0.8766 
2023-10-27 10:11:29.559597: val_loss -0.8898 
2023-10-27 10:11:29.559856: Pseudo dice [0.883, 0.9064, 0.9697, 0.8798, 0.947] 
2023-10-27 10:11:29.560085: Epoch time: 3.4 s 
2023-10-27 10:11:30.584070:  
2023-10-27 10:11:30.584461: Epoch 466 
2023-10-27 10:11:30.584696: Current learning rate: 0.00569 
2023-10-27 10:11:33.990205: train_loss -0.8645 
2023-10-27 10:11:33.990588: val_loss -0.8593 
2023-10-27 10:11:33.990846: Pseudo dice [0.8792, 0.908, 0.97, 0.1359, 0.9479] 
2023-10-27 10:11:33.991079: Epoch time: 3.41 s 
2023-10-27 10:11:35.015180:  
2023-10-27 10:11:35.015553: Epoch 467 
2023-10-27 10:11:35.015798: Current learning rate: 0.00568 
2023-10-27 10:11:38.396658: train_loss -0.8574 
2023-10-27 10:11:38.397106: val_loss -0.8884 
2023-10-27 10:11:38.397489: Pseudo dice [0.8914, 0.9059, 0.9697, 0.8649, 0.9469] 
2023-10-27 10:11:38.397803: Epoch time: 3.38 s 
2023-10-27 10:11:39.425421:  
2023-10-27 10:11:39.429336: Epoch 468 
2023-10-27 10:11:39.429580: Current learning rate: 0.00567 
2023-10-27 10:11:42.829535: train_loss -0.8427 
2023-10-27 10:11:42.829915: val_loss -0.8877 
2023-10-27 10:11:42.830164: Pseudo dice [0.866, 0.9051, 0.9706, 0.8897, 0.9376] 
2023-10-27 10:11:42.830395: Epoch time: 3.4 s 
2023-10-27 10:11:43.851413:  
2023-10-27 10:11:43.851781: Epoch 469 
2023-10-27 10:11:43.852027: Current learning rate: 0.00566 
2023-10-27 10:11:47.235779: train_loss -0.8519 
2023-10-27 10:11:47.236148: val_loss -0.8777 
2023-10-27 10:11:47.236409: Pseudo dice [0.8815, 0.911, 0.9701, 0.7672, 0.9458] 
2023-10-27 10:11:47.236624: Epoch time: 3.38 s 
2023-10-27 10:11:48.262735:  
2023-10-27 10:11:48.263014: Epoch 470 
2023-10-27 10:11:48.263260: Current learning rate: 0.00565 
2023-10-27 10:11:51.580871: train_loss -0.8593 
2023-10-27 10:11:51.581255: val_loss -0.8935 
2023-10-27 10:11:51.581531: Pseudo dice [0.8927, 0.9103, 0.9711, 0.8885, 0.9495] 
2023-10-27 10:11:51.581748: Epoch time: 3.32 s 
2023-10-27 10:11:52.745347:  
2023-10-27 10:11:52.745697: Epoch 471 
2023-10-27 10:11:52.745936: Current learning rate: 0.00564 
2023-10-27 10:11:56.199738: train_loss -0.8695 
2023-10-27 10:11:56.200194: val_loss -0.8946 
2023-10-27 10:11:56.200557: Pseudo dice [0.8898, 0.9125, 0.9681, 0.8698, 0.9478] 
2023-10-27 10:11:56.200904: Epoch time: 3.45 s 
2023-10-27 10:11:57.229671:  
2023-10-27 10:11:57.229953: Epoch 472 
2023-10-27 10:11:57.230226: Current learning rate: 0.00563 
2023-10-27 10:12:00.702237: train_loss -0.866 
2023-10-27 10:12:00.702617: val_loss -0.9007 
2023-10-27 10:12:00.702872: Pseudo dice [0.8911, 0.9142, 0.9716, 0.8775, 0.9494] 
2023-10-27 10:12:00.703087: Epoch time: 3.47 s 
2023-10-27 10:12:01.734618:  
2023-10-27 10:12:01.734897: Epoch 473 
2023-10-27 10:12:01.735136: Current learning rate: 0.00562 
2023-10-27 10:12:05.137750: train_loss -0.8694 
2023-10-27 10:12:05.138142: val_loss -0.8961 
2023-10-27 10:12:05.138388: Pseudo dice [0.8909, 0.9148, 0.9725, 0.8769, 0.9473] 
2023-10-27 10:12:05.138623: Epoch time: 3.4 s 
2023-10-27 10:12:06.163894:  
2023-10-27 10:12:06.164183: Epoch 474 
2023-10-27 10:12:06.164428: Current learning rate: 0.00561 
2023-10-27 10:12:09.504696: train_loss -0.8726 
2023-10-27 10:12:09.505052: val_loss -0.8941 
2023-10-27 10:12:09.505298: Pseudo dice [0.8836, 0.9082, 0.9707, 0.8874, 0.9486] 
2023-10-27 10:12:09.505525: Epoch time: 3.34 s 
2023-10-27 10:12:10.527428:  
2023-10-27 10:12:10.527791: Epoch 475 
2023-10-27 10:12:10.528021: Current learning rate: 0.0056 
2023-10-27 10:12:13.905522: train_loss -0.8694 
2023-10-27 10:12:13.905939: val_loss -0.8938 
2023-10-27 10:12:13.906183: Pseudo dice [0.8869, 0.9123, 0.9721, 0.8615, 0.9473] 
2023-10-27 10:12:13.906417: Epoch time: 3.38 s 
2023-10-27 10:12:14.933253:  
2023-10-27 10:12:14.933564: Epoch 476 
2023-10-27 10:12:14.933814: Current learning rate: 0.00559 
2023-10-27 10:12:18.308701: train_loss -0.8734 
2023-10-27 10:12:18.309117: val_loss -0.8972 
2023-10-27 10:12:18.309359: Pseudo dice [0.8911, 0.9201, 0.9708, 0.8566, 0.9469] 
2023-10-27 10:12:18.309594: Epoch time: 3.38 s 
2023-10-27 10:12:19.468469:  
2023-10-27 10:12:19.468836: Epoch 477 
2023-10-27 10:12:19.469110: Current learning rate: 0.00558 
2023-10-27 10:12:22.909953: train_loss -0.8677 
2023-10-27 10:12:22.910483: val_loss -0.8871 
2023-10-27 10:12:22.910736: Pseudo dice [0.8837, 0.905, 0.9689, 0.8497, 0.9451] 
2023-10-27 10:12:22.910954: Epoch time: 3.44 s 
2023-10-27 10:12:23.945629:  
2023-10-27 10:12:23.945916: Epoch 478 
2023-10-27 10:12:23.946160: Current learning rate: 0.00557 
2023-10-27 10:12:27.337190: train_loss -0.8566 
2023-10-27 10:12:27.337792: val_loss -0.8939 
2023-10-27 10:12:27.338052: Pseudo dice [0.8846, 0.91, 0.9726, 0.8969, 0.9473] 
2023-10-27 10:12:27.338273: Epoch time: 3.39 s 
2023-10-27 10:12:28.377522:  
2023-10-27 10:12:28.377808: Epoch 479 
2023-10-27 10:12:28.378042: Current learning rate: 0.00556 
2023-10-27 10:12:31.728886: train_loss -0.8724 
2023-10-27 10:12:31.729306: val_loss -0.8926 
2023-10-27 10:12:31.729574: Pseudo dice [0.8918, 0.9092, 0.9702, 0.8589, 0.9466] 
2023-10-27 10:12:31.729800: Epoch time: 3.35 s 
2023-10-27 10:12:32.770409:  
2023-10-27 10:12:32.770808: Epoch 480 
2023-10-27 10:12:32.771063: Current learning rate: 0.00555 
2023-10-27 10:12:36.140273: train_loss -0.8774 
2023-10-27 10:12:36.140668: val_loss -0.8943 
2023-10-27 10:12:36.140928: Pseudo dice [0.8918, 0.9192, 0.9723, 0.8344, 0.946] 
2023-10-27 10:12:36.141157: Epoch time: 3.37 s 
2023-10-27 10:12:37.180649:  
2023-10-27 10:12:37.181016: Epoch 481 
2023-10-27 10:12:37.181264: Current learning rate: 0.00554 
2023-10-27 10:12:40.553810: train_loss -0.8801 
2023-10-27 10:12:40.554214: val_loss -0.8978 
2023-10-27 10:12:40.554469: Pseudo dice [0.8922, 0.913, 0.9721, 0.8758, 0.9468] 
2023-10-27 10:12:40.554685: Epoch time: 3.37 s 
2023-10-27 10:12:41.592044:  
2023-10-27 10:12:41.592370: Epoch 482 
2023-10-27 10:12:41.592611: Current learning rate: 0.00553 
2023-10-27 10:12:44.965742: train_loss -0.8825 
2023-10-27 10:12:44.966139: val_loss -0.8988 
2023-10-27 10:12:44.966413: Pseudo dice [0.894, 0.9151, 0.9715, 0.8436, 0.9486] 
2023-10-27 10:12:44.966638: Epoch time: 3.37 s 
2023-10-27 10:12:46.003289:  
2023-10-27 10:12:46.003606: Epoch 483 
2023-10-27 10:12:46.003843: Current learning rate: 0.00552 
2023-10-27 10:12:49.309895: train_loss -0.8782 
2023-10-27 10:12:49.310238: val_loss -0.8442 
2023-10-27 10:12:49.310488: Pseudo dice [0.8896, 0.9136, 0.9698, 0.1753, 0.9439] 
2023-10-27 10:12:49.310709: Epoch time: 3.31 s 
2023-10-27 10:12:50.484476:  
2023-10-27 10:12:50.484768: Epoch 484 
2023-10-27 10:12:50.484997: Current learning rate: 0.00551 
2023-10-27 10:12:53.949619: train_loss -0.8771 
2023-10-27 10:12:53.949995: val_loss -0.887 
2023-10-27 10:12:53.950276: Pseudo dice [0.8912, 0.9143, 0.9703, 0.7746, 0.9491] 
2023-10-27 10:12:53.950552: Epoch time: 3.47 s 
2023-10-27 10:12:54.982874:  
2023-10-27 10:12:54.983182: Epoch 485 
2023-10-27 10:12:54.983426: Current learning rate: 0.0055 
2023-10-27 10:12:58.427677: train_loss -0.8797 
2023-10-27 10:12:58.428068: val_loss -0.8913 
2023-10-27 10:12:58.428312: Pseudo dice [0.8903, 0.9123, 0.9713, 0.8181, 0.9476] 
2023-10-27 10:12:58.428556: Epoch time: 3.45 s 
2023-10-27 10:12:59.466270:  
2023-10-27 10:12:59.466566: Epoch 486 
2023-10-27 10:12:59.466818: Current learning rate: 0.00549 
2023-10-27 10:13:02.818596: train_loss -0.8798 
2023-10-27 10:13:02.819166: val_loss -0.8974 
2023-10-27 10:13:02.819442: Pseudo dice [0.8925, 0.912, 0.9729, 0.8514, 0.9479] 
2023-10-27 10:13:02.819655: Epoch time: 3.35 s 
2023-10-27 10:13:03.857470:  
2023-10-27 10:13:03.857800: Epoch 487 
2023-10-27 10:13:03.858034: Current learning rate: 0.00548 
2023-10-27 10:13:07.205981: train_loss -0.8777 
2023-10-27 10:13:07.206352: val_loss -0.8917 
2023-10-27 10:13:07.206619: Pseudo dice [0.8854, 0.9132, 0.9708, 0.8294, 0.9417] 
2023-10-27 10:13:07.206852: Epoch time: 3.35 s 
2023-10-27 10:13:08.244679:  
2023-10-27 10:13:08.245064: Epoch 488 
2023-10-27 10:13:08.245308: Current learning rate: 0.00547 
2023-10-27 10:13:11.794944: train_loss -0.871 
2023-10-27 10:13:11.795307: val_loss -0.8917 
2023-10-27 10:13:11.795566: Pseudo dice [0.8919, 0.9109, 0.9725, 0.8326, 0.9457] 
2023-10-27 10:13:11.795792: Epoch time: 3.55 s 
2023-10-27 10:13:12.835379:  
2023-10-27 10:13:12.835766: Epoch 489 
2023-10-27 10:13:12.836002: Current learning rate: 0.00546 
2023-10-27 10:13:16.194806: train_loss -0.8707 
2023-10-27 10:13:16.195290: val_loss -0.8864 
2023-10-27 10:13:16.195585: Pseudo dice [0.883, 0.9042, 0.9678, 0.8775, 0.9417] 
2023-10-27 10:13:16.195832: Epoch time: 3.36 s 
2023-10-27 10:13:17.376840:  
2023-10-27 10:13:17.377240: Epoch 490 
2023-10-27 10:13:17.377477: Current learning rate: 0.00546 
2023-10-27 10:13:20.765326: train_loss -0.8746 
2023-10-27 10:13:20.765679: val_loss -0.8942 
2023-10-27 10:13:20.765927: Pseudo dice [0.8932, 0.9163, 0.9678, 0.8589, 0.946] 
2023-10-27 10:13:20.766148: Epoch time: 3.39 s 
2023-10-27 10:13:21.799417:  
2023-10-27 10:13:21.799739: Epoch 491 
2023-10-27 10:13:21.799983: Current learning rate: 0.00545 
2023-10-27 10:13:25.207371: train_loss -0.8766 
2023-10-27 10:13:25.207834: val_loss -0.8699 
2023-10-27 10:13:25.208236: Pseudo dice [0.8886, 0.9101, 0.9703, 0.498, 0.9458] 
2023-10-27 10:13:25.208577: Epoch time: 3.41 s 
2023-10-27 10:13:26.252085:  
2023-10-27 10:13:26.252464: Epoch 492 
2023-10-27 10:13:26.252719: Current learning rate: 0.00544 
2023-10-27 10:13:29.628213: train_loss -0.8756 
2023-10-27 10:13:29.628757: val_loss -0.893 
2023-10-27 10:13:29.629004: Pseudo dice [0.8931, 0.9165, 0.9707, 0.8713, 0.9454] 
2023-10-27 10:13:29.629231: Epoch time: 3.38 s 
2023-10-27 10:13:30.673273:  
2023-10-27 10:13:30.673638: Epoch 493 
2023-10-27 10:13:30.673947: Current learning rate: 0.00543 
2023-10-27 10:13:34.079887: train_loss -0.868 
2023-10-27 10:13:34.080260: val_loss -0.8932 
2023-10-27 10:13:34.080530: Pseudo dice [0.8799, 0.9057, 0.9691, 0.8877, 0.9448] 
2023-10-27 10:13:34.080757: Epoch time: 3.41 s 
2023-10-27 10:13:35.122700:  
2023-10-27 10:13:35.123039: Epoch 494 
2023-10-27 10:13:35.123270: Current learning rate: 0.00542 
2023-10-27 10:13:38.543649: train_loss -0.8821 
2023-10-27 10:13:38.544040: val_loss -0.8801 
2023-10-27 10:13:38.544302: Pseudo dice [0.8823, 0.9055, 0.9703, 0.895, 0.94] 
2023-10-27 10:13:38.544548: Epoch time: 3.42 s 
2023-10-27 10:13:39.595640:  
2023-10-27 10:13:39.595929: Epoch 495 
2023-10-27 10:13:39.596161: Current learning rate: 0.00541 
2023-10-27 10:13:43.036259: train_loss -0.8726 
2023-10-27 10:13:43.036850: val_loss -0.8943 
2023-10-27 10:13:43.037102: Pseudo dice [0.8876, 0.9083, 0.9709, 0.8754, 0.9486] 
2023-10-27 10:13:43.037320: Epoch time: 3.44 s 
2023-10-27 10:13:44.073472:  
2023-10-27 10:13:44.073941: Epoch 496 
2023-10-27 10:13:44.074184: Current learning rate: 0.0054 
2023-10-27 10:13:47.460797: train_loss -0.8758 
2023-10-27 10:13:47.461170: val_loss -0.8926 
2023-10-27 10:13:47.461431: Pseudo dice [0.8849, 0.9076, 0.9693, 0.8807, 0.9466] 
2023-10-27 10:13:47.461661: Epoch time: 3.39 s 
2023-10-27 10:13:48.636234:  
2023-10-27 10:13:48.636530: Epoch 497 
2023-10-27 10:13:48.636761: Current learning rate: 0.00539 
2023-10-27 10:13:52.080125: train_loss -0.8682 
2023-10-27 10:13:52.080477: val_loss -0.8709 
2023-10-27 10:13:52.080728: Pseudo dice [0.8824, 0.9064, 0.9714, 0.6697, 0.9411] 
2023-10-27 10:13:52.080944: Epoch time: 3.44 s 
2023-10-27 10:13:53.114441:  
2023-10-27 10:13:53.114783: Epoch 498 
2023-10-27 10:13:53.115022: Current learning rate: 0.00538 
2023-10-27 10:13:56.462392: train_loss -0.8586 
2023-10-27 10:13:56.462778: val_loss -0.8855 
2023-10-27 10:13:56.463024: Pseudo dice [0.8826, 0.9107, 0.9694, 0.8254, 0.943] 
2023-10-27 10:13:56.463241: Epoch time: 3.35 s 
2023-10-27 10:13:57.494118:  
2023-10-27 10:13:57.494433: Epoch 499 
2023-10-27 10:13:57.494704: Current learning rate: 0.00537 
2023-10-27 10:14:00.855875: train_loss -0.8613 
2023-10-27 10:14:00.856344: val_loss -0.8852 
2023-10-27 10:14:00.856591: Pseudo dice [0.89, 0.9103, 0.9687, 0.8758, 0.9464] 
2023-10-27 10:14:00.856807: Epoch time: 3.36 s 
2023-10-27 10:14:01.988581:  
2023-10-27 10:14:01.988935: Epoch 500 
2023-10-27 10:14:01.989172: Current learning rate: 0.00536 
2023-10-27 10:14:05.361334: train_loss -0.8589 
2023-10-27 10:14:05.361702: val_loss -0.8912 
2023-10-27 10:14:05.361945: Pseudo dice [0.8808, 0.9138, 0.971, 0.8419, 0.9478] 
2023-10-27 10:14:05.362165: Epoch time: 3.37 s 
2023-10-27 10:14:06.401145:  
2023-10-27 10:14:06.401455: Epoch 501 
2023-10-27 10:14:06.401699: Current learning rate: 0.00535 
2023-10-27 10:14:09.833996: train_loss -0.8609 
2023-10-27 10:14:09.834408: val_loss -0.8912 
2023-10-27 10:14:09.834671: Pseudo dice [0.8863, 0.9098, 0.9684, 0.8445, 0.9475] 
2023-10-27 10:14:09.834897: Epoch time: 3.43 s 
2023-10-27 10:14:11.013518:  
2023-10-27 10:14:11.013890: Epoch 502 
2023-10-27 10:14:11.014119: Current learning rate: 0.00534 
2023-10-27 10:14:14.416263: train_loss -0.8653 
2023-10-27 10:14:14.416695: val_loss -0.8923 
2023-10-27 10:14:14.416956: Pseudo dice [0.8741, 0.9076, 0.9711, 0.8763, 0.9453] 
2023-10-27 10:14:14.417212: Epoch time: 3.4 s 
2023-10-27 10:14:15.443164:  
2023-10-27 10:14:15.443452: Epoch 503 
2023-10-27 10:14:15.443699: Current learning rate: 0.00533 
2023-10-27 10:14:18.847255: train_loss -0.8703 
2023-10-27 10:14:18.847613: val_loss -0.874 
2023-10-27 10:14:18.847851: Pseudo dice [0.8883, 0.9114, 0.9698, 0.6741, 0.9422] 
2023-10-27 10:14:18.848071: Epoch time: 3.4 s 
2023-10-27 10:14:19.882318:  
2023-10-27 10:14:19.882636: Epoch 504 
2023-10-27 10:14:19.882874: Current learning rate: 0.00532 
2023-10-27 10:14:23.369992: train_loss -0.8725 
2023-10-27 10:14:23.370347: val_loss -0.8912 
2023-10-27 10:14:23.370589: Pseudo dice [0.8903, 0.9152, 0.9686, 0.8315, 0.9477] 
2023-10-27 10:14:23.370803: Epoch time: 3.49 s 
2023-10-27 10:14:24.402681:  
2023-10-27 10:14:24.402970: Epoch 505 
2023-10-27 10:14:24.403210: Current learning rate: 0.00531 
2023-10-27 10:14:27.731390: train_loss -0.868 
2023-10-27 10:14:27.731746: val_loss -0.8908 
2023-10-27 10:14:27.731995: Pseudo dice [0.8839, 0.9113, 0.9687, 0.8391, 0.9466] 
2023-10-27 10:14:27.732221: Epoch time: 3.33 s 
2023-10-27 10:14:28.770372:  
2023-10-27 10:14:28.770700: Epoch 506 
2023-10-27 10:14:28.770927: Current learning rate: 0.0053 
2023-10-27 10:14:32.147027: train_loss -0.8762 
2023-10-27 10:14:32.147449: val_loss -0.8926 
2023-10-27 10:14:32.147725: Pseudo dice [0.8853, 0.9056, 0.9712, 0.8344, 0.948] 
2023-10-27 10:14:32.147963: Epoch time: 3.38 s 
2023-10-27 10:14:33.192384:  
2023-10-27 10:14:33.192917: Epoch 507 
2023-10-27 10:14:33.193153: Current learning rate: 0.00529 
2023-10-27 10:14:36.757383: train_loss -0.8723 
2023-10-27 10:14:36.757963: val_loss -0.8979 
2023-10-27 10:14:36.758243: Pseudo dice [0.8926, 0.9112, 0.9701, 0.8781, 0.9498] 
2023-10-27 10:14:36.758459: Epoch time: 3.57 s 
2023-10-27 10:14:37.799032:  
2023-10-27 10:14:37.799314: Epoch 508 
2023-10-27 10:14:37.799564: Current learning rate: 0.00528 
2023-10-27 10:14:41.197450: train_loss -0.8749 
2023-10-27 10:14:41.197832: val_loss -0.8919 
2023-10-27 10:14:41.198090: Pseudo dice [0.8844, 0.9066, 0.9694, 0.8483, 0.9459] 
2023-10-27 10:14:41.198330: Epoch time: 3.4 s 
2023-10-27 10:14:42.374331:  
2023-10-27 10:14:42.374665: Epoch 509 
2023-10-27 10:14:42.374933: Current learning rate: 0.00527 
2023-10-27 10:14:45.767158: train_loss -0.8733 
2023-10-27 10:14:45.767516: val_loss -0.893 
2023-10-27 10:14:45.767756: Pseudo dice [0.8841, 0.9113, 0.9693, 0.8554, 0.9482] 
2023-10-27 10:14:45.767965: Epoch time: 3.39 s 
2023-10-27 10:14:46.804803:  
2023-10-27 10:14:46.805165: Epoch 510 
2023-10-27 10:14:46.805408: Current learning rate: 0.00526 
2023-10-27 10:14:50.161803: train_loss -0.8662 
2023-10-27 10:14:50.162255: val_loss -0.8819 
2023-10-27 10:14:50.162509: Pseudo dice [0.882, 0.9071, 0.969, 0.7852, 0.9445] 
2023-10-27 10:14:50.162737: Epoch time: 3.36 s 
2023-10-27 10:14:51.201184:  
2023-10-27 10:14:51.201481: Epoch 511 
2023-10-27 10:14:51.201723: Current learning rate: 0.00525 
2023-10-27 10:14:54.572947: train_loss -0.8628 
2023-10-27 10:14:54.573303: val_loss -0.878 
2023-10-27 10:14:54.573558: Pseudo dice [0.887, 0.9115, 0.9714, 0.8214, 0.9468] 
2023-10-27 10:14:54.573773: Epoch time: 3.37 s 
2023-10-27 10:14:55.611844:  
2023-10-27 10:14:55.612138: Epoch 512 
2023-10-27 10:14:55.612371: Current learning rate: 0.00524 
2023-10-27 10:14:58.935402: train_loss -0.8725 
2023-10-27 10:14:58.935787: val_loss -0.8903 
2023-10-27 10:14:58.936033: Pseudo dice [0.8884, 0.9128, 0.9686, 0.8101, 0.9464] 
2023-10-27 10:14:58.936253: Epoch time: 3.32 s 
2023-10-27 10:14:59.973945:  
2023-10-27 10:14:59.974283: Epoch 513 
2023-10-27 10:14:59.974536: Current learning rate: 0.00523 
2023-10-27 10:15:03.372146: train_loss -0.8707 
2023-10-27 10:15:03.372585: val_loss -0.8856 
2023-10-27 10:15:03.372849: Pseudo dice [0.8813, 0.9009, 0.9694, 0.8728, 0.9427] 
2023-10-27 10:15:03.373070: Epoch time: 3.4 s 
2023-10-27 10:15:04.413332:  
2023-10-27 10:15:04.413648: Epoch 514 
2023-10-27 10:15:04.413883: Current learning rate: 0.00522 
2023-10-27 10:15:07.802235: train_loss -0.8737 
2023-10-27 10:15:07.802610: val_loss -0.8901 
2023-10-27 10:15:07.802857: Pseudo dice [0.8836, 0.9071, 0.9675, 0.8418, 0.9466] 
2023-10-27 10:15:07.803076: Epoch time: 3.39 s 
2023-10-27 10:15:08.977593:  
2023-10-27 10:15:08.977940: Epoch 515 
2023-10-27 10:15:08.978183: Current learning rate: 0.00521 
2023-10-27 10:15:12.318270: train_loss -0.874 
2023-10-27 10:15:12.318621: val_loss -0.8908 
2023-10-27 10:15:12.318864: Pseudo dice [0.8903, 0.9114, 0.9702, 0.8661, 0.9479] 
2023-10-27 10:15:12.319080: Epoch time: 3.34 s 
2023-10-27 10:15:13.389418:  
2023-10-27 10:15:13.389775: Epoch 516 
2023-10-27 10:15:13.390061: Current learning rate: 0.0052 
2023-10-27 10:15:16.737412: train_loss -0.8612 
2023-10-27 10:15:16.737772: val_loss -0.8738 
2023-10-27 10:15:16.738033: Pseudo dice [0.8836, 0.8972, 0.9706, 0.7423, 0.942] 
2023-10-27 10:15:16.738262: Epoch time: 3.35 s 
2023-10-27 10:15:17.781868:  
2023-10-27 10:15:17.782156: Epoch 517 
2023-10-27 10:15:17.782388: Current learning rate: 0.00519 
2023-10-27 10:15:21.116094: train_loss -0.8742 
2023-10-27 10:15:21.116457: val_loss -0.8888 
2023-10-27 10:15:21.116708: Pseudo dice [0.8829, 0.9137, 0.9712, 0.8692, 0.9432] 
2023-10-27 10:15:21.116922: Epoch time: 3.33 s 
2023-10-27 10:15:22.162953:  
2023-10-27 10:15:22.163245: Epoch 518 
2023-10-27 10:15:22.163473: Current learning rate: 0.00518 
2023-10-27 10:15:25.525339: train_loss -0.8718 
2023-10-27 10:15:25.525805: val_loss -0.891 
2023-10-27 10:15:25.526238: Pseudo dice [0.8777, 0.9094, 0.9689, 0.8576, 0.9434] 
2023-10-27 10:15:25.526521: Epoch time: 3.36 s 
2023-10-27 10:15:26.568480:  
2023-10-27 10:15:26.568796: Epoch 519 
2023-10-27 10:15:26.569022: Current learning rate: 0.00518 
2023-10-27 10:15:29.958937: train_loss -0.8621 
2023-10-27 10:15:29.959297: val_loss -0.8824 
2023-10-27 10:15:29.959543: Pseudo dice [0.8822, 0.9109, 0.9702, 0.8476, 0.9385] 
2023-10-27 10:15:29.959751: Epoch time: 3.39 s 
2023-10-27 10:15:31.005318:  
2023-10-27 10:15:31.005614: Epoch 520 
2023-10-27 10:15:31.005856: Current learning rate: 0.00517 
2023-10-27 10:15:34.332410: train_loss -0.8592 
2023-10-27 10:15:34.332933: val_loss -0.8803 
2023-10-27 10:15:34.333274: Pseudo dice [0.887, 0.9137, 0.9683, 0.8123, 0.9477] 
2023-10-27 10:15:34.333588: Epoch time: 3.33 s 
2023-10-27 10:15:35.372298:  
2023-10-27 10:15:35.372589: Epoch 521 
2023-10-27 10:15:35.372824: Current learning rate: 0.00516 
2023-10-27 10:15:38.678192: train_loss -0.8543 
2023-10-27 10:15:38.678553: val_loss -0.8797 
2023-10-27 10:15:38.678800: Pseudo dice [0.8756, 0.9083, 0.9709, 0.8095, 0.9287] 
2023-10-27 10:15:38.679012: Epoch time: 3.31 s 
2023-10-27 10:15:39.863334:  
2023-10-27 10:15:39.863704: Epoch 522 
2023-10-27 10:15:39.863940: Current learning rate: 0.00515 
2023-10-27 10:15:43.228961: train_loss -0.8683 
2023-10-27 10:15:43.229311: val_loss -0.885 
2023-10-27 10:15:43.229577: Pseudo dice [0.888, 0.9058, 0.9694, 0.8124, 0.9444] 
2023-10-27 10:15:43.229812: Epoch time: 3.37 s 
2023-10-27 10:15:44.270336:  
2023-10-27 10:15:44.270697: Epoch 523 
2023-10-27 10:15:44.270941: Current learning rate: 0.00514 
2023-10-27 10:15:47.577445: train_loss -0.867 
2023-10-27 10:15:47.577801: val_loss -0.8871 
2023-10-27 10:15:47.578049: Pseudo dice [0.8855, 0.9113, 0.9697, 0.8485, 0.946] 
2023-10-27 10:15:47.578280: Epoch time: 3.31 s 
2023-10-27 10:15:48.621964:  
2023-10-27 10:15:48.622294: Epoch 524 
2023-10-27 10:15:48.622545: Current learning rate: 0.00513 
2023-10-27 10:15:51.972567: train_loss -0.8774 
2023-10-27 10:15:51.973060: val_loss -0.8945 
2023-10-27 10:15:51.973319: Pseudo dice [0.8904, 0.9136, 0.9697, 0.8769, 0.9497] 
2023-10-27 10:15:51.973571: Epoch time: 3.35 s 
2023-10-27 10:15:53.024161:  
2023-10-27 10:15:53.024674: Epoch 525 
2023-10-27 10:15:53.024970: Current learning rate: 0.00512 
2023-10-27 10:15:56.394344: train_loss -0.8781 
2023-10-27 10:15:56.394713: val_loss -0.886 
2023-10-27 10:15:56.394992: Pseudo dice [0.885, 0.9071, 0.9689, 0.8126, 0.9439] 
2023-10-27 10:15:56.395380: Epoch time: 3.37 s 
2023-10-27 10:15:57.442145:  
2023-10-27 10:15:57.442456: Epoch 526 
2023-10-27 10:15:57.442704: Current learning rate: 0.00511 
2023-10-27 10:16:00.799760: train_loss -0.8698 
2023-10-27 10:16:00.800141: val_loss -0.8879 
2023-10-27 10:16:00.800390: Pseudo dice [0.8879, 0.9051, 0.9695, 0.8357, 0.9423] 
2023-10-27 10:16:00.800637: Epoch time: 3.36 s 
2023-10-27 10:16:01.851197:  
2023-10-27 10:16:01.851537: Epoch 527 
2023-10-27 10:16:01.851784: Current learning rate: 0.0051 
2023-10-27 10:16:05.257564: train_loss -0.867 
2023-10-27 10:16:05.257918: val_loss -0.8928 
2023-10-27 10:16:05.258171: Pseudo dice [0.8833, 0.9121, 0.9718, 0.8786, 0.9464] 
2023-10-27 10:16:05.258405: Epoch time: 3.41 s 
2023-10-27 10:16:06.437598:  
2023-10-27 10:16:06.437906: Epoch 528 
2023-10-27 10:16:06.438139: Current learning rate: 0.00509 
2023-10-27 10:16:09.835597: train_loss -0.8735 
2023-10-27 10:16:09.836004: val_loss -0.8867 
2023-10-27 10:16:09.836251: Pseudo dice [0.8858, 0.9002, 0.9709, 0.8427, 0.9472] 
2023-10-27 10:16:09.836480: Epoch time: 3.4 s 
2023-10-27 10:16:10.880118:  
2023-10-27 10:16:10.880467: Epoch 529 
2023-10-27 10:16:10.880707: Current learning rate: 0.00508 
2023-10-27 10:16:14.275986: train_loss -0.8642 
2023-10-27 10:16:14.276344: val_loss -0.8906 
2023-10-27 10:16:14.276604: Pseudo dice [0.892, 0.9101, 0.9718, 0.8544, 0.9465] 
2023-10-27 10:16:14.276828: Epoch time: 3.4 s 
2023-10-27 10:16:15.327668:  
2023-10-27 10:16:15.327988: Epoch 530 
2023-10-27 10:16:15.328234: Current learning rate: 0.00507 
2023-10-27 10:16:18.743345: train_loss -0.8613 
2023-10-27 10:16:18.743720: val_loss -0.8931 
2023-10-27 10:16:18.743967: Pseudo dice [0.8957, 0.912, 0.972, 0.8749, 0.9478] 
2023-10-27 10:16:18.744181: Epoch time: 3.42 s 
2023-10-27 10:16:19.784216:  
2023-10-27 10:16:19.784508: Epoch 531 
2023-10-27 10:16:19.784752: Current learning rate: 0.00506 
2023-10-27 10:16:23.187459: train_loss -0.86 
2023-10-27 10:16:23.187808: val_loss -0.8899 
2023-10-27 10:16:23.188051: Pseudo dice [0.8806, 0.9116, 0.9694, 0.8617, 0.9474] 
2023-10-27 10:16:23.188290: Epoch time: 3.4 s 
2023-10-27 10:16:24.232242:  
2023-10-27 10:16:24.232520: Epoch 532 
2023-10-27 10:16:24.232759: Current learning rate: 0.00505 
2023-10-27 10:16:27.567060: train_loss -0.8612 
2023-10-27 10:16:27.567425: val_loss -0.8863 
2023-10-27 10:16:27.567679: Pseudo dice [0.8844, 0.9099, 0.9705, 0.8858, 0.9468] 
2023-10-27 10:16:27.567909: Epoch time: 3.34 s 
2023-10-27 10:16:28.609782:  
2023-10-27 10:16:28.610093: Epoch 533 
2023-10-27 10:16:28.610331: Current learning rate: 0.00504 
2023-10-27 10:16:32.013205: train_loss -0.8738 
2023-10-27 10:16:32.013599: val_loss -0.8894 
2023-10-27 10:16:32.013858: Pseudo dice [0.8887, 0.9159, 0.9704, 0.8455, 0.9478] 
2023-10-27 10:16:32.014084: Epoch time: 3.4 s 
2023-10-27 10:16:33.196581:  
2023-10-27 10:16:33.196885: Epoch 534 
2023-10-27 10:16:33.197123: Current learning rate: 0.00503 
2023-10-27 10:16:36.630592: train_loss -0.8686 
2023-10-27 10:16:36.630946: val_loss -0.8899 
2023-10-27 10:16:36.631196: Pseudo dice [0.8805, 0.9083, 0.9654, 0.8983, 0.9433] 
2023-10-27 10:16:36.631420: Epoch time: 3.43 s 
2023-10-27 10:16:37.671877:  
2023-10-27 10:16:37.672182: Epoch 535 
2023-10-27 10:16:37.672423: Current learning rate: 0.00502 
2023-10-27 10:16:41.023165: train_loss -0.8702 
2023-10-27 10:16:41.023540: val_loss -0.8937 
2023-10-27 10:16:41.023807: Pseudo dice [0.8888, 0.9103, 0.9709, 0.8774, 0.9468] 
2023-10-27 10:16:41.024033: Epoch time: 3.35 s 
2023-10-27 10:16:42.071384:  
2023-10-27 10:16:42.071732: Epoch 536 
2023-10-27 10:16:42.071984: Current learning rate: 0.00501 
2023-10-27 10:16:45.432569: train_loss -0.8703 
2023-10-27 10:16:45.433016: val_loss -0.8918 
2023-10-27 10:16:45.433281: Pseudo dice [0.8786, 0.912, 0.9693, 0.8678, 0.9409] 
2023-10-27 10:16:45.433533: Epoch time: 3.36 s 
2023-10-27 10:16:46.479537:  
2023-10-27 10:16:46.479896: Epoch 537 
2023-10-27 10:16:46.480158: Current learning rate: 0.005 
2023-10-27 10:16:49.943678: train_loss -0.8687 
2023-10-27 10:16:49.944151: val_loss -0.8899 
2023-10-27 10:16:49.944427: Pseudo dice [0.8893, 0.9058, 0.9704, 0.8681, 0.9459] 
2023-10-27 10:16:49.944667: Epoch time: 3.46 s 
2023-10-27 10:16:50.990222:  
2023-10-27 10:16:50.990539: Epoch 538 
2023-10-27 10:16:50.990794: Current learning rate: 0.00499 
2023-10-27 10:16:54.419637: train_loss -0.8657 
2023-10-27 10:16:54.420040: val_loss -0.8935 
2023-10-27 10:16:54.420295: Pseudo dice [0.8906, 0.911, 0.9708, 0.866, 0.9455] 
2023-10-27 10:16:54.420622: Epoch time: 3.43 s 
2023-10-27 10:16:55.468325:  
2023-10-27 10:16:55.468868: Epoch 539 
2023-10-27 10:16:55.469154: Current learning rate: 0.00498 
2023-10-27 10:16:58.876319: train_loss -0.8708 
2023-10-27 10:16:58.876673: val_loss -0.8657 
2023-10-27 10:16:58.876926: Pseudo dice [0.8782, 0.8955, 0.9689, 0.8346, 0.8936] 
2023-10-27 10:16:58.877160: Epoch time: 3.41 s 
2023-10-27 10:16:59.929536:  
2023-10-27 10:16:59.929915: Epoch 540 
2023-10-27 10:16:59.930203: Current learning rate: 0.00497 
2023-10-27 10:17:03.422642: train_loss -0.8503 
2023-10-27 10:17:03.423093: val_loss -0.8852 
2023-10-27 10:17:03.423379: Pseudo dice [0.8848, 0.9031, 0.9694, 0.8556, 0.9438] 
2023-10-27 10:17:03.423760: Epoch time: 3.49 s 
2023-10-27 10:17:04.613282:  
2023-10-27 10:17:04.613594: Epoch 541 
2023-10-27 10:17:04.613834: Current learning rate: 0.00496 
2023-10-27 10:17:08.008386: train_loss -0.8577 
2023-10-27 10:17:08.008997: val_loss -0.8876 
2023-10-27 10:17:08.009305: Pseudo dice [0.8815, 0.9096, 0.9694, 0.8821, 0.9361] 
2023-10-27 10:17:08.009767: Epoch time: 3.4 s 
2023-10-27 10:17:09.052691:  
2023-10-27 10:17:09.053011: Epoch 542 
2023-10-27 10:17:09.053249: Current learning rate: 0.00495 
2023-10-27 10:17:12.381790: train_loss -0.8526 
2023-10-27 10:17:12.382143: val_loss -0.8758 
2023-10-27 10:17:12.382386: Pseudo dice [0.8834, 0.9113, 0.9667, 0.6836, 0.9421] 
2023-10-27 10:17:12.382623: Epoch time: 3.33 s 
2023-10-27 10:17:13.448999:  
2023-10-27 10:17:13.449474: Epoch 543 
2023-10-27 10:17:13.449957: Current learning rate: 0.00494 
2023-10-27 10:17:16.811309: train_loss -0.8645 
2023-10-27 10:17:16.811707: val_loss -0.8912 
2023-10-27 10:17:16.811959: Pseudo dice [0.8848, 0.9075, 0.9675, 0.8738, 0.9443] 
2023-10-27 10:17:16.812190: Epoch time: 3.36 s 
2023-10-27 10:17:17.858024:  
2023-10-27 10:17:17.858353: Epoch 544 
2023-10-27 10:17:17.858596: Current learning rate: 0.00493 
2023-10-27 10:17:21.226713: train_loss -0.8631 
2023-10-27 10:17:21.227071: val_loss -0.8955 
2023-10-27 10:17:21.227334: Pseudo dice [0.8899, 0.9104, 0.9692, 0.8645, 0.9471] 
2023-10-27 10:17:21.227563: Epoch time: 3.37 s 
2023-10-27 10:17:22.271728:  
2023-10-27 10:17:22.272052: Epoch 545 
2023-10-27 10:17:22.272289: Current learning rate: 0.00492 
2023-10-27 10:17:25.674063: train_loss -0.8639 
2023-10-27 10:17:25.674431: val_loss -0.8916 
2023-10-27 10:17:25.674695: Pseudo dice [0.8907, 0.9119, 0.9693, 0.8824, 0.9477] 
2023-10-27 10:17:25.674925: Epoch time: 3.4 s 
2023-10-27 10:17:26.725165:  
2023-10-27 10:17:26.725546: Epoch 546 
2023-10-27 10:17:26.725865: Current learning rate: 0.00491 
2023-10-27 10:17:30.115250: train_loss -0.8736 
2023-10-27 10:17:30.115761: val_loss -0.893 
2023-10-27 10:17:30.116021: Pseudo dice [0.885, 0.9132, 0.9696, 0.8885, 0.9457] 
2023-10-27 10:17:30.116248: Epoch time: 3.39 s 
2023-10-27 10:17:31.309468:  
2023-10-27 10:17:31.309870: Epoch 547 
2023-10-27 10:17:31.310132: Current learning rate: 0.0049 
2023-10-27 10:17:34.844246: train_loss -0.8579 
2023-10-27 10:17:34.844645: val_loss -0.893 
2023-10-27 10:17:34.844912: Pseudo dice [0.8761, 0.9045, 0.9685, 0.8992, 0.9424] 
2023-10-27 10:17:34.845142: Epoch time: 3.54 s 
2023-10-27 10:17:35.891833:  
2023-10-27 10:17:35.892189: Epoch 548 
2023-10-27 10:17:35.892459: Current learning rate: 0.00489 
2023-10-27 10:17:39.274790: train_loss -0.8615 
2023-10-27 10:17:39.275140: val_loss -0.898 
2023-10-27 10:17:39.275379: Pseudo dice [0.8788, 0.9112, 0.9696, 0.8936, 0.9505] 
2023-10-27 10:17:39.275604: Epoch time: 3.38 s 
2023-10-27 10:17:40.319040:  
2023-10-27 10:17:40.319355: Epoch 549 
2023-10-27 10:17:40.319597: Current learning rate: 0.00488 
2023-10-27 10:17:43.726036: train_loss -0.8663 
2023-10-27 10:17:43.726641: val_loss -0.8856 
2023-10-27 10:17:43.726905: Pseudo dice [0.8857, 0.9088, 0.9688, 0.8542, 0.941] 
2023-10-27 10:17:43.727145: Epoch time: 3.41 s 
2023-10-27 10:17:44.862191:  
2023-10-27 10:17:44.862690: Epoch 550 
2023-10-27 10:17:44.862940: Current learning rate: 0.00487 
2023-10-27 10:17:48.284172: train_loss -0.8721 
2023-10-27 10:17:48.284603: val_loss -0.8845 
2023-10-27 10:17:48.284864: Pseudo dice [0.8775, 0.9021, 0.9672, 0.8566, 0.9453] 
2023-10-27 10:17:48.285089: Epoch time: 3.42 s 
2023-10-27 10:17:49.331954:  
2023-10-27 10:17:49.332323: Epoch 551 
2023-10-27 10:17:49.332590: Current learning rate: 0.00486 
2023-10-27 10:17:52.831978: train_loss -0.8662 
2023-10-27 10:17:52.832395: val_loss -0.8916 
2023-10-27 10:17:52.832688: Pseudo dice [0.8811, 0.9077, 0.9699, 0.8921, 0.9486] 
2023-10-27 10:17:52.832923: Epoch time: 3.5 s 
2023-10-27 10:17:53.885808:  
2023-10-27 10:17:53.886160: Epoch 552 
2023-10-27 10:17:53.886404: Current learning rate: 0.00485 
2023-10-27 10:17:57.385849: train_loss -0.8722 
2023-10-27 10:17:57.386273: val_loss -0.893 
2023-10-27 10:17:57.386545: Pseudo dice [0.8875, 0.9079, 0.9695, 0.8863, 0.9444] 
2023-10-27 10:17:57.386791: Epoch time: 3.5 s 
2023-10-27 10:17:58.574910:  
2023-10-27 10:17:58.575304: Epoch 553 
2023-10-27 10:17:58.575701: Current learning rate: 0.00484 
2023-10-27 10:18:02.014989: train_loss -0.8706 
2023-10-27 10:18:02.015368: val_loss -0.8925 
2023-10-27 10:18:02.015645: Pseudo dice [0.8843, 0.9087, 0.9707, 0.8698, 0.9408] 
2023-10-27 10:18:02.015875: Epoch time: 3.44 s 
2023-10-27 10:18:03.074641:  
2023-10-27 10:18:03.075039: Epoch 554 
2023-10-27 10:18:03.075429: Current learning rate: 0.00484 
2023-10-27 10:18:06.579428: train_loss -0.8685 
2023-10-27 10:18:06.579874: val_loss -0.8908 
2023-10-27 10:18:06.580133: Pseudo dice [0.8862, 0.9122, 0.9683, 0.8614, 0.9466] 
2023-10-27 10:18:06.580354: Epoch time: 3.51 s 
2023-10-27 10:18:07.631490:  
2023-10-27 10:18:07.631806: Epoch 555 
2023-10-27 10:18:07.632045: Current learning rate: 0.00483 
2023-10-27 10:18:11.072891: train_loss -0.87 
2023-10-27 10:18:11.073282: val_loss -0.8894 
2023-10-27 10:18:11.073554: Pseudo dice [0.8828, 0.9123, 0.9689, 0.8422, 0.9455] 
2023-10-27 10:18:11.073791: Epoch time: 3.44 s 
2023-10-27 10:18:12.134773:  
2023-10-27 10:18:12.135157: Epoch 556 
2023-10-27 10:18:12.135404: Current learning rate: 0.00482 
2023-10-27 10:18:15.717108: train_loss -0.8787 
2023-10-27 10:18:15.717495: val_loss -0.8925 
2023-10-27 10:18:15.717755: Pseudo dice [0.8903, 0.9097, 0.9701, 0.829, 0.9491] 
2023-10-27 10:18:15.717994: Epoch time: 3.58 s 
2023-10-27 10:18:16.774973:  
2023-10-27 10:18:16.775404: Epoch 557 
2023-10-27 10:18:16.775680: Current learning rate: 0.00481 
2023-10-27 10:18:20.264550: train_loss -0.873 
2023-10-27 10:18:20.264946: val_loss -0.8883 
2023-10-27 10:18:20.265197: Pseudo dice [0.8812, 0.9097, 0.9685, 0.8494, 0.9486] 
2023-10-27 10:18:20.265550: Epoch time: 3.49 s 
2023-10-27 10:18:21.315079:  
2023-10-27 10:18:21.315375: Epoch 558 
2023-10-27 10:18:21.315626: Current learning rate: 0.0048 
2023-10-27 10:18:24.733468: train_loss -0.8734 
2023-10-27 10:18:24.733894: val_loss -0.8781 
2023-10-27 10:18:24.734203: Pseudo dice [0.8827, 0.899, 0.9695, 0.7238, 0.9472] 
2023-10-27 10:18:24.734483: Epoch time: 3.42 s 
2023-10-27 10:18:25.773813:  
2023-10-27 10:18:25.774139: Epoch 559 
2023-10-27 10:18:25.774371: Current learning rate: 0.00479 
2023-10-27 10:18:29.130412: train_loss -0.8666 
2023-10-27 10:18:29.130815: val_loss -0.894 
2023-10-27 10:18:29.131069: Pseudo dice [0.8805, 0.9076, 0.9708, 0.8676, 0.9443] 
2023-10-27 10:18:29.131301: Epoch time: 3.36 s 
2023-10-27 10:18:30.319342:  
2023-10-27 10:18:30.319715: Epoch 560 
2023-10-27 10:18:30.319953: Current learning rate: 0.00478 
2023-10-27 10:18:33.688760: train_loss -0.8771 
2023-10-27 10:18:33.689138: val_loss -0.8961 
2023-10-27 10:18:33.689385: Pseudo dice [0.8907, 0.9034, 0.9679, 0.8591, 0.9492] 
2023-10-27 10:18:33.689616: Epoch time: 3.37 s 
2023-10-27 10:18:34.737841:  
2023-10-27 10:18:34.738174: Epoch 561 
2023-10-27 10:18:34.738432: Current learning rate: 0.00477 
2023-10-27 10:18:38.105330: train_loss -0.8803 
2023-10-27 10:18:38.105705: val_loss -0.874 
2023-10-27 10:18:38.105968: Pseudo dice [0.8894, 0.9042, 0.969, 0.5897, 0.9483] 
2023-10-27 10:18:38.106187: Epoch time: 3.37 s 
2023-10-27 10:18:39.151725:  
2023-10-27 10:18:39.152028: Epoch 562 
2023-10-27 10:18:39.152272: Current learning rate: 0.00476 
2023-10-27 10:18:42.629037: train_loss -0.8708 
2023-10-27 10:18:42.629388: val_loss -0.8918 
2023-10-27 10:18:42.629648: Pseudo dice [0.8865, 0.9089, 0.9708, 0.8736, 0.9423] 
2023-10-27 10:18:42.629867: Epoch time: 3.48 s 
2023-10-27 10:18:43.676594:  
2023-10-27 10:18:43.676962: Epoch 563 
2023-10-27 10:18:43.677248: Current learning rate: 0.00475 
2023-10-27 10:18:46.997728: train_loss -0.8561 
2023-10-27 10:18:46.998140: val_loss -0.8925 
2023-10-27 10:18:46.998391: Pseudo dice [0.8884, 0.9089, 0.9704, 0.8543, 0.9454] 
2023-10-27 10:18:46.998635: Epoch time: 3.32 s 
2023-10-27 10:18:48.054905:  
2023-10-27 10:18:48.055249: Epoch 564 
2023-10-27 10:18:48.055485: Current learning rate: 0.00474 
2023-10-27 10:18:51.436675: train_loss -0.8658 
2023-10-27 10:18:51.437067: val_loss -0.8903 
2023-10-27 10:18:51.437322: Pseudo dice [0.893, 0.9107, 0.9714, 0.8585, 0.9462] 
2023-10-27 10:18:51.437563: Epoch time: 3.38 s 
2023-10-27 10:18:52.490341:  
2023-10-27 10:18:52.490679: Epoch 565 
2023-10-27 10:18:52.490911: Current learning rate: 0.00473 
2023-10-27 10:18:55.872482: train_loss -0.8781 
2023-10-27 10:18:55.872827: val_loss -0.891 
2023-10-27 10:18:55.873095: Pseudo dice [0.8942, 0.9111, 0.9729, 0.8612, 0.945] 
2023-10-27 10:18:55.873311: Epoch time: 3.38 s 
2023-10-27 10:18:57.049902:  
2023-10-27 10:18:57.050234: Epoch 566 
2023-10-27 10:18:57.050466: Current learning rate: 0.00472 
2023-10-27 10:19:00.397369: train_loss -0.8639 
2023-10-27 10:19:00.397738: val_loss -0.8714 
2023-10-27 10:19:00.397994: Pseudo dice [0.8809, 0.8909, 0.9679, 0.7354, 0.9429] 
2023-10-27 10:19:00.398216: Epoch time: 3.35 s 
2023-10-27 10:19:01.448207:  
2023-10-27 10:19:01.448588: Epoch 567 
2023-10-27 10:19:01.448882: Current learning rate: 0.00471 
2023-10-27 10:19:04.784092: train_loss -0.8507 
2023-10-27 10:19:04.784460: val_loss -0.8707 
2023-10-27 10:19:04.784714: Pseudo dice [0.8732, 0.899, 0.9685, 0.6925, 0.9442] 
2023-10-27 10:19:04.784934: Epoch time: 3.34 s 
2023-10-27 10:19:05.834016:  
2023-10-27 10:19:05.834372: Epoch 568 
2023-10-27 10:19:05.834623: Current learning rate: 0.0047 
2023-10-27 10:19:09.135552: train_loss -0.8689 
2023-10-27 10:19:09.136014: val_loss -0.8948 
2023-10-27 10:19:09.136265: Pseudo dice [0.8881, 0.9063, 0.9724, 0.8611, 0.9448] 
2023-10-27 10:19:09.136498: Epoch time: 3.3 s 
2023-10-27 10:19:10.182935:  
2023-10-27 10:19:10.183284: Epoch 569 
2023-10-27 10:19:10.183530: Current learning rate: 0.00469 
2023-10-27 10:19:13.509597: train_loss -0.8805 
2023-10-27 10:19:13.509955: val_loss -0.8911 
2023-10-27 10:19:13.510214: Pseudo dice [0.8904, 0.915, 0.969, 0.8684, 0.9413] 
2023-10-27 10:19:13.510437: Epoch time: 3.33 s 
2023-10-27 10:19:14.550751:  
2023-10-27 10:19:14.554419: Epoch 570 
2023-10-27 10:19:14.554662: Current learning rate: 0.00468 
2023-10-27 10:19:17.900059: train_loss -0.8841 
2023-10-27 10:19:17.900444: val_loss -0.8904 
2023-10-27 10:19:17.900709: Pseudo dice [0.8916, 0.9104, 0.9716, 0.8293, 0.9472] 
2023-10-27 10:19:17.900949: Epoch time: 3.35 s 
2023-10-27 10:19:18.938195:  
2023-10-27 10:19:18.941312: Epoch 571 
2023-10-27 10:19:18.941846: Current learning rate: 0.00467 
2023-10-27 10:19:22.328343: train_loss -0.8773 
2023-10-27 10:19:22.328874: val_loss -0.8909 
2023-10-27 10:19:22.329125: Pseudo dice [0.8787, 0.8965, 0.9701, 0.8973, 0.9439] 
2023-10-27 10:19:22.329621: Epoch time: 3.39 s 
2023-10-27 10:19:23.518837:  
2023-10-27 10:19:23.519217: Epoch 572 
2023-10-27 10:19:23.519448: Current learning rate: 0.00466 
2023-10-27 10:19:26.911687: train_loss -0.8779 
2023-10-27 10:19:26.912214: val_loss -0.8917 
2023-10-27 10:19:26.912642: Pseudo dice [0.8887, 0.9097, 0.9701, 0.891, 0.9441] 
2023-10-27 10:19:26.912934: Epoch time: 3.39 s 
2023-10-27 10:19:27.962700:  
2023-10-27 10:19:27.963028: Epoch 573 
2023-10-27 10:19:27.963274: Current learning rate: 0.00465 
2023-10-27 10:19:31.360249: train_loss -0.8748 
2023-10-27 10:19:31.360600: val_loss -0.8939 
2023-10-27 10:19:31.360845: Pseudo dice [0.8909, 0.9084, 0.972, 0.8495, 0.948] 
2023-10-27 10:19:31.361072: Epoch time: 3.4 s 
2023-10-27 10:19:32.417746:  
2023-10-27 10:19:32.418068: Epoch 574 
2023-10-27 10:19:32.418298: Current learning rate: 0.00464 
2023-10-27 10:19:35.791477: train_loss -0.8748 
2023-10-27 10:19:35.791942: val_loss -0.8853 
2023-10-27 10:19:35.792191: Pseudo dice [0.8784, 0.9034, 0.9701, 0.8221, 0.9429] 
2023-10-27 10:19:35.792438: Epoch time: 3.37 s 
2023-10-27 10:19:36.851806:  
2023-10-27 10:19:36.852131: Epoch 575 
2023-10-27 10:19:36.852361: Current learning rate: 0.00463 
2023-10-27 10:19:40.203249: train_loss -0.8813 
2023-10-27 10:19:40.203607: val_loss -0.8928 
2023-10-27 10:19:40.203871: Pseudo dice [0.8809, 0.9027, 0.9704, 0.8715, 0.9468] 
2023-10-27 10:19:40.204108: Epoch time: 3.35 s 
2023-10-27 10:19:41.265884:  
2023-10-27 10:19:41.266200: Epoch 576 
2023-10-27 10:19:41.266494: Current learning rate: 0.00462 
2023-10-27 10:19:44.665691: train_loss -0.883 
2023-10-27 10:19:44.666067: val_loss -0.8948 
2023-10-27 10:19:44.666336: Pseudo dice [0.8926, 0.9135, 0.972, 0.887, 0.9445] 
2023-10-27 10:19:44.666561: Epoch time: 3.4 s 
2023-10-27 10:19:45.738564:  
2023-10-27 10:19:45.738869: Epoch 577 
2023-10-27 10:19:45.739120: Current learning rate: 0.00461 
2023-10-27 10:19:49.118912: train_loss -0.8725 
2023-10-27 10:19:49.119283: val_loss -0.8889 
2023-10-27 10:19:49.119537: Pseudo dice [0.893, 0.9117, 0.9708, 0.8568, 0.9456] 
2023-10-27 10:19:49.119753: Epoch time: 3.38 s 
2023-10-27 10:19:50.176471:  
2023-10-27 10:19:50.176786: Epoch 578 
2023-10-27 10:19:50.177029: Current learning rate: 0.0046 
2023-10-27 10:19:53.570803: train_loss -0.8825 
2023-10-27 10:19:53.571172: val_loss -0.8973 
2023-10-27 10:19:53.571443: Pseudo dice [0.8885, 0.911, 0.9724, 0.8784, 0.9475] 
2023-10-27 10:19:53.571674: Epoch time: 3.39 s 
2023-10-27 10:19:54.770307:  
2023-10-27 10:19:54.770606: Epoch 579 
2023-10-27 10:19:54.770837: Current learning rate: 0.00459 
2023-10-27 10:19:58.139941: train_loss -0.8777 
2023-10-27 10:19:58.140324: val_loss -0.8928 
2023-10-27 10:19:58.140582: Pseudo dice [0.8951, 0.9111, 0.9727, 0.8809, 0.9462] 
2023-10-27 10:19:58.140816: Epoch time: 3.37 s 
2023-10-27 10:19:59.199526:  
2023-10-27 10:19:59.199859: Epoch 580 
2023-10-27 10:19:59.200097: Current learning rate: 0.00458 
2023-10-27 10:20:02.573907: train_loss -0.884 
2023-10-27 10:20:02.574305: val_loss -0.8951 
2023-10-27 10:20:02.574605: Pseudo dice [0.8827, 0.911, 0.9709, 0.8607, 0.9479] 
2023-10-27 10:20:02.574850: Epoch time: 3.37 s 
2023-10-27 10:20:03.634817:  
2023-10-27 10:20:03.635155: Epoch 581 
2023-10-27 10:20:03.635386: Current learning rate: 0.00457 
2023-10-27 10:20:07.010217: train_loss -0.8812 
2023-10-27 10:20:07.010717: val_loss -0.8954 
2023-10-27 10:20:07.010979: Pseudo dice [0.892, 0.9138, 0.9717, 0.8844, 0.9463] 
2023-10-27 10:20:07.011204: Epoch time: 3.38 s 
2023-10-27 10:20:08.075046:  
2023-10-27 10:20:08.075341: Epoch 582 
2023-10-27 10:20:08.075581: Current learning rate: 0.00456 
2023-10-27 10:20:11.450780: train_loss -0.8829 
2023-10-27 10:20:11.451140: val_loss -0.899 
2023-10-27 10:20:11.451386: Pseudo dice [0.8889, 0.9163, 0.9712, 0.8811, 0.9468] 
2023-10-27 10:20:11.451627: Epoch time: 3.38 s 
2023-10-27 10:20:12.515579:  
2023-10-27 10:20:12.515869: Epoch 583 
2023-10-27 10:20:12.516115: Current learning rate: 0.00455 
2023-10-27 10:20:15.845906: train_loss -0.886 
2023-10-27 10:20:15.846262: val_loss -0.8934 
2023-10-27 10:20:15.846529: Pseudo dice [0.8855, 0.9098, 0.971, 0.8875, 0.9467] 
2023-10-27 10:20:15.846783: Epoch time: 3.33 s 
2023-10-27 10:20:16.901580:  
2023-10-27 10:20:16.901868: Epoch 584 
2023-10-27 10:20:16.902109: Current learning rate: 0.00454 
2023-10-27 10:20:20.238210: train_loss -0.8812 
2023-10-27 10:20:20.238587: val_loss -0.8957 
2023-10-27 10:20:20.238830: Pseudo dice [0.8842, 0.9067, 0.9703, 0.8498, 0.9483] 
2023-10-27 10:20:20.239057: Epoch time: 3.34 s 
2023-10-27 10:20:21.428311:  
2023-10-27 10:20:21.428668: Epoch 585 
2023-10-27 10:20:21.428909: Current learning rate: 0.00453 
2023-10-27 10:20:24.756219: train_loss -0.8881 
2023-10-27 10:20:24.756596: val_loss -0.8962 
2023-10-27 10:20:24.756854: Pseudo dice [0.8944, 0.9105, 0.9713, 0.8711, 0.9464] 
2023-10-27 10:20:24.757096: Epoch time: 3.33 s 
2023-10-27 10:20:25.814539:  
2023-10-27 10:20:25.814862: Epoch 586 
2023-10-27 10:20:25.815150: Current learning rate: 0.00452 
2023-10-27 10:20:29.102641: train_loss -0.8806 
2023-10-27 10:20:29.102992: val_loss -0.8963 
2023-10-27 10:20:29.103238: Pseudo dice [0.885, 0.907, 0.9703, 0.8539, 0.9471] 
2023-10-27 10:20:29.103459: Epoch time: 3.29 s 
2023-10-27 10:20:30.158150:  
2023-10-27 10:20:30.158480: Epoch 587 
2023-10-27 10:20:30.158707: Current learning rate: 0.00451 
2023-10-27 10:20:33.505315: train_loss -0.8901 
2023-10-27 10:20:33.505669: val_loss -0.8853 
2023-10-27 10:20:33.505908: Pseudo dice [0.8814, 0.911, 0.9716, 0.8074, 0.9392] 
2023-10-27 10:20:33.506140: Epoch time: 3.35 s 
2023-10-27 10:20:34.579975:  
2023-10-27 10:20:34.580258: Epoch 588 
2023-10-27 10:20:34.580489: Current learning rate: 0.0045 
2023-10-27 10:20:37.999055: train_loss -0.8874 
2023-10-27 10:20:37.999407: val_loss -0.8996 
2023-10-27 10:20:37.999658: Pseudo dice [0.8876, 0.9105, 0.9711, 0.8742, 0.948] 
2023-10-27 10:20:37.999875: Epoch time: 3.42 s 
2023-10-27 10:20:39.051287:  
2023-10-27 10:20:39.051637: Epoch 589 
2023-10-27 10:20:39.051874: Current learning rate: 0.00449 
2023-10-27 10:20:42.415345: train_loss -0.89 
2023-10-27 10:20:42.415704: val_loss -0.8931 
2023-10-27 10:20:42.415956: Pseudo dice [0.8874, 0.9106, 0.9712, 0.8534, 0.9446] 
2023-10-27 10:20:42.416178: Epoch time: 3.36 s 
2023-10-27 10:20:43.470679:  
2023-10-27 10:20:43.471008: Epoch 590 
2023-10-27 10:20:43.471246: Current learning rate: 0.00448 
2023-10-27 10:20:46.813704: train_loss -0.8819 
2023-10-27 10:20:46.814058: val_loss -0.8889 
2023-10-27 10:20:46.814307: Pseudo dice [0.8889, 0.9088, 0.9702, 0.8648, 0.9459] 
2023-10-27 10:20:46.814539: Epoch time: 3.34 s 
2023-10-27 10:20:48.003207:  
2023-10-27 10:20:48.003509: Epoch 591 
2023-10-27 10:20:48.003745: Current learning rate: 0.00447 
2023-10-27 10:20:51.327394: train_loss -0.8751 
2023-10-27 10:20:51.327906: val_loss -0.895 
2023-10-27 10:20:51.328164: Pseudo dice [0.8939, 0.9093, 0.9712, 0.8438, 0.9459] 
2023-10-27 10:20:51.328383: Epoch time: 3.32 s 
2023-10-27 10:20:52.383441:  
2023-10-27 10:20:52.383801: Epoch 592 
2023-10-27 10:20:52.384034: Current learning rate: 0.00446 
2023-10-27 10:20:55.748981: train_loss -0.886 
2023-10-27 10:20:55.749337: val_loss -0.8879 
2023-10-27 10:20:55.749596: Pseudo dice [0.8933, 0.9058, 0.9703, 0.8007, 0.9455] 
2023-10-27 10:20:55.749818: Epoch time: 3.37 s 
2023-10-27 10:20:56.803879:  
2023-10-27 10:20:56.804201: Epoch 593 
2023-10-27 10:20:56.804448: Current learning rate: 0.00445 
2023-10-27 10:21:00.119116: train_loss -0.885 
2023-10-27 10:21:00.119843: val_loss -0.8922 
2023-10-27 10:21:00.120158: Pseudo dice [0.8894, 0.9098, 0.9698, 0.8332, 0.9447] 
2023-10-27 10:21:00.120431: Epoch time: 3.32 s 
2023-10-27 10:21:01.173725:  
2023-10-27 10:21:01.174123: Epoch 594 
2023-10-27 10:21:01.174361: Current learning rate: 0.00444 
2023-10-27 10:21:04.600131: train_loss -0.8783 
2023-10-27 10:21:04.600526: val_loss -0.8907 
2023-10-27 10:21:04.600776: Pseudo dice [0.8865, 0.9077, 0.9709, 0.8511, 0.9446] 
2023-10-27 10:21:04.600989: Epoch time: 3.43 s 
2023-10-27 10:21:05.656960:  
2023-10-27 10:21:05.657251: Epoch 595 
2023-10-27 10:21:05.657494: Current learning rate: 0.00443 
2023-10-27 10:21:09.063055: train_loss -0.884 
2023-10-27 10:21:09.063443: val_loss -0.8912 
2023-10-27 10:21:09.063691: Pseudo dice [0.8898, 0.9083, 0.9701, 0.8205, 0.9459] 
2023-10-27 10:21:09.063907: Epoch time: 3.41 s 
2023-10-27 10:21:10.123514:  
2023-10-27 10:21:10.123842: Epoch 596 
2023-10-27 10:21:10.124074: Current learning rate: 0.00442 
2023-10-27 10:21:13.453670: train_loss -0.8878 
2023-10-27 10:21:13.454040: val_loss -0.8936 
2023-10-27 10:21:13.454285: Pseudo dice [0.887, 0.906, 0.9711, 0.83, 0.9466] 
2023-10-27 10:21:13.454520: Epoch time: 3.33 s 
2023-10-27 10:21:14.645620:  
2023-10-27 10:21:14.645904: Epoch 597 
2023-10-27 10:21:14.646139: Current learning rate: 0.00441 
2023-10-27 10:21:18.032321: train_loss -0.8836 
2023-10-27 10:21:18.032672: val_loss -0.8929 
2023-10-27 10:21:18.032921: Pseudo dice [0.8917, 0.9113, 0.9709, 0.8592, 0.9461] 
2023-10-27 10:21:18.033149: Epoch time: 3.39 s 
2023-10-27 10:21:19.087003:  
2023-10-27 10:21:19.087296: Epoch 598 
2023-10-27 10:21:19.087541: Current learning rate: 0.0044 
2023-10-27 10:21:22.522071: train_loss -0.8732 
2023-10-27 10:21:22.522485: val_loss -0.855 
2023-10-27 10:21:22.522747: Pseudo dice [0.8879, 0.9057, 0.9716, 0.099, 0.9477] 
2023-10-27 10:21:22.522992: Epoch time: 3.44 s 
2023-10-27 10:21:23.577525:  
2023-10-27 10:21:23.577820: Epoch 599 
2023-10-27 10:21:23.578059: Current learning rate: 0.00439 
2023-10-27 10:21:26.958650: train_loss -0.8679 
2023-10-27 10:21:26.959101: val_loss -0.8885 
2023-10-27 10:21:26.959359: Pseudo dice [0.8903, 0.9092, 0.9712, 0.815, 0.9474] 
2023-10-27 10:21:26.959606: Epoch time: 3.38 s 
2023-10-27 10:21:28.107670:  
2023-10-27 10:21:28.107942: Epoch 600 
2023-10-27 10:21:28.108177: Current learning rate: 0.00438 
2023-10-27 10:21:31.479260: train_loss -0.8722 
2023-10-27 10:21:31.479628: val_loss -0.8912 
2023-10-27 10:21:31.479882: Pseudo dice [0.8904, 0.9101, 0.9731, 0.867, 0.942] 
2023-10-27 10:21:31.480102: Epoch time: 3.37 s 
2023-10-27 10:21:32.535540:  
2023-10-27 10:21:32.535827: Epoch 601 
2023-10-27 10:21:32.536062: Current learning rate: 0.00437 
2023-10-27 10:21:35.907636: train_loss -0.8717 
2023-10-27 10:21:35.908026: val_loss -0.8967 
2023-10-27 10:21:35.908274: Pseudo dice [0.8921, 0.9118, 0.9694, 0.8811, 0.9488] 
2023-10-27 10:21:35.908502: Epoch time: 3.37 s 
2023-10-27 10:21:36.964050:  
2023-10-27 10:21:36.964337: Epoch 602 
2023-10-27 10:21:36.964577: Current learning rate: 0.00436 
2023-10-27 10:21:40.311013: train_loss -0.8735 
2023-10-27 10:21:40.311450: val_loss -0.8901 
2023-10-27 10:21:40.311862: Pseudo dice [0.8895, 0.9123, 0.9699, 0.8345, 0.9467] 
2023-10-27 10:21:40.312104: Epoch time: 3.35 s 
2023-10-27 10:21:41.511941:  
2023-10-27 10:21:41.512229: Epoch 603 
2023-10-27 10:21:41.512455: Current learning rate: 0.00435 
2023-10-27 10:21:44.948206: train_loss -0.8791 
2023-10-27 10:21:44.948612: val_loss -0.8918 
2023-10-27 10:21:44.948867: Pseudo dice [0.8931, 0.9062, 0.9692, 0.8836, 0.9493] 
2023-10-27 10:21:44.949098: Epoch time: 3.44 s 
2023-10-27 10:21:46.000615:  
2023-10-27 10:21:46.000925: Epoch 604 
2023-10-27 10:21:46.001168: Current learning rate: 0.00434 
2023-10-27 10:21:49.332753: train_loss -0.8752 
2023-10-27 10:21:49.333226: val_loss -0.8917 
2023-10-27 10:21:49.333499: Pseudo dice [0.8833, 0.9072, 0.9707, 0.8562, 0.945] 
2023-10-27 10:21:49.333736: Epoch time: 3.33 s 
2023-10-27 10:21:50.389218:  
2023-10-27 10:21:50.389560: Epoch 605 
2023-10-27 10:21:50.389886: Current learning rate: 0.00433 
2023-10-27 10:21:53.810931: train_loss -0.8769 
2023-10-27 10:21:53.811738: val_loss -0.8907 
2023-10-27 10:21:53.812140: Pseudo dice [0.8894, 0.9122, 0.9688, 0.8499, 0.9471] 
2023-10-27 10:21:53.812565: Epoch time: 3.42 s 
2023-10-27 10:21:54.883611:  
2023-10-27 10:21:54.883910: Epoch 606 
2023-10-27 10:21:54.884153: Current learning rate: 0.00432 
2023-10-27 10:21:58.244790: train_loss -0.8745 
2023-10-27 10:21:58.245152: val_loss -0.8939 
2023-10-27 10:21:58.245403: Pseudo dice [0.8878, 0.907, 0.9711, 0.8734, 0.9473] 
2023-10-27 10:21:58.245621: Epoch time: 3.36 s 
2023-10-27 10:21:59.311084:  
2023-10-27 10:21:59.311429: Epoch 607 
2023-10-27 10:21:59.311687: Current learning rate: 0.00431 
2023-10-27 10:22:02.663004: train_loss -0.8803 
2023-10-27 10:22:02.663354: val_loss -0.8967 
2023-10-27 10:22:02.663604: Pseudo dice [0.8871, 0.9138, 0.9718, 0.8713, 0.9441] 
2023-10-27 10:22:02.663821: Epoch time: 3.35 s 
2023-10-27 10:22:03.723119:  
2023-10-27 10:22:03.723457: Epoch 608 
2023-10-27 10:22:03.723705: Current learning rate: 0.0043 
2023-10-27 10:22:07.139155: train_loss -0.8816 
2023-10-27 10:22:07.139571: val_loss -0.8931 
2023-10-27 10:22:07.139852: Pseudo dice [0.8886, 0.9086, 0.9696, 0.9068, 0.9428] 
2023-10-27 10:22:07.140124: Epoch time: 3.42 s 
2023-10-27 10:22:08.336351:  
2023-10-27 10:22:08.336659: Epoch 609 
2023-10-27 10:22:08.336889: Current learning rate: 0.00429 
2023-10-27 10:22:11.863434: train_loss -0.893 
2023-10-27 10:22:11.863812: val_loss -0.8959 
2023-10-27 10:22:11.864066: Pseudo dice [0.8902, 0.9131, 0.9705, 0.8592, 0.9492] 
2023-10-27 10:22:11.864289: Epoch time: 3.53 s 
2023-10-27 10:22:12.923217:  
2023-10-27 10:22:12.923525: Epoch 610 
2023-10-27 10:22:12.923778: Current learning rate: 0.00429 
2023-10-27 10:22:16.419715: train_loss -0.8868 
2023-10-27 10:22:16.420116: val_loss -0.8975 
2023-10-27 10:22:16.420366: Pseudo dice [0.887, 0.9097, 0.9715, 0.9051, 0.9485] 
2023-10-27 10:22:16.420602: Epoch time: 3.5 s 
2023-10-27 10:22:17.491138:  
2023-10-27 10:22:17.491475: Epoch 611 
2023-10-27 10:22:17.491708: Current learning rate: 0.00428 
2023-10-27 10:22:20.840471: train_loss -0.8876 
2023-10-27 10:22:20.840832: val_loss -0.8657 
2023-10-27 10:22:20.841092: Pseudo dice [0.8826, 0.9002, 0.972, 0.5708, 0.9462] 
2023-10-27 10:22:20.841310: Epoch time: 3.35 s 
2023-10-27 10:22:21.914582:  
2023-10-27 10:22:21.914923: Epoch 612 
2023-10-27 10:22:21.915159: Current learning rate: 0.00427 
2023-10-27 10:22:25.259816: train_loss -0.8801 
2023-10-27 10:22:25.260191: val_loss -0.8941 
2023-10-27 10:22:25.260452: Pseudo dice [0.8846, 0.9079, 0.9699, 0.873, 0.9451] 
2023-10-27 10:22:25.260672: Epoch time: 3.35 s 
2023-10-27 10:22:26.324370:  
2023-10-27 10:22:26.324823: Epoch 613 
2023-10-27 10:22:26.325070: Current learning rate: 0.00426 
2023-10-27 10:22:29.641452: train_loss -0.8857 
2023-10-27 10:22:29.641816: val_loss -0.8957 
2023-10-27 10:22:29.642067: Pseudo dice [0.8873, 0.9135, 0.9736, 0.8827, 0.9443] 
2023-10-27 10:22:29.642281: Epoch time: 3.32 s 
2023-10-27 10:22:30.704152:  
2023-10-27 10:22:30.704545: Epoch 614 
2023-10-27 10:22:30.704784: Current learning rate: 0.00425 
2023-10-27 10:22:34.118986: train_loss -0.8809 
2023-10-27 10:22:34.119352: val_loss -0.8951 
2023-10-27 10:22:34.119607: Pseudo dice [0.8859, 0.9061, 0.9718, 0.8681, 0.948] 
2023-10-27 10:22:34.119833: Epoch time: 3.42 s 
2023-10-27 10:22:35.179939:  
2023-10-27 10:22:35.180224: Epoch 615 
2023-10-27 10:22:35.180462: Current learning rate: 0.00424 
2023-10-27 10:22:38.670443: train_loss -0.8843 
2023-10-27 10:22:38.670832: val_loss -0.891 
2023-10-27 10:22:38.671081: Pseudo dice [0.8895, 0.9119, 0.9712, 0.8286, 0.943] 
2023-10-27 10:22:38.671357: Epoch time: 3.49 s 
2023-10-27 10:22:39.726789:  
2023-10-27 10:22:39.727091: Epoch 616 
2023-10-27 10:22:39.727328: Current learning rate: 0.00423 
2023-10-27 10:22:43.119065: train_loss -0.8816 
2023-10-27 10:22:43.119436: val_loss -0.8897 
2023-10-27 10:22:43.119697: Pseudo dice [0.8898, 0.9131, 0.9691, 0.8535, 0.9477] 
2023-10-27 10:22:43.119912: Epoch time: 3.39 s 
2023-10-27 10:22:44.178702:  
2023-10-27 10:22:44.178989: Epoch 617 
2023-10-27 10:22:44.179227: Current learning rate: 0.00422 
2023-10-27 10:22:47.624924: train_loss -0.8839 
2023-10-27 10:22:47.625282: val_loss -0.894 
2023-10-27 10:22:47.625539: Pseudo dice [0.8905, 0.9141, 0.9712, 0.8724, 0.9447] 
2023-10-27 10:22:47.625755: Epoch time: 3.45 s 
2023-10-27 10:22:48.687968:  
2023-10-27 10:22:48.688246: Epoch 618 
2023-10-27 10:22:48.688489: Current learning rate: 0.00421 
2023-10-27 10:22:52.033305: train_loss -0.8786 
2023-10-27 10:22:52.033701: val_loss -0.859 
2023-10-27 10:22:52.033954: Pseudo dice [0.867, 0.9134, 0.9679, 0.5967, 0.9371] 
2023-10-27 10:22:52.034187: Epoch time: 3.35 s 
2023-10-27 10:22:53.093850:  
2023-10-27 10:22:53.094240: Epoch 619 
2023-10-27 10:22:53.094496: Current learning rate: 0.0042 
2023-10-27 10:22:56.426067: train_loss -0.8743 
2023-10-27 10:22:56.426442: val_loss -0.8899 
2023-10-27 10:22:56.426694: Pseudo dice [0.8865, 0.9135, 0.9697, 0.8556, 0.9438] 
2023-10-27 10:22:56.426941: Epoch time: 3.33 s 
2023-10-27 10:22:57.487175:  
2023-10-27 10:22:57.487551: Epoch 620 
2023-10-27 10:22:57.487786: Current learning rate: 0.00419 
2023-10-27 10:23:00.826871: train_loss -0.8772 
2023-10-27 10:23:00.827236: val_loss -0.8954 
2023-10-27 10:23:00.827519: Pseudo dice [0.8891, 0.9119, 0.9709, 0.8907, 0.9462] 
2023-10-27 10:23:00.827743: Epoch time: 3.34 s 
2023-10-27 10:23:01.888322:  
2023-10-27 10:23:01.888673: Epoch 621 
2023-10-27 10:23:01.888915: Current learning rate: 0.00418 
2023-10-27 10:23:05.225620: train_loss -0.881 
2023-10-27 10:23:05.225979: val_loss -0.8697 
2023-10-27 10:23:05.226228: Pseudo dice [0.8887, 0.8957, 0.9701, 0.4328, 0.9488] 
2023-10-27 10:23:05.226465: Epoch time: 3.34 s 
2023-10-27 10:23:06.427694:  
2023-10-27 10:23:06.428057: Epoch 622 
2023-10-27 10:23:06.428288: Current learning rate: 0.00417 
2023-10-27 10:23:09.754319: train_loss -0.8748 
2023-10-27 10:23:09.754717: val_loss -0.8942 
2023-10-27 10:23:09.754977: Pseudo dice [0.8901, 0.9109, 0.9713, 0.9006, 0.9461] 
2023-10-27 10:23:09.755208: Epoch time: 3.33 s 
2023-10-27 10:23:10.816756:  
2023-10-27 10:23:10.817047: Epoch 623 
2023-10-27 10:23:10.817282: Current learning rate: 0.00416 
2023-10-27 10:23:14.193454: train_loss -0.8785 
2023-10-27 10:23:14.193807: val_loss -0.8628 
2023-10-27 10:23:14.194053: Pseudo dice [0.8891, 0.907, 0.9661, 0.7847, 0.9441] 
2023-10-27 10:23:14.194274: Epoch time: 3.38 s 
2023-10-27 10:23:15.264061:  
2023-10-27 10:23:15.264405: Epoch 624 
2023-10-27 10:23:15.264652: Current learning rate: 0.00415 
2023-10-27 10:23:18.638391: train_loss -0.8713 
2023-10-27 10:23:18.638773: val_loss -0.8836 
2023-10-27 10:23:18.639033: Pseudo dice [0.8848, 0.9119, 0.9696, 0.8365, 0.9378] 
2023-10-27 10:23:18.639266: Epoch time: 3.37 s 
2023-10-27 10:23:19.699671:  
2023-10-27 10:23:19.699965: Epoch 625 
2023-10-27 10:23:19.700217: Current learning rate: 0.00414 
2023-10-27 10:23:23.090964: train_loss -0.874 
2023-10-27 10:23:23.091322: val_loss -0.8929 
2023-10-27 10:23:23.091580: Pseudo dice [0.8923, 0.9119, 0.9716, 0.8452, 0.9463] 
2023-10-27 10:23:23.091810: Epoch time: 3.39 s 
2023-10-27 10:23:24.154571:  
2023-10-27 10:23:24.154862: Epoch 626 
2023-10-27 10:23:24.155098: Current learning rate: 0.00413 
2023-10-27 10:23:27.504159: train_loss -0.8884 
2023-10-27 10:23:27.504663: val_loss -0.8497 
2023-10-27 10:23:27.505062: Pseudo dice [0.8782, 0.9076, 0.9714, 0.3817, 0.9452] 
2023-10-27 10:23:27.505390: Epoch time: 3.35 s 
2023-10-27 10:23:28.561862:  
2023-10-27 10:23:28.562186: Epoch 627 
2023-10-27 10:23:28.562447: Current learning rate: 0.00412 
2023-10-27 10:23:32.080343: train_loss -0.8772 
2023-10-27 10:23:32.080754: val_loss -0.8487 
2023-10-27 10:23:32.081010: Pseudo dice [0.8843, 0.9118, 0.9708, 0.2591, 0.9482] 
2023-10-27 10:23:32.081237: Epoch time: 3.52 s 
2023-10-27 10:23:33.283531:  
2023-10-27 10:23:33.283871: Epoch 628 
2023-10-27 10:23:33.284111: Current learning rate: 0.00411 
2023-10-27 10:23:36.718632: train_loss -0.8649 
2023-10-27 10:23:36.719023: val_loss -0.8933 
2023-10-27 10:23:36.719275: Pseudo dice [0.8832, 0.912, 0.9708, 0.8864, 0.9469] 
2023-10-27 10:23:36.719599: Epoch time: 3.44 s 
2023-10-27 10:23:37.779803:  
2023-10-27 10:23:37.780150: Epoch 629 
2023-10-27 10:23:37.780406: Current learning rate: 0.0041 
2023-10-27 10:23:41.129431: train_loss -0.8814 
2023-10-27 10:23:41.129784: val_loss -0.889 
2023-10-27 10:23:41.130032: Pseudo dice [0.8837, 0.9051, 0.9712, 0.7978, 0.9428] 
2023-10-27 10:23:41.130250: Epoch time: 3.35 s 
2023-10-27 10:23:42.192745:  
2023-10-27 10:23:42.193097: Epoch 630 
2023-10-27 10:23:42.193337: Current learning rate: 0.00409 
2023-10-27 10:23:45.575080: train_loss -0.8757 
2023-10-27 10:23:45.575713: val_loss -0.8943 
2023-10-27 10:23:45.576142: Pseudo dice [0.89, 0.9113, 0.9718, 0.8622, 0.9441] 
2023-10-27 10:23:45.576374: Epoch time: 3.38 s 
2023-10-27 10:23:46.639337:  
2023-10-27 10:23:46.639664: Epoch 631 
2023-10-27 10:23:46.639905: Current learning rate: 0.00408 
2023-10-27 10:23:50.202810: train_loss -0.8838 
2023-10-27 10:23:50.203179: val_loss -0.8935 
2023-10-27 10:23:50.203444: Pseudo dice [0.8817, 0.9076, 0.9717, 0.8879, 0.9465] 
2023-10-27 10:23:50.203659: Epoch time: 3.56 s 
2023-10-27 10:23:51.269869:  
2023-10-27 10:23:51.270215: Epoch 632 
2023-10-27 10:23:51.270469: Current learning rate: 0.00407 
2023-10-27 10:23:54.715470: train_loss -0.8823 
2023-10-27 10:23:54.715833: val_loss -0.8899 
2023-10-27 10:23:54.716090: Pseudo dice [0.8856, 0.9028, 0.9698, 0.8633, 0.948] 
2023-10-27 10:23:54.716330: Epoch time: 3.45 s 
2023-10-27 10:23:55.783136:  
2023-10-27 10:23:55.783466: Epoch 633 
2023-10-27 10:23:55.783704: Current learning rate: 0.00406 
2023-10-27 10:23:59.118582: train_loss -0.8833 
2023-10-27 10:23:59.118978: val_loss -0.8942 
2023-10-27 10:23:59.119232: Pseudo dice [0.8909, 0.9067, 0.9706, 0.835, 0.9494] 
2023-10-27 10:23:59.119458: Epoch time: 3.34 s 
2023-10-27 10:24:00.323303:  
2023-10-27 10:24:00.323679: Epoch 634 
2023-10-27 10:24:00.323976: Current learning rate: 0.00405 
2023-10-27 10:24:03.721976: train_loss -0.8832 
2023-10-27 10:24:03.722369: val_loss -0.8966 
2023-10-27 10:24:03.722628: Pseudo dice [0.8914, 0.9103, 0.9714, 0.8464, 0.9482] 
2023-10-27 10:24:03.722861: Epoch time: 3.4 s 
2023-10-27 10:24:04.788812:  
2023-10-27 10:24:04.789436: Epoch 635 
2023-10-27 10:24:04.789675: Current learning rate: 0.00404 
2023-10-27 10:24:08.150900: train_loss -0.8766 
2023-10-27 10:24:08.151318: val_loss -0.8953 
2023-10-27 10:24:08.151589: Pseudo dice [0.8842, 0.9134, 0.9702, 0.8713, 0.9482] 
2023-10-27 10:24:08.151813: Epoch time: 3.36 s 
2023-10-27 10:24:09.214120:  
2023-10-27 10:24:09.214408: Epoch 636 
2023-10-27 10:24:09.214651: Current learning rate: 0.00403 
2023-10-27 10:24:12.671943: train_loss -0.8755 
2023-10-27 10:24:12.672325: val_loss -0.8904 
2023-10-27 10:24:12.672581: Pseudo dice [0.8871, 0.9119, 0.9713, 0.7876, 0.9465] 
2023-10-27 10:24:12.672809: Epoch time: 3.46 s 
2023-10-27 10:24:13.732351:  
2023-10-27 10:24:13.732720: Epoch 637 
2023-10-27 10:24:13.732957: Current learning rate: 0.00402 
2023-10-27 10:24:17.097231: train_loss -0.8777 
2023-10-27 10:24:17.097610: val_loss -0.8884 
2023-10-27 10:24:17.097859: Pseudo dice [0.89, 0.9137, 0.9704, 0.832, 0.9491] 
2023-10-27 10:24:17.098087: Epoch time: 3.37 s 
2023-10-27 10:24:18.159360:  
2023-10-27 10:24:18.159686: Epoch 638 
2023-10-27 10:24:18.159925: Current learning rate: 0.00401 
2023-10-27 10:24:21.528632: train_loss -0.8741 
2023-10-27 10:24:21.528983: val_loss -0.8382 
2023-10-27 10:24:21.529234: Pseudo dice [0.8831, 0.9001, 0.968, 0.0, 0.9414] 
2023-10-27 10:24:21.529458: Epoch time: 3.37 s 
2023-10-27 10:24:22.589302:  
2023-10-27 10:24:22.589614: Epoch 639 
2023-10-27 10:24:22.589855: Current learning rate: 0.004 
2023-10-27 10:24:26.178262: train_loss -0.8586 
2023-10-27 10:24:26.178626: val_loss -0.8825 
2023-10-27 10:24:26.178887: Pseudo dice [0.8834, 0.8991, 0.9709, 0.8236, 0.9445] 
2023-10-27 10:24:26.179109: Epoch time: 3.59 s 
2023-10-27 10:24:27.376379:  
2023-10-27 10:24:27.376657: Epoch 640 
2023-10-27 10:24:27.376891: Current learning rate: 0.00399 
2023-10-27 10:24:30.791740: train_loss -0.8743 
2023-10-27 10:24:30.792142: val_loss -0.8323 
2023-10-27 10:24:30.792408: Pseudo dice [0.8839, 0.9103, 0.9698, 0.0, 0.946] 
2023-10-27 10:24:30.792646: Epoch time: 3.42 s 
2023-10-27 10:24:31.850214:  
2023-10-27 10:24:31.850553: Epoch 641 
2023-10-27 10:24:31.850790: Current learning rate: 0.00398 
2023-10-27 10:24:35.347239: train_loss -0.8708 
2023-10-27 10:24:35.347604: val_loss -0.8864 
2023-10-27 10:24:35.347963: Pseudo dice [0.8897, 0.9095, 0.9706, 0.8345, 0.9439] 
2023-10-27 10:24:35.348284: Epoch time: 3.5 s 
2023-10-27 10:24:36.411986:  
2023-10-27 10:24:36.412306: Epoch 642 
2023-10-27 10:24:36.412544: Current learning rate: 0.00397 
2023-10-27 10:24:40.098440: train_loss -0.8759 
2023-10-27 10:24:40.098903: val_loss -0.8881 
2023-10-27 10:24:40.099172: Pseudo dice [0.8882, 0.9125, 0.9674, 0.8838, 0.946] 
2023-10-27 10:24:40.099411: Epoch time: 3.69 s 
2023-10-27 10:24:41.163990:  
2023-10-27 10:24:41.164303: Epoch 643 
2023-10-27 10:24:41.164547: Current learning rate: 0.00396 
2023-10-27 10:24:44.642244: train_loss -0.8743 
2023-10-27 10:24:44.642610: val_loss -0.8824 
2023-10-27 10:24:44.642880: Pseudo dice [0.8841, 0.913, 0.9714, 0.8301, 0.9433] 
2023-10-27 10:24:44.643112: Epoch time: 3.48 s 
2023-10-27 10:24:45.709993:  
2023-10-27 10:24:45.710328: Epoch 644 
2023-10-27 10:24:45.710593: Current learning rate: 0.00395 
2023-10-27 10:24:49.179758: train_loss -0.8657 
2023-10-27 10:24:49.180186: val_loss -0.8932 
2023-10-27 10:24:49.180454: Pseudo dice [0.891, 0.913, 0.9693, 0.8717, 0.9465] 
2023-10-27 10:24:49.180715: Epoch time: 3.47 s 
2023-10-27 10:24:50.249083:  
2023-10-27 10:24:50.249425: Epoch 645 
2023-10-27 10:24:50.249665: Current learning rate: 0.00394 
2023-10-27 10:24:53.623543: train_loss -0.8793 
2023-10-27 10:24:53.623895: val_loss -0.8887 
2023-10-27 10:24:53.624147: Pseudo dice [0.8875, 0.9072, 0.9713, 0.8214, 0.9442] 
2023-10-27 10:24:53.624369: Epoch time: 3.38 s 
2023-10-27 10:24:54.825273:  
2023-10-27 10:24:54.825569: Epoch 646 
2023-10-27 10:24:54.825798: Current learning rate: 0.00393 
2023-10-27 10:24:58.226298: train_loss -0.877 
2023-10-27 10:24:58.226678: val_loss -0.8974 
2023-10-27 10:24:58.226928: Pseudo dice [0.8925, 0.9094, 0.9716, 0.8674, 0.9507] 
2023-10-27 10:24:58.227151: Epoch time: 3.4 s 
2023-10-27 10:24:59.297075:  
2023-10-27 10:24:59.297367: Epoch 647 
2023-10-27 10:24:59.297609: Current learning rate: 0.00392 
2023-10-27 10:25:02.830045: train_loss -0.8831 
2023-10-27 10:25:02.830431: val_loss -0.8941 
2023-10-27 10:25:02.830693: Pseudo dice [0.8834, 0.909, 0.9705, 0.8656, 0.947] 
2023-10-27 10:25:02.830918: Epoch time: 3.53 s 
2023-10-27 10:25:03.905685:  
2023-10-27 10:25:03.906011: Epoch 648 
2023-10-27 10:25:03.906247: Current learning rate: 0.00391 
2023-10-27 10:25:07.284022: train_loss -0.8775 
2023-10-27 10:25:07.284410: val_loss -0.891 
2023-10-27 10:25:07.284669: Pseudo dice [0.8898, 0.913, 0.9691, 0.8362, 0.9433] 
2023-10-27 10:25:07.284903: Epoch time: 3.38 s 
2023-10-27 10:25:08.348585:  
2023-10-27 10:25:08.348907: Epoch 649 
2023-10-27 10:25:08.349137: Current learning rate: 0.0039 
2023-10-27 10:25:11.669951: train_loss -0.881 
2023-10-27 10:25:11.670298: val_loss -0.893 
2023-10-27 10:25:11.670555: Pseudo dice [0.8868, 0.9066, 0.9696, 0.89, 0.9458] 
2023-10-27 10:25:11.670775: Epoch time: 3.32 s 
2023-10-27 10:25:12.829784:  
2023-10-27 10:25:12.830093: Epoch 650 
2023-10-27 10:25:12.830331: Current learning rate: 0.00389 
2023-10-27 10:25:16.189096: train_loss -0.8807 
2023-10-27 10:25:16.189488: val_loss -0.8913 
2023-10-27 10:25:16.189744: Pseudo dice [0.8902, 0.9156, 0.9708, 0.8878, 0.9405] 
2023-10-27 10:25:16.189962: Epoch time: 3.36 s 
2023-10-27 10:25:17.251783:  
2023-10-27 10:25:17.252141: Epoch 651 
2023-10-27 10:25:17.252391: Current learning rate: 0.00388 
2023-10-27 10:25:20.611753: train_loss -0.8845 
2023-10-27 10:25:20.612111: val_loss -0.8933 
2023-10-27 10:25:20.612428: Pseudo dice [0.8864, 0.9017, 0.9732, 0.8516, 0.9482] 
2023-10-27 10:25:20.612651: Epoch time: 3.36 s 
2023-10-27 10:25:21.810903:  
2023-10-27 10:25:21.811229: Epoch 652 
2023-10-27 10:25:21.811455: Current learning rate: 0.00387 
2023-10-27 10:25:25.212759: train_loss -0.8822 
2023-10-27 10:25:25.213151: val_loss -0.8971 
2023-10-27 10:25:25.213502: Pseudo dice [0.8916, 0.9149, 0.9697, 0.874, 0.9478] 
2023-10-27 10:25:25.213744: Epoch time: 3.4 s 
2023-10-27 10:25:26.274990:  
2023-10-27 10:25:26.275317: Epoch 653 
2023-10-27 10:25:26.275576: Current learning rate: 0.00386 
2023-10-27 10:25:29.624945: train_loss -0.8816 
2023-10-27 10:25:29.625325: val_loss -0.8863 
2023-10-27 10:25:29.625587: Pseudo dice [0.8864, 0.9101, 0.969, 0.7939, 0.9434] 
2023-10-27 10:25:29.625806: Epoch time: 3.35 s 
2023-10-27 10:25:30.707562:  
2023-10-27 10:25:30.707894: Epoch 654 
2023-10-27 10:25:30.708126: Current learning rate: 0.00385 
2023-10-27 10:25:34.037932: train_loss -0.8915 
2023-10-27 10:25:34.038308: val_loss -0.8964 
2023-10-27 10:25:34.038565: Pseudo dice [0.8975, 0.9095, 0.9689, 0.8494, 0.9491] 
2023-10-27 10:25:34.038801: Epoch time: 3.33 s 
2023-10-27 10:25:35.099555:  
2023-10-27 10:25:35.099885: Epoch 655 
2023-10-27 10:25:35.100123: Current learning rate: 0.00384 
2023-10-27 10:25:38.441844: train_loss -0.8813 
2023-10-27 10:25:38.442203: val_loss -0.894 
2023-10-27 10:25:38.442461: Pseudo dice [0.889, 0.9124, 0.97, 0.8379, 0.9477] 
2023-10-27 10:25:38.442688: Epoch time: 3.34 s 
2023-10-27 10:25:39.506527:  
2023-10-27 10:25:39.506803: Epoch 656 
2023-10-27 10:25:39.507042: Current learning rate: 0.00383 
2023-10-27 10:25:43.005335: train_loss -0.8893 
2023-10-27 10:25:43.005734: val_loss -0.9033 
2023-10-27 10:25:43.005992: Pseudo dice [0.897, 0.919, 0.9707, 0.874, 0.9517] 
2023-10-27 10:25:43.006222: Epoch time: 3.5 s 
2023-10-27 10:25:44.075344:  
2023-10-27 10:25:44.075718: Epoch 657 
2023-10-27 10:25:44.075966: Current learning rate: 0.00382 
2023-10-27 10:25:47.384126: train_loss -0.8887 
2023-10-27 10:25:47.384485: val_loss -0.8953 
2023-10-27 10:25:47.384741: Pseudo dice [0.8833, 0.9009, 0.9722, 0.8791, 0.9475] 
2023-10-27 10:25:47.384979: Epoch time: 3.31 s 
2023-10-27 10:25:48.589303:  
2023-10-27 10:25:48.589717: Epoch 658 
2023-10-27 10:25:48.589940: Current learning rate: 0.00381 
2023-10-27 10:25:52.030282: train_loss -0.8893 
2023-10-27 10:25:52.030726: val_loss -0.8998 
2023-10-27 10:25:52.030988: Pseudo dice [0.8874, 0.9148, 0.9726, 0.8795, 0.9489] 
2023-10-27 10:25:52.031222: Epoch time: 3.44 s 
2023-10-27 10:25:53.122113:  
2023-10-27 10:25:53.122430: Epoch 659 
2023-10-27 10:25:53.122691: Current learning rate: 0.0038 
2023-10-27 10:25:56.526883: train_loss -0.8874 
2023-10-27 10:25:56.527249: val_loss -0.8944 
2023-10-27 10:25:56.527523: Pseudo dice [0.8905, 0.9077, 0.9726, 0.8423, 0.9473] 
2023-10-27 10:25:56.527749: Epoch time: 3.41 s 
2023-10-27 10:25:57.591658:  
2023-10-27 10:25:57.592063: Epoch 660 
2023-10-27 10:25:57.592310: Current learning rate: 0.00379 
2023-10-27 10:26:00.981736: train_loss -0.8932 
2023-10-27 10:26:00.982103: val_loss -0.8946 
2023-10-27 10:26:00.982366: Pseudo dice [0.8919, 0.9129, 0.9709, 0.8674, 0.9481] 
2023-10-27 10:26:00.982601: Epoch time: 3.39 s 
2023-10-27 10:26:02.048542:  
2023-10-27 10:26:02.048874: Epoch 661 
2023-10-27 10:26:02.049109: Current learning rate: 0.00378 
2023-10-27 10:26:05.476847: train_loss -0.8847 
2023-10-27 10:26:05.477271: val_loss -0.8956 
2023-10-27 10:26:05.477603: Pseudo dice [0.8883, 0.9071, 0.9724, 0.8751, 0.9475] 
2023-10-27 10:26:05.477845: Epoch time: 3.43 s 
2023-10-27 10:26:06.552866:  
2023-10-27 10:26:06.553265: Epoch 662 
2023-10-27 10:26:06.553509: Current learning rate: 0.00377 
2023-10-27 10:26:10.098733: train_loss -0.8908 
2023-10-27 10:26:10.099105: val_loss -0.8959 
2023-10-27 10:26:10.099362: Pseudo dice [0.8876, 0.9083, 0.9709, 0.8705, 0.9483] 
2023-10-27 10:26:10.099604: Epoch time: 3.55 s 
2023-10-27 10:26:11.171500:  
2023-10-27 10:26:11.171832: Epoch 663 
2023-10-27 10:26:11.172072: Current learning rate: 0.00376 
2023-10-27 10:26:14.684639: train_loss -0.8804 
2023-10-27 10:26:14.685271: val_loss -0.8589 
2023-10-27 10:26:14.685633: Pseudo dice [0.889, 0.9047, 0.9694, 0.3791, 0.9455] 
2023-10-27 10:26:14.685907: Epoch time: 3.51 s 
2023-10-27 10:26:15.762606:  
2023-10-27 10:26:15.762951: Epoch 664 
2023-10-27 10:26:15.763193: Current learning rate: 0.00375 
2023-10-27 10:26:19.190310: train_loss -0.8817 
2023-10-27 10:26:19.190687: val_loss -0.8659 
2023-10-27 10:26:19.190946: Pseudo dice [0.8856, 0.9013, 0.9727, 0.4926, 0.9449] 
2023-10-27 10:26:19.191181: Epoch time: 3.43 s 
2023-10-27 10:26:20.399314:  
2023-10-27 10:26:20.399614: Epoch 665 
2023-10-27 10:26:20.399854: Current learning rate: 0.00374 
2023-10-27 10:26:23.792866: train_loss -0.8804 
2023-10-27 10:26:23.793236: val_loss -0.8973 
2023-10-27 10:26:23.793498: Pseudo dice [0.8933, 0.9075, 0.9712, 0.8995, 0.9465] 
2023-10-27 10:26:23.793728: Epoch time: 3.39 s 
2023-10-27 10:26:24.863957:  
2023-10-27 10:26:24.864280: Epoch 666 
2023-10-27 10:26:24.864604: Current learning rate: 0.00373 
2023-10-27 10:26:28.247247: train_loss -0.883 
2023-10-27 10:26:28.247613: val_loss -0.8897 
2023-10-27 10:26:28.247865: Pseudo dice [0.8901, 0.913, 0.9704, 0.8438, 0.9451] 
2023-10-27 10:26:28.248091: Epoch time: 3.38 s 
2023-10-27 10:26:29.317340:  
2023-10-27 10:26:29.317727: Epoch 667 
2023-10-27 10:26:29.317980: Current learning rate: 0.00372 
2023-10-27 10:26:32.717666: train_loss -0.8857 
2023-10-27 10:26:32.718061: val_loss -0.8956 
2023-10-27 10:26:32.718321: Pseudo dice [0.8857, 0.915, 0.97, 0.8944, 0.9438] 
2023-10-27 10:26:32.718562: Epoch time: 3.4 s 
2023-10-27 10:26:33.791909:  
2023-10-27 10:26:33.792270: Epoch 668 
2023-10-27 10:26:33.792518: Current learning rate: 0.00371 
2023-10-27 10:26:37.188876: train_loss -0.8849 
2023-10-27 10:26:37.189252: val_loss -0.8624 
2023-10-27 10:26:37.189535: Pseudo dice [0.8799, 0.9094, 0.9732, 0.2959, 0.9444] 
2023-10-27 10:26:37.189770: Epoch time: 3.4 s 
2023-10-27 10:26:38.265524:  
2023-10-27 10:26:38.265811: Epoch 669 
2023-10-27 10:26:38.266054: Current learning rate: 0.0037 
2023-10-27 10:26:41.672628: train_loss -0.88 
2023-10-27 10:26:41.673007: val_loss -0.893 
2023-10-27 10:26:41.673271: Pseudo dice [0.8872, 0.9097, 0.9718, 0.8852, 0.9478] 
2023-10-27 10:26:41.673510: Epoch time: 3.41 s 
2023-10-27 10:26:42.748552:  
2023-10-27 10:26:42.748865: Epoch 670 
2023-10-27 10:26:42.749115: Current learning rate: 0.00369 
2023-10-27 10:26:46.070838: train_loss -0.8871 
2023-10-27 10:26:46.071297: val_loss -0.8745 
2023-10-27 10:26:46.071564: Pseudo dice [0.8863, 0.9135, 0.9699, 0.8763, 0.922] 
2023-10-27 10:26:46.071805: Epoch time: 3.32 s 
2023-10-27 10:26:47.284448:  
2023-10-27 10:26:47.284775: Epoch 671 
2023-10-27 10:26:47.285009: Current learning rate: 0.00368 
2023-10-27 10:26:50.672568: train_loss -0.8743 
2023-10-27 10:26:50.672969: val_loss -0.8899 
2023-10-27 10:26:50.673216: Pseudo dice [0.8868, 0.9136, 0.9696, 0.8829, 0.9435] 
2023-10-27 10:26:50.673449: Epoch time: 3.39 s 
2023-10-27 10:26:51.745654:  
2023-10-27 10:26:51.745937: Epoch 672 
2023-10-27 10:26:51.746177: Current learning rate: 0.00367 
2023-10-27 10:26:55.118245: train_loss -0.8896 
2023-10-27 10:26:55.118600: val_loss -0.8895 
2023-10-27 10:26:55.118852: Pseudo dice [0.8842, 0.9084, 0.9719, 0.8694, 0.9465] 
2023-10-27 10:26:55.119068: Epoch time: 3.37 s 
2023-10-27 10:26:56.190230:  
2023-10-27 10:26:56.190562: Epoch 673 
2023-10-27 10:26:56.190800: Current learning rate: 0.00366 
2023-10-27 10:26:59.552331: train_loss -0.8838 
2023-10-27 10:26:59.552706: val_loss -0.8932 
2023-10-27 10:26:59.552966: Pseudo dice [0.8879, 0.9096, 0.9726, 0.8396, 0.9447] 
2023-10-27 10:26:59.553191: Epoch time: 3.36 s 
2023-10-27 10:27:00.628340:  
2023-10-27 10:27:00.628660: Epoch 674 
2023-10-27 10:27:00.628895: Current learning rate: 0.00365 
2023-10-27 10:27:04.059496: train_loss -0.8853 
2023-10-27 10:27:04.060014: val_loss -0.8885 
2023-10-27 10:27:04.060277: Pseudo dice [0.8929, 0.908, 0.9707, 0.8291, 0.9455] 
2023-10-27 10:27:04.060519: Epoch time: 3.43 s 
2023-10-27 10:27:05.154888:  
2023-10-27 10:27:05.155334: Epoch 675 
2023-10-27 10:27:05.155689: Current learning rate: 0.00364 
2023-10-27 10:27:08.783361: train_loss -0.8848 
2023-10-27 10:27:08.783801: val_loss -0.891 
2023-10-27 10:27:08.784064: Pseudo dice [0.8918, 0.9126, 0.9724, 0.8369, 0.9434] 
2023-10-27 10:27:08.784291: Epoch time: 3.63 s 
2023-10-27 10:27:09.865820:  
2023-10-27 10:27:09.866151: Epoch 676 
2023-10-27 10:27:09.866408: Current learning rate: 0.00363 
2023-10-27 10:27:13.289966: train_loss -0.8812 
2023-10-27 10:27:13.290320: val_loss -0.8888 
2023-10-27 10:27:13.290580: Pseudo dice [0.8817, 0.9107, 0.9689, 0.8368, 0.9407] 
2023-10-27 10:27:13.290807: Epoch time: 3.42 s 
2023-10-27 10:27:14.503798:  
2023-10-27 10:27:14.504097: Epoch 677 
2023-10-27 10:27:14.504339: Current learning rate: 0.00362 
2023-10-27 10:27:17.847022: train_loss -0.8802 
2023-10-27 10:27:17.847420: val_loss -0.8862 
2023-10-27 10:27:17.847680: Pseudo dice [0.8885, 0.9064, 0.9684, 0.8456, 0.9393] 
2023-10-27 10:27:17.847914: Epoch time: 3.34 s 
2023-10-27 10:27:18.934038:  
2023-10-27 10:27:18.934773: Epoch 678 
2023-10-27 10:27:18.935025: Current learning rate: 0.00361 
2023-10-27 10:27:22.286772: train_loss -0.8681 
2023-10-27 10:27:22.287230: val_loss -0.8668 
2023-10-27 10:27:22.287500: Pseudo dice [0.8867, 0.9067, 0.9736, 0.5473, 0.9434] 
2023-10-27 10:27:22.287751: Epoch time: 3.35 s 
2023-10-27 10:27:23.369848:  
2023-10-27 10:27:23.370142: Epoch 679 
2023-10-27 10:27:23.370412: Current learning rate: 0.0036 
2023-10-27 10:27:26.754671: train_loss -0.8758 
2023-10-27 10:27:26.755070: val_loss -0.884 
2023-10-27 10:27:26.755339: Pseudo dice [0.8865, 0.9116, 0.9718, 0.8723, 0.9279] 
2023-10-27 10:27:26.755570: Epoch time: 3.39 s 
2023-10-27 10:27:27.835627:  
2023-10-27 10:27:27.835912: Epoch 680 
2023-10-27 10:27:27.836160: Current learning rate: 0.00359 
2023-10-27 10:27:31.226799: train_loss -0.8676 
2023-10-27 10:27:31.227196: val_loss -0.7799 
2023-10-27 10:27:31.227481: Pseudo dice [0.8866, 0.9096, 0.9689, 0.0829, 0.8325] 
2023-10-27 10:27:31.227744: Epoch time: 3.39 s 
2023-10-27 10:27:32.303894:  
2023-10-27 10:27:32.304183: Epoch 681 
2023-10-27 10:27:32.304439: Current learning rate: 0.00358 
2023-10-27 10:27:35.703118: train_loss -0.8687 
2023-10-27 10:27:35.703478: val_loss -0.8855 
2023-10-27 10:27:35.703751: Pseudo dice [0.8888, 0.9109, 0.9723, 0.8741, 0.9423] 
2023-10-27 10:27:35.703982: Epoch time: 3.4 s 
2023-10-27 10:27:36.785738:  
2023-10-27 10:27:36.786028: Epoch 682 
2023-10-27 10:27:36.786273: Current learning rate: 0.00357 
2023-10-27 10:27:40.125453: train_loss -0.8787 
2023-10-27 10:27:40.125822: val_loss -0.8848 
2023-10-27 10:27:40.126078: Pseudo dice [0.8918, 0.9102, 0.9718, 0.5894, 0.9495] 
2023-10-27 10:27:40.126304: Epoch time: 3.34 s 
2023-10-27 10:27:41.343331:  
2023-10-27 10:27:41.343662: Epoch 683 
2023-10-27 10:27:41.343910: Current learning rate: 0.00356 
2023-10-27 10:27:44.706487: train_loss -0.8692 
2023-10-27 10:27:44.706905: val_loss -0.8841 
2023-10-27 10:27:44.707183: Pseudo dice [0.8878, 0.91, 0.9716, 0.827, 0.9253] 
2023-10-27 10:27:44.707460: Epoch time: 3.36 s 
2023-10-27 10:27:45.789286:  
2023-10-27 10:27:45.789640: Epoch 684 
2023-10-27 10:27:45.789882: Current learning rate: 0.00355 
2023-10-27 10:27:49.275199: train_loss -0.8662 
2023-10-27 10:27:49.275680: val_loss -0.888 
2023-10-27 10:27:49.275993: Pseudo dice [0.896, 0.9061, 0.9691, 0.8795, 0.946] 
2023-10-27 10:27:49.276302: Epoch time: 3.49 s 
2023-10-27 10:27:50.356553:  
2023-10-27 10:27:50.356892: Epoch 685 
2023-10-27 10:27:50.357127: Current learning rate: 0.00354 
2023-10-27 10:27:53.755791: train_loss -0.8672 
2023-10-27 10:27:53.756194: val_loss -0.8898 
2023-10-27 10:27:53.756467: Pseudo dice [0.889, 0.91, 0.9718, 0.9035, 0.9416] 
2023-10-27 10:27:53.756707: Epoch time: 3.4 s 
2023-10-27 10:27:54.842256:  
2023-10-27 10:27:54.842562: Epoch 686 
2023-10-27 10:27:54.842814: Current learning rate: 0.00353 
2023-10-27 10:27:58.228308: train_loss -0.8743 
2023-10-27 10:27:58.228676: val_loss -0.8928 
2023-10-27 10:27:58.228937: Pseudo dice [0.8948, 0.9174, 0.9721, 0.858, 0.9467] 
2023-10-27 10:27:58.229163: Epoch time: 3.39 s 
2023-10-27 10:27:59.303023:  
2023-10-27 10:27:59.303357: Epoch 687 
2023-10-27 10:27:59.303602: Current learning rate: 0.00352 
2023-10-27 10:28:02.707850: train_loss -0.876 
2023-10-27 10:28:02.708218: val_loss -0.8915 
2023-10-27 10:28:02.708478: Pseudo dice [0.8886, 0.9113, 0.9707, 0.8751, 0.9438] 
2023-10-27 10:28:02.708702: Epoch time: 3.41 s 
2023-10-27 10:28:03.789540:  
2023-10-27 10:28:03.789826: Epoch 688 
2023-10-27 10:28:03.790082: Current learning rate: 0.00351 
2023-10-27 10:28:07.119196: train_loss -0.8811 
2023-10-27 10:28:07.119559: val_loss -0.891 
2023-10-27 10:28:07.119819: Pseudo dice [0.8876, 0.9167, 0.9725, 0.8506, 0.9458] 
2023-10-27 10:28:07.120048: Epoch time: 3.33 s 
2023-10-27 10:28:08.338631:  
2023-10-27 10:28:08.338901: Epoch 689 
2023-10-27 10:28:08.339151: Current learning rate: 0.0035 
2023-10-27 10:28:11.757041: train_loss -0.88 
2023-10-27 10:28:11.757458: val_loss -0.8931 
2023-10-27 10:28:11.757734: Pseudo dice [0.8878, 0.9065, 0.9714, 0.8642, 0.9472] 
2023-10-27 10:28:11.757972: Epoch time: 3.42 s 
2023-10-27 10:28:12.845183:  
2023-10-27 10:28:12.845535: Epoch 690 
2023-10-27 10:28:12.845776: Current learning rate: 0.00349 
2023-10-27 10:28:16.254858: train_loss -0.8834 
2023-10-27 10:28:16.255241: val_loss -0.8912 
2023-10-27 10:28:16.255491: Pseudo dice [0.8861, 0.9088, 0.9693, 0.8899, 0.9471] 
2023-10-27 10:28:16.255904: Epoch time: 3.41 s 
2023-10-27 10:28:17.337745:  
2023-10-27 10:28:17.338069: Epoch 691 
2023-10-27 10:28:17.338324: Current learning rate: 0.00348 
2023-10-27 10:28:20.799219: train_loss -0.8737 
2023-10-27 10:28:20.799579: val_loss -0.8778 
2023-10-27 10:28:20.799845: Pseudo dice [0.8893, 0.9096, 0.9696, 0.7901, 0.9457] 
2023-10-27 10:28:20.800189: Epoch time: 3.46 s 
2023-10-27 10:28:21.882324:  
2023-10-27 10:28:21.882837: Epoch 692 
2023-10-27 10:28:21.883084: Current learning rate: 0.00346 
2023-10-27 10:28:25.383159: train_loss -0.8804 
2023-10-27 10:28:25.383538: val_loss -0.8929 
2023-10-27 10:28:25.383795: Pseudo dice [0.895, 0.9108, 0.9704, 0.8892, 0.9452] 
2023-10-27 10:28:25.384015: Epoch time: 3.5 s 
2023-10-27 10:28:26.462625:  
2023-10-27 10:28:26.462930: Epoch 693 
2023-10-27 10:28:26.463188: Current learning rate: 0.00345 
2023-10-27 10:28:29.827073: train_loss -0.8895 
2023-10-27 10:28:29.827442: val_loss -0.8936 
2023-10-27 10:28:29.827725: Pseudo dice [0.8872, 0.9071, 0.9722, 0.8639, 0.9461] 
2023-10-27 10:28:29.827948: Epoch time: 3.37 s 
2023-10-27 10:28:30.917732:  
2023-10-27 10:28:30.918023: Epoch 694 
2023-10-27 10:28:30.918260: Current learning rate: 0.00344 
2023-10-27 10:28:34.309669: train_loss -0.8839 
2023-10-27 10:28:34.310051: val_loss -0.8741 
2023-10-27 10:28:34.310302: Pseudo dice [0.8936, 0.9132, 0.9736, 0.5646, 0.9474] 
2023-10-27 10:28:34.310526: Epoch time: 3.39 s 
2023-10-27 10:28:35.534476:  
2023-10-27 10:28:35.534789: Epoch 695 
2023-10-27 10:28:35.535048: Current learning rate: 0.00343 
2023-10-27 10:28:38.927685: train_loss -0.8869 
2023-10-27 10:28:38.928053: val_loss -0.8981 
2023-10-27 10:28:38.928323: Pseudo dice [0.8909, 0.9126, 0.9693, 0.9128, 0.9474] 
2023-10-27 10:28:38.928570: Epoch time: 3.39 s 
2023-10-27 10:28:40.016325:  
2023-10-27 10:28:40.016617: Epoch 696 
2023-10-27 10:28:40.016858: Current learning rate: 0.00342 
2023-10-27 10:28:43.451923: train_loss -0.8891 
2023-10-27 10:28:43.452296: val_loss -0.8949 
2023-10-27 10:28:43.452578: Pseudo dice [0.8908, 0.9093, 0.9718, 0.8562, 0.9474] 
2023-10-27 10:28:43.452796: Epoch time: 3.44 s 
2023-10-27 10:28:44.536841:  
2023-10-27 10:28:44.537180: Epoch 697 
2023-10-27 10:28:44.537419: Current learning rate: 0.00341 
2023-10-27 10:28:47.939737: train_loss -0.8894 
2023-10-27 10:28:47.940281: val_loss -0.8897 
2023-10-27 10:28:47.940749: Pseudo dice [0.8894, 0.9073, 0.9707, 0.8585, 0.9459] 
2023-10-27 10:28:47.941047: Epoch time: 3.4 s 
2023-10-27 10:28:49.024848:  
2023-10-27 10:28:49.025225: Epoch 698 
2023-10-27 10:28:49.025473: Current learning rate: 0.0034 
2023-10-27 10:28:52.382699: train_loss -0.8875 
2023-10-27 10:28:52.383053: val_loss -0.893 
2023-10-27 10:28:52.383466: Pseudo dice [0.8904, 0.9067, 0.9716, 0.8837, 0.9485] 
2023-10-27 10:28:52.384209: Epoch time: 3.36 s 
2023-10-27 10:28:53.471163:  
2023-10-27 10:28:53.471571: Epoch 699 
2023-10-27 10:28:53.471818: Current learning rate: 0.00339 
2023-10-27 10:28:56.940910: train_loss -0.8793 
2023-10-27 10:28:56.941324: val_loss -0.8118 
2023-10-27 10:28:56.941616: Pseudo dice [0.8934, 0.902, 0.9694, 0.0, 0.9488] 
2023-10-27 10:28:56.941859: Epoch time: 3.47 s 
2023-10-27 10:28:58.256907:  
2023-10-27 10:28:58.257236: Epoch 700 
2023-10-27 10:28:58.257483: Current learning rate: 0.00338 
2023-10-27 10:29:01.782929: train_loss -0.8695 
2023-10-27 10:29:01.783271: val_loss -0.8927 
2023-10-27 10:29:01.783533: Pseudo dice [0.8882, 0.9114, 0.9725, 0.8876, 0.9459] 
2023-10-27 10:29:01.783756: Epoch time: 3.53 s 
2023-10-27 10:29:02.858867:  
2023-10-27 10:29:02.859203: Epoch 701 
2023-10-27 10:29:02.859456: Current learning rate: 0.00337 
2023-10-27 10:29:06.275699: train_loss -0.891 
2023-10-27 10:29:06.276109: val_loss -0.8982 
2023-10-27 10:29:06.276359: Pseudo dice [0.8929, 0.9173, 0.9709, 0.8873, 0.9472] 
2023-10-27 10:29:06.276579: Epoch time: 3.42 s 
2023-10-27 10:29:07.355759:  
2023-10-27 10:29:07.356086: Epoch 702 
2023-10-27 10:29:07.356313: Current learning rate: 0.00336 
2023-10-27 10:29:10.751868: train_loss -0.8827 
2023-10-27 10:29:10.752229: val_loss -0.8914 
2023-10-27 10:29:10.752483: Pseudo dice [0.8944, 0.9122, 0.9706, 0.8798, 0.9477] 
2023-10-27 10:29:10.752713: Epoch time: 3.4 s 
2023-10-27 10:29:11.827584:  
2023-10-27 10:29:11.827860: Epoch 703 
2023-10-27 10:29:11.828096: Current learning rate: 0.00335 
2023-10-27 10:29:15.197820: train_loss -0.8826 
2023-10-27 10:29:15.198315: val_loss -0.8801 
2023-10-27 10:29:15.198733: Pseudo dice [0.8884, 0.9108, 0.9692, 0.7177, 0.9454] 
2023-10-27 10:29:15.199011: Epoch time: 3.37 s 
2023-10-27 10:29:16.275300:  
2023-10-27 10:29:16.275682: Epoch 704 
2023-10-27 10:29:16.275923: Current learning rate: 0.00334 
2023-10-27 10:29:19.619069: train_loss -0.8902 
2023-10-27 10:29:19.619450: val_loss -0.8943 
2023-10-27 10:29:19.619703: Pseudo dice [0.8938, 0.9129, 0.9716, 0.7411, 0.9452] 
2023-10-27 10:29:19.619925: Epoch time: 3.34 s 
2023-10-27 10:29:20.710018:  
2023-10-27 10:29:20.710348: Epoch 705 
2023-10-27 10:29:20.710598: Current learning rate: 0.00333 
2023-10-27 10:29:24.102846: train_loss -0.8874 
2023-10-27 10:29:24.103203: val_loss -0.8868 
2023-10-27 10:29:24.103467: Pseudo dice [0.889, 0.9131, 0.9686, 0.8297, 0.9421] 
2023-10-27 10:29:24.103693: Epoch time: 3.39 s 
2023-10-27 10:29:25.324624:  
2023-10-27 10:29:25.324904: Epoch 706 
2023-10-27 10:29:25.325119: Current learning rate: 0.00332 
2023-10-27 10:29:28.741293: train_loss -0.888 
2023-10-27 10:29:28.741665: val_loss -0.8998 
2023-10-27 10:29:28.741931: Pseudo dice [0.8904, 0.9158, 0.9724, 0.8567, 0.95] 
2023-10-27 10:29:28.742154: Epoch time: 3.42 s 
2023-10-27 10:29:29.817474:  
2023-10-27 10:29:29.817803: Epoch 707 
2023-10-27 10:29:29.818048: Current learning rate: 0.00331 
2023-10-27 10:29:33.370384: train_loss -0.8947 
2023-10-27 10:29:33.370771: val_loss -0.8713 
2023-10-27 10:29:33.371030: Pseudo dice [0.8866, 0.9091, 0.9708, 0.6432, 0.9415] 
2023-10-27 10:29:33.371257: Epoch time: 3.55 s 
2023-10-27 10:29:34.457580:  
2023-10-27 10:29:34.457881: Epoch 708 
2023-10-27 10:29:34.458120: Current learning rate: 0.0033 
2023-10-27 10:29:37.863831: train_loss -0.8827 
2023-10-27 10:29:37.864227: val_loss -0.8867 
2023-10-27 10:29:37.864484: Pseudo dice [0.8839, 0.9086, 0.9703, 0.8578, 0.9478] 
2023-10-27 10:29:37.864724: Epoch time: 3.41 s 
2023-10-27 10:29:38.946919:  
2023-10-27 10:29:38.947197: Epoch 709 
2023-10-27 10:29:38.947441: Current learning rate: 0.00329 
2023-10-27 10:29:42.303589: train_loss -0.8837 
2023-10-27 10:29:42.303941: val_loss -0.8938 
2023-10-27 10:29:42.304175: Pseudo dice [0.886, 0.9094, 0.9696, 0.873, 0.9462] 
2023-10-27 10:29:42.304385: Epoch time: 3.36 s 
2023-10-27 10:29:43.390898:  
2023-10-27 10:29:43.391200: Epoch 710 
2023-10-27 10:29:43.391443: Current learning rate: 0.00328 
2023-10-27 10:29:46.785308: train_loss -0.8896 
2023-10-27 10:29:46.785683: val_loss -0.8963 
2023-10-27 10:29:46.785930: Pseudo dice [0.8901, 0.9129, 0.9714, 0.8901, 0.9474] 
2023-10-27 10:29:46.786227: Epoch time: 3.39 s 
2023-10-27 10:29:47.872891:  
2023-10-27 10:29:47.873291: Epoch 711 
2023-10-27 10:29:47.873535: Current learning rate: 0.00327 
2023-10-27 10:29:51.248263: train_loss -0.8884 
2023-10-27 10:29:51.248679: val_loss -0.8987 
2023-10-27 10:29:51.248921: Pseudo dice [0.8941, 0.9237, 0.9714, 0.8407, 0.9484] 
2023-10-27 10:29:51.249138: Epoch time: 3.38 s 
2023-10-27 10:29:52.471823:  
2023-10-27 10:29:52.472142: Epoch 712 
2023-10-27 10:29:52.472432: Current learning rate: 0.00326 
2023-10-27 10:29:55.864875: train_loss -0.8868 
2023-10-27 10:29:55.865393: val_loss -0.898 
2023-10-27 10:29:55.865756: Pseudo dice [0.8882, 0.9199, 0.9711, 0.8784, 0.9476] 
2023-10-27 10:29:55.866038: Epoch time: 3.39 s 
2023-10-27 10:29:56.945792:  
2023-10-27 10:29:56.946157: Epoch 713 
2023-10-27 10:29:56.946391: Current learning rate: 0.00325 
2023-10-27 10:30:00.309014: train_loss -0.8922 
2023-10-27 10:30:00.309362: val_loss -0.897 
2023-10-27 10:30:00.309623: Pseudo dice [0.8912, 0.9138, 0.9716, 0.8484, 0.9473] 
2023-10-27 10:30:00.309854: Epoch time: 3.36 s 
2023-10-27 10:30:01.386545:  
2023-10-27 10:30:01.386848: Epoch 714 
2023-10-27 10:30:01.387097: Current learning rate: 0.00324 
2023-10-27 10:30:04.779494: train_loss -0.8933 
2023-10-27 10:30:04.779843: val_loss -0.8923 
2023-10-27 10:30:04.780093: Pseudo dice [0.8921, 0.9109, 0.971, 0.8524, 0.9444] 
2023-10-27 10:30:04.780316: Epoch time: 3.39 s 
2023-10-27 10:30:05.864279:  
2023-10-27 10:30:05.864613: Epoch 715 
2023-10-27 10:30:05.864863: Current learning rate: 0.00323 
2023-10-27 10:30:09.267933: train_loss -0.8884 
2023-10-27 10:30:09.268411: val_loss -0.8938 
2023-10-27 10:30:09.268732: Pseudo dice [0.8927, 0.9101, 0.9712, 0.8742, 0.9479] 
2023-10-27 10:30:09.268986: Epoch time: 3.4 s 
2023-10-27 10:30:10.350882:  
2023-10-27 10:30:10.351223: Epoch 716 
2023-10-27 10:30:10.351473: Current learning rate: 0.00322 
2023-10-27 10:30:13.733876: train_loss -0.8889 
2023-10-27 10:30:13.734282: val_loss -0.8979 
2023-10-27 10:30:13.734541: Pseudo dice [0.8887, 0.9086, 0.9722, 0.8426, 0.9485] 
2023-10-27 10:30:13.734806: Epoch time: 3.38 s 
2023-10-27 10:30:14.817293:  
2023-10-27 10:30:14.817611: Epoch 717 
2023-10-27 10:30:14.817852: Current learning rate: 0.00321 
2023-10-27 10:30:18.148216: train_loss -0.8886 
2023-10-27 10:30:18.148587: val_loss -0.8892 
2023-10-27 10:30:18.148843: Pseudo dice [0.8862, 0.9094, 0.97, 0.825, 0.9364] 
2023-10-27 10:30:18.149060: Epoch time: 3.33 s 
2023-10-27 10:30:19.371901:  
2023-10-27 10:30:19.372231: Epoch 718 
2023-10-27 10:30:19.372463: Current learning rate: 0.0032 
2023-10-27 10:30:22.758220: train_loss -0.8898 
2023-10-27 10:30:22.758572: val_loss -0.8775 
2023-10-27 10:30:22.758810: Pseudo dice [0.8851, 0.9125, 0.972, 0.6293, 0.9432] 
2023-10-27 10:30:22.759038: Epoch time: 3.39 s 
2023-10-27 10:30:23.838505:  
2023-10-27 10:30:23.838795: Epoch 719 
2023-10-27 10:30:23.839029: Current learning rate: 0.00319 
2023-10-27 10:30:27.204309: train_loss -0.8857 
2023-10-27 10:30:27.204663: val_loss -0.8925 
2023-10-27 10:30:27.204911: Pseudo dice [0.8874, 0.907, 0.971, 0.8636, 0.9484] 
2023-10-27 10:30:27.205135: Epoch time: 3.37 s 
2023-10-27 10:30:28.296353:  
2023-10-27 10:30:28.296681: Epoch 720 
2023-10-27 10:30:28.296922: Current learning rate: 0.00318 
2023-10-27 10:30:31.662761: train_loss -0.8742 
2023-10-27 10:30:31.663157: val_loss -0.8841 
2023-10-27 10:30:31.663418: Pseudo dice [0.8818, 0.9125, 0.9712, 0.7508, 0.9447] 
2023-10-27 10:30:31.663654: Epoch time: 3.37 s 
2023-10-27 10:30:32.752744:  
2023-10-27 10:30:32.753096: Epoch 721 
2023-10-27 10:30:32.753345: Current learning rate: 0.00317 
2023-10-27 10:30:36.132958: train_loss -0.8706 
2023-10-27 10:30:36.133307: val_loss -0.8824 
2023-10-27 10:30:36.133561: Pseudo dice [0.8868, 0.9071, 0.9701, 0.8416, 0.9343] 
2023-10-27 10:30:36.133778: Epoch time: 3.38 s 
2023-10-27 10:30:37.226100:  
2023-10-27 10:30:37.226463: Epoch 722 
2023-10-27 10:30:37.226700: Current learning rate: 0.00316 
2023-10-27 10:30:40.631259: train_loss -0.8832 
2023-10-27 10:30:40.631617: val_loss -0.892 
2023-10-27 10:30:40.631866: Pseudo dice [0.8925, 0.9163, 0.9708, 0.8565, 0.9476] 
2023-10-27 10:30:40.632089: Epoch time: 3.41 s 
2023-10-27 10:30:41.714881:  
2023-10-27 10:30:41.715249: Epoch 723 
2023-10-27 10:30:41.715484: Current learning rate: 0.00315 
2023-10-27 10:30:45.085665: train_loss -0.8818 
2023-10-27 10:30:45.086015: val_loss -0.893 
2023-10-27 10:30:45.086279: Pseudo dice [0.8911, 0.9106, 0.9716, 0.8608, 0.9438] 
2023-10-27 10:30:45.086559: Epoch time: 3.37 s 
2023-10-27 10:30:46.304582:  
2023-10-27 10:30:46.304917: Epoch 724 
2023-10-27 10:30:46.305157: Current learning rate: 0.00314 
2023-10-27 10:30:49.684454: train_loss -0.88 
2023-10-27 10:30:49.684909: val_loss -0.8936 
2023-10-27 10:30:49.685184: Pseudo dice [0.8876, 0.907, 0.972, 0.8426, 0.9462] 
2023-10-27 10:30:49.685422: Epoch time: 3.38 s 
2023-10-27 10:30:50.766186:  
2023-10-27 10:30:50.766526: Epoch 725 
2023-10-27 10:30:50.766781: Current learning rate: 0.00313 
2023-10-27 10:30:54.141645: train_loss -0.8881 
2023-10-27 10:30:54.142010: val_loss -0.8963 
2023-10-27 10:30:54.142269: Pseudo dice [0.8896, 0.9127, 0.9717, 0.8606, 0.9485] 
2023-10-27 10:30:54.142498: Epoch time: 3.38 s 
2023-10-27 10:30:55.224686:  
2023-10-27 10:30:55.224969: Epoch 726 
2023-10-27 10:30:55.225199: Current learning rate: 0.00312 
2023-10-27 10:30:58.625045: train_loss -0.8782 
2023-10-27 10:30:58.625527: val_loss -0.8917 
2023-10-27 10:30:58.625784: Pseudo dice [0.8876, 0.9071, 0.9698, 0.8798, 0.9498] 
2023-10-27 10:30:58.626010: Epoch time: 3.4 s 
2023-10-27 10:30:59.712675:  
2023-10-27 10:30:59.713010: Epoch 727 
2023-10-27 10:30:59.713249: Current learning rate: 0.00311 
2023-10-27 10:31:03.090670: train_loss -0.883 
2023-10-27 10:31:03.091032: val_loss -0.8939 
2023-10-27 10:31:03.091286: Pseudo dice [0.8817, 0.9142, 0.9703, 0.8907, 0.9437] 
2023-10-27 10:31:03.091521: Epoch time: 3.38 s 
2023-10-27 10:31:04.180283:  
2023-10-27 10:31:04.180612: Epoch 728 
2023-10-27 10:31:04.180853: Current learning rate: 0.0031 
2023-10-27 10:31:07.518125: train_loss -0.8817 
2023-10-27 10:31:07.518496: val_loss -0.8904 
2023-10-27 10:31:07.518752: Pseudo dice [0.8826, 0.9067, 0.9709, 0.9185, 0.9414] 
2023-10-27 10:31:07.518973: Epoch time: 3.34 s 
2023-10-27 10:31:08.608618:  
2023-10-27 10:31:08.608982: Epoch 729 
2023-10-27 10:31:08.609243: Current learning rate: 0.00309 
2023-10-27 10:31:11.935678: train_loss -0.8809 
2023-10-27 10:31:11.936028: val_loss -0.8945 
2023-10-27 10:31:11.936280: Pseudo dice [0.883, 0.9109, 0.9721, 0.8651, 0.9488] 
2023-10-27 10:31:11.936510: Epoch time: 3.33 s 
2023-10-27 10:31:13.160893:  
2023-10-27 10:31:13.161309: Epoch 730 
2023-10-27 10:31:13.161558: Current learning rate: 0.00308 
2023-10-27 10:31:16.632241: train_loss -0.8901 
2023-10-27 10:31:16.632641: val_loss -0.8938 
2023-10-27 10:31:16.632910: Pseudo dice [0.8868, 0.9067, 0.9727, 0.8677, 0.9439] 
2023-10-27 10:31:16.633142: Epoch time: 3.47 s 
2023-10-27 10:31:17.710725:  
2023-10-27 10:31:17.711029: Epoch 731 
2023-10-27 10:31:17.711268: Current learning rate: 0.00307 
2023-10-27 10:31:21.278461: train_loss -0.8871 
2023-10-27 10:31:21.278875: val_loss -0.8953 
2023-10-27 10:31:21.279143: Pseudo dice [0.8874, 0.9144, 0.9711, 0.8683, 0.9456] 
2023-10-27 10:31:21.279366: Epoch time: 3.57 s 
2023-10-27 10:31:22.362324:  
2023-10-27 10:31:22.362657: Epoch 732 
2023-10-27 10:31:22.362900: Current learning rate: 0.00306 
2023-10-27 10:31:25.832574: train_loss -0.8869 
2023-10-27 10:31:25.832961: val_loss -0.8935 
2023-10-27 10:31:25.833229: Pseudo dice [0.8874, 0.9085, 0.9718, 0.8903, 0.9443] 
2023-10-27 10:31:25.833501: Epoch time: 3.47 s 
2023-10-27 10:31:26.921069:  
2023-10-27 10:31:26.921415: Epoch 733 
2023-10-27 10:31:26.921667: Current learning rate: 0.00305 
2023-10-27 10:31:30.297677: train_loss -0.8871 
2023-10-27 10:31:30.298040: val_loss -0.885 
2023-10-27 10:31:30.298290: Pseudo dice [0.8838, 0.9016, 0.9709, 0.8352, 0.9453] 
2023-10-27 10:31:30.298523: Epoch time: 3.38 s 
2023-10-27 10:31:31.388115:  
2023-10-27 10:31:31.388508: Epoch 734 
2023-10-27 10:31:31.388746: Current learning rate: 0.00304 
2023-10-27 10:31:34.793115: train_loss -0.8778 
2023-10-27 10:31:34.793511: val_loss -0.8878 
2023-10-27 10:31:34.793766: Pseudo dice [0.8865, 0.9081, 0.9717, 0.8733, 0.9464] 
2023-10-27 10:31:34.793993: Epoch time: 3.41 s 
2023-10-27 10:31:35.881661:  
2023-10-27 10:31:35.881957: Epoch 735 
2023-10-27 10:31:35.882194: Current learning rate: 0.00303 
2023-10-27 10:31:39.235391: train_loss -0.8902 
2023-10-27 10:31:39.235748: val_loss -0.8907 
2023-10-27 10:31:39.235987: Pseudo dice [0.8896, 0.9097, 0.9701, 0.8628, 0.9459] 
2023-10-27 10:31:39.236202: Epoch time: 3.35 s 
2023-10-27 10:31:40.458864:  
2023-10-27 10:31:40.459154: Epoch 736 
2023-10-27 10:31:40.459404: Current learning rate: 0.00302 
2023-10-27 10:31:43.877043: train_loss -0.8885 
2023-10-27 10:31:43.877458: val_loss -0.8991 
2023-10-27 10:31:43.877786: Pseudo dice [0.8888, 0.915, 0.9715, 0.8731, 0.9468] 
2023-10-27 10:31:43.878133: Epoch time: 3.42 s 
2023-10-27 10:31:44.957039:  
2023-10-27 10:31:44.957455: Epoch 737 
2023-10-27 10:31:44.957699: Current learning rate: 0.00301 
2023-10-27 10:31:48.363310: train_loss -0.8807 
2023-10-27 10:31:48.363673: val_loss -0.8911 
2023-10-27 10:31:48.363925: Pseudo dice [0.8867, 0.9105, 0.9724, 0.8389, 0.9462] 
2023-10-27 10:31:48.364153: Epoch time: 3.41 s 
2023-10-27 10:31:49.452831:  
2023-10-27 10:31:49.453159: Epoch 738 
2023-10-27 10:31:49.453417: Current learning rate: 0.003 
2023-10-27 10:31:52.875538: train_loss -0.8897 
2023-10-27 10:31:52.875990: val_loss -0.8926 
2023-10-27 10:31:52.876257: Pseudo dice [0.8854, 0.9057, 0.9719, 0.8689, 0.9486] 
2023-10-27 10:31:52.876508: Epoch time: 3.42 s 
2023-10-27 10:31:53.957778:  
2023-10-27 10:31:53.958118: Epoch 739 
2023-10-27 10:31:53.958355: Current learning rate: 0.00299 
2023-10-27 10:31:57.396734: train_loss -0.8874 
2023-10-27 10:31:57.397109: val_loss -0.8927 
2023-10-27 10:31:57.397369: Pseudo dice [0.8845, 0.9107, 0.9728, 0.8564, 0.9432] 
2023-10-27 10:31:57.397627: Epoch time: 3.44 s 
2023-10-27 10:31:58.480098:  
2023-10-27 10:31:58.480424: Epoch 740 
2023-10-27 10:31:58.480675: Current learning rate: 0.00297 
2023-10-27 10:32:01.817125: train_loss -0.8899 
2023-10-27 10:32:01.817537: val_loss -0.887 
2023-10-27 10:32:01.817816: Pseudo dice [0.8806, 0.9112, 0.9726, 0.8486, 0.9404] 
2023-10-27 10:32:01.818068: Epoch time: 3.34 s 
2023-10-27 10:32:02.901841:  
2023-10-27 10:32:02.902149: Epoch 741 
2023-10-27 10:32:02.902390: Current learning rate: 0.00296 
2023-10-27 10:32:06.396317: train_loss -0.891 
2023-10-27 10:32:06.396717: val_loss -0.8727 
2023-10-27 10:32:06.397003: Pseudo dice [0.8907, 0.9079, 0.9698, 0.715, 0.9471] 
2023-10-27 10:32:06.397241: Epoch time: 3.5 s 
2023-10-27 10:32:07.667355:  
2023-10-27 10:32:07.667747: Epoch 742 
2023-10-27 10:32:07.668010: Current learning rate: 0.00295 
2023-10-27 10:32:11.325050: train_loss -0.8913 
2023-10-27 10:32:11.325450: val_loss -0.8918 
2023-10-27 10:32:11.325713: Pseudo dice [0.8862, 0.9109, 0.9698, 0.8356, 0.9479] 
2023-10-27 10:32:11.325944: Epoch time: 3.66 s 
2023-10-27 10:32:12.415331:  
2023-10-27 10:32:12.415643: Epoch 743 
2023-10-27 10:32:12.415896: Current learning rate: 0.00294 
2023-10-27 10:32:15.852687: train_loss -0.8942 
2023-10-27 10:32:15.853245: val_loss -0.8924 
2023-10-27 10:32:15.853617: Pseudo dice [0.8894, 0.9135, 0.9697, 0.8465, 0.9448] 
2023-10-27 10:32:15.853923: Epoch time: 3.44 s 
2023-10-27 10:32:16.940796:  
2023-10-27 10:32:16.941100: Epoch 744 
2023-10-27 10:32:16.941343: Current learning rate: 0.00293 
2023-10-27 10:32:20.308149: train_loss -0.8956 
2023-10-27 10:32:20.308627: val_loss -0.8941 
2023-10-27 10:32:20.308972: Pseudo dice [0.8927, 0.9118, 0.9707, 0.8436, 0.9504] 
2023-10-27 10:32:20.309279: Epoch time: 3.37 s 
2023-10-27 10:32:21.393601:  
2023-10-27 10:32:21.393996: Epoch 745 
2023-10-27 10:32:21.394233: Current learning rate: 0.00292 
2023-10-27 10:32:24.770660: train_loss -0.8924 
2023-10-27 10:32:24.771037: val_loss -0.8901 
2023-10-27 10:32:24.771301: Pseudo dice [0.8903, 0.9114, 0.9713, 0.8576, 0.9435] 
2023-10-27 10:32:24.771536: Epoch time: 3.38 s 
2023-10-27 10:32:25.857529:  
2023-10-27 10:32:25.857820: Epoch 746 
2023-10-27 10:32:25.858059: Current learning rate: 0.00291 
2023-10-27 10:32:29.277394: train_loss -0.8899 
2023-10-27 10:32:29.277851: val_loss -0.8949 
2023-10-27 10:32:29.278105: Pseudo dice [0.8877, 0.9144, 0.9696, 0.8532, 0.9465] 
2023-10-27 10:32:29.278352: Epoch time: 3.42 s 
2023-10-27 10:32:30.364098:  
2023-10-27 10:32:30.364405: Epoch 747 
2023-10-27 10:32:30.364647: Current learning rate: 0.0029 
2023-10-27 10:32:33.764238: train_loss -0.8908 
2023-10-27 10:32:33.764610: val_loss -0.8927 
2023-10-27 10:32:33.764869: Pseudo dice [0.8951, 0.9162, 0.9732, 0.8528, 0.9489] 
2023-10-27 10:32:33.765114: Epoch time: 3.4 s 
2023-10-27 10:32:35.001965:  
2023-10-27 10:32:35.002307: Epoch 748 
2023-10-27 10:32:35.002540: Current learning rate: 0.00289 
2023-10-27 10:32:38.405734: train_loss -0.8923 
2023-10-27 10:32:38.406125: val_loss -0.889 
2023-10-27 10:32:38.406389: Pseudo dice [0.8874, 0.9063, 0.9722, 0.8322, 0.9466] 
2023-10-27 10:32:38.406624: Epoch time: 3.4 s 
2023-10-27 10:32:39.484963:  
2023-10-27 10:32:39.485245: Epoch 749 
2023-10-27 10:32:39.485504: Current learning rate: 0.00288 
2023-10-27 10:32:42.873838: train_loss -0.8887 
2023-10-27 10:32:42.874198: val_loss -0.8915 
2023-10-27 10:32:42.874460: Pseudo dice [0.887, 0.9075, 0.9733, 0.8293, 0.9452] 
2023-10-27 10:32:42.874686: Epoch time: 3.39 s 
2023-10-27 10:32:44.059249:  
2023-10-27 10:32:44.059550: Epoch 750 
2023-10-27 10:32:44.059783: Current learning rate: 0.00287 
2023-10-27 10:32:47.449979: train_loss -0.8913 
2023-10-27 10:32:47.450329: val_loss -0.8934 
2023-10-27 10:32:47.450578: Pseudo dice [0.8841, 0.9131, 0.9711, 0.8726, 0.9422] 
2023-10-27 10:32:47.450819: Epoch time: 3.39 s 
2023-10-27 10:32:48.535642:  
2023-10-27 10:32:48.535918: Epoch 751 
2023-10-27 10:32:48.536160: Current learning rate: 0.00286 
2023-10-27 10:32:51.939510: train_loss -0.8902 
2023-10-27 10:32:51.940119: val_loss -0.8951 
2023-10-27 10:32:51.940459: Pseudo dice [0.8933, 0.9136, 0.9726, 0.8472, 0.9462] 
2023-10-27 10:32:51.940727: Epoch time: 3.4 s 
2023-10-27 10:32:53.026861:  
2023-10-27 10:32:53.027157: Epoch 752 
2023-10-27 10:32:53.027408: Current learning rate: 0.00285 
2023-10-27 10:32:56.353857: train_loss -0.893 
2023-10-27 10:32:56.354259: val_loss -0.8923 
2023-10-27 10:32:56.354524: Pseudo dice [0.885, 0.9081, 0.9724, 0.8431, 0.9437] 
2023-10-27 10:32:56.354753: Epoch time: 3.33 s 
2023-10-27 10:32:57.432862:  
2023-10-27 10:32:57.433236: Epoch 753 
2023-10-27 10:32:57.433477: Current learning rate: 0.00284 
2023-10-27 10:33:00.795222: train_loss -0.8919 
2023-10-27 10:33:00.795584: val_loss -0.8562 
2023-10-27 10:33:00.795835: Pseudo dice [0.8871, 0.9133, 0.97, 0.2793, 0.9437] 
2023-10-27 10:33:00.796068: Epoch time: 3.36 s 
2023-10-27 10:33:01.882934:  
2023-10-27 10:33:01.883320: Epoch 754 
2023-10-27 10:33:01.883559: Current learning rate: 0.00283 
2023-10-27 10:33:05.258454: train_loss -0.8836 
2023-10-27 10:33:05.258917: val_loss -0.888 
2023-10-27 10:33:05.259169: Pseudo dice [0.8857, 0.9082, 0.9719, 0.8223, 0.9384] 
2023-10-27 10:33:05.259391: Epoch time: 3.38 s 
2023-10-27 10:33:06.491578:  
2023-10-27 10:33:06.491946: Epoch 755 
2023-10-27 10:33:06.492192: Current learning rate: 0.00282 
2023-10-27 10:33:09.915875: train_loss -0.896 
2023-10-27 10:33:09.916273: val_loss -0.8937 
2023-10-27 10:33:09.916567: Pseudo dice [0.8864, 0.9108, 0.9724, 0.8549, 0.9468] 
2023-10-27 10:33:09.916800: Epoch time: 3.42 s 
2023-10-27 10:33:11.000518:  
2023-10-27 10:33:11.000856: Epoch 756 
2023-10-27 10:33:11.001096: Current learning rate: 0.00281 
2023-10-27 10:33:14.355190: train_loss -0.8901 
2023-10-27 10:33:14.355683: val_loss -0.8929 
2023-10-27 10:33:14.355931: Pseudo dice [0.8837, 0.9131, 0.9709, 0.8468, 0.9449] 
2023-10-27 10:33:14.356160: Epoch time: 3.36 s 
2023-10-27 10:33:15.442070:  
2023-10-27 10:33:15.442362: Epoch 757 
2023-10-27 10:33:15.442605: Current learning rate: 0.0028 
2023-10-27 10:33:18.793932: train_loss -0.8966 
2023-10-27 10:33:18.794292: val_loss -0.896 
2023-10-27 10:33:18.794557: Pseudo dice [0.8936, 0.9164, 0.9727, 0.8668, 0.9464] 
2023-10-27 10:33:18.794785: Epoch time: 3.35 s 
2023-10-27 10:33:19.878063:  
2023-10-27 10:33:19.878389: Epoch 758 
2023-10-27 10:33:19.878636: Current learning rate: 0.00279 
2023-10-27 10:33:23.396708: train_loss -0.89 
2023-10-27 10:33:23.397187: val_loss -0.8947 
2023-10-27 10:33:23.397452: Pseudo dice [0.8891, 0.9105, 0.9714, 0.8624, 0.9479] 
2023-10-27 10:33:23.397671: Epoch time: 3.52 s 
2023-10-27 10:33:24.483468:  
2023-10-27 10:33:24.483760: Epoch 759 
2023-10-27 10:33:24.483992: Current learning rate: 0.00278 
2023-10-27 10:33:27.879528: train_loss -0.8918 
2023-10-27 10:33:27.879957: val_loss -0.8923 
2023-10-27 10:33:27.880218: Pseudo dice [0.8899, 0.9107, 0.9716, 0.8483, 0.9443] 
2023-10-27 10:33:27.880461: Epoch time: 3.4 s 
2023-10-27 10:33:28.970932:  
2023-10-27 10:33:28.971230: Epoch 760 
2023-10-27 10:33:28.971471: Current learning rate: 0.00277 
2023-10-27 10:33:32.491024: train_loss -0.8925 
2023-10-27 10:33:32.491361: val_loss -0.888 
2023-10-27 10:33:32.491617: Pseudo dice [0.8886, 0.9125, 0.968, 0.8435, 0.946] 
2023-10-27 10:33:32.491837: Epoch time: 3.52 s 
2023-10-27 10:33:33.573487:  
2023-10-27 10:33:33.573827: Epoch 761 
2023-10-27 10:33:33.574059: Current learning rate: 0.00276 
2023-10-27 10:33:36.928008: train_loss -0.8934 
2023-10-27 10:33:36.928346: val_loss -0.8942 
2023-10-27 10:33:36.928597: Pseudo dice [0.887, 0.9138, 0.972, 0.8506, 0.948] 
2023-10-27 10:33:36.928815: Epoch time: 3.36 s 
2023-10-27 10:33:38.017600:  
2023-10-27 10:33:38.017927: Epoch 762 
2023-10-27 10:33:38.018185: Current learning rate: 0.00275 
2023-10-27 10:33:41.381304: train_loss -0.8925 
2023-10-27 10:33:41.381655: val_loss -0.8902 
2023-10-27 10:33:41.381899: Pseudo dice [0.8798, 0.9133, 0.9712, 0.8511, 0.9352] 
2023-10-27 10:33:41.382121: Epoch time: 3.36 s 
2023-10-27 10:33:42.482005:  
2023-10-27 10:33:42.482336: Epoch 763 
2023-10-27 10:33:42.482586: Current learning rate: 0.00274 
2023-10-27 10:33:45.871864: train_loss -0.8857 
2023-10-27 10:33:45.872291: val_loss -0.8906 
2023-10-27 10:33:45.872574: Pseudo dice [0.8837, 0.9139, 0.971, 0.8889, 0.9452] 
2023-10-27 10:33:45.872805: Epoch time: 3.39 s 
2023-10-27 10:33:46.967672:  
2023-10-27 10:33:46.968004: Epoch 764 
2023-10-27 10:33:46.968251: Current learning rate: 0.00273 
2023-10-27 10:33:50.346170: train_loss -0.8917 
2023-10-27 10:33:50.346734: val_loss -0.89 
2023-10-27 10:33:50.347087: Pseudo dice [0.8844, 0.912, 0.9714, 0.8088, 0.9484] 
2023-10-27 10:33:50.347508: Epoch time: 3.38 s 
2023-10-27 10:33:51.441724:  
2023-10-27 10:33:51.442059: Epoch 765 
2023-10-27 10:33:51.442298: Current learning rate: 0.00272 
2023-10-27 10:33:54.900900: train_loss -0.8945 
2023-10-27 10:33:54.901259: val_loss -0.8922 
2023-10-27 10:33:54.901529: Pseudo dice [0.8897, 0.9127, 0.9698, 0.8513, 0.9436] 
2023-10-27 10:33:54.901764: Epoch time: 3.46 s 
2023-10-27 10:33:56.147545:  
2023-10-27 10:33:56.148320: Epoch 766 
2023-10-27 10:33:56.148572: Current learning rate: 0.00271 
2023-10-27 10:33:59.660734: train_loss -0.888 
2023-10-27 10:33:59.661165: val_loss -0.8938 
2023-10-27 10:33:59.661434: Pseudo dice [0.8856, 0.9094, 0.972, 0.8504, 0.9461] 
2023-10-27 10:33:59.661673: Epoch time: 3.51 s 
2023-10-27 10:34:00.756791:  
2023-10-27 10:34:00.757094: Epoch 767 
2023-10-27 10:34:00.757334: Current learning rate: 0.0027 
2023-10-27 10:34:04.254018: train_loss -0.878 
2023-10-27 10:34:04.254416: val_loss -0.8849 
2023-10-27 10:34:04.254668: Pseudo dice [0.8829, 0.9116, 0.97, 0.755, 0.9423] 
2023-10-27 10:34:04.254892: Epoch time: 3.5 s 
2023-10-27 10:34:05.354007:  
2023-10-27 10:34:05.354335: Epoch 768 
2023-10-27 10:34:05.354574: Current learning rate: 0.00268 
2023-10-27 10:34:08.864953: train_loss -0.8869 
2023-10-27 10:34:08.865626: val_loss -0.8961 
2023-10-27 10:34:08.866045: Pseudo dice [0.8872, 0.9139, 0.9706, 0.8502, 0.945] 
2023-10-27 10:34:08.866453: Epoch time: 3.51 s 
2023-10-27 10:34:09.971838:  
2023-10-27 10:34:09.972141: Epoch 769 
2023-10-27 10:34:09.972380: Current learning rate: 0.00267 
2023-10-27 10:34:13.373099: train_loss -0.8803 
2023-10-27 10:34:13.373480: val_loss -0.8724 
2023-10-27 10:34:13.373746: Pseudo dice [0.875, 0.9103, 0.973, 0.6608, 0.9427] 
2023-10-27 10:34:13.374001: Epoch time: 3.4 s 
2023-10-27 10:34:14.462006:  
2023-10-27 10:34:14.462341: Epoch 770 
2023-10-27 10:34:14.462715: Current learning rate: 0.00266 
2023-10-27 10:34:18.072688: train_loss -0.8922 
2023-10-27 10:34:18.082075: val_loss -0.8846 
2023-10-27 10:34:18.082486: Pseudo dice [0.8789, 0.9107, 0.9697, 0.7957, 0.9421] 
2023-10-27 10:34:18.082715: Epoch time: 3.61 s 
2023-10-27 10:34:19.244087:  
2023-10-27 10:34:19.244496: Epoch 771 
2023-10-27 10:34:19.244751: Current learning rate: 0.00265 
2023-10-27 10:34:22.595323: train_loss -0.8892 
2023-10-27 10:34:22.595775: val_loss -0.8841 
2023-10-27 10:34:22.596032: Pseudo dice [0.8807, 0.9102, 0.9726, 0.7859, 0.9447] 
2023-10-27 10:34:22.596257: Epoch time: 3.35 s 
2023-10-27 10:34:23.915180:  
2023-10-27 10:34:23.915528: Epoch 772 
2023-10-27 10:34:23.915785: Current learning rate: 0.00264 
2023-10-27 10:34:27.318609: train_loss -0.8908 
2023-10-27 10:34:27.319360: val_loss -0.8857 
2023-10-27 10:34:27.319719: Pseudo dice [0.8843, 0.9105, 0.9707, 0.6684, 0.9453] 
2023-10-27 10:34:27.320018: Epoch time: 3.4 s 
2023-10-27 10:34:28.413905:  
2023-10-27 10:34:28.414293: Epoch 773 
2023-10-27 10:34:28.414587: Current learning rate: 0.00263 
2023-10-27 10:34:31.811207: train_loss -0.8936 
2023-10-27 10:34:31.811585: val_loss -0.889 
2023-10-27 10:34:31.811828: Pseudo dice [0.8878, 0.9176, 0.9718, 0.8005, 0.9441] 
2023-10-27 10:34:31.812045: Epoch time: 3.4 s 
2023-10-27 10:34:32.915291:  
2023-10-27 10:34:32.915653: Epoch 774 
2023-10-27 10:34:32.915891: Current learning rate: 0.00262 
2023-10-27 10:34:36.309048: train_loss -0.8962 
2023-10-27 10:34:36.309427: val_loss -0.891 
2023-10-27 10:34:36.309688: Pseudo dice [0.8888, 0.9124, 0.9711, 0.8297, 0.9454] 
2023-10-27 10:34:36.309924: Epoch time: 3.39 s 
2023-10-27 10:34:37.410757:  
2023-10-27 10:34:37.411052: Epoch 775 
2023-10-27 10:34:37.411297: Current learning rate: 0.00261 
2023-10-27 10:34:40.857696: train_loss -0.8909 
2023-10-27 10:34:40.858088: val_loss -0.8906 
2023-10-27 10:34:40.858348: Pseudo dice [0.8912, 0.9137, 0.9716, 0.8372, 0.9456] 
2023-10-27 10:34:40.858592: Epoch time: 3.45 s 
2023-10-27 10:34:41.954086:  
2023-10-27 10:34:41.954370: Epoch 776 
2023-10-27 10:34:41.954616: Current learning rate: 0.0026 
2023-10-27 10:34:45.371255: train_loss -0.8918 
2023-10-27 10:34:45.371614: val_loss -0.8892 
2023-10-27 10:34:45.371863: Pseudo dice [0.8869, 0.9084, 0.9702, 0.8291, 0.9431] 
2023-10-27 10:34:45.372103: Epoch time: 3.42 s 
2023-10-27 10:34:46.470450:  
2023-10-27 10:34:46.470755: Epoch 777 
2023-10-27 10:34:46.471011: Current learning rate: 0.00259 
2023-10-27 10:34:49.828855: train_loss -0.8824 
2023-10-27 10:34:49.829201: val_loss -0.8739 
2023-10-27 10:34:49.829603: Pseudo dice [0.8933, 0.9183, 0.9676, 0.7015, 0.9408] 
2023-10-27 10:34:49.829824: Epoch time: 3.36 s 
2023-10-27 10:34:51.067678:  
2023-10-27 10:34:51.068004: Epoch 778 
2023-10-27 10:34:51.068229: Current learning rate: 0.00258 
2023-10-27 10:34:54.449814: train_loss -0.884 
2023-10-27 10:34:54.450170: val_loss -0.8911 
2023-10-27 10:34:54.450448: Pseudo dice [0.8889, 0.9102, 0.9708, 0.8725, 0.9428] 
2023-10-27 10:34:54.450678: Epoch time: 3.38 s 
2023-10-27 10:34:55.535635:  
2023-10-27 10:34:55.535994: Epoch 779 
2023-10-27 10:34:55.536239: Current learning rate: 0.00257 
2023-10-27 10:34:58.934447: train_loss -0.8863 
2023-10-27 10:34:58.934803: val_loss -0.8833 
2023-10-27 10:34:58.935057: Pseudo dice [0.8822, 0.9088, 0.9706, 0.8208, 0.9332] 
2023-10-27 10:34:58.935283: Epoch time: 3.4 s 
2023-10-27 10:35:00.030855:  
2023-10-27 10:35:00.031193: Epoch 780 
2023-10-27 10:35:00.031433: Current learning rate: 0.00256 
2023-10-27 10:35:03.359208: train_loss -0.8806 
2023-10-27 10:35:03.359683: val_loss -0.8858 
2023-10-27 10:35:03.359934: Pseudo dice [0.8904, 0.9182, 0.9695, 0.8451, 0.9401] 
2023-10-27 10:35:03.360169: Epoch time: 3.33 s 
2023-10-27 10:35:04.455557:  
2023-10-27 10:35:04.455859: Epoch 781 
2023-10-27 10:35:04.456110: Current learning rate: 0.00255 
2023-10-27 10:35:07.818226: train_loss -0.8845 
2023-10-27 10:35:07.818723: val_loss -0.8875 
2023-10-27 10:35:07.818988: Pseudo dice [0.8887, 0.909, 0.9718, 0.8129, 0.9439] 
2023-10-27 10:35:07.819261: Epoch time: 3.36 s 
2023-10-27 10:35:08.913948:  
2023-10-27 10:35:08.914234: Epoch 782 
2023-10-27 10:35:08.914470: Current learning rate: 0.00254 
2023-10-27 10:35:12.256061: train_loss -0.8908 
2023-10-27 10:35:12.256427: val_loss -0.8924 
2023-10-27 10:35:12.256693: Pseudo dice [0.8892, 0.9113, 0.9718, 0.8417, 0.9456] 
2023-10-27 10:35:12.256928: Epoch time: 3.34 s 
2023-10-27 10:35:13.350674:  
2023-10-27 10:35:13.350996: Epoch 783 
2023-10-27 10:35:13.351238: Current learning rate: 0.00253 
2023-10-27 10:35:16.676403: train_loss -0.8756 
2023-10-27 10:35:16.676758: val_loss -0.8739 
2023-10-27 10:35:16.677022: Pseudo dice [0.8892, 0.9077, 0.9721, 0.7883, 0.9228] 
2023-10-27 10:35:16.677248: Epoch time: 3.33 s 
2023-10-27 10:35:17.912269:  
2023-10-27 10:35:17.912603: Epoch 784 
2023-10-27 10:35:17.912840: Current learning rate: 0.00252 
2023-10-27 10:35:21.261288: train_loss -0.8819 
2023-10-27 10:35:21.261665: val_loss -0.8891 
2023-10-27 10:35:21.261916: Pseudo dice [0.8871, 0.9176, 0.9705, 0.8238, 0.9447] 
2023-10-27 10:35:21.262142: Epoch time: 3.35 s 
2023-10-27 10:35:22.357104:  
2023-10-27 10:35:22.357449: Epoch 785 
2023-10-27 10:35:22.357688: Current learning rate: 0.00251 
2023-10-27 10:35:25.718649: train_loss -0.8825 
2023-10-27 10:35:25.719123: val_loss -0.8922 
2023-10-27 10:35:25.719514: Pseudo dice [0.8913, 0.9137, 0.9709, 0.7969, 0.9488] 
2023-10-27 10:35:25.719752: Epoch time: 3.36 s 
2023-10-27 10:35:26.813361:  
2023-10-27 10:35:26.813725: Epoch 786 
2023-10-27 10:35:26.813960: Current learning rate: 0.0025 
2023-10-27 10:35:30.147918: train_loss -0.8884 
2023-10-27 10:35:30.148423: val_loss -0.8888 
2023-10-27 10:35:30.148741: Pseudo dice [0.8916, 0.9131, 0.97, 0.8229, 0.9456] 
2023-10-27 10:35:30.148975: Epoch time: 3.34 s 
2023-10-27 10:35:31.241264:  
2023-10-27 10:35:31.241560: Epoch 787 
2023-10-27 10:35:31.241802: Current learning rate: 0.00249 
2023-10-27 10:35:34.570458: train_loss -0.8821 
2023-10-27 10:35:34.571146: val_loss -0.8937 
2023-10-27 10:35:34.571388: Pseudo dice [0.8866, 0.914, 0.9699, 0.8655, 0.9469] 
2023-10-27 10:35:34.571624: Epoch time: 3.33 s 
2023-10-27 10:35:35.668410:  
2023-10-27 10:35:35.668743: Epoch 788 
2023-10-27 10:35:35.668979: Current learning rate: 0.00248 
2023-10-27 10:35:38.994942: train_loss -0.8896 
2023-10-27 10:35:38.995318: val_loss -0.8903 
2023-10-27 10:35:38.995579: Pseudo dice [0.8845, 0.9118, 0.9702, 0.832, 0.9446] 
2023-10-27 10:35:38.995797: Epoch time: 3.33 s 
2023-10-27 10:35:40.091294:  
2023-10-27 10:35:40.091623: Epoch 789 
2023-10-27 10:35:40.091860: Current learning rate: 0.00247 
2023-10-27 10:35:43.461409: train_loss -0.8917 
2023-10-27 10:35:43.461895: val_loss -0.8868 
2023-10-27 10:35:43.462141: Pseudo dice [0.8886, 0.9149, 0.9708, 0.8322, 0.9398] 
2023-10-27 10:35:43.462362: Epoch time: 3.37 s 
2023-10-27 10:35:44.694497:  
2023-10-27 10:35:44.694866: Epoch 790 
2023-10-27 10:35:44.695106: Current learning rate: 0.00245 
2023-10-27 10:35:48.051434: train_loss -0.8861 
2023-10-27 10:35:48.051783: val_loss -0.8669 
2023-10-27 10:35:48.052030: Pseudo dice [0.8871, 0.9136, 0.9702, 0.789, 0.9324] 
2023-10-27 10:35:48.052245: Epoch time: 3.36 s 
2023-10-27 10:35:49.149504:  
2023-10-27 10:35:49.149825: Epoch 791 
2023-10-27 10:35:49.150067: Current learning rate: 0.00244 
2023-10-27 10:35:52.470203: train_loss -0.8824 
2023-10-27 10:35:52.470592: val_loss -0.8933 
2023-10-27 10:35:52.470851: Pseudo dice [0.8886, 0.9141, 0.9712, 0.8758, 0.9477] 
2023-10-27 10:35:52.471082: Epoch time: 3.32 s 
2023-10-27 10:35:53.567981:  
2023-10-27 10:35:53.568300: Epoch 792 
2023-10-27 10:35:53.568544: Current learning rate: 0.00243 
2023-10-27 10:35:56.871012: train_loss -0.8856 
2023-10-27 10:35:56.871362: val_loss -0.8919 
2023-10-27 10:35:56.871606: Pseudo dice [0.8817, 0.9066, 0.9707, 0.839, 0.9466] 
2023-10-27 10:35:56.871837: Epoch time: 3.3 s 
2023-10-27 10:35:57.968589:  
2023-10-27 10:35:57.968884: Epoch 793 
2023-10-27 10:35:57.969121: Current learning rate: 0.00242 
2023-10-27 10:36:01.281825: train_loss -0.8878 
2023-10-27 10:36:01.282249: val_loss -0.8913 
2023-10-27 10:36:01.282512: Pseudo dice [0.8881, 0.9137, 0.971, 0.8523, 0.9409] 
2023-10-27 10:36:01.282734: Epoch time: 3.31 s 
2023-10-27 10:36:02.378080:  
2023-10-27 10:36:02.378371: Epoch 794 
2023-10-27 10:36:02.378618: Current learning rate: 0.00241 
2023-10-27 10:36:05.727482: train_loss -0.8915 
2023-10-27 10:36:05.727902: val_loss -0.896 
2023-10-27 10:36:05.728171: Pseudo dice [0.8875, 0.9131, 0.972, 0.8245, 0.9479] 
2023-10-27 10:36:05.728491: Epoch time: 3.35 s 
2023-10-27 10:36:06.823082:  
2023-10-27 10:36:06.823424: Epoch 795 
2023-10-27 10:36:06.823665: Current learning rate: 0.0024 
2023-10-27 10:36:10.154626: train_loss -0.8932 
2023-10-27 10:36:10.155007: val_loss -0.8894 
2023-10-27 10:36:10.155250: Pseudo dice [0.8794, 0.9089, 0.9717, 0.8121, 0.9453] 
2023-10-27 10:36:10.155470: Epoch time: 3.33 s 
2023-10-27 10:36:11.387559:  
2023-10-27 10:36:11.387929: Epoch 796 
2023-10-27 10:36:11.388193: Current learning rate: 0.00239 
2023-10-27 10:36:14.768936: train_loss -0.8916 
2023-10-27 10:36:14.769425: val_loss -0.8891 
2023-10-27 10:36:14.769731: Pseudo dice [0.8804, 0.9093, 0.9697, 0.8502, 0.9407] 
2023-10-27 10:36:14.770029: Epoch time: 3.38 s 
2023-10-27 10:36:15.862967:  
2023-10-27 10:36:15.863287: Epoch 797 
2023-10-27 10:36:15.863539: Current learning rate: 0.00238 
2023-10-27 10:36:19.209423: train_loss -0.8941 
2023-10-27 10:36:19.209786: val_loss -0.8913 
2023-10-27 10:36:19.210043: Pseudo dice [0.8861, 0.9058, 0.9693, 0.8697, 0.9473] 
2023-10-27 10:36:19.210261: Epoch time: 3.35 s 
2023-10-27 10:36:20.306592:  
2023-10-27 10:36:20.306909: Epoch 798 
2023-10-27 10:36:20.307147: Current learning rate: 0.00237 
2023-10-27 10:36:23.659165: train_loss -0.8923 
2023-10-27 10:36:23.659536: val_loss -0.8942 
2023-10-27 10:36:23.659790: Pseudo dice [0.8886, 0.9141, 0.9727, 0.8616, 0.9438] 
2023-10-27 10:36:23.660011: Epoch time: 3.35 s 
2023-10-27 10:36:24.759419:  
2023-10-27 10:36:24.759713: Epoch 799 
2023-10-27 10:36:24.759961: Current learning rate: 0.00236 
2023-10-27 10:36:28.126482: train_loss -0.8931 
2023-10-27 10:36:28.126836: val_loss -0.8961 
2023-10-27 10:36:28.127090: Pseudo dice [0.8891, 0.9095, 0.9713, 0.8682, 0.9447] 
2023-10-27 10:36:28.127308: Epoch time: 3.37 s 
2023-10-27 10:36:29.322606:  
2023-10-27 10:36:29.322894: Epoch 800 
2023-10-27 10:36:29.323124: Current learning rate: 0.00235 
2023-10-27 10:36:32.631232: train_loss -0.8903 
2023-10-27 10:36:32.631593: val_loss -0.8905 
2023-10-27 10:36:32.631845: Pseudo dice [0.8821, 0.9072, 0.9717, 0.8308, 0.9456] 
2023-10-27 10:36:32.632061: Epoch time: 3.31 s 
2023-10-27 10:36:33.848453:  
2023-10-27 10:36:33.848740: Epoch 801 
2023-10-27 10:36:33.848982: Current learning rate: 0.00234 
2023-10-27 10:36:37.186168: train_loss -0.8851 
2023-10-27 10:36:37.186541: val_loss -0.894 
2023-10-27 10:36:37.186788: Pseudo dice [0.8865, 0.9117, 0.9709, 0.8497, 0.9471] 
2023-10-27 10:36:37.187002: Epoch time: 3.34 s 
2023-10-27 10:36:38.277751:  
2023-10-27 10:36:38.278078: Epoch 802 
2023-10-27 10:36:38.278320: Current learning rate: 0.00233 
2023-10-27 10:36:41.599129: train_loss -0.8938 
2023-10-27 10:36:41.599628: val_loss -0.89 
2023-10-27 10:36:41.599904: Pseudo dice [0.8815, 0.9059, 0.9716, 0.8561, 0.9371] 
2023-10-27 10:36:41.600126: Epoch time: 3.32 s 
2023-10-27 10:36:42.695760:  
2023-10-27 10:36:42.696134: Epoch 803 
2023-10-27 10:36:42.696392: Current learning rate: 0.00232 
2023-10-27 10:36:46.024561: train_loss -0.8929 
2023-10-27 10:36:46.024915: val_loss -0.8934 
2023-10-27 10:36:46.025166: Pseudo dice [0.8884, 0.9079, 0.9723, 0.8277, 0.9472] 
2023-10-27 10:36:46.025385: Epoch time: 3.33 s 
2023-10-27 10:36:47.122291:  
2023-10-27 10:36:47.122651: Epoch 804 
2023-10-27 10:36:47.122887: Current learning rate: 0.00231 
2023-10-27 10:36:50.622886: train_loss -0.8935 
2023-10-27 10:36:50.623237: val_loss -0.8858 
2023-10-27 10:36:50.623476: Pseudo dice [0.886, 0.9075, 0.9694, 0.861, 0.9351] 
2023-10-27 10:36:50.623690: Epoch time: 3.5 s 
2023-10-27 10:36:51.732677:  
2023-10-27 10:36:51.732954: Epoch 805 
2023-10-27 10:36:51.733186: Current learning rate: 0.0023 
2023-10-27 10:36:55.057132: train_loss -0.889 
2023-10-27 10:36:55.057500: val_loss -0.8905 
2023-10-27 10:36:55.057756: Pseudo dice [0.8821, 0.9077, 0.9724, 0.8329, 0.9453] 
2023-10-27 10:36:55.057972: Epoch time: 3.33 s 
2023-10-27 10:36:56.158797:  
2023-10-27 10:36:56.159091: Epoch 806 
2023-10-27 10:36:56.159328: Current learning rate: 0.00229 
2023-10-27 10:36:59.497143: train_loss -0.8877 
2023-10-27 10:36:59.497508: val_loss -0.8903 
2023-10-27 10:36:59.497793: Pseudo dice [0.8904, 0.9097, 0.9718, 0.7787, 0.9476] 
2023-10-27 10:36:59.498020: Epoch time: 3.34 s 
2023-10-27 10:37:00.734566:  
2023-10-27 10:37:00.734943: Epoch 807 
2023-10-27 10:37:00.735175: Current learning rate: 0.00228 
2023-10-27 10:37:04.089432: train_loss -0.8853 
2023-10-27 10:37:04.089896: val_loss -0.8906 
2023-10-27 10:37:04.090240: Pseudo dice [0.8845, 0.9074, 0.9728, 0.786, 0.9459] 
2023-10-27 10:37:04.090564: Epoch time: 3.36 s 
2023-10-27 10:37:05.214987:  
2023-10-27 10:37:05.215314: Epoch 808 
2023-10-27 10:37:05.215559: Current learning rate: 0.00226 
2023-10-27 10:37:08.535982: train_loss -0.8939 
2023-10-27 10:37:08.536518: val_loss -0.8888 
2023-10-27 10:37:08.536767: Pseudo dice [0.8896, 0.9073, 0.9729, 0.7956, 0.9432] 
2023-10-27 10:37:08.536983: Epoch time: 3.32 s 
2023-10-27 10:37:09.641418:  
2023-10-27 10:37:09.641754: Epoch 809 
2023-10-27 10:37:09.641999: Current learning rate: 0.00225 
2023-10-27 10:37:12.982105: train_loss -0.8978 
2023-10-27 10:37:12.982669: val_loss -0.8924 
2023-10-27 10:37:12.983087: Pseudo dice [0.8907, 0.9127, 0.9733, 0.8053, 0.9469] 
2023-10-27 10:37:12.983425: Epoch time: 3.34 s 
2023-10-27 10:37:14.081393:  
2023-10-27 10:37:14.081728: Epoch 810 
2023-10-27 10:37:14.081976: Current learning rate: 0.00224 
2023-10-27 10:37:17.407884: train_loss -0.8916 
2023-10-27 10:37:17.408234: val_loss -0.8918 
2023-10-27 10:37:17.408475: Pseudo dice [0.8931, 0.9115, 0.9711, 0.814, 0.9471] 
2023-10-27 10:37:17.408687: Epoch time: 3.33 s 
2023-10-27 10:37:18.509631:  
2023-10-27 10:37:18.509915: Epoch 811 
2023-10-27 10:37:18.510160: Current learning rate: 0.00223 
2023-10-27 10:37:21.827726: train_loss -0.8927 
2023-10-27 10:37:21.828113: val_loss -0.8923 
2023-10-27 10:37:21.828377: Pseudo dice [0.8847, 0.915, 0.9709, 0.8155, 0.9444] 
2023-10-27 10:37:21.828631: Epoch time: 3.32 s 
2023-10-27 10:37:22.931946:  
2023-10-27 10:37:22.932409: Epoch 812 
2023-10-27 10:37:22.932659: Current learning rate: 0.00222 
2023-10-27 10:37:26.244081: train_loss -0.8909 
2023-10-27 10:37:26.244451: val_loss -0.8859 
2023-10-27 10:37:26.244708: Pseudo dice [0.8821, 0.9094, 0.9724, 0.8418, 0.9269] 
2023-10-27 10:37:26.244924: Epoch time: 3.31 s 
2023-10-27 10:37:27.485071:  
2023-10-27 10:37:27.485536: Epoch 813 
2023-10-27 10:37:27.485783: Current learning rate: 0.00221 
2023-10-27 10:37:30.831803: train_loss -0.894 
2023-10-27 10:37:30.832208: val_loss -0.8724 
2023-10-27 10:37:30.832477: Pseudo dice [0.8841, 0.9114, 0.9682, 0.8412, 0.9105] 
2023-10-27 10:37:30.832734: Epoch time: 3.35 s 
2023-10-27 10:37:31.933808:  
2023-10-27 10:37:31.934207: Epoch 814 
2023-10-27 10:37:31.934516: Current learning rate: 0.0022 
2023-10-27 10:37:35.279362: train_loss -0.891 
2023-10-27 10:37:35.279732: val_loss -0.8912 
2023-10-27 10:37:35.279985: Pseudo dice [0.8861, 0.9089, 0.9709, 0.8186, 0.9481] 
2023-10-27 10:37:35.280217: Epoch time: 3.35 s 
2023-10-27 10:37:36.380365:  
2023-10-27 10:37:36.380711: Epoch 815 
2023-10-27 10:37:36.380960: Current learning rate: 0.00219 
2023-10-27 10:37:39.704533: train_loss -0.8953 
2023-10-27 10:37:39.704906: val_loss -0.8906 
2023-10-27 10:37:39.705164: Pseudo dice [0.8836, 0.9059, 0.9723, 0.8068, 0.9453] 
2023-10-27 10:37:39.705400: Epoch time: 3.32 s 
2023-10-27 10:37:40.823549:  
2023-10-27 10:37:40.823848: Epoch 816 
2023-10-27 10:37:40.824088: Current learning rate: 0.00218 
2023-10-27 10:37:44.295698: train_loss -0.8899 
2023-10-27 10:37:44.296100: val_loss -0.8941 
2023-10-27 10:37:44.296356: Pseudo dice [0.8888, 0.9071, 0.9713, 0.8227, 0.948] 
2023-10-27 10:37:44.296584: Epoch time: 3.47 s 
2023-10-27 10:37:45.395034:  
2023-10-27 10:37:45.395369: Epoch 817 
2023-10-27 10:37:45.395625: Current learning rate: 0.00217 
2023-10-27 10:37:48.886142: train_loss -0.8777 
2023-10-27 10:37:48.886528: val_loss -0.8934 
2023-10-27 10:37:48.886792: Pseudo dice [0.8891, 0.91, 0.9738, 0.8409, 0.9493] 
2023-10-27 10:37:48.887024: Epoch time: 3.49 s 
2023-10-27 10:37:49.986742:  
2023-10-27 10:37:49.987029: Epoch 818 
2023-10-27 10:37:49.987267: Current learning rate: 0.00216 
2023-10-27 10:37:53.371605: train_loss -0.8891 
2023-10-27 10:37:53.372018: val_loss -0.8893 
2023-10-27 10:37:53.372291: Pseudo dice [0.8843, 0.9073, 0.9716, 0.8237, 0.9493] 
2023-10-27 10:37:53.372535: Epoch time: 3.39 s 
2023-10-27 10:37:54.613692:  
2023-10-27 10:37:54.614000: Epoch 819 
2023-10-27 10:37:54.614240: Current learning rate: 0.00215 
2023-10-27 10:37:58.005602: train_loss -0.8905 
2023-10-27 10:37:58.006003: val_loss -0.8874 
2023-10-27 10:37:58.006253: Pseudo dice [0.8883, 0.9105, 0.9722, 0.8011, 0.9478] 
2023-10-27 10:37:58.006487: Epoch time: 3.39 s 
2023-10-27 10:37:59.056617:  
2023-10-27 10:37:59.056903: Epoch 820 
2023-10-27 10:37:59.057137: Current learning rate: 0.00214 
2023-10-27 10:38:02.522242: train_loss -0.8899 
2023-10-27 10:38:02.522642: val_loss -0.8852 
2023-10-27 10:38:02.522910: Pseudo dice [0.8851, 0.9069, 0.9715, 0.8053, 0.9435] 
2023-10-27 10:38:02.523160: Epoch time: 3.47 s 
2023-10-27 10:38:03.579750:  
2023-10-27 10:38:03.580058: Epoch 821 
2023-10-27 10:38:03.580318: Current learning rate: 0.00213 
2023-10-27 10:38:06.915468: train_loss -0.8892 
2023-10-27 10:38:06.915845: val_loss -0.8905 
2023-10-27 10:38:06.916092: Pseudo dice [0.8866, 0.9082, 0.9717, 0.8166, 0.9498] 
2023-10-27 10:38:06.916319: Epoch time: 3.34 s 
2023-10-27 10:38:07.973982:  
2023-10-27 10:38:07.976499: Epoch 822 
2023-10-27 10:38:07.977110: Current learning rate: 0.00212 
2023-10-27 10:38:11.288349: train_loss -0.8906 
2023-10-27 10:38:11.288726: val_loss -0.8894 
2023-10-27 10:38:11.289008: Pseudo dice [0.8807, 0.9058, 0.9702, 0.8082, 0.9466] 
2023-10-27 10:38:11.289241: Epoch time: 3.31 s 
2023-10-27 10:38:12.346894:  
2023-10-27 10:38:12.347256: Epoch 823 
2023-10-27 10:38:12.347493: Current learning rate: 0.0021 
2023-10-27 10:38:15.683221: train_loss -0.8935 
2023-10-27 10:38:15.683688: val_loss -0.8921 
2023-10-27 10:38:15.684029: Pseudo dice [0.8785, 0.9082, 0.9708, 0.8339, 0.9426] 
2023-10-27 10:38:15.684271: Epoch time: 3.34 s 
2023-10-27 10:38:16.738368:  
2023-10-27 10:38:16.738806: Epoch 824 
2023-10-27 10:38:16.739044: Current learning rate: 0.00209 
2023-10-27 10:38:20.059899: train_loss -0.8904 
2023-10-27 10:38:20.060240: val_loss -0.8914 
2023-10-27 10:38:20.060484: Pseudo dice [0.8798, 0.9104, 0.9698, 0.838, 0.9439] 
2023-10-27 10:38:20.060711: Epoch time: 3.32 s 
2023-10-27 10:38:21.246052:  
2023-10-27 10:38:21.246351: Epoch 825 
2023-10-27 10:38:21.246584: Current learning rate: 0.00208 
2023-10-27 10:38:24.603534: train_loss -0.8949 
2023-10-27 10:38:24.604035: val_loss -0.8873 
2023-10-27 10:38:24.604287: Pseudo dice [0.8861, 0.9139, 0.9709, 0.8051, 0.9451] 
2023-10-27 10:38:24.604525: Epoch time: 3.36 s 
2023-10-27 10:38:25.659999:  
2023-10-27 10:38:25.660323: Epoch 826 
2023-10-27 10:38:25.660568: Current learning rate: 0.00207 
2023-10-27 10:38:28.985726: train_loss -0.8935 
2023-10-27 10:38:28.986075: val_loss -0.8848 
2023-10-27 10:38:28.986321: Pseudo dice [0.8804, 0.908, 0.972, 0.7802, 0.9451] 
2023-10-27 10:38:28.986548: Epoch time: 3.33 s 
2023-10-27 10:38:30.040090:  
2023-10-27 10:38:30.040600: Epoch 827 
2023-10-27 10:38:30.040837: Current learning rate: 0.00206 
2023-10-27 10:38:33.369440: train_loss -0.8976 
2023-10-27 10:38:33.369828: val_loss -0.8852 
2023-10-27 10:38:33.370085: Pseudo dice [0.8781, 0.9015, 0.9707, 0.8266, 0.9442] 
2023-10-27 10:38:33.370329: Epoch time: 3.33 s 
2023-10-27 10:38:34.423600:  
2023-10-27 10:38:34.423955: Epoch 828 
2023-10-27 10:38:34.424187: Current learning rate: 0.00205 
2023-10-27 10:38:37.766217: train_loss -0.8951 
2023-10-27 10:38:37.766577: val_loss -0.8864 
2023-10-27 10:38:37.766827: Pseudo dice [0.8804, 0.9042, 0.9711, 0.821, 0.9418] 
2023-10-27 10:38:37.767046: Epoch time: 3.34 s 
2023-10-27 10:38:38.819760:  
2023-10-27 10:38:38.820088: Epoch 829 
2023-10-27 10:38:38.820317: Current learning rate: 0.00204 
2023-10-27 10:38:42.191890: train_loss -0.9004 
2023-10-27 10:38:42.192307: val_loss -0.8939 
2023-10-27 10:38:42.192576: Pseudo dice [0.8897, 0.91, 0.9714, 0.842, 0.9466] 
2023-10-27 10:38:42.192832: Epoch time: 3.37 s 
2023-10-27 10:38:43.247989:  
2023-10-27 10:38:43.248422: Epoch 830 
2023-10-27 10:38:43.248666: Current learning rate: 0.00203 
2023-10-27 10:38:46.621256: train_loss -0.8989 
2023-10-27 10:38:46.621617: val_loss -0.8894 
2023-10-27 10:38:46.621865: Pseudo dice [0.8803, 0.9054, 0.971, 0.8167, 0.9444] 
2023-10-27 10:38:46.622088: Epoch time: 3.37 s 
2023-10-27 10:38:47.808959:  
2023-10-27 10:38:47.809259: Epoch 831 
2023-10-27 10:38:47.809483: Current learning rate: 0.00202 
2023-10-27 10:38:51.138379: train_loss -0.9018 
2023-10-27 10:38:51.138755: val_loss -0.8874 
2023-10-27 10:38:51.139020: Pseudo dice [0.8792, 0.904, 0.9718, 0.8101, 0.9424] 
2023-10-27 10:38:51.139249: Epoch time: 3.33 s 
2023-10-27 10:38:52.177175:  
2023-10-27 10:38:52.177510: Epoch 832 
2023-10-27 10:38:52.177732: Current learning rate: 0.00201 
2023-10-27 10:38:55.536714: train_loss -0.8915 
2023-10-27 10:38:55.537066: val_loss -0.8907 
2023-10-27 10:38:55.537320: Pseudo dice [0.8815, 0.9088, 0.9711, 0.8407, 0.9425] 
2023-10-27 10:38:55.537551: Epoch time: 3.36 s 
2023-10-27 10:38:56.586275:  
2023-10-27 10:38:56.586602: Epoch 833 
2023-10-27 10:38:56.586835: Current learning rate: 0.002 
2023-10-27 10:38:59.902732: train_loss -0.8976 
2023-10-27 10:38:59.903120: val_loss -0.8848 
2023-10-27 10:38:59.903384: Pseudo dice [0.8809, 0.9098, 0.9718, 0.8012, 0.9446] 
2023-10-27 10:38:59.903621: Epoch time: 3.32 s 
2023-10-27 10:39:00.943619:  
2023-10-27 10:39:00.943938: Epoch 834 
2023-10-27 10:39:00.944182: Current learning rate: 0.00199 
2023-10-27 10:39:04.286582: train_loss -0.9012 
2023-10-27 10:39:04.286980: val_loss -0.8853 
2023-10-27 10:39:04.287283: Pseudo dice [0.8846, 0.9058, 0.9714, 0.8348, 0.9378] 
2023-10-27 10:39:04.287521: Epoch time: 3.34 s 
2023-10-27 10:39:05.332005:  
2023-10-27 10:39:05.332323: Epoch 835 
2023-10-27 10:39:05.332566: Current learning rate: 0.00198 
2023-10-27 10:39:08.675314: train_loss -0.8961 
2023-10-27 10:39:08.675654: val_loss -0.8885 
2023-10-27 10:39:08.675915: Pseudo dice [0.8798, 0.9104, 0.9696, 0.8371, 0.946] 
2023-10-27 10:39:08.676129: Epoch time: 3.34 s 
2023-10-27 10:39:09.717703:  
2023-10-27 10:39:09.718027: Epoch 836 
2023-10-27 10:39:09.718262: Current learning rate: 0.00196 
2023-10-27 10:39:13.047110: train_loss -0.8976 
2023-10-27 10:39:13.047461: val_loss -0.8911 
2023-10-27 10:39:13.047723: Pseudo dice [0.8857, 0.9052, 0.97, 0.8446, 0.9445] 
2023-10-27 10:39:13.047958: Epoch time: 3.33 s 
2023-10-27 10:39:14.091583:  
2023-10-27 10:39:14.091868: Epoch 837 
2023-10-27 10:39:14.092084: Current learning rate: 0.00195 
2023-10-27 10:39:17.461774: train_loss -0.9008 
2023-10-27 10:39:17.462156: val_loss -0.8944 
2023-10-27 10:39:17.462420: Pseudo dice [0.8916, 0.9146, 0.9705, 0.8387, 0.947] 
2023-10-27 10:39:17.462649: Epoch time: 3.37 s 
2023-10-27 10:39:18.650732:  
2023-10-27 10:39:18.651180: Epoch 838 
2023-10-27 10:39:18.651414: Current learning rate: 0.00194 
2023-10-27 10:39:22.042575: train_loss -0.8994 
2023-10-27 10:39:22.042953: val_loss -0.8851 
2023-10-27 10:39:22.043229: Pseudo dice [0.889, 0.9053, 0.9707, 0.8449, 0.9429] 
2023-10-27 10:39:22.043456: Epoch time: 3.39 s 
2023-10-27 10:39:23.084991:  
2023-10-27 10:39:23.085352: Epoch 839 
2023-10-27 10:39:23.085605: Current learning rate: 0.00193 
2023-10-27 10:39:26.471843: train_loss -0.8979 
2023-10-27 10:39:26.472229: val_loss -0.8921 
2023-10-27 10:39:26.472486: Pseudo dice [0.8853, 0.9102, 0.972, 0.8289, 0.9444] 
2023-10-27 10:39:26.472724: Epoch time: 3.39 s 
2023-10-27 10:39:27.515859:  
2023-10-27 10:39:27.516150: Epoch 840 
2023-10-27 10:39:27.516379: Current learning rate: 0.00192 
2023-10-27 10:39:30.873954: train_loss -0.8927 
2023-10-27 10:39:30.874304: val_loss -0.8759 
2023-10-27 10:39:30.874567: Pseudo dice [0.8819, 0.9136, 0.9695, 0.8302, 0.9437] 
2023-10-27 10:39:30.874817: Epoch time: 3.36 s 
2023-10-27 10:39:31.920961:  
2023-10-27 10:39:31.921251: Epoch 841 
2023-10-27 10:39:31.921487: Current learning rate: 0.00191 
2023-10-27 10:39:35.285803: train_loss -0.9002 
2023-10-27 10:39:35.286190: val_loss -0.8852 
2023-10-27 10:39:35.286437: Pseudo dice [0.8866, 0.9129, 0.9697, 0.8285, 0.9439] 
2023-10-27 10:39:35.286704: Epoch time: 3.37 s 
2023-10-27 10:39:36.357969:  
2023-10-27 10:39:36.358258: Epoch 842 
2023-10-27 10:39:36.358525: Current learning rate: 0.0019 
2023-10-27 10:39:39.738075: train_loss -0.8958 
2023-10-27 10:39:39.738594: val_loss -0.8863 
2023-10-27 10:39:39.738954: Pseudo dice [0.8835, 0.9038, 0.9713, 0.8182, 0.942] 
2023-10-27 10:39:39.739275: Epoch time: 3.38 s 
2023-10-27 10:39:40.779001:  
2023-10-27 10:39:40.779290: Epoch 843 
2023-10-27 10:39:40.779535: Current learning rate: 0.00189 
2023-10-27 10:39:44.149538: train_loss -0.8969 
2023-10-27 10:39:44.149929: val_loss -0.893 
2023-10-27 10:39:44.150177: Pseudo dice [0.8906, 0.9117, 0.9722, 0.8517, 0.9465] 
2023-10-27 10:39:44.150422: Epoch time: 3.37 s 
2023-10-27 10:39:45.335461:  
2023-10-27 10:39:45.335801: Epoch 844 
2023-10-27 10:39:45.336048: Current learning rate: 0.00188 
2023-10-27 10:39:48.858807: train_loss -0.8947 
2023-10-27 10:39:48.859207: val_loss -0.8941 
2023-10-27 10:39:48.859465: Pseudo dice [0.8816, 0.9101, 0.9722, 0.8359, 0.9449] 
2023-10-27 10:39:48.859682: Epoch time: 3.52 s 
2023-10-27 10:39:49.898622:  
2023-10-27 10:39:49.898943: Epoch 845 
2023-10-27 10:39:49.899171: Current learning rate: 0.00187 
2023-10-27 10:39:53.281491: train_loss -0.9008 
2023-10-27 10:39:53.281847: val_loss -0.889 
2023-10-27 10:39:53.282104: Pseudo dice [0.8842, 0.9065, 0.9719, 0.8246, 0.9398] 
2023-10-27 10:39:53.282327: Epoch time: 3.38 s 
2023-10-27 10:39:54.327569:  
2023-10-27 10:39:54.327897: Epoch 846 
2023-10-27 10:39:54.328122: Current learning rate: 0.00186 
2023-10-27 10:39:57.716226: train_loss -0.8977 
2023-10-27 10:39:57.716657: val_loss -0.888 
2023-10-27 10:39:57.716918: Pseudo dice [0.8843, 0.9063, 0.9711, 0.8371, 0.9456] 
2023-10-27 10:39:57.717139: Epoch time: 3.39 s 
2023-10-27 10:39:58.760211:  
2023-10-27 10:39:58.760543: Epoch 847 
2023-10-27 10:39:58.760771: Current learning rate: 0.00185 
2023-10-27 10:40:02.139533: train_loss -0.9003 
2023-10-27 10:40:02.139931: val_loss -0.8873 
2023-10-27 10:40:02.140183: Pseudo dice [0.8835, 0.9106, 0.9702, 0.8071, 0.9396] 
2023-10-27 10:40:02.140411: Epoch time: 3.38 s 
2023-10-27 10:40:03.182586:  
2023-10-27 10:40:03.184588: Epoch 848 
2023-10-27 10:40:03.184953: Current learning rate: 0.00184 
2023-10-27 10:40:06.584367: train_loss -0.8977 
2023-10-27 10:40:06.584821: val_loss -0.8899 
2023-10-27 10:40:06.585098: Pseudo dice [0.8879, 0.9128, 0.9711, 0.835, 0.9412] 
2023-10-27 10:40:06.585343: Epoch time: 3.4 s 
2023-10-27 10:40:07.625171:  
2023-10-27 10:40:07.626705: Epoch 849 
2023-10-27 10:40:07.626942: Current learning rate: 0.00182 
2023-10-27 10:40:10.983423: train_loss -0.9009 
2023-10-27 10:40:10.983793: val_loss -0.8654 
2023-10-27 10:40:10.984046: Pseudo dice [0.8838, 0.9134, 0.9689, 0.8629, 0.8973] 
2023-10-27 10:40:10.984270: Epoch time: 3.36 s 
2023-10-27 10:40:12.266820:  
2023-10-27 10:40:12.267146: Epoch 850 
2023-10-27 10:40:12.267378: Current learning rate: 0.00181 
2023-10-27 10:40:15.646215: train_loss -0.8877 
2023-10-27 10:40:15.646589: val_loss -0.8925 
2023-10-27 10:40:15.646846: Pseudo dice [0.8889, 0.9174, 0.9714, 0.8464, 0.9457] 
2023-10-27 10:40:15.647074: Epoch time: 3.38 s 
2023-10-27 10:40:16.677899:  
2023-10-27 10:40:16.678242: Epoch 851 
2023-10-27 10:40:16.678482: Current learning rate: 0.0018 
2023-10-27 10:40:20.075572: train_loss -0.9003 
2023-10-27 10:40:20.075940: val_loss -0.8905 
2023-10-27 10:40:20.076205: Pseudo dice [0.8896, 0.9099, 0.9707, 0.846, 0.9488] 
2023-10-27 10:40:20.076452: Epoch time: 3.4 s 
2023-10-27 10:40:21.110928:  
2023-10-27 10:40:21.111237: Epoch 852 
2023-10-27 10:40:21.111476: Current learning rate: 0.00179 
2023-10-27 10:40:24.521794: train_loss -0.8977 
2023-10-27 10:40:24.522294: val_loss -0.8946 
2023-10-27 10:40:24.522580: Pseudo dice [0.8891, 0.9126, 0.9721, 0.8413, 0.944] 
2023-10-27 10:40:24.522804: Epoch time: 3.41 s 
2023-10-27 10:40:25.560439:  
2023-10-27 10:40:25.560757: Epoch 853 
2023-10-27 10:40:25.561002: Current learning rate: 0.00178 
2023-10-27 10:40:29.026360: train_loss -0.8986 
2023-10-27 10:40:29.027871: val_loss -0.8917 
2023-10-27 10:40:29.028246: Pseudo dice [0.8824, 0.9093, 0.9721, 0.826, 0.9479] 
2023-10-27 10:40:29.028471: Epoch time: 3.47 s 
2023-10-27 10:40:30.099708:  
2023-10-27 10:40:30.100044: Epoch 854 
2023-10-27 10:40:30.100277: Current learning rate: 0.00177 
2023-10-27 10:40:35.876125: train_loss -0.8945 
2023-10-27 10:40:35.877569: val_loss -0.8902 
2023-10-27 10:40:35.877833: Pseudo dice [0.8901, 0.9133, 0.9712, 0.8162, 0.942] 
2023-10-27 10:40:35.878337: Epoch time: 5.78 s 
2023-10-27 10:40:36.934449:  
2023-10-27 10:40:36.934865: Epoch 855 
2023-10-27 10:40:37.054368: Current learning rate: 0.00176 
2023-10-27 10:40:40.648125: train_loss -0.8969 
2023-10-27 10:40:40.648673: val_loss -0.89 
2023-10-27 10:40:40.648921: Pseudo dice [0.8816, 0.9077, 0.9719, 0.8241, 0.9463] 
2023-10-27 10:40:40.649821: Epoch time: 3.71 s 
2023-10-27 10:40:41.700998:  
2023-10-27 10:40:41.701347: Epoch 856 
2023-10-27 10:40:41.701572: Current learning rate: 0.00175 
2023-10-27 10:40:45.106752: train_loss -0.8913 
2023-10-27 10:40:45.107114: val_loss -0.8896 
2023-10-27 10:40:45.107364: Pseudo dice [0.8891, 0.9118, 0.9708, 0.8283, 0.9446] 
2023-10-27 10:40:45.107588: Epoch time: 3.41 s 
2023-10-27 10:40:47.250891:  
2023-10-27 10:40:47.251235: Epoch 857 
2023-10-27 10:40:47.251476: Current learning rate: 0.00174 
2023-10-27 10:40:51.697360: train_loss -0.8934 
2023-10-27 10:40:51.697769: val_loss -0.8764 
2023-10-27 10:40:51.698025: Pseudo dice [0.8812, 0.9043, 0.9691, 0.8173, 0.9395] 
2023-10-27 10:40:51.698257: Epoch time: 4.45 s 
2023-10-27 10:40:52.733736:  
2023-10-27 10:40:52.734129: Epoch 858 
2023-10-27 10:40:52.734385: Current learning rate: 0.00173 
2023-10-27 10:40:56.557595: train_loss -0.881 
2023-10-27 10:40:56.565013: val_loss -0.8845 
2023-10-27 10:40:56.565272: Pseudo dice [0.8844, 0.909, 0.9695, 0.8524, 0.9402] 
2023-10-27 10:40:56.565505: Epoch time: 3.82 s 
2023-10-27 10:40:57.668622:  
2023-10-27 10:40:57.668900: Epoch 859 
2023-10-27 10:40:57.669143: Current learning rate: 0.00172 
2023-10-27 10:41:01.165563: train_loss -0.893 
2023-10-27 10:41:01.165913: val_loss -0.8817 
2023-10-27 10:41:01.166166: Pseudo dice [0.8872, 0.9034, 0.968, 0.8377, 0.9383] 
2023-10-27 10:41:01.166393: Epoch time: 3.5 s 
2023-10-27 10:41:02.198575:  
2023-10-27 10:41:02.198860: Epoch 860 
2023-10-27 10:41:02.199085: Current learning rate: 0.0017 
2023-10-27 10:41:05.711781: train_loss -0.8977 
2023-10-27 10:41:05.712146: val_loss -0.8875 
2023-10-27 10:41:05.712395: Pseudo dice [0.8879, 0.9082, 0.9714, 0.8487, 0.942] 
2023-10-27 10:41:05.712636: Epoch time: 3.51 s 
2023-10-27 10:41:06.750594:  
2023-10-27 10:41:06.750941: Epoch 861 
2023-10-27 10:41:06.751163: Current learning rate: 0.00169 
2023-10-27 10:41:10.227212: train_loss -0.9007 
2023-10-27 10:41:10.227587: val_loss -0.8841 
2023-10-27 10:41:10.227853: Pseudo dice [0.8829, 0.8999, 0.9694, 0.8527, 0.9447] 
2023-10-27 10:41:10.228085: Epoch time: 3.48 s 
2023-10-27 10:41:11.257667:  
2023-10-27 10:41:11.257960: Epoch 862 
2023-10-27 10:41:11.258188: Current learning rate: 0.00168 
2023-10-27 10:41:14.791478: train_loss -0.8972 
2023-10-27 10:41:14.791971: val_loss -0.8684 
2023-10-27 10:41:14.792298: Pseudo dice [0.8892, 0.9107, 0.969, 0.7892, 0.9366] 
2023-10-27 10:41:14.792691: Epoch time: 3.53 s 
2023-10-27 10:41:15.831317:  
2023-10-27 10:41:15.831643: Epoch 863 
2023-10-27 10:41:15.831877: Current learning rate: 0.00167 
2023-10-27 10:41:19.240375: train_loss -0.8935 
2023-10-27 10:41:19.240754: val_loss -0.8857 
2023-10-27 10:41:19.240996: Pseudo dice [0.8887, 0.9102, 0.9698, 0.8537, 0.9371] 
2023-10-27 10:41:19.241229: Epoch time: 3.41 s 
2023-10-27 10:41:21.030034:  
2023-10-27 10:41:21.030328: Epoch 864 
2023-10-27 10:41:21.030693: Current learning rate: 0.00166 
2023-10-27 10:41:24.370876: train_loss -0.8948 
2023-10-27 10:41:24.371223: val_loss -0.8936 
2023-10-27 10:41:24.371478: Pseudo dice [0.885, 0.9079, 0.9716, 0.8555, 0.9408] 
2023-10-27 10:41:24.371705: Epoch time: 3.34 s 
2023-10-27 10:41:25.406113:  
2023-10-27 10:41:25.406430: Epoch 865 
2023-10-27 10:41:25.406662: Current learning rate: 0.00165 
2023-10-27 10:41:28.745717: train_loss -0.901 
2023-10-27 10:41:28.746062: val_loss -0.8929 
2023-10-27 10:41:28.746314: Pseudo dice [0.884, 0.9107, 0.9707, 0.8576, 0.9416] 
2023-10-27 10:41:28.746540: Epoch time: 3.34 s 
2023-10-27 10:41:29.775506:  
2023-10-27 10:41:29.775848: Epoch 866 
2023-10-27 10:41:29.776076: Current learning rate: 0.00164 
2023-10-27 10:41:33.104821: train_loss -0.8977 
2023-10-27 10:41:33.105169: val_loss -0.8909 
2023-10-27 10:41:33.105429: Pseudo dice [0.8845, 0.9121, 0.9722, 0.8342, 0.9451] 
2023-10-27 10:41:33.105667: Epoch time: 3.33 s 
2023-10-27 10:41:34.134663:  
2023-10-27 10:41:34.134952: Epoch 867 
2023-10-27 10:41:34.135182: Current learning rate: 0.00163 
2023-10-27 10:41:37.460979: train_loss -0.9017 
2023-10-27 10:41:37.461351: val_loss -0.8887 
2023-10-27 10:41:37.461611: Pseudo dice [0.8876, 0.9066, 0.9702, 0.8495, 0.9435] 
2023-10-27 10:41:37.461837: Epoch time: 3.33 s 
2023-10-27 10:41:38.492840:  
2023-10-27 10:41:38.493135: Epoch 868 
2023-10-27 10:41:38.493360: Current learning rate: 0.00162 
2023-10-27 10:41:41.763926: train_loss -0.8979 
2023-10-27 10:41:41.764306: val_loss -0.8865 
2023-10-27 10:41:41.764565: Pseudo dice [0.8841, 0.9098, 0.97, 0.8342, 0.9466] 
2023-10-27 10:41:41.764787: Epoch time: 3.27 s 
2023-10-27 10:41:42.801327:  
2023-10-27 10:41:42.801674: Epoch 869 
2023-10-27 10:41:42.801932: Current learning rate: 0.00161 
2023-10-27 10:41:46.167945: train_loss -0.897 
2023-10-27 10:41:46.168348: val_loss -0.8669 
2023-10-27 10:41:46.168605: Pseudo dice [0.8798, 0.9067, 0.971, 0.5508, 0.9436] 
2023-10-27 10:41:46.168837: Epoch time: 3.37 s 
2023-10-27 10:41:47.343098:  
2023-10-27 10:41:47.343394: Epoch 870 
2023-10-27 10:41:47.343638: Current learning rate: 0.00159 
2023-10-27 10:41:50.702253: train_loss -0.8995 
2023-10-27 10:41:50.702602: val_loss -0.8868 
2023-10-27 10:41:50.702864: Pseudo dice [0.8865, 0.9063, 0.9714, 0.802, 0.9446] 
2023-10-27 10:41:50.703093: Epoch time: 3.36 s 
2023-10-27 10:41:51.735223:  
2023-10-27 10:41:51.735519: Epoch 871 
2023-10-27 10:41:51.735757: Current learning rate: 0.00158 
2023-10-27 10:41:55.076617: train_loss -0.8972 
2023-10-27 10:41:55.076956: val_loss -0.8861 
2023-10-27 10:41:55.077205: Pseudo dice [0.8831, 0.9051, 0.9702, 0.832, 0.9446] 
2023-10-27 10:41:55.077462: Epoch time: 3.34 s 
2023-10-27 10:41:56.110707:  
2023-10-27 10:41:56.111000: Epoch 872 
2023-10-27 10:41:56.111230: Current learning rate: 0.00157 
2023-10-27 10:41:59.431411: train_loss -0.8947 
2023-10-27 10:41:59.431757: val_loss -0.89 
2023-10-27 10:41:59.432015: Pseudo dice [0.8876, 0.9151, 0.9712, 0.857, 0.9485] 
2023-10-27 10:41:59.432232: Epoch time: 3.32 s 
2023-10-27 10:42:00.464102:  
2023-10-27 10:42:00.464448: Epoch 873 
2023-10-27 10:42:00.464681: Current learning rate: 0.00156 
2023-10-27 10:42:03.840769: train_loss -0.9021 
2023-10-27 10:42:03.841197: val_loss -0.8785 
2023-10-27 10:42:03.841470: Pseudo dice [0.8748, 0.8981, 0.9684, 0.8143, 0.9363] 
2023-10-27 10:42:03.841701: Epoch time: 3.38 s 
2023-10-27 10:42:04.878805:  
2023-10-27 10:42:04.879163: Epoch 874 
2023-10-27 10:42:04.879395: Current learning rate: 0.00155 
2023-10-27 10:42:08.246810: train_loss -0.9009 
2023-10-27 10:42:08.247156: val_loss -0.8778 
2023-10-27 10:42:08.247413: Pseudo dice [0.8855, 0.9083, 0.9727, 0.7005, 0.9419] 
2023-10-27 10:42:08.247641: Epoch time: 3.37 s 
2023-10-27 10:42:09.283771:  
2023-10-27 10:42:09.284087: Epoch 875 
2023-10-27 10:42:09.284309: Current learning rate: 0.00154 
2023-10-27 10:42:12.740231: train_loss -0.8986 
2023-10-27 10:42:12.740617: val_loss -0.8861 
2023-10-27 10:42:12.740868: Pseudo dice [0.8847, 0.9043, 0.9705, 0.7894, 0.9448] 
2023-10-27 10:42:12.741118: Epoch time: 3.46 s 
2023-10-27 10:42:13.779736:  
2023-10-27 10:42:13.780021: Epoch 876 
2023-10-27 10:42:13.780242: Current learning rate: 0.00153 
2023-10-27 10:42:17.211259: train_loss -0.8942 
2023-10-27 10:42:17.211785: val_loss -0.8712 
2023-10-27 10:42:17.212071: Pseudo dice [0.8814, 0.9049, 0.9729, 0.796, 0.9373] 
2023-10-27 10:42:17.212292: Epoch time: 3.43 s 
2023-10-27 10:42:18.395536:  
2023-10-27 10:42:18.395883: Epoch 877 
2023-10-27 10:42:18.396108: Current learning rate: 0.00152 
2023-10-27 10:42:21.806810: train_loss -0.8975 
2023-10-27 10:42:21.807276: val_loss -0.8841 
2023-10-27 10:42:21.807598: Pseudo dice [0.8859, 0.9106, 0.9708, 0.8038, 0.9438] 
2023-10-27 10:42:21.807883: Epoch time: 3.41 s 
2023-10-27 10:42:22.839230:  
2023-10-27 10:42:22.839568: Epoch 878 
2023-10-27 10:42:22.839822: Current learning rate: 0.00151 
2023-10-27 10:42:26.220733: train_loss -0.8962 
2023-10-27 10:42:26.221073: val_loss -0.8847 
2023-10-27 10:42:26.221324: Pseudo dice [0.8839, 0.9055, 0.9731, 0.6748, 0.9464] 
2023-10-27 10:42:26.221567: Epoch time: 3.38 s 
2023-10-27 10:42:27.257555:  
2023-10-27 10:42:27.257833: Epoch 879 
2023-10-27 10:42:27.258061: Current learning rate: 0.00149 
2023-10-27 10:42:30.637784: train_loss -0.8939 
2023-10-27 10:42:30.638212: val_loss -0.8862 
2023-10-27 10:42:30.638493: Pseudo dice [0.8746, 0.9117, 0.973, 0.8201, 0.9234] 
2023-10-27 10:42:30.638731: Epoch time: 3.38 s 
2023-10-27 10:42:31.671412:  
2023-10-27 10:42:31.671717: Epoch 880 
2023-10-27 10:42:31.671960: Current learning rate: 0.00148 
2023-10-27 10:42:35.111253: train_loss -0.8906 
2023-10-27 10:42:35.111613: val_loss -0.8892 
2023-10-27 10:42:35.111879: Pseudo dice [0.8856, 0.9092, 0.971, 0.7869, 0.9447] 
2023-10-27 10:42:35.112110: Epoch time: 3.44 s 
2023-10-27 10:42:36.151401:  
2023-10-27 10:42:36.151688: Epoch 881 
2023-10-27 10:42:36.151940: Current learning rate: 0.00147 
2023-10-27 10:42:39.557583: train_loss -0.9011 
2023-10-27 10:42:39.557932: val_loss -0.889 
2023-10-27 10:42:39.558195: Pseudo dice [0.8807, 0.9051, 0.9705, 0.8207, 0.9467] 
2023-10-27 10:42:39.558418: Epoch time: 3.41 s 
2023-10-27 10:42:40.593236:  
2023-10-27 10:42:40.593529: Epoch 882 
2023-10-27 10:42:40.593765: Current learning rate: 0.00146 
2023-10-27 10:42:43.986084: train_loss -0.9021 
2023-10-27 10:42:43.986523: val_loss -0.889 
2023-10-27 10:42:43.986845: Pseudo dice [0.8852, 0.9067, 0.9716, 0.8193, 0.9468] 
2023-10-27 10:42:43.987091: Epoch time: 3.39 s 
2023-10-27 10:42:45.164927:  
2023-10-27 10:42:45.165209: Epoch 883 
2023-10-27 10:42:45.165457: Current learning rate: 0.00145 
2023-10-27 10:42:48.615350: train_loss -0.9004 
2023-10-27 10:42:48.615948: val_loss -0.8925 
2023-10-27 10:42:48.616325: Pseudo dice [0.8802, 0.9078, 0.9725, 0.8514, 0.9418] 
2023-10-27 10:42:48.616601: Epoch time: 3.45 s 
2023-10-27 10:42:49.656090:  
2023-10-27 10:42:49.656371: Epoch 884 
2023-10-27 10:42:49.656626: Current learning rate: 0.00144 
2023-10-27 10:42:53.071525: train_loss -0.8993 
2023-10-27 10:42:53.071889: val_loss -0.8893 
2023-10-27 10:42:53.072181: Pseudo dice [0.8923, 0.9122, 0.97, 0.8141, 0.9471] 
2023-10-27 10:42:53.072527: Epoch time: 3.42 s 
2023-10-27 10:42:54.109503:  
2023-10-27 10:42:54.109794: Epoch 885 
2023-10-27 10:42:54.110022: Current learning rate: 0.00143 
2023-10-27 10:42:57.485509: train_loss -0.9036 
2023-10-27 10:42:57.485867: val_loss -0.8871 
2023-10-27 10:42:57.486138: Pseudo dice [0.8833, 0.9086, 0.9708, 0.7298, 0.9472] 
2023-10-27 10:42:57.486366: Epoch time: 3.38 s 
2023-10-27 10:42:58.523983:  
2023-10-27 10:42:58.524318: Epoch 886 
2023-10-27 10:42:58.524552: Current learning rate: 0.00142 
2023-10-27 10:43:01.878500: train_loss -0.8909 
2023-10-27 10:43:01.878852: val_loss -0.8887 
2023-10-27 10:43:01.879096: Pseudo dice [0.8817, 0.9102, 0.9711, 0.8103, 0.9375] 
2023-10-27 10:43:01.879331: Epoch time: 3.36 s 
2023-10-27 10:43:02.912017:  
2023-10-27 10:43:02.912326: Epoch 887 
2023-10-27 10:43:02.912590: Current learning rate: 0.00141 
2023-10-27 10:43:06.328976: train_loss -0.8971 
2023-10-27 10:43:06.329339: val_loss -0.888 
2023-10-27 10:43:06.329586: Pseudo dice [0.8854, 0.9092, 0.9694, 0.8372, 0.9397] 
2023-10-27 10:43:06.329802: Epoch time: 3.42 s 
2023-10-27 10:43:07.363388:  
2023-10-27 10:43:07.363679: Epoch 888 
2023-10-27 10:43:07.363911: Current learning rate: 0.00139 
2023-10-27 10:43:10.730433: train_loss -0.8988 
2023-10-27 10:43:10.730776: val_loss -0.8699 
2023-10-27 10:43:10.731025: Pseudo dice [0.8779, 0.9061, 0.9686, 0.8386, 0.907] 
2023-10-27 10:43:10.731250: Epoch time: 3.37 s 
2023-10-27 10:43:11.764276:  
2023-10-27 10:43:11.764592: Epoch 889 
2023-10-27 10:43:11.764807: Current learning rate: 0.00138 
2023-10-27 10:43:15.152423: train_loss -0.8988 
2023-10-27 10:43:15.152778: val_loss -0.8895 
2023-10-27 10:43:15.153123: Pseudo dice [0.8943, 0.916, 0.9705, 0.8339, 0.9466] 
2023-10-27 10:43:15.153362: Epoch time: 3.39 s 
2023-10-27 10:43:16.334083:  
2023-10-27 10:43:16.334518: Epoch 890 
2023-10-27 10:43:16.334751: Current learning rate: 0.00137 
2023-10-27 10:43:19.716743: train_loss -0.9034 
2023-10-27 10:43:19.717116: val_loss -0.888 
2023-10-27 10:43:19.717373: Pseudo dice [0.881, 0.9078, 0.971, 0.8358, 0.9379] 
2023-10-27 10:43:19.717611: Epoch time: 3.38 s 
2023-10-27 10:43:20.753587:  
2023-10-27 10:43:20.753879: Epoch 891 
2023-10-27 10:43:20.754103: Current learning rate: 0.00136 
2023-10-27 10:43:24.152527: train_loss -0.9051 
2023-10-27 10:43:24.152899: val_loss -0.8843 
2023-10-27 10:43:24.153149: Pseudo dice [0.8834, 0.9107, 0.9692, 0.8195, 0.9369] 
2023-10-27 10:43:24.153366: Epoch time: 3.4 s 
2023-10-27 10:43:25.185738:  
2023-10-27 10:43:25.186058: Epoch 892 
2023-10-27 10:43:25.186290: Current learning rate: 0.00135 
2023-10-27 10:43:28.561099: train_loss -0.9019 
2023-10-27 10:43:28.561470: val_loss -0.8861 
2023-10-27 10:43:28.561727: Pseudo dice [0.889, 0.9077, 0.9716, 0.8164, 0.9427] 
2023-10-27 10:43:28.561948: Epoch time: 3.38 s 
2023-10-27 10:43:29.600749:  
2023-10-27 10:43:29.601128: Epoch 893 
2023-10-27 10:43:29.601354: Current learning rate: 0.00134 
2023-10-27 10:43:32.974348: train_loss -0.9002 
2023-10-27 10:43:32.974704: val_loss -0.8927 
2023-10-27 10:43:32.974962: Pseudo dice [0.8852, 0.9096, 0.97, 0.8395, 0.9456] 
2023-10-27 10:43:32.975188: Epoch time: 3.37 s 
2023-10-27 10:43:34.008673:  
2023-10-27 10:43:34.008960: Epoch 894 
2023-10-27 10:43:34.009184: Current learning rate: 0.00133 
2023-10-27 10:43:37.400427: train_loss -0.8985 
2023-10-27 10:43:37.401044: val_loss -0.8827 
2023-10-27 10:43:37.401300: Pseudo dice [0.885, 0.9097, 0.9715, 0.8097, 0.9395] 
2023-10-27 10:43:37.401544: Epoch time: 3.39 s 
2023-10-27 10:43:38.435592:  
2023-10-27 10:43:38.435932: Epoch 895 
2023-10-27 10:43:38.436164: Current learning rate: 0.00132 
2023-10-27 10:43:41.818743: train_loss -0.9052 
2023-10-27 10:43:41.819085: val_loss -0.8917 
2023-10-27 10:43:41.819341: Pseudo dice [0.884, 0.9115, 0.9715, 0.8156, 0.942] 
2023-10-27 10:43:41.819558: Epoch time: 3.38 s 
2023-10-27 10:43:42.858020:  
2023-10-27 10:43:42.858371: Epoch 896 
2023-10-27 10:43:42.858640: Current learning rate: 0.0013 
2023-10-27 10:43:46.255676: train_loss -0.8977 
2023-10-27 10:43:46.256024: val_loss -0.8823 
2023-10-27 10:43:46.256277: Pseudo dice [0.8888, 0.9102, 0.9698, 0.8237, 0.9324] 
2023-10-27 10:43:46.256523: Epoch time: 3.4 s 
2023-10-27 10:43:47.434431:  
2023-10-27 10:43:47.434770: Epoch 897 
2023-10-27 10:43:47.434999: Current learning rate: 0.00129 
2023-10-27 10:43:50.814860: train_loss -0.9002 
2023-10-27 10:43:50.815234: val_loss -0.8875 
2023-10-27 10:43:50.815489: Pseudo dice [0.8753, 0.9054, 0.9698, 0.8164, 0.9387] 
2023-10-27 10:43:50.815704: Epoch time: 3.38 s 
2023-10-27 10:43:51.851599:  
2023-10-27 10:43:51.851997: Epoch 898 
2023-10-27 10:43:51.852289: Current learning rate: 0.00128 
2023-10-27 10:43:55.242246: train_loss -0.8966 
2023-10-27 10:43:55.242614: val_loss -0.8905 
2023-10-27 10:43:55.242877: Pseudo dice [0.8805, 0.9121, 0.9709, 0.8389, 0.941] 
2023-10-27 10:43:55.243102: Epoch time: 3.39 s 
2023-10-27 10:43:56.278982:  
2023-10-27 10:43:56.279289: Epoch 899 
2023-10-27 10:43:56.279538: Current learning rate: 0.00127 
2023-10-27 10:43:59.707458: train_loss -0.9051 
2023-10-27 10:43:59.707981: val_loss -0.8902 
2023-10-27 10:43:59.708247: Pseudo dice [0.8865, 0.9097, 0.9702, 0.8516, 0.9443] 
2023-10-27 10:43:59.708463: Epoch time: 3.43 s 
2023-10-27 10:44:00.972862:  
2023-10-27 10:44:00.973152: Epoch 900 
2023-10-27 10:44:00.973391: Current learning rate: 0.00126 
2023-10-27 10:44:04.400477: train_loss -0.8995 
2023-10-27 10:44:04.400901: val_loss -0.8874 
2023-10-27 10:44:04.401149: Pseudo dice [0.8876, 0.9098, 0.9709, 0.8389, 0.9408] 
2023-10-27 10:44:04.401366: Epoch time: 3.43 s 
2023-10-27 10:44:05.432758:  
2023-10-27 10:44:05.433081: Epoch 901 
2023-10-27 10:44:05.433312: Current learning rate: 0.00125 
2023-10-27 10:44:08.822966: train_loss -0.8976 
2023-10-27 10:44:08.823311: val_loss -0.883 
2023-10-27 10:44:08.823639: Pseudo dice [0.8879, 0.909, 0.9696, 0.8323, 0.935] 
2023-10-27 10:44:08.823862: Epoch time: 3.39 s 
2023-10-27 10:44:09.853848:  
2023-10-27 10:44:09.854131: Epoch 902 
2023-10-27 10:44:09.854366: Current learning rate: 0.00124 
2023-10-27 10:44:13.308609: train_loss -0.9034 
2023-10-27 10:44:13.308985: val_loss -0.8865 
2023-10-27 10:44:13.309230: Pseudo dice [0.877, 0.9064, 0.9703, 0.8316, 0.9396] 
2023-10-27 10:44:13.309455: Epoch time: 3.46 s 
2023-10-27 10:44:14.474457:  
2023-10-27 10:44:14.474751: Epoch 903 
2023-10-27 10:44:14.474980: Current learning rate: 0.00122 
2023-10-27 10:44:18.003941: train_loss -0.9039 
2023-10-27 10:44:18.004301: val_loss -0.8864 
2023-10-27 10:44:18.004570: Pseudo dice [0.887, 0.9122, 0.9709, 0.8133, 0.9363] 
2023-10-27 10:44:18.004794: Epoch time: 3.53 s 
2023-10-27 10:44:19.044599:  
2023-10-27 10:44:19.044878: Epoch 904 
2023-10-27 10:44:19.045121: Current learning rate: 0.00121 
2023-10-27 10:44:22.437212: train_loss -0.903 
2023-10-27 10:44:22.437592: val_loss -0.8887 
2023-10-27 10:44:22.437853: Pseudo dice [0.8817, 0.9099, 0.9717, 0.8312, 0.9385] 
2023-10-27 10:44:22.438079: Epoch time: 3.39 s 
2023-10-27 10:44:23.483126:  
2023-10-27 10:44:23.483461: Epoch 905 
2023-10-27 10:44:23.483690: Current learning rate: 0.0012 
2023-10-27 10:44:26.879885: train_loss -0.9007 
2023-10-27 10:44:26.880240: val_loss -0.8851 
2023-10-27 10:44:26.880506: Pseudo dice [0.8858, 0.9078, 0.9708, 0.8532, 0.9372] 
2023-10-27 10:44:26.880744: Epoch time: 3.4 s 
2023-10-27 10:44:27.916053:  
2023-10-27 10:44:27.916342: Epoch 906 
2023-10-27 10:44:27.916574: Current learning rate: 0.00119 
2023-10-27 10:44:31.296084: train_loss -0.9044 
2023-10-27 10:44:31.296663: val_loss -0.8898 
2023-10-27 10:44:31.296940: Pseudo dice [0.8857, 0.9075, 0.9715, 0.8408, 0.9402] 
2023-10-27 10:44:31.297157: Epoch time: 3.38 s 
2023-10-27 10:44:32.339800:  
2023-10-27 10:44:32.340117: Epoch 907 
2023-10-27 10:44:32.340342: Current learning rate: 0.00118 
2023-10-27 10:44:35.771151: train_loss -0.8999 
2023-10-27 10:44:35.771497: val_loss -0.8827 
2023-10-27 10:44:35.771750: Pseudo dice [0.8856, 0.9091, 0.9705, 0.824, 0.9347] 
2023-10-27 10:44:35.771968: Epoch time: 3.43 s 
2023-10-27 10:44:36.805826:  
2023-10-27 10:44:36.806122: Epoch 908 
2023-10-27 10:44:36.806364: Current learning rate: 0.00117 
2023-10-27 10:44:40.334907: train_loss -0.9008 
2023-10-27 10:44:40.335327: val_loss -0.8835 
2023-10-27 10:44:40.335597: Pseudo dice [0.8766, 0.9034, 0.9711, 0.8374, 0.9452] 
2023-10-27 10:44:40.335872: Epoch time: 3.53 s 
2023-10-27 10:44:41.510693:  
2023-10-27 10:44:41.511019: Epoch 909 
2023-10-27 10:44:41.511242: Current learning rate: 0.00116 
2023-10-27 10:44:44.974378: train_loss -0.9048 
2023-10-27 10:44:44.974745: val_loss -0.8945 
2023-10-27 10:44:44.975002: Pseudo dice [0.8853, 0.9089, 0.9716, 0.8625, 0.9435] 
2023-10-27 10:44:44.975231: Epoch time: 3.46 s 
2023-10-27 10:44:46.015909:  
2023-10-27 10:44:46.016272: Epoch 910 
2023-10-27 10:44:46.016519: Current learning rate: 0.00115 
2023-10-27 10:44:49.571034: train_loss -0.9033 
2023-10-27 10:44:49.571826: val_loss -0.889 
2023-10-27 10:44:49.572077: Pseudo dice [0.8829, 0.9068, 0.971, 0.852, 0.9426] 
2023-10-27 10:44:49.572319: Epoch time: 3.56 s 
2023-10-27 10:44:50.616319:  
2023-10-27 10:44:50.616608: Epoch 911 
2023-10-27 10:44:50.616831: Current learning rate: 0.00113 
2023-10-27 10:44:54.072515: train_loss -0.9024 
2023-10-27 10:44:54.072870: val_loss -0.8896 
2023-10-27 10:44:54.073115: Pseudo dice [0.8845, 0.9069, 0.9712, 0.8404, 0.9401] 
2023-10-27 10:44:54.073362: Epoch time: 3.46 s 
2023-10-27 10:44:55.104788:  
2023-10-27 10:44:55.105129: Epoch 912 
2023-10-27 10:44:55.105364: Current learning rate: 0.00112 
2023-10-27 10:44:58.474263: train_loss -0.9023 
2023-10-27 10:44:58.474658: val_loss -0.8875 
2023-10-27 10:44:58.474929: Pseudo dice [0.8771, 0.9089, 0.9702, 0.8624, 0.9378] 
2023-10-27 10:44:58.475156: Epoch time: 3.37 s 
2023-10-27 10:44:59.515105:  
2023-10-27 10:44:59.515429: Epoch 913 
2023-10-27 10:44:59.515678: Current learning rate: 0.00111 
2023-10-27 10:45:02.934538: train_loss -0.9048 
2023-10-27 10:45:02.934930: val_loss -0.8891 
2023-10-27 10:45:02.935193: Pseudo dice [0.8846, 0.9101, 0.9708, 0.825, 0.939] 
2023-10-27 10:45:02.935443: Epoch time: 3.42 s 
2023-10-27 10:45:03.976412:  
2023-10-27 10:45:03.976714: Epoch 914 
2023-10-27 10:45:03.976940: Current learning rate: 0.0011 
2023-10-27 10:45:07.389503: train_loss -0.8985 
2023-10-27 10:45:07.389892: val_loss -0.8906 
2023-10-27 10:45:07.390146: Pseudo dice [0.884, 0.9021, 0.9726, 0.8426, 0.9443] 
2023-10-27 10:45:07.390388: Epoch time: 3.41 s 
2023-10-27 10:45:08.432495:  
2023-10-27 10:45:08.432791: Epoch 915 
2023-10-27 10:45:08.433026: Current learning rate: 0.00109 
2023-10-27 10:45:11.901988: train_loss -0.8996 
2023-10-27 10:45:11.902370: val_loss -0.8863 
2023-10-27 10:45:11.902635: Pseudo dice [0.8831, 0.9059, 0.9688, 0.8236, 0.9439] 
2023-10-27 10:45:11.902859: Epoch time: 3.47 s 
2023-10-27 10:45:13.099808:  
2023-10-27 10:45:13.100203: Epoch 916 
2023-10-27 10:45:13.100514: Current learning rate: 0.00108 
2023-10-27 10:45:16.564929: train_loss -0.9006 
2023-10-27 10:45:16.565304: val_loss -0.8857 
2023-10-27 10:45:16.565589: Pseudo dice [0.8798, 0.8995, 0.9693, 0.807, 0.9428] 
2023-10-27 10:45:16.565820: Epoch time: 3.47 s 
2023-10-27 10:45:17.611476:  
2023-10-27 10:45:17.611805: Epoch 917 
2023-10-27 10:45:17.612053: Current learning rate: 0.00106 
2023-10-27 10:45:21.013405: train_loss -0.9023 
2023-10-27 10:45:21.013766: val_loss -0.8868 
2023-10-27 10:45:21.014023: Pseudo dice [0.8849, 0.9069, 0.9717, 0.8265, 0.9447] 
2023-10-27 10:45:21.014248: Epoch time: 3.4 s 
2023-10-27 10:45:22.057091:  
2023-10-27 10:45:22.057410: Epoch 918 
2023-10-27 10:45:22.057709: Current learning rate: 0.00105 
2023-10-27 10:45:25.496815: train_loss -0.9021 
2023-10-27 10:45:25.497217: val_loss -0.8852 
2023-10-27 10:45:25.497479: Pseudo dice [0.8813, 0.9043, 0.9692, 0.8332, 0.9398] 
2023-10-27 10:45:25.497713: Epoch time: 3.44 s 
2023-10-27 10:45:26.539695:  
2023-10-27 10:45:26.540011: Epoch 919 
2023-10-27 10:45:26.540258: Current learning rate: 0.00104 
2023-10-27 10:45:29.972092: train_loss -0.9025 
2023-10-27 10:45:29.972454: val_loss -0.8906 
2023-10-27 10:45:29.972718: Pseudo dice [0.8852, 0.9071, 0.9719, 0.8095, 0.9454] 
2023-10-27 10:45:29.972952: Epoch time: 3.43 s 
2023-10-27 10:45:31.015504:  
2023-10-27 10:45:31.015795: Epoch 920 
2023-10-27 10:45:31.016060: Current learning rate: 0.00103 
2023-10-27 10:45:34.420119: train_loss -0.9076 
2023-10-27 10:45:34.420544: val_loss -0.8873 
2023-10-27 10:45:34.420835: Pseudo dice [0.8904, 0.9071, 0.9707, 0.8238, 0.9452] 
2023-10-27 10:45:34.421161: Epoch time: 3.41 s 
2023-10-27 10:45:35.467281:  
2023-10-27 10:45:35.467680: Epoch 921 
2023-10-27 10:45:35.467982: Current learning rate: 0.00102 
2023-10-27 10:45:38.883088: train_loss -0.8978 
2023-10-27 10:45:38.883464: val_loss -0.8897 
2023-10-27 10:45:38.883726: Pseudo dice [0.8865, 0.9082, 0.9721, 0.8428, 0.9448] 
2023-10-27 10:45:38.883951: Epoch time: 3.42 s 
2023-10-27 10:45:39.928488:  
2023-10-27 10:45:39.928793: Epoch 922 
2023-10-27 10:45:39.929045: Current learning rate: 0.00101 
2023-10-27 10:45:43.361118: train_loss -0.8963 
2023-10-27 10:45:43.361487: val_loss -0.887 
2023-10-27 10:45:43.361749: Pseudo dice [0.885, 0.9073, 0.9696, 0.8267, 0.9465] 
2023-10-27 10:45:43.361984: Epoch time: 3.43 s 
2023-10-27 10:45:44.550794:  
2023-10-27 10:45:44.551105: Epoch 923 
2023-10-27 10:45:44.551355: Current learning rate: 0.001 
2023-10-27 10:45:47.933963: train_loss -0.8982 
2023-10-27 10:45:47.934333: val_loss -0.8861 
2023-10-27 10:45:47.934610: Pseudo dice [0.8836, 0.9062, 0.972, 0.8127, 0.9354] 
2023-10-27 10:45:47.934851: Epoch time: 3.38 s 
2023-10-27 10:45:48.980992:  
2023-10-27 10:45:48.981327: Epoch 924 
2023-10-27 10:45:48.981591: Current learning rate: 0.00098 
2023-10-27 10:45:52.367317: train_loss -0.9108 
2023-10-27 10:45:52.367733: val_loss -0.8874 
2023-10-27 10:45:52.368002: Pseudo dice [0.8803, 0.9095, 0.971, 0.8092, 0.943] 
2023-10-27 10:45:52.368242: Epoch time: 3.39 s 
2023-10-27 10:45:53.410954:  
2023-10-27 10:45:53.411307: Epoch 925 
2023-10-27 10:45:53.411580: Current learning rate: 0.00097 
2023-10-27 10:45:56.786748: train_loss -0.8973 
2023-10-27 10:45:56.787123: val_loss -0.884 
2023-10-27 10:45:56.787380: Pseudo dice [0.8817, 0.9072, 0.9694, 0.8644, 0.944] 
2023-10-27 10:45:56.787622: Epoch time: 3.38 s 
2023-10-27 10:45:57.833867:  
2023-10-27 10:45:57.834232: Epoch 926 
2023-10-27 10:45:57.834471: Current learning rate: 0.00096 
2023-10-27 10:46:01.212148: train_loss -0.8994 
2023-10-27 10:46:01.212597: val_loss -0.8812 
2023-10-27 10:46:01.212875: Pseudo dice [0.8825, 0.9071, 0.971, 0.7795, 0.9428] 
2023-10-27 10:46:01.213125: Epoch time: 3.38 s 
2023-10-27 10:46:02.257639:  
2023-10-27 10:46:02.257940: Epoch 927 
2023-10-27 10:46:02.258191: Current learning rate: 0.00095 
2023-10-27 10:46:05.690016: train_loss -0.9038 
2023-10-27 10:46:05.690395: val_loss -0.8858 
2023-10-27 10:46:05.690674: Pseudo dice [0.8839, 0.9105, 0.9706, 0.8472, 0.938] 
2023-10-27 10:46:05.690924: Epoch time: 3.43 s 
2023-10-27 10:46:06.732369:  
2023-10-27 10:46:06.732679: Epoch 928 
2023-10-27 10:46:06.732938: Current learning rate: 0.00094 
2023-10-27 10:46:10.213572: train_loss -0.9019 
2023-10-27 10:46:10.213933: val_loss -0.8857 
2023-10-27 10:46:10.214189: Pseudo dice [0.8885, 0.9147, 0.9702, 0.8449, 0.9385] 
2023-10-27 10:46:10.214415: Epoch time: 3.48 s 
2023-10-27 10:46:11.394228:  
2023-10-27 10:46:11.394561: Epoch 929 
2023-10-27 10:46:11.394790: Current learning rate: 0.00092 
2023-10-27 10:46:14.792883: train_loss -0.905 
2023-10-27 10:46:14.793224: val_loss -0.8909 
2023-10-27 10:46:14.793467: Pseudo dice [0.8895, 0.9141, 0.9704, 0.8552, 0.9476] 
2023-10-27 10:46:14.793695: Epoch time: 3.4 s 
2023-10-27 10:46:15.823058:  
2023-10-27 10:46:15.823429: Epoch 930 
2023-10-27 10:46:15.823659: Current learning rate: 0.00091 
2023-10-27 10:46:19.254364: train_loss -0.898 
2023-10-27 10:46:19.254815: val_loss -0.8941 
2023-10-27 10:46:19.255083: Pseudo dice [0.8897, 0.9128, 0.9717, 0.835, 0.9462] 
2023-10-27 10:46:19.255319: Epoch time: 3.43 s 
2023-10-27 10:46:20.299072:  
2023-10-27 10:46:20.299485: Epoch 931 
2023-10-27 10:46:20.299725: Current learning rate: 0.0009 
2023-10-27 10:46:23.670367: train_loss -0.9035 
2023-10-27 10:46:23.670714: val_loss -0.8901 
2023-10-27 10:46:23.670965: Pseudo dice [0.8807, 0.91, 0.9714, 0.8205, 0.9447] 
2023-10-27 10:46:23.671186: Epoch time: 3.37 s 
2023-10-27 10:46:24.704750:  
2023-10-27 10:46:24.705071: Epoch 932 
2023-10-27 10:46:24.705295: Current learning rate: 0.00089 
2023-10-27 10:46:28.088879: train_loss -0.9012 
2023-10-27 10:46:28.089325: val_loss -0.8883 
2023-10-27 10:46:28.089574: Pseudo dice [0.8857, 0.9066, 0.972, 0.853, 0.9398] 
2023-10-27 10:46:28.089794: Epoch time: 3.38 s 
2023-10-27 10:46:29.123297:  
2023-10-27 10:46:29.123586: Epoch 933 
2023-10-27 10:46:29.123814: Current learning rate: 0.00088 
2023-10-27 10:46:32.496829: train_loss -0.8989 
2023-10-27 10:46:32.497300: val_loss -0.8861 
2023-10-27 10:46:32.497602: Pseudo dice [0.878, 0.9114, 0.9703, 0.8327, 0.9435] 
2023-10-27 10:46:32.497941: Epoch time: 3.37 s 
2023-10-27 10:46:33.538671:  
2023-10-27 10:46:33.538973: Epoch 934 
2023-10-27 10:46:33.539209: Current learning rate: 0.00087 
2023-10-27 10:46:36.946792: train_loss -0.9006 
2023-10-27 10:46:36.947188: val_loss -0.8856 
2023-10-27 10:46:36.947452: Pseudo dice [0.8872, 0.9098, 0.9701, 0.832, 0.942] 
2023-10-27 10:46:36.947684: Epoch time: 3.41 s 
2023-10-27 10:46:37.986694:  
2023-10-27 10:46:37.987021: Epoch 935 
2023-10-27 10:46:37.987256: Current learning rate: 0.00085 
2023-10-27 10:46:41.390332: train_loss -0.9017 
2023-10-27 10:46:41.390674: val_loss -0.8905 
2023-10-27 10:46:41.390924: Pseudo dice [0.8894, 0.9121, 0.9712, 0.8275, 0.9467] 
2023-10-27 10:46:41.391137: Epoch time: 3.4 s 
2023-10-27 10:46:42.573515:  
2023-10-27 10:46:42.573936: Epoch 936 
2023-10-27 10:46:42.574192: Current learning rate: 0.00084 
2023-10-27 10:46:46.074588: train_loss -0.9019 
2023-10-27 10:46:46.075214: val_loss -0.886 
2023-10-27 10:46:46.075540: Pseudo dice [0.8857, 0.9076, 0.9702, 0.8289, 0.9455] 
2023-10-27 10:46:46.075843: Epoch time: 3.5 s 
2023-10-27 10:46:47.121014:  
2023-10-27 10:46:47.121413: Epoch 937 
2023-10-27 10:46:47.121677: Current learning rate: 0.00083 
2023-10-27 10:46:50.574971: train_loss -0.9054 
2023-10-27 10:46:50.575331: val_loss -0.8853 
2023-10-27 10:46:50.575579: Pseudo dice [0.8757, 0.9054, 0.9702, 0.82, 0.9398] 
2023-10-27 10:46:50.575812: Epoch time: 3.45 s 
2023-10-27 10:46:51.622949:  
2023-10-27 10:46:51.623328: Epoch 938 
2023-10-27 10:46:51.623584: Current learning rate: 0.00082 
2023-10-27 10:46:55.130112: train_loss -0.8988 
2023-10-27 10:46:55.130518: val_loss -0.8883 
2023-10-27 10:46:55.130836: Pseudo dice [0.884, 0.9052, 0.9697, 0.8169, 0.9426] 
2023-10-27 10:46:55.131140: Epoch time: 3.51 s 
2023-10-27 10:46:56.170722:  
2023-10-27 10:46:56.171013: Epoch 939 
2023-10-27 10:46:56.171249: Current learning rate: 0.00081 
2023-10-27 10:46:59.642579: train_loss -0.9038 
2023-10-27 10:46:59.643229: val_loss -0.8907 
2023-10-27 10:46:59.643535: Pseudo dice [0.8779, 0.9068, 0.9708, 0.82, 0.9413] 
2023-10-27 10:46:59.643822: Epoch time: 3.47 s 
2023-10-27 10:47:00.681790:  
2023-10-27 10:47:00.682087: Epoch 940 
2023-10-27 10:47:00.682329: Current learning rate: 0.00079 
2023-10-27 10:47:04.179382: train_loss -0.9047 
2023-10-27 10:47:04.179760: val_loss -0.8921 
2023-10-27 10:47:04.180010: Pseudo dice [0.8852, 0.9093, 0.9709, 0.831, 0.9433] 
2023-10-27 10:47:04.180242: Epoch time: 3.5 s 
2023-10-27 10:47:05.219206:  
2023-10-27 10:47:05.219550: Epoch 941 
2023-10-27 10:47:05.219795: Current learning rate: 0.00078 
2023-10-27 10:47:08.627937: train_loss -0.9064 
2023-10-27 10:47:08.628289: val_loss -0.8902 
2023-10-27 10:47:08.628542: Pseudo dice [0.8877, 0.9069, 0.9714, 0.8326, 0.9427] 
2023-10-27 10:47:08.628767: Epoch time: 3.41 s 
2023-10-27 10:47:09.664514:  
2023-10-27 10:47:09.664808: Epoch 942 
2023-10-27 10:47:09.665055: Current learning rate: 0.00077 
2023-10-27 10:47:13.325016: train_loss -0.9064 
2023-10-27 10:47:13.325451: val_loss -0.8895 
2023-10-27 10:47:13.325805: Pseudo dice [0.8846, 0.9104, 0.9715, 0.8041, 0.9435] 
2023-10-27 10:47:13.326071: Epoch time: 3.66 s 
2023-10-27 10:47:14.361336:  
2023-10-27 10:47:14.361683: Epoch 943 
2023-10-27 10:47:14.361925: Current learning rate: 0.00076 
2023-10-27 10:47:17.915572: train_loss -0.9068 
2023-10-27 10:47:17.916081: val_loss -0.8905 
2023-10-27 10:47:17.916339: Pseudo dice [0.8869, 0.9048, 0.9716, 0.849, 0.9421] 
2023-10-27 10:47:17.916573: Epoch time: 3.55 s 
2023-10-27 10:47:18.957247:  
2023-10-27 10:47:18.957583: Epoch 944 
2023-10-27 10:47:18.957827: Current learning rate: 0.00075 
2023-10-27 10:47:22.344765: train_loss -0.9057 
2023-10-27 10:47:22.345120: val_loss -0.8938 
2023-10-27 10:47:22.345379: Pseudo dice [0.8864, 0.9119, 0.9714, 0.8298, 0.9412] 
2023-10-27 10:47:22.345610: Epoch time: 3.39 s 
2023-10-27 10:47:23.383543:  
2023-10-27 10:47:23.383899: Epoch 945 
2023-10-27 10:47:23.384134: Current learning rate: 0.00074 
2023-10-27 10:47:26.727489: train_loss -0.9026 
2023-10-27 10:47:26.727847: val_loss -0.8897 
2023-10-27 10:47:26.728114: Pseudo dice [0.8856, 0.9068, 0.9717, 0.8776, 0.9476] 
2023-10-27 10:47:26.728350: Epoch time: 3.34 s 
2023-10-27 10:47:27.773648:  
2023-10-27 10:47:27.774008: Epoch 946 
2023-10-27 10:47:27.774248: Current learning rate: 0.00072 
2023-10-27 10:47:31.126893: train_loss -0.906 
2023-10-27 10:47:31.127330: val_loss -0.888 
2023-10-27 10:47:31.127597: Pseudo dice [0.8816, 0.9097, 0.9704, 0.8542, 0.9461] 
2023-10-27 10:47:31.127826: Epoch time: 3.35 s 
2023-10-27 10:47:32.169441:  
2023-10-27 10:47:32.169768: Epoch 947 
2023-10-27 10:47:32.170009: Current learning rate: 0.00071 
2023-10-27 10:47:35.494580: train_loss -0.9046 
2023-10-27 10:47:35.494945: val_loss -0.8914 
2023-10-27 10:47:35.495196: Pseudo dice [0.8804, 0.909, 0.9716, 0.8411, 0.9461] 
2023-10-27 10:47:35.495423: Epoch time: 3.33 s 
2023-10-27 10:47:36.540174:  
2023-10-27 10:47:36.540561: Epoch 948 
2023-10-27 10:47:36.540805: Current learning rate: 0.0007 
2023-10-27 10:47:39.866547: train_loss -0.9056 
2023-10-27 10:47:39.866889: val_loss -0.8942 
2023-10-27 10:47:39.867127: Pseudo dice [0.8862, 0.9112, 0.9727, 0.85, 0.9446] 
2023-10-27 10:47:39.867348: Epoch time: 3.33 s 
2023-10-27 10:47:41.046466:  
2023-10-27 10:47:41.046793: Epoch 949 
2023-10-27 10:47:41.047013: Current learning rate: 0.00069 
2023-10-27 10:47:44.404018: train_loss -0.9045 
2023-10-27 10:47:44.404409: val_loss -0.8566 
2023-10-27 10:47:44.404742: Pseudo dice [0.885, 0.908, 0.9703, 0.4432, 0.9394] 
2023-10-27 10:47:44.404966: Epoch time: 3.36 s 
2023-10-27 10:47:45.550341:  
2023-10-27 10:47:45.550669: Epoch 950 
2023-10-27 10:47:45.550904: Current learning rate: 0.00067 
2023-10-27 10:47:49.193605: train_loss -0.9026 
2023-10-27 10:47:49.194046: val_loss -0.8845 
2023-10-27 10:47:49.194372: Pseudo dice [0.8807, 0.907, 0.9716, 0.8078, 0.9444] 
2023-10-27 10:47:49.194663: Epoch time: 3.64 s 
2023-10-27 10:47:50.232671:  
2023-10-27 10:47:50.232973: Epoch 951 
2023-10-27 10:47:50.233203: Current learning rate: 0.00066 
2023-10-27 10:47:53.845971: train_loss -0.9059 
2023-10-27 10:47:53.846332: val_loss -0.8824 
2023-10-27 10:47:53.846597: Pseudo dice [0.8849, 0.9138, 0.9713, 0.7896, 0.942] 
2023-10-27 10:47:53.846825: Epoch time: 3.61 s 
2023-10-27 10:47:54.888327:  
2023-10-27 10:47:54.888666: Epoch 952 
2023-10-27 10:47:54.888905: Current learning rate: 0.00065 
2023-10-27 10:47:58.391358: train_loss -0.9053 
2023-10-27 10:47:58.392261: val_loss -0.8923 
2023-10-27 10:47:58.392639: Pseudo dice [0.8877, 0.9073, 0.9708, 0.8466, 0.9462] 
2023-10-27 10:47:58.392976: Epoch time: 3.5 s 
2023-10-27 10:47:59.430263:  
2023-10-27 10:47:59.430604: Epoch 953 
2023-10-27 10:47:59.430846: Current learning rate: 0.00064 
2023-10-27 10:48:02.857201: train_loss -0.904 
2023-10-27 10:48:02.857864: val_loss -0.8877 
2023-10-27 10:48:02.858124: Pseudo dice [0.8864, 0.9086, 0.9714, 0.8318, 0.9451] 
2023-10-27 10:48:02.858357: Epoch time: 3.43 s 
2023-10-27 10:48:03.910857:  
2023-10-27 10:48:03.911145: Epoch 954 
2023-10-27 10:48:03.911387: Current learning rate: 0.00063 
2023-10-27 10:48:07.283671: train_loss -0.9024 
2023-10-27 10:48:07.284388: val_loss -0.8878 
2023-10-27 10:48:07.284670: Pseudo dice [0.8819, 0.9075, 0.971, 0.8397, 0.9472] 
2023-10-27 10:48:07.284924: Epoch time: 3.37 s 
2023-10-27 10:48:08.344888:  
2023-10-27 10:48:08.345215: Epoch 955 
2023-10-27 10:48:08.345473: Current learning rate: 0.00061 
2023-10-27 10:48:11.724593: train_loss -0.9037 
2023-10-27 10:48:11.725118: val_loss -0.8894 
2023-10-27 10:48:11.725383: Pseudo dice [0.8803, 0.9098, 0.9714, 0.8409, 0.9398] 
2023-10-27 10:48:11.725646: Epoch time: 3.38 s 
2023-10-27 10:48:12.930058:  
2023-10-27 10:48:12.930373: Epoch 956 
2023-10-27 10:48:12.930642: Current learning rate: 0.0006 
2023-10-27 10:48:16.355329: train_loss -0.903 
2023-10-27 10:48:16.355690: val_loss -0.8876 
2023-10-27 10:48:16.355947: Pseudo dice [0.8816, 0.91, 0.9707, 0.8376, 0.9433] 
2023-10-27 10:48:16.356186: Epoch time: 3.43 s 
2023-10-27 10:48:17.418172:  
2023-10-27 10:48:17.418550: Epoch 957 
2023-10-27 10:48:17.418813: Current learning rate: 0.00059 
2023-10-27 10:48:20.768020: train_loss -0.9028 
2023-10-27 10:48:20.768424: val_loss -0.8859 
2023-10-27 10:48:20.768693: Pseudo dice [0.8876, 0.9105, 0.9718, 0.8332, 0.9404] 
2023-10-27 10:48:20.768934: Epoch time: 3.35 s 
2023-10-27 10:48:21.825833:  
2023-10-27 10:48:21.826207: Epoch 958 
2023-10-27 10:48:21.826461: Current learning rate: 0.00058 
2023-10-27 10:48:25.199727: train_loss -0.9019 
2023-10-27 10:48:25.200081: val_loss -0.8925 
2023-10-27 10:48:25.200335: Pseudo dice [0.8762, 0.9072, 0.9714, 0.8345, 0.9457] 
2023-10-27 10:48:25.200559: Epoch time: 3.37 s 
2023-10-27 10:48:26.255857:  
2023-10-27 10:48:26.256220: Epoch 959 
2023-10-27 10:48:26.256463: Current learning rate: 0.00056 
2023-10-27 10:48:29.695280: train_loss -0.9061 
2023-10-27 10:48:29.695728: val_loss -0.8923 
2023-10-27 10:48:29.696013: Pseudo dice [0.8859, 0.9093, 0.9695, 0.8546, 0.9481] 
2023-10-27 10:48:29.696244: Epoch time: 3.44 s 
2023-10-27 10:48:30.751875:  
2023-10-27 10:48:30.752214: Epoch 960 
2023-10-27 10:48:30.752469: Current learning rate: 0.00055 
2023-10-27 10:48:34.161493: train_loss -0.9023 
2023-10-27 10:48:34.161878: val_loss -0.8882 
2023-10-27 10:48:34.162137: Pseudo dice [0.8899, 0.908, 0.9713, 0.8287, 0.9462] 
2023-10-27 10:48:34.162453: Epoch time: 3.41 s 
2023-10-27 10:48:35.218395:  
2023-10-27 10:48:35.218741: Epoch 961 
2023-10-27 10:48:35.218983: Current learning rate: 0.00054 
2023-10-27 10:48:38.608847: train_loss -0.9068 
2023-10-27 10:48:38.609236: val_loss -0.8789 
2023-10-27 10:48:38.609488: Pseudo dice [0.884, 0.9088, 0.9712, 0.8203, 0.9397] 
2023-10-27 10:48:38.609726: Epoch time: 3.39 s 
2023-10-27 10:48:39.799113:  
2023-10-27 10:48:39.799451: Epoch 962 
2023-10-27 10:48:39.799711: Current learning rate: 0.00053 
2023-10-27 10:48:43.211853: train_loss -0.9087 
2023-10-27 10:48:43.212209: val_loss -0.8898 
2023-10-27 10:48:43.212477: Pseudo dice [0.8887, 0.911, 0.9716, 0.8219, 0.9422] 
2023-10-27 10:48:43.212728: Epoch time: 3.41 s 
2023-10-27 10:48:44.267376:  
2023-10-27 10:48:44.267669: Epoch 963 
2023-10-27 10:48:44.267929: Current learning rate: 0.00051 
2023-10-27 10:48:47.664269: train_loss -0.9051 
2023-10-27 10:48:47.664663: val_loss -0.8912 
2023-10-27 10:48:47.664932: Pseudo dice [0.8843, 0.911, 0.9715, 0.8646, 0.9468] 
2023-10-27 10:48:47.665167: Epoch time: 3.4 s 
2023-10-27 10:48:48.726240:  
2023-10-27 10:48:48.726547: Epoch 964 
2023-10-27 10:48:48.726797: Current learning rate: 0.0005 
2023-10-27 10:48:52.122453: train_loss -0.903 
2023-10-27 10:48:52.122857: val_loss -0.8884 
2023-10-27 10:48:52.123140: Pseudo dice [0.8852, 0.9013, 0.971, 0.8636, 0.9455] 
2023-10-27 10:48:52.123370: Epoch time: 3.4 s 
2023-10-27 10:48:53.181892:  
2023-10-27 10:48:53.182272: Epoch 965 
2023-10-27 10:48:53.182659: Current learning rate: 0.00049 
2023-10-27 10:48:56.572846: train_loss -0.9046 
2023-10-27 10:48:56.573416: val_loss -0.8932 
2023-10-27 10:48:56.573687: Pseudo dice [0.8856, 0.9059, 0.9723, 0.8561, 0.9462] 
2023-10-27 10:48:56.573908: Epoch time: 3.39 s 
2023-10-27 10:48:57.632601:  
2023-10-27 10:48:57.632936: Epoch 966 
2023-10-27 10:48:57.633181: Current learning rate: 0.00048 
2023-10-27 10:49:00.976393: train_loss -0.8994 
2023-10-27 10:49:00.976779: val_loss -0.887 
2023-10-27 10:49:00.977037: Pseudo dice [0.8828, 0.9042, 0.9703, 0.8361, 0.946] 
2023-10-27 10:49:00.977276: Epoch time: 3.34 s 
2023-10-27 10:49:02.029382:  
2023-10-27 10:49:02.029747: Epoch 967 
2023-10-27 10:49:02.029991: Current learning rate: 0.00046 
2023-10-27 10:49:05.435390: train_loss -0.8994 
2023-10-27 10:49:05.435794: val_loss -0.8883 
2023-10-27 10:49:05.436057: Pseudo dice [0.884, 0.9112, 0.9706, 0.8223, 0.9404] 
2023-10-27 10:49:05.436286: Epoch time: 3.41 s 
2023-10-27 10:49:06.635792:  
2023-10-27 10:49:06.636122: Epoch 968 
2023-10-27 10:49:06.636350: Current learning rate: 0.00045 
2023-10-27 10:49:10.012251: train_loss -0.9027 
2023-10-27 10:49:10.012623: val_loss -0.8854 
2023-10-27 10:49:10.012879: Pseudo dice [0.8808, 0.9114, 0.9704, 0.8189, 0.9392] 
2023-10-27 10:49:10.013113: Epoch time: 3.38 s 
2023-10-27 10:49:11.067528:  
2023-10-27 10:49:11.067846: Epoch 969 
2023-10-27 10:49:11.068099: Current learning rate: 0.00044 
2023-10-27 10:49:14.466516: train_loss -0.9019 
2023-10-27 10:49:14.466860: val_loss -0.886 
2023-10-27 10:49:14.467115: Pseudo dice [0.8832, 0.9057, 0.9709, 0.832, 0.9434] 
2023-10-27 10:49:14.467338: Epoch time: 3.4 s 
2023-10-27 10:49:15.527293:  
2023-10-27 10:49:15.527587: Epoch 970 
2023-10-27 10:49:15.527827: Current learning rate: 0.00043 
2023-10-27 10:49:18.895710: train_loss -0.9109 
2023-10-27 10:49:18.896065: val_loss -0.8871 
2023-10-27 10:49:18.896317: Pseudo dice [0.8808, 0.9069, 0.9711, 0.8319, 0.9399] 
2023-10-27 10:49:18.896557: Epoch time: 3.37 s 
2023-10-27 10:49:19.963625:  
2023-10-27 10:49:19.963946: Epoch 971 
2023-10-27 10:49:19.964189: Current learning rate: 0.00041 
2023-10-27 10:49:23.405620: train_loss -0.902 
2023-10-27 10:49:23.406068: val_loss -0.8902 
2023-10-27 10:49:23.406364: Pseudo dice [0.8841, 0.9072, 0.9712, 0.8342, 0.9425] 
2023-10-27 10:49:23.406607: Epoch time: 3.44 s 
2023-10-27 10:49:24.475223:  
2023-10-27 10:49:24.475615: Epoch 972 
2023-10-27 10:49:24.475868: Current learning rate: 0.0004 
2023-10-27 10:49:27.837733: train_loss -0.9075 
2023-10-27 10:49:27.838211: val_loss -0.8843 
2023-10-27 10:49:27.838553: Pseudo dice [0.8854, 0.9103, 0.9701, 0.8384, 0.9394] 
2023-10-27 10:49:27.838967: Epoch time: 3.36 s 
2023-10-27 10:49:28.897450:  
2023-10-27 10:49:28.897755: Epoch 973 
2023-10-27 10:49:28.898012: Current learning rate: 0.00039 
2023-10-27 10:49:32.514066: train_loss -0.9013 
2023-10-27 10:49:32.514435: val_loss -0.8926 
2023-10-27 10:49:32.514689: Pseudo dice [0.8864, 0.9096, 0.9727, 0.8306, 0.9455] 
2023-10-27 10:49:32.514909: Epoch time: 3.62 s 
2023-10-27 10:49:33.585320:  
2023-10-27 10:49:33.585629: Epoch 974 
2023-10-27 10:49:33.585892: Current learning rate: 0.00037 
2023-10-27 10:49:36.948654: train_loss -0.9047 
2023-10-27 10:49:36.949039: val_loss -0.8892 
2023-10-27 10:49:36.949294: Pseudo dice [0.8813, 0.9065, 0.9709, 0.8443, 0.946] 
2023-10-27 10:49:36.949542: Epoch time: 3.36 s 
2023-10-27 10:49:38.158022:  
2023-10-27 10:49:38.158406: Epoch 975 
2023-10-27 10:49:38.158672: Current learning rate: 0.00036 
2023-10-27 10:49:41.611175: train_loss -0.9035 
2023-10-27 10:49:41.612000: val_loss -0.8895 
2023-10-27 10:49:41.612425: Pseudo dice [0.8863, 0.914, 0.9712, 0.8265, 0.9447] 
2023-10-27 10:49:41.612703: Epoch time: 3.45 s 
2023-10-27 10:49:42.675479:  
2023-10-27 10:49:42.675850: Epoch 976 
2023-10-27 10:49:42.676111: Current learning rate: 0.00035 
2023-10-27 10:49:46.090964: train_loss -0.9046 
2023-10-27 10:49:46.091685: val_loss -0.8844 
2023-10-27 10:49:46.091983: Pseudo dice [0.8875, 0.9093, 0.9714, 0.8278, 0.942] 
2023-10-27 10:49:46.092230: Epoch time: 3.42 s 
2023-10-27 10:49:47.167749:  
2023-10-27 10:49:47.168200: Epoch 977 
2023-10-27 10:49:47.168521: Current learning rate: 0.00034 
2023-10-27 10:49:50.550190: train_loss -0.8986 
2023-10-27 10:49:50.550562: val_loss -0.8922 
2023-10-27 10:49:50.550822: Pseudo dice [0.8861, 0.9086, 0.9719, 0.8052, 0.9451] 
2023-10-27 10:49:50.551061: Epoch time: 3.38 s 
2023-10-27 10:49:51.613328:  
2023-10-27 10:49:51.613633: Epoch 978 
2023-10-27 10:49:51.613884: Current learning rate: 0.00032 
2023-10-27 10:49:54.985919: train_loss -0.9045 
2023-10-27 10:49:54.986276: val_loss -0.889 
2023-10-27 10:49:54.986544: Pseudo dice [0.8803, 0.9045, 0.9707, 0.8325, 0.9432] 
2023-10-27 10:49:54.986778: Epoch time: 3.37 s 
2023-10-27 10:49:56.052475:  
2023-10-27 10:49:56.052767: Epoch 979 
2023-10-27 10:49:56.053011: Current learning rate: 0.00031 
2023-10-27 10:49:59.410423: train_loss -0.9101 
2023-10-27 10:49:59.410795: val_loss -0.8868 
2023-10-27 10:49:59.411058: Pseudo dice [0.8805, 0.9065, 0.9704, 0.8289, 0.9403] 
2023-10-27 10:49:59.411288: Epoch time: 3.36 s 
2023-10-27 10:50:00.477493:  
2023-10-27 10:50:00.477826: Epoch 980 
2023-10-27 10:50:00.478066: Current learning rate: 0.0003 
2023-10-27 10:50:03.880310: train_loss -0.9056 
2023-10-27 10:50:03.880758: val_loss -0.8859 
2023-10-27 10:50:03.881043: Pseudo dice [0.8838, 0.9087, 0.9713, 0.8343, 0.9395] 
2023-10-27 10:50:03.881418: Epoch time: 3.4 s 
2023-10-27 10:50:05.083699:  
2023-10-27 10:50:05.084084: Epoch 981 
2023-10-27 10:50:05.084317: Current learning rate: 0.00028 
2023-10-27 10:50:08.468197: train_loss -0.9076 
2023-10-27 10:50:08.468589: val_loss -0.8875 
2023-10-27 10:50:08.468839: Pseudo dice [0.8808, 0.906, 0.971, 0.8475, 0.9412] 
2023-10-27 10:50:08.469059: Epoch time: 3.39 s 
2023-10-27 10:50:09.529767:  
2023-10-27 10:50:09.530058: Epoch 982 
2023-10-27 10:50:09.530301: Current learning rate: 0.00027 
2023-10-27 10:50:12.948714: train_loss -0.9054 
2023-10-27 10:50:12.949180: val_loss -0.8912 
2023-10-27 10:50:12.949453: Pseudo dice [0.8803, 0.9096, 0.9708, 0.8417, 0.9435] 
2023-10-27 10:50:12.949689: Epoch time: 3.42 s 
2023-10-27 10:50:14.007533:  
2023-10-27 10:50:14.007831: Epoch 983 
2023-10-27 10:50:14.008082: Current learning rate: 0.00026 
2023-10-27 10:50:17.371333: train_loss -0.907 
2023-10-27 10:50:17.371793: val_loss -0.888 
2023-10-27 10:50:17.372048: Pseudo dice [0.883, 0.9072, 0.971, 0.8458, 0.9442] 
2023-10-27 10:50:17.372267: Epoch time: 3.36 s 
2023-10-27 10:50:18.437306:  
2023-10-27 10:50:18.437608: Epoch 984 
2023-10-27 10:50:18.437849: Current learning rate: 0.00024 
2023-10-27 10:50:21.882539: train_loss -0.9047 
2023-10-27 10:50:21.882905: val_loss -0.8853 
2023-10-27 10:50:21.883246: Pseudo dice [0.8849, 0.906, 0.9704, 0.8322, 0.941] 
2023-10-27 10:50:21.883485: Epoch time: 3.45 s 
2023-10-27 10:50:22.970010:  
2023-10-27 10:50:22.970344: Epoch 985 
2023-10-27 10:50:22.970594: Current learning rate: 0.00023 
2023-10-27 10:50:26.342847: train_loss -0.9049 
2023-10-27 10:50:26.343204: val_loss -0.8839 
2023-10-27 10:50:26.343480: Pseudo dice [0.8887, 0.9113, 0.9716, 0.8294, 0.9402] 
2023-10-27 10:50:26.343704: Epoch time: 3.37 s 
2023-10-27 10:50:27.403974:  
2023-10-27 10:50:27.404298: Epoch 986 
2023-10-27 10:50:27.404556: Current learning rate: 0.00021 
2023-10-27 10:50:30.738257: train_loss -0.9061 
2023-10-27 10:50:30.738656: val_loss -0.8892 
2023-10-27 10:50:30.738915: Pseudo dice [0.8874, 0.9104, 0.9714, 0.8502, 0.9446] 
2023-10-27 10:50:30.739150: Epoch time: 3.33 s 
2023-10-27 10:50:31.805320:  
2023-10-27 10:50:31.805630: Epoch 987 
2023-10-27 10:50:31.805873: Current learning rate: 0.0002 
2023-10-27 10:50:35.206014: train_loss -0.903 
2023-10-27 10:50:35.206442: val_loss -0.8878 
2023-10-27 10:50:35.206698: Pseudo dice [0.887, 0.9055, 0.9717, 0.8442, 0.945] 
2023-10-27 10:50:35.206924: Epoch time: 3.4 s 
2023-10-27 10:50:36.410142:  
2023-10-27 10:50:36.410458: Epoch 988 
2023-10-27 10:50:36.410715: Current learning rate: 0.00019 
2023-10-27 10:50:39.775806: train_loss -0.9063 
2023-10-27 10:50:39.776193: val_loss -0.8918 
2023-10-27 10:50:39.776470: Pseudo dice [0.8828, 0.9082, 0.9715, 0.844, 0.9452] 
2023-10-27 10:50:39.776743: Epoch time: 3.37 s 
2023-10-27 10:50:40.834808:  
2023-10-27 10:50:40.835145: Epoch 989 
2023-10-27 10:50:40.835390: Current learning rate: 0.00017 
2023-10-27 10:50:44.256535: train_loss -0.9077 
2023-10-27 10:50:44.256900: val_loss -0.8847 
2023-10-27 10:50:44.257175: Pseudo dice [0.8841, 0.902, 0.9713, 0.8303, 0.9383] 
2023-10-27 10:50:44.257417: Epoch time: 3.42 s 
2023-10-27 10:50:45.318628:  
2023-10-27 10:50:45.318928: Epoch 990 
2023-10-27 10:50:45.319176: Current learning rate: 0.00016 
2023-10-27 10:50:48.657963: train_loss -0.9115 
2023-10-27 10:50:48.658423: val_loss -0.8863 
2023-10-27 10:50:48.658843: Pseudo dice [0.8836, 0.9061, 0.9712, 0.8066, 0.9407] 
2023-10-27 10:50:48.659085: Epoch time: 3.34 s 
2023-10-27 10:50:49.718729:  
2023-10-27 10:50:49.719084: Epoch 991 
2023-10-27 10:50:49.719336: Current learning rate: 0.00014 
2023-10-27 10:50:53.401903: train_loss -0.9086 
2023-10-27 10:50:53.402264: val_loss -0.8861 
2023-10-27 10:50:53.402524: Pseudo dice [0.8893, 0.9073, 0.971, 0.8314, 0.9431] 
2023-10-27 10:50:53.402758: Epoch time: 3.68 s 
2023-10-27 10:50:54.461992:  
2023-10-27 10:50:54.462294: Epoch 992 
2023-10-27 10:50:54.462559: Current learning rate: 0.00013 
2023-10-27 10:50:58.080126: train_loss -0.9072 
2023-10-27 10:50:58.080499: val_loss -0.8835 
2023-10-27 10:50:58.080755: Pseudo dice [0.8874, 0.9091, 0.9719, 0.8133, 0.9382] 
2023-10-27 10:50:58.080988: Epoch time: 3.62 s 
2023-10-27 10:50:59.148668:  
2023-10-27 10:50:59.149017: Epoch 993 
2023-10-27 10:50:59.149262: Current learning rate: 0.00011 
2023-10-27 10:51:02.510626: train_loss -0.9086 
2023-10-27 10:51:02.511300: val_loss -0.8865 
2023-10-27 10:51:02.511625: Pseudo dice [0.8802, 0.9071, 0.9709, 0.8272, 0.9402] 
2023-10-27 10:51:02.511974: Epoch time: 3.36 s 
2023-10-27 10:51:03.708615:  
2023-10-27 10:51:03.708955: Epoch 994 
2023-10-27 10:51:03.709216: Current learning rate: 0.0001 
2023-10-27 10:51:07.104954: train_loss -0.9027 
2023-10-27 10:51:07.105322: val_loss -0.8891 
2023-10-27 10:51:07.105585: Pseudo dice [0.8889, 0.9077, 0.971, 0.8432, 0.9421] 
2023-10-27 10:51:07.105807: Epoch time: 3.4 s 
2023-10-27 10:51:08.158574:  
2023-10-27 10:51:08.158915: Epoch 995 
2023-10-27 10:51:08.174152: Current learning rate: 8e-05 
2023-10-27 10:51:11.588381: train_loss -0.903 
2023-10-27 10:51:11.588890: val_loss -0.8842 
2023-10-27 10:51:11.589149: Pseudo dice [0.8809, 0.9052, 0.9708, 0.8288, 0.9395] 
2023-10-27 10:51:11.589376: Epoch time: 3.43 s 
2023-10-27 10:51:12.654102:  
2023-10-27 10:51:12.654426: Epoch 996 
2023-10-27 10:51:12.654677: Current learning rate: 7e-05 
2023-10-27 10:51:16.048105: train_loss -0.9092 
2023-10-27 10:51:16.048522: val_loss -0.8838 
2023-10-27 10:51:16.048787: Pseudo dice [0.8865, 0.9082, 0.971, 0.8363, 0.9395] 
2023-10-27 10:51:16.049018: Epoch time: 3.39 s 
2023-10-27 10:51:17.115533:  
2023-10-27 10:51:17.115951: Epoch 997 
2023-10-27 10:51:17.116224: Current learning rate: 5e-05 
2023-10-27 10:51:20.526281: train_loss -0.9082 
2023-10-27 10:51:20.526643: val_loss -0.8877 
2023-10-27 10:51:20.526911: Pseudo dice [0.8832, 0.9082, 0.9719, 0.8307, 0.9429] 
2023-10-27 10:51:20.527141: Epoch time: 3.41 s 
2023-10-27 10:51:21.582968:  
2023-10-27 10:51:21.583254: Epoch 998 
2023-10-27 10:51:21.583492: Current learning rate: 4e-05 
2023-10-27 10:51:24.984994: train_loss -0.9106 
2023-10-27 10:51:24.985357: val_loss -0.8903 
2023-10-27 10:51:24.985628: Pseudo dice [0.8874, 0.908, 0.9716, 0.8349, 0.9424] 
2023-10-27 10:51:24.985851: Epoch time: 3.4 s 
2023-10-27 10:51:26.046266:  
2023-10-27 10:51:26.046622: Epoch 999 
2023-10-27 10:51:26.046867: Current learning rate: 2e-05 
2023-10-27 10:51:29.428295: train_loss -0.9119 
2023-10-27 10:51:29.428962: val_loss -0.889 
2023-10-27 10:51:29.429236: Pseudo dice [0.876, 0.9075, 0.9709, 0.8497, 0.943] 
2023-10-27 10:51:29.429479: Epoch time: 3.38 s 
2023-10-27 10:51:30.877875: Training done. 
2023-10-27 10:51:30.990692: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-27 10:51:31.021312: The split file contains 5 splits. 
2023-10-27 10:51:31.021561: Desired fold for training: 4 
2023-10-27 10:51:31.021750: This split has 36 training and 8 validation cases. 
2023-10-27 10:51:31.022011: predicting t2_haste_tra_2_2mm_008 
2023-10-27 10:51:34.105844: predicting t2_haste_tra_2_2mm_009 
2023-10-27 10:51:34.129548: predicting t2_haste_tra_2_2mm_016 
2023-10-27 10:51:34.152284: predicting t2_haste_tra_2_2mm_108 
2023-10-27 10:51:34.175268: predicting t2_haste_tra_2_2mm_109 
2023-10-27 10:51:34.198130: predicting t2_haste_tra_2_2mm_116 
2023-10-27 10:51:34.220737: predicting t2_haste_tra_2_2mm_208 
2023-10-27 10:51:34.243232: predicting t2_haste_tra_2_2mm_209 
2023-10-27 10:53:19.654453: Validation complete 
2023-10-27 10:53:19.655647: Mean Validation Dice:  0.9145979987353217 
