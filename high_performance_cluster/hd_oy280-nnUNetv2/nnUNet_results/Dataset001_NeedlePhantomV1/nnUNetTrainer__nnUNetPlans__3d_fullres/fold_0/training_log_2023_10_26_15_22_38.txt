
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [24, 56, 40], 'median_image_size_in_voxels': [22.0, 56.0, 36.0], 'spacing': [2.419999837875366, 1.46875, 1.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [2, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_NeedlePhantomV1', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.419999837875366, 1.46875, 1.46875], 'original_median_shape_after_transp': [22, 56, 36], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1610.0, 'mean': 465.1097412109375, 'median': 478.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 1246.0, 'std': 241.4937744140625}}} 
 
2023-10-26 15:22:39.650166: unpacking dataset... 
2023-10-26 15:22:46.978205: unpacking done... 
2023-10-26 15:22:47.015890: do_dummy_2d_data_aug: False 
2023-10-26 15:22:47.017572: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-26 15:22:47.032469: The split file contains 5 splits. 
2023-10-26 15:22:47.032666: Desired fold for training: 0 
2023-10-26 15:22:47.032895: This split has 34 training and 10 validation cases. 
2023-10-26 15:23:01.839866: Unable to plot network architecture: 
2023-10-26 15:23:01.841589: 'torch._C.Node' object is not subscriptable 
2023-10-26 15:23:01.866851:  
2023-10-26 15:23:01.867093: Epoch 0 
2023-10-26 15:23:01.867427: Current learning rate: 0.01 
2023-10-26 15:23:10.769340: train_loss -0.2066 
2023-10-26 15:23:10.769759: val_loss -0.5235 
2023-10-26 15:23:10.770213: Pseudo dice [0.7706, 0.819, 0.9511, 0.0, 0.8567] 
2023-10-26 15:23:10.770681: Epoch time: 8.89 s 
2023-10-26 15:23:10.771002: Yayy! New best EMA pseudo Dice: 0.6795 
2023-10-26 15:23:12.197625:  
2023-10-26 15:23:12.197900: Epoch 1 
2023-10-26 15:23:12.198136: Current learning rate: 0.00999 
2023-10-26 15:23:16.187773: train_loss -0.5247 
2023-10-26 15:23:16.188136: val_loss -0.5811 
2023-10-26 15:23:16.188383: Pseudo dice [0.8054, 0.8601, 0.9503, 0.0, 0.886] 
2023-10-26 15:23:16.188599: Epoch time: 3.99 s 
2023-10-26 15:23:16.188801: Yayy! New best EMA pseudo Dice: 0.6816 
2023-10-26 15:23:17.276426:  
2023-10-26 15:23:17.276761: Epoch 2 
2023-10-26 15:23:17.277058: Current learning rate: 0.00998 
2023-10-26 15:23:21.135279: train_loss -0.5836 
2023-10-26 15:23:21.135668: val_loss -0.6684 
2023-10-26 15:23:21.135926: Pseudo dice [0.8026, 0.8769, 0.9607, 0.0, 0.9184] 
2023-10-26 15:23:21.136162: Epoch time: 3.86 s 
2023-10-26 15:23:21.136396: Yayy! New best EMA pseudo Dice: 0.6846 
2023-10-26 15:23:22.432402:  
2023-10-26 15:23:22.432703: Epoch 3 
2023-10-26 15:23:22.432960: Current learning rate: 0.00997 
2023-10-26 15:23:26.392963: train_loss -0.615 
2023-10-26 15:23:26.393341: val_loss -0.6506 
2023-10-26 15:23:26.393585: Pseudo dice [0.8125, 0.8791, 0.9538, 0.0, 0.9002] 
2023-10-26 15:23:26.393806: Epoch time: 3.96 s 
2023-10-26 15:23:26.394036: Yayy! New best EMA pseudo Dice: 0.687 
2023-10-26 15:23:27.574896:  
2023-10-26 15:23:27.575192: Epoch 4 
2023-10-26 15:23:27.575442: Current learning rate: 0.00996 
2023-10-26 15:23:31.538868: train_loss -0.6316 
2023-10-26 15:23:31.539241: val_loss -0.6573 
2023-10-26 15:23:31.539499: Pseudo dice [0.8225, 0.8942, 0.9618, 0.0, 0.9134] 
2023-10-26 15:23:31.539763: Epoch time: 3.96 s 
2023-10-26 15:23:31.539986: Yayy! New best EMA pseudo Dice: 0.6902 
2023-10-26 15:23:32.726624:  
2023-10-26 15:23:32.726959: Epoch 5 
2023-10-26 15:23:32.727202: Current learning rate: 0.00995 
2023-10-26 15:23:36.667756: train_loss -0.6376 
2023-10-26 15:23:36.668138: val_loss -0.6746 
2023-10-26 15:23:36.668393: Pseudo dice [0.8324, 0.8957, 0.9628, 0.0, 0.9107] 
2023-10-26 15:23:36.668665: Epoch time: 3.94 s 
2023-10-26 15:23:36.668883: Yayy! New best EMA pseudo Dice: 0.6932 
2023-10-26 15:23:37.810971:  
2023-10-26 15:23:37.811274: Epoch 6 
2023-10-26 15:23:37.811529: Current learning rate: 0.00995 
2023-10-26 15:23:41.823765: train_loss -0.6499 
2023-10-26 15:23:41.824178: val_loss -0.6796 
2023-10-26 15:23:41.824591: Pseudo dice [0.8295, 0.8933, 0.9641, 0.0, 0.9242] 
2023-10-26 15:23:41.824894: Epoch time: 4.01 s 
2023-10-26 15:23:41.825184: Yayy! New best EMA pseudo Dice: 0.6961 
2023-10-26 15:23:42.920244:  
2023-10-26 15:23:42.920540: Epoch 7 
2023-10-26 15:23:42.920780: Current learning rate: 0.00994 
2023-10-26 15:23:46.805351: train_loss -0.643 
2023-10-26 15:23:46.805744: val_loss -0.6906 
2023-10-26 15:23:46.806023: Pseudo dice [0.845, 0.9048, 0.9608, 0.5044, 0.9098] 
2023-10-26 15:23:46.806261: Epoch time: 3.89 s 
2023-10-26 15:23:46.806469: Yayy! New best EMA pseudo Dice: 0.709 
2023-10-26 15:23:47.937470:  
2023-10-26 15:23:47.937798: Epoch 8 
2023-10-26 15:23:47.938107: Current learning rate: 0.00993 
2023-10-26 15:23:51.930364: train_loss -0.6606 
2023-10-26 15:23:51.930747: val_loss -0.6907 
2023-10-26 15:23:51.931028: Pseudo dice [0.8392, 0.9081, 0.9623, 0.562, 0.9122] 
2023-10-26 15:23:51.931274: Epoch time: 3.99 s 
2023-10-26 15:23:51.931513: Yayy! New best EMA pseudo Dice: 0.7218 
2023-10-26 15:23:53.238129:  
2023-10-26 15:23:53.238434: Epoch 9 
2023-10-26 15:23:53.238678: Current learning rate: 0.00992 
2023-10-26 15:23:57.050463: train_loss -0.6741 
2023-10-26 15:23:57.050862: val_loss -0.701 
2023-10-26 15:23:57.051179: Pseudo dice [0.8304, 0.897, 0.9639, 0.6343, 0.915] 
2023-10-26 15:23:57.051424: Epoch time: 3.81 s 
2023-10-26 15:23:57.051653: Yayy! New best EMA pseudo Dice: 0.7344 
2023-10-26 15:23:58.155855:  
2023-10-26 15:23:58.156167: Epoch 10 
2023-10-26 15:23:58.156403: Current learning rate: 0.00991 
2023-10-26 15:24:02.033885: train_loss -0.6816 
2023-10-26 15:24:02.034249: val_loss -0.7085 
2023-10-26 15:24:02.034551: Pseudo dice [0.8332, 0.9011, 0.9646, 0.5907, 0.9207] 
2023-10-26 15:24:02.034810: Epoch time: 3.88 s 
2023-10-26 15:24:02.035095: Yayy! New best EMA pseudo Dice: 0.7452 
2023-10-26 15:24:03.140051:  
2023-10-26 15:24:03.140366: Epoch 11 
2023-10-26 15:24:03.140658: Current learning rate: 0.0099 
2023-10-26 15:24:07.149090: train_loss -0.6839 
2023-10-26 15:24:07.149467: val_loss -0.7063 
2023-10-26 15:24:07.149746: Pseudo dice [0.8388, 0.905, 0.9587, 0.6237, 0.9159] 
2023-10-26 15:24:07.149989: Epoch time: 4.01 s 
2023-10-26 15:24:07.150202: Yayy! New best EMA pseudo Dice: 0.7555 
2023-10-26 15:24:08.254050:  
2023-10-26 15:24:08.254576: Epoch 12 
2023-10-26 15:24:08.254822: Current learning rate: 0.00989 
2023-10-26 15:24:12.097109: train_loss -0.6782 
2023-10-26 15:24:12.097484: val_loss -0.7077 
2023-10-26 15:24:12.097742: Pseudo dice [0.835, 0.9098, 0.9642, 0.707, 0.9157] 
2023-10-26 15:24:12.098015: Epoch time: 3.84 s 
2023-10-26 15:24:12.098239: Yayy! New best EMA pseudo Dice: 0.7666 
2023-10-26 15:24:13.261974:  
2023-10-26 15:24:13.262372: Epoch 13 
2023-10-26 15:24:13.262681: Current learning rate: 0.00988 
2023-10-26 15:24:17.062641: train_loss -0.6774 
2023-10-26 15:24:17.063046: val_loss -0.7216 
2023-10-26 15:24:17.063529: Pseudo dice [0.8519, 0.907, 0.9666, 0.6962, 0.924] 
2023-10-26 15:24:17.063848: Epoch time: 3.8 s 
2023-10-26 15:24:17.064150: Yayy! New best EMA pseudo Dice: 0.7768 
2023-10-26 15:24:18.167115:  
2023-10-26 15:24:18.167414: Epoch 14 
2023-10-26 15:24:18.167655: Current learning rate: 0.00987 
2023-10-26 15:24:22.059964: train_loss -0.6874 
2023-10-26 15:24:22.060327: val_loss -0.7195 
2023-10-26 15:24:22.060573: Pseudo dice [0.839, 0.9088, 0.9685, 0.7235, 0.9266] 
2023-10-26 15:24:22.060785: Epoch time: 3.89 s 
2023-10-26 15:24:22.060991: Yayy! New best EMA pseudo Dice: 0.7865 
2023-10-26 15:24:23.176188:  
2023-10-26 15:24:23.176478: Epoch 15 
2023-10-26 15:24:23.176711: Current learning rate: 0.00986 
2023-10-26 15:24:27.353646: train_loss -0.6874 
2023-10-26 15:24:27.354053: val_loss -0.725 
2023-10-26 15:24:27.354341: Pseudo dice [0.8636, 0.9121, 0.9623, 0.7075, 0.9282] 
2023-10-26 15:24:27.354579: Epoch time: 4.18 s 
2023-10-26 15:24:27.354787: Yayy! New best EMA pseudo Dice: 0.7953 
2023-10-26 15:24:28.483548:  
2023-10-26 15:24:28.483846: Epoch 16 
2023-10-26 15:24:28.484092: Current learning rate: 0.00986 
2023-10-26 15:24:32.508598: train_loss -0.6872 
2023-10-26 15:24:32.508975: val_loss -0.7192 
2023-10-26 15:24:32.509257: Pseudo dice [0.8488, 0.9098, 0.9656, 0.7401, 0.9339] 
2023-10-26 15:24:32.509489: Epoch time: 4.03 s 
2023-10-26 15:24:32.509781: Yayy! New best EMA pseudo Dice: 0.8037 
2023-10-26 15:24:33.666115:  
2023-10-26 15:24:33.666426: Epoch 17 
2023-10-26 15:24:33.666684: Current learning rate: 0.00985 
2023-10-26 15:24:37.636912: train_loss -0.6898 
2023-10-26 15:24:37.637358: val_loss -0.7233 
2023-10-26 15:24:37.637662: Pseudo dice [0.8484, 0.9133, 0.9701, 0.7153, 0.9219] 
2023-10-26 15:24:37.637904: Epoch time: 3.97 s 
2023-10-26 15:24:37.638141: Yayy! New best EMA pseudo Dice: 0.8107 
2023-10-26 15:24:38.775558:  
2023-10-26 15:24:38.775869: Epoch 18 
2023-10-26 15:24:38.776106: Current learning rate: 0.00984 
2023-10-26 15:24:42.784621: train_loss -0.6908 
2023-10-26 15:24:42.785010: val_loss -0.711 
2023-10-26 15:24:42.785277: Pseudo dice [0.8483, 0.9062, 0.9601, 0.6689, 0.9162] 
2023-10-26 15:24:42.785502: Epoch time: 4.01 s 
2023-10-26 15:24:42.785708: Yayy! New best EMA pseudo Dice: 0.8157 
2023-10-26 15:24:43.934461:  
2023-10-26 15:24:43.934745: Epoch 19 
2023-10-26 15:24:43.934983: Current learning rate: 0.00983 
2023-10-26 15:24:47.926460: train_loss -0.6949 
2023-10-26 15:24:47.926844: val_loss -0.7214 
2023-10-26 15:24:47.927137: Pseudo dice [0.8482, 0.915, 0.9678, 0.715, 0.9258] 
2023-10-26 15:24:47.927372: Epoch time: 3.99 s 
2023-10-26 15:24:47.927586: Yayy! New best EMA pseudo Dice: 0.8215 
2023-10-26 15:24:49.073520:  
2023-10-26 15:24:49.073816: Epoch 20 
2023-10-26 15:24:49.074056: Current learning rate: 0.00982 
2023-10-26 15:24:53.054845: train_loss -0.6934 
2023-10-26 15:24:53.055256: val_loss -0.7165 
2023-10-26 15:24:53.055515: Pseudo dice [0.8402, 0.913, 0.9675, 0.7254, 0.9156] 
2023-10-26 15:24:53.055756: Epoch time: 3.98 s 
2023-10-26 15:24:53.055965: Yayy! New best EMA pseudo Dice: 0.8266 
2023-10-26 15:24:54.422842:  
2023-10-26 15:24:54.423160: Epoch 21 
2023-10-26 15:24:54.423433: Current learning rate: 0.00981 
2023-10-26 15:24:58.345914: train_loss -0.6892 
2023-10-26 15:24:58.346336: val_loss -0.7206 
2023-10-26 15:24:58.346611: Pseudo dice [0.8307, 0.9128, 0.9673, 0.715, 0.925] 
2023-10-26 15:24:58.346833: Epoch time: 3.92 s 
2023-10-26 15:24:58.347057: Yayy! New best EMA pseudo Dice: 0.831 
2023-10-26 15:24:59.447209:  
2023-10-26 15:24:59.447512: Epoch 22 
2023-10-26 15:24:59.447769: Current learning rate: 0.0098 
2023-10-26 15:25:03.251143: train_loss -0.695 
2023-10-26 15:25:03.251564: val_loss -0.7063 
2023-10-26 15:25:03.251840: Pseudo dice [0.853, 0.9128, 0.9636, 0.6489, 0.917] 
2023-10-26 15:25:03.252158: Epoch time: 3.8 s 
2023-10-26 15:25:03.252369: Yayy! New best EMA pseudo Dice: 0.8338 
2023-10-26 15:25:04.398411:  
2023-10-26 15:25:04.398703: Epoch 23 
2023-10-26 15:25:04.398959: Current learning rate: 0.00979 
2023-10-26 15:25:08.295690: train_loss -0.6905 
2023-10-26 15:25:08.296136: val_loss -0.7147 
2023-10-26 15:25:08.296658: Pseudo dice [0.8391, 0.9138, 0.9633, 0.6497, 0.9232] 
2023-10-26 15:25:08.297045: Epoch time: 3.9 s 
2023-10-26 15:25:08.297372: Yayy! New best EMA pseudo Dice: 0.8362 
2023-10-26 15:25:09.412983:  
2023-10-26 15:25:09.413291: Epoch 24 
2023-10-26 15:25:09.413573: Current learning rate: 0.00978 
2023-10-26 15:25:13.347556: train_loss -0.691 
2023-10-26 15:25:13.347936: val_loss -0.7206 
2023-10-26 15:25:13.348223: Pseudo dice [0.8579, 0.9181, 0.9699, 0.7284, 0.9164] 
2023-10-26 15:25:13.348505: Epoch time: 3.94 s 
2023-10-26 15:25:13.348714: Yayy! New best EMA pseudo Dice: 0.8404 
2023-10-26 15:25:14.458339:  
2023-10-26 15:25:14.458626: Epoch 25 
2023-10-26 15:25:14.458869: Current learning rate: 0.00977 
2023-10-26 15:25:18.389608: train_loss -0.6966 
2023-10-26 15:25:18.390037: val_loss -0.7164 
2023-10-26 15:25:18.390321: Pseudo dice [0.8523, 0.9183, 0.9667, 0.7067, 0.9189] 
2023-10-26 15:25:18.390647: Epoch time: 3.93 s 
2023-10-26 15:25:18.391051: Yayy! New best EMA pseudo Dice: 0.8436 
2023-10-26 15:25:19.492782:  
2023-10-26 15:25:19.493092: Epoch 26 
2023-10-26 15:25:19.493340: Current learning rate: 0.00977 
2023-10-26 15:25:23.410996: train_loss -0.7023 
2023-10-26 15:25:23.411358: val_loss -0.7157 
2023-10-26 15:25:23.411641: Pseudo dice [0.8487, 0.9142, 0.9681, 0.712, 0.9229] 
2023-10-26 15:25:23.411861: Epoch time: 3.92 s 
2023-10-26 15:25:23.412092: Yayy! New best EMA pseudo Dice: 0.8466 
2023-10-26 15:25:24.508684:  
2023-10-26 15:25:24.508993: Epoch 27 
2023-10-26 15:25:24.509228: Current learning rate: 0.00976 
2023-10-26 15:25:28.443650: train_loss -0.6954 
2023-10-26 15:25:28.444011: val_loss -0.7178 
2023-10-26 15:25:28.444261: Pseudo dice [0.864, 0.9223, 0.9672, 0.7192, 0.9215] 
2023-10-26 15:25:28.444483: Epoch time: 3.94 s 
2023-10-26 15:25:28.444687: Yayy! New best EMA pseudo Dice: 0.8498 
2023-10-26 15:25:29.720988:  
2023-10-26 15:25:29.721271: Epoch 28 
2023-10-26 15:25:29.721507: Current learning rate: 0.00975 
2023-10-26 15:25:33.752723: train_loss -0.7011 
2023-10-26 15:25:33.753080: val_loss -0.7171 
2023-10-26 15:25:33.753409: Pseudo dice [0.8508, 0.9191, 0.9665, 0.7132, 0.9341] 
2023-10-26 15:25:33.753634: Epoch time: 4.03 s 
2023-10-26 15:25:33.753858: Yayy! New best EMA pseudo Dice: 0.8525 
2023-10-26 15:25:35.012078:  
2023-10-26 15:25:35.012390: Epoch 29 
2023-10-26 15:25:35.012630: Current learning rate: 0.00974 
2023-10-26 15:25:39.060772: train_loss -0.7046 
2023-10-26 15:25:39.061133: val_loss -0.7234 
2023-10-26 15:25:39.061402: Pseudo dice [0.8676, 0.9203, 0.9702, 0.7084, 0.9264] 
2023-10-26 15:25:39.061628: Epoch time: 4.05 s 
2023-10-26 15:25:39.061837: Yayy! New best EMA pseudo Dice: 0.8551 
2023-10-26 15:25:40.186735:  
2023-10-26 15:25:40.187035: Epoch 30 
2023-10-26 15:25:40.187283: Current learning rate: 0.00973 
2023-10-26 15:25:44.161828: train_loss -0.6971 
2023-10-26 15:25:44.162196: val_loss -0.7226 
2023-10-26 15:25:44.162468: Pseudo dice [0.8555, 0.9168, 0.9648, 0.7334, 0.9178] 
2023-10-26 15:25:44.162692: Epoch time: 3.98 s 
2023-10-26 15:25:44.162900: Yayy! New best EMA pseudo Dice: 0.8573 
2023-10-26 15:25:45.306192:  
2023-10-26 15:25:45.306489: Epoch 31 
2023-10-26 15:25:45.306733: Current learning rate: 0.00972 
2023-10-26 15:25:49.259264: train_loss -0.696 
2023-10-26 15:25:49.259653: val_loss -0.7113 
2023-10-26 15:25:49.260078: Pseudo dice [0.8565, 0.913, 0.9649, 0.6603, 0.9092] 
2023-10-26 15:25:49.260379: Epoch time: 3.95 s 
2023-10-26 15:25:49.260677: Yayy! New best EMA pseudo Dice: 0.8577 
2023-10-26 15:25:50.367266:  
2023-10-26 15:25:50.367567: Epoch 32 
2023-10-26 15:25:50.367810: Current learning rate: 0.00971 
2023-10-26 15:25:54.384838: train_loss -0.7033 
2023-10-26 15:25:54.385210: val_loss -0.7404 
2023-10-26 15:25:54.385471: Pseudo dice [0.8569, 0.9155, 0.9694, 0.7193, 0.9265] 
2023-10-26 15:25:54.385706: Epoch time: 4.02 s 
2023-10-26 15:25:54.385921: Yayy! New best EMA pseudo Dice: 0.8597 
2023-10-26 15:25:55.498449:  
2023-10-26 15:25:55.498741: Epoch 33 
2023-10-26 15:25:55.498973: Current learning rate: 0.0097 
2023-10-26 15:25:59.552098: train_loss -0.7034 
2023-10-26 15:25:59.552472: val_loss -0.7273 
2023-10-26 15:25:59.552743: Pseudo dice [0.8565, 0.9178, 0.9689, 0.6318, 0.9347] 
2023-10-26 15:25:59.552982: Epoch time: 4.05 s 
2023-10-26 15:25:59.553185: Yayy! New best EMA pseudo Dice: 0.8599 
2023-10-26 15:26:00.807290:  
2023-10-26 15:26:00.807620: Epoch 34 
2023-10-26 15:26:00.807867: Current learning rate: 0.00969 
2023-10-26 15:26:04.895903: train_loss -0.7079 
2023-10-26 15:26:04.898798: val_loss -0.7157 
2023-10-26 15:26:04.899072: Pseudo dice [0.8493, 0.9199, 0.968, 0.6797, 0.9231] 
2023-10-26 15:26:04.899292: Epoch time: 4.09 s 
2023-10-26 15:26:04.899505: Yayy! New best EMA pseudo Dice: 0.8607 
2023-10-26 15:26:06.019537:  
2023-10-26 15:26:06.019835: Epoch 35 
2023-10-26 15:26:06.020076: Current learning rate: 0.00968 
2023-10-26 15:26:10.049268: train_loss -0.7032 
2023-10-26 15:26:10.049637: val_loss -0.7212 
2023-10-26 15:26:10.049911: Pseudo dice [0.8519, 0.9217, 0.9689, 0.7021, 0.9278] 
2023-10-26 15:26:10.050157: Epoch time: 4.03 s 
2023-10-26 15:26:10.050380: Yayy! New best EMA pseudo Dice: 0.8621 
2023-10-26 15:26:11.211920:  
2023-10-26 15:26:11.212236: Epoch 36 
2023-10-26 15:26:11.212512: Current learning rate: 0.00968 
2023-10-26 15:26:15.114934: train_loss -0.6978 
2023-10-26 15:26:15.115311: val_loss -0.7061 
2023-10-26 15:26:15.115572: Pseudo dice [0.8346, 0.9172, 0.9665, 0.68, 0.9183] 
2023-10-26 15:26:15.115800: Epoch time: 3.9 s 
2023-10-26 15:26:15.116030: Yayy! New best EMA pseudo Dice: 0.8622 
2023-10-26 15:26:16.255293:  
2023-10-26 15:26:16.255577: Epoch 37 
2023-10-26 15:26:16.255818: Current learning rate: 0.00967 
2023-10-26 15:26:20.126842: train_loss -0.7023 
2023-10-26 15:26:20.127261: val_loss -0.7313 
2023-10-26 15:26:20.127563: Pseudo dice [0.872, 0.9212, 0.9704, 0.6932, 0.9336] 
2023-10-26 15:26:20.127794: Epoch time: 3.87 s 
2023-10-26 15:26:20.128070: Yayy! New best EMA pseudo Dice: 0.8638 
2023-10-26 15:26:21.282529:  
2023-10-26 15:26:21.282818: Epoch 38 
2023-10-26 15:26:21.283101: Current learning rate: 0.00966 
2023-10-26 15:26:25.194263: train_loss -0.6989 
2023-10-26 15:26:25.194649: val_loss -0.727 
2023-10-26 15:26:25.194929: Pseudo dice [0.8641, 0.9176, 0.9676, 0.686, 0.925] 
2023-10-26 15:26:25.195158: Epoch time: 3.91 s 
2023-10-26 15:26:25.195373: Yayy! New best EMA pseudo Dice: 0.8646 
2023-10-26 15:26:26.492247:  
2023-10-26 15:26:26.492537: Epoch 39 
2023-10-26 15:26:26.492774: Current learning rate: 0.00965 
2023-10-26 15:26:30.394184: train_loss -0.7087 
2023-10-26 15:26:30.394556: val_loss -0.7673 
2023-10-26 15:26:30.394817: Pseudo dice [0.8433, 0.9091, 0.9688, 0.3928, 0.9227] 
2023-10-26 15:26:30.395046: Epoch time: 3.9 s 
2023-10-26 15:26:31.527802:  
2023-10-26 15:26:31.528114: Epoch 40 
2023-10-26 15:26:31.528360: Current learning rate: 0.00964 
2023-10-26 15:26:35.532990: train_loss -0.7681 
2023-10-26 15:26:35.533408: val_loss -0.8178 
2023-10-26 15:26:35.533672: Pseudo dice [0.8175, 0.902, 0.9618, 0.0, 0.9142] 
2023-10-26 15:26:35.533908: Epoch time: 4.01 s 
2023-10-26 15:26:36.645788:  
2023-10-26 15:26:36.646073: Epoch 41 
2023-10-26 15:26:36.646322: Current learning rate: 0.00963 
2023-10-26 15:26:40.597156: train_loss -0.7901 
2023-10-26 15:26:40.597568: val_loss -0.8213 
2023-10-26 15:26:40.597842: Pseudo dice [0.8477, 0.9046, 0.9642, 0.1523, 0.9049] 
2023-10-26 15:26:40.598102: Epoch time: 3.95 s 
2023-10-26 15:26:41.654541:  
2023-10-26 15:26:41.654909: Epoch 42 
2023-10-26 15:26:41.655161: Current learning rate: 0.00962 
2023-10-26 15:26:45.604342: train_loss -0.7771 
2023-10-26 15:26:45.604802: val_loss -0.8095 
2023-10-26 15:26:45.605338: Pseudo dice [0.8465, 0.9022, 0.9613, 0.0, 0.8936] 
2023-10-26 15:26:45.605927: Epoch time: 3.95 s 
2023-10-26 15:26:46.645097:  
2023-10-26 15:26:46.645388: Epoch 43 
2023-10-26 15:26:46.645622: Current learning rate: 0.00961 
2023-10-26 15:26:50.507523: train_loss -0.7862 
2023-10-26 15:26:50.508008: val_loss -0.853 
2023-10-26 15:26:50.508335: Pseudo dice [0.8747, 0.909, 0.9669, 0.0, 0.9245] 
2023-10-26 15:26:50.508636: Epoch time: 3.86 s 
2023-10-26 15:26:51.598748:  
2023-10-26 15:26:51.599120: Epoch 44 
2023-10-26 15:26:51.599364: Current learning rate: 0.0096 
2023-10-26 15:26:55.443967: train_loss -0.8075 
2023-10-26 15:26:55.444341: val_loss -0.835 
2023-10-26 15:26:55.444598: Pseudo dice [0.8399, 0.9101, 0.9652, 0.2079, 0.9297] 
2023-10-26 15:26:55.444826: Epoch time: 3.85 s 
2023-10-26 15:26:56.677326:  
2023-10-26 15:26:56.677672: Epoch 45 
2023-10-26 15:26:56.677936: Current learning rate: 0.00959 
2023-10-26 15:27:00.599700: train_loss -0.8101 
2023-10-26 15:27:00.600074: val_loss -0.8285 
2023-10-26 15:27:00.600325: Pseudo dice [0.8628, 0.8842, 0.9638, 0.1056, 0.9198] 
2023-10-26 15:27:00.600549: Epoch time: 3.92 s 
2023-10-26 15:27:01.641059:  
2023-10-26 15:27:01.641371: Epoch 46 
2023-10-26 15:27:01.641621: Current learning rate: 0.00959 
2023-10-26 15:27:05.772986: train_loss -0.7936 
2023-10-26 15:27:05.773345: val_loss -0.8592 
2023-10-26 15:27:05.773618: Pseudo dice [0.8593, 0.9068, 0.9601, 0.6667, 0.9235] 
2023-10-26 15:27:05.773847: Epoch time: 4.13 s 
2023-10-26 15:27:06.914968:  
2023-10-26 15:27:06.915271: Epoch 47 
2023-10-26 15:27:06.915514: Current learning rate: 0.00958 
2023-10-26 15:27:10.864417: train_loss -0.7847 
2023-10-26 15:27:10.864783: val_loss -0.8128 
2023-10-26 15:27:10.865069: Pseudo dice [0.8249, 0.9013, 0.9624, 0.0205, 0.9187] 
2023-10-26 15:27:10.865394: Epoch time: 3.95 s 
2023-10-26 15:27:11.914140:  
2023-10-26 15:27:11.914443: Epoch 48 
2023-10-26 15:27:11.914683: Current learning rate: 0.00957 
2023-10-26 15:27:15.807923: train_loss -0.7898 
2023-10-26 15:27:15.808297: val_loss -0.788 
2023-10-26 15:27:15.808561: Pseudo dice [0.8506, 0.9155, 0.9531, 0.7012, 0.8289] 
2023-10-26 15:27:15.808779: Epoch time: 3.89 s 
2023-10-26 15:27:16.846025:  
2023-10-26 15:27:16.846308: Epoch 49 
2023-10-26 15:27:16.846547: Current learning rate: 0.00956 
2023-10-26 15:27:20.646750: train_loss -0.7955 
2023-10-26 15:27:20.647365: val_loss -0.8002 
2023-10-26 15:27:20.647791: Pseudo dice [0.8494, 0.9051, 0.963, 0.6899, 0.9178] 
2023-10-26 15:27:20.648236: Epoch time: 3.8 s 
2023-10-26 15:27:21.759484:  
2023-10-26 15:27:21.759769: Epoch 50 
2023-10-26 15:27:21.760010: Current learning rate: 0.00955 
2023-10-26 15:27:25.707204: train_loss -0.7558 
2023-10-26 15:27:25.707596: val_loss -0.8071 
2023-10-26 15:27:25.707846: Pseudo dice [0.8289, 0.8895, 0.9618, 0.0, 0.8967] 
2023-10-26 15:27:25.708081: Epoch time: 3.95 s 
2023-10-26 15:27:26.758224:  
2023-10-26 15:27:26.758518: Epoch 51 
2023-10-26 15:27:26.758759: Current learning rate: 0.00954 
2023-10-26 15:27:30.707217: train_loss -0.7712 
2023-10-26 15:27:30.707599: val_loss -0.8338 
2023-10-26 15:27:30.707865: Pseudo dice [0.8431, 0.9096, 0.966, 0.0, 0.9121] 
2023-10-26 15:27:30.708112: Epoch time: 3.95 s 
2023-10-26 15:27:31.964240:  
2023-10-26 15:27:31.964638: Epoch 52 
2023-10-26 15:27:31.964950: Current learning rate: 0.00953 
2023-10-26 15:27:35.895053: train_loss -0.7915 
2023-10-26 15:27:35.895448: val_loss -0.8115 
2023-10-26 15:27:35.895716: Pseudo dice [0.8429, 0.9114, 0.964, 0.0, 0.9108] 
2023-10-26 15:27:35.895959: Epoch time: 3.93 s 
2023-10-26 15:27:36.955099:  
2023-10-26 15:27:36.955456: Epoch 53 
2023-10-26 15:27:36.955761: Current learning rate: 0.00952 
2023-10-26 15:27:40.879562: train_loss -0.7937 
2023-10-26 15:27:40.880032: val_loss -0.8259 
2023-10-26 15:27:40.880323: Pseudo dice [0.8516, 0.9131, 0.9684, 0.0, 0.9202] 
2023-10-26 15:27:40.880579: Epoch time: 3.93 s 
2023-10-26 15:27:41.930383:  
2023-10-26 15:27:41.930710: Epoch 54 
2023-10-26 15:27:41.930954: Current learning rate: 0.00951 
2023-10-26 15:27:45.813780: train_loss -0.7966 
2023-10-26 15:27:45.814170: val_loss -0.8225 
2023-10-26 15:27:45.814432: Pseudo dice [0.84, 0.9184, 0.9654, 0.0, 0.9214] 
2023-10-26 15:27:45.814671: Epoch time: 3.88 s 
2023-10-26 15:27:46.876348:  
2023-10-26 15:27:46.876656: Epoch 55 
2023-10-26 15:27:46.876924: Current learning rate: 0.0095 
2023-10-26 15:27:50.794371: train_loss -0.8091 
2023-10-26 15:27:50.794769: val_loss -0.8311 
2023-10-26 15:27:50.795057: Pseudo dice [0.8554, 0.912, 0.9646, 0.0, 0.9118] 
2023-10-26 15:27:50.795313: Epoch time: 3.92 s 
2023-10-26 15:27:51.931330:  
2023-10-26 15:27:51.931640: Epoch 56 
2023-10-26 15:27:51.931898: Current learning rate: 0.00949 
2023-10-26 15:27:55.890654: train_loss -0.7997 
2023-10-26 15:27:55.891045: val_loss -0.8443 
2023-10-26 15:27:55.891316: Pseudo dice [0.8591, 0.9191, 0.9672, 0.0, 0.9205] 
2023-10-26 15:27:55.891554: Epoch time: 3.96 s 
2023-10-26 15:27:56.977939:  
2023-10-26 15:27:56.978338: Epoch 57 
2023-10-26 15:27:56.978662: Current learning rate: 0.00949 
2023-10-26 15:28:00.861465: train_loss -0.802 
2023-10-26 15:28:00.861848: val_loss -0.8365 
2023-10-26 15:28:00.862109: Pseudo dice [0.8602, 0.9161, 0.9677, 0.0, 0.9201] 
2023-10-26 15:28:00.862348: Epoch time: 3.88 s 
2023-10-26 15:28:02.076535:  
2023-10-26 15:28:02.076853: Epoch 58 
2023-10-26 15:28:02.077107: Current learning rate: 0.00948 
2023-10-26 15:28:06.150290: train_loss -0.795 
2023-10-26 15:28:06.150666: val_loss -0.8283 
2023-10-26 15:28:06.151071: Pseudo dice [0.8571, 0.9186, 0.9677, 0.0, 0.9268] 
2023-10-26 15:28:06.151384: Epoch time: 4.07 s 
2023-10-26 15:28:07.221857:  
2023-10-26 15:28:07.222198: Epoch 59 
2023-10-26 15:28:07.222436: Current learning rate: 0.00947 
2023-10-26 15:28:11.198020: train_loss -0.802 
2023-10-26 15:28:11.198400: val_loss -0.8436 
2023-10-26 15:28:11.198663: Pseudo dice [0.8499, 0.9156, 0.9639, 0.0, 0.9156] 
2023-10-26 15:28:11.198904: Epoch time: 3.98 s 
2023-10-26 15:28:12.260009:  
2023-10-26 15:28:12.260310: Epoch 60 
2023-10-26 15:28:12.260569: Current learning rate: 0.00946 
2023-10-26 15:28:16.334918: train_loss -0.8047 
2023-10-26 15:28:16.335287: val_loss -0.8523 
2023-10-26 15:28:16.335641: Pseudo dice [0.8576, 0.9173, 0.968, 0.0, 0.9338] 
2023-10-26 15:28:16.336033: Epoch time: 4.08 s 
2023-10-26 15:28:17.435442:  
2023-10-26 15:28:17.435764: Epoch 61 
2023-10-26 15:28:17.436025: Current learning rate: 0.00945 
2023-10-26 15:28:21.459334: train_loss -0.8026 
2023-10-26 15:28:21.459692: val_loss -0.8435 
2023-10-26 15:28:21.459942: Pseudo dice [0.8646, 0.9185, 0.9695, 0.0, 0.9237] 
2023-10-26 15:28:21.460163: Epoch time: 4.02 s 
2023-10-26 15:28:22.513848:  
2023-10-26 15:28:22.514153: Epoch 62 
2023-10-26 15:28:22.514401: Current learning rate: 0.00944 
2023-10-26 15:28:26.530105: train_loss -0.8021 
2023-10-26 15:28:26.530480: val_loss -0.8446 
2023-10-26 15:28:26.530739: Pseudo dice [0.873, 0.9211, 0.9679, 0.0, 0.9272] 
2023-10-26 15:28:26.530988: Epoch time: 4.02 s 
2023-10-26 15:28:27.585205:  
2023-10-26 15:28:27.585519: Epoch 63 
2023-10-26 15:28:27.585811: Current learning rate: 0.00943 
2023-10-26 15:28:31.540943: train_loss -0.8073 
2023-10-26 15:28:31.541366: val_loss -0.8373 
2023-10-26 15:28:31.541697: Pseudo dice [0.8524, 0.9143, 0.9669, 0.0, 0.9274] 
2023-10-26 15:28:31.542031: Epoch time: 3.96 s 
2023-10-26 15:28:32.757869:  
2023-10-26 15:28:32.758175: Epoch 64 
2023-10-26 15:28:32.758403: Current learning rate: 0.00942 
2023-10-26 15:28:36.799734: train_loss -0.8079 
2023-10-26 15:28:36.800586: val_loss -0.8421 
2023-10-26 15:28:36.800912: Pseudo dice [0.8652, 0.9162, 0.9682, 0.0, 0.9257] 
2023-10-26 15:28:36.801252: Epoch time: 4.04 s 
2023-10-26 15:28:37.898799:  
2023-10-26 15:28:37.899120: Epoch 65 
2023-10-26 15:28:37.899362: Current learning rate: 0.00941 
2023-10-26 15:28:41.954707: train_loss -0.8133 
2023-10-26 15:28:41.955223: val_loss -0.8412 
2023-10-26 15:28:41.955647: Pseudo dice [0.8616, 0.9207, 0.9701, 0.0, 0.9264] 
2023-10-26 15:28:41.956064: Epoch time: 4.06 s 
2023-10-26 15:28:43.018621:  
2023-10-26 15:28:43.018959: Epoch 66 
2023-10-26 15:28:43.019200: Current learning rate: 0.0094 
2023-10-26 15:28:47.018659: train_loss -0.8199 
2023-10-26 15:28:47.019069: val_loss -0.8234 
2023-10-26 15:28:47.019343: Pseudo dice [0.8399, 0.9008, 0.964, 0.0345, 0.9285] 
2023-10-26 15:28:47.019688: Epoch time: 4.0 s 
2023-10-26 15:28:48.092784:  
2023-10-26 15:28:48.093124: Epoch 67 
2023-10-26 15:28:48.093394: Current learning rate: 0.00939 
2023-10-26 15:28:51.967634: train_loss -0.799 
2023-10-26 15:28:51.968037: val_loss -0.8496 
2023-10-26 15:28:51.968332: Pseudo dice [0.8639, 0.9046, 0.968, 0.5856, 0.9325] 
2023-10-26 15:28:51.968599: Epoch time: 3.88 s 
2023-10-26 15:28:53.051249:  
2023-10-26 15:28:53.051563: Epoch 68 
2023-10-26 15:28:53.051803: Current learning rate: 0.00939 
2023-10-26 15:28:56.976910: train_loss -0.8181 
2023-10-26 15:28:56.977312: val_loss -0.8476 
2023-10-26 15:28:56.977594: Pseudo dice [0.8545, 0.9136, 0.9653, 0.5064, 0.9222] 
2023-10-26 15:28:56.977834: Epoch time: 3.93 s 
2023-10-26 15:28:58.099413:  
2023-10-26 15:28:58.099741: Epoch 69 
2023-10-26 15:28:58.099997: Current learning rate: 0.00938 
2023-10-26 15:29:02.038004: train_loss -0.8164 
2023-10-26 15:29:02.038461: val_loss -0.8399 
2023-10-26 15:29:02.038740: Pseudo dice [0.8641, 0.9114, 0.967, 0.6195, 0.9157] 
2023-10-26 15:29:02.038993: Epoch time: 3.94 s 
2023-10-26 15:29:03.332907:  
2023-10-26 15:29:03.333242: Epoch 70 
2023-10-26 15:29:03.333495: Current learning rate: 0.00937 
2023-10-26 15:29:07.318989: train_loss -0.8091 
2023-10-26 15:29:07.319372: val_loss -0.8273 
2023-10-26 15:29:07.319646: Pseudo dice [0.8625, 0.907, 0.964, 0.5636, 0.9338] 
2023-10-26 15:29:07.319894: Epoch time: 3.99 s 
2023-10-26 15:29:08.413806:  
2023-10-26 15:29:08.414114: Epoch 71 
2023-10-26 15:29:08.414402: Current learning rate: 0.00936 
2023-10-26 15:29:12.368985: train_loss -0.8177 
2023-10-26 15:29:12.369393: val_loss -0.8375 
2023-10-26 15:29:12.369679: Pseudo dice [0.8409, 0.9106, 0.9642, 0.685, 0.922] 
2023-10-26 15:29:12.369923: Epoch time: 3.96 s 
2023-10-26 15:29:13.460656:  
2023-10-26 15:29:13.460996: Epoch 72 
2023-10-26 15:29:13.461263: Current learning rate: 0.00935 
2023-10-26 15:29:17.355586: train_loss -0.8131 
2023-10-26 15:29:17.355981: val_loss -0.8349 
2023-10-26 15:29:17.356278: Pseudo dice [0.838, 0.9059, 0.9646, 0.2818, 0.9148] 
2023-10-26 15:29:17.356519: Epoch time: 3.9 s 
2023-10-26 15:29:18.453034:  
2023-10-26 15:29:18.453354: Epoch 73 
2023-10-26 15:29:18.453599: Current learning rate: 0.00934 
2023-10-26 15:29:22.336032: train_loss -0.8213 
2023-10-26 15:29:22.336405: val_loss -0.8497 
2023-10-26 15:29:22.336663: Pseudo dice [0.8604, 0.9137, 0.9675, 0.7122, 0.9154] 
2023-10-26 15:29:22.336890: Epoch time: 3.88 s 
2023-10-26 15:29:23.445936:  
2023-10-26 15:29:23.446228: Epoch 74 
2023-10-26 15:29:23.446500: Current learning rate: 0.00933 
2023-10-26 15:29:27.355269: train_loss -0.8261 
2023-10-26 15:29:27.355638: val_loss -0.8566 
2023-10-26 15:29:27.355914: Pseudo dice [0.8441, 0.9124, 0.9681, 0.7101, 0.9213] 
2023-10-26 15:29:27.356212: Epoch time: 3.91 s 
2023-10-26 15:29:28.457582:  
2023-10-26 15:29:28.457919: Epoch 75 
2023-10-26 15:29:28.458215: Current learning rate: 0.00932 
2023-10-26 15:29:32.526103: train_loss -0.822 
2023-10-26 15:29:32.526451: val_loss -0.8509 
2023-10-26 15:29:32.526709: Pseudo dice [0.8629, 0.9161, 0.9646, 0.6985, 0.9119] 
2023-10-26 15:29:32.526944: Epoch time: 4.07 s 
2023-10-26 15:29:33.759161:  
2023-10-26 15:29:33.759468: Epoch 76 
2023-10-26 15:29:33.759727: Current learning rate: 0.00931 
2023-10-26 15:29:37.812772: train_loss -0.8251 
2023-10-26 15:29:37.813241: val_loss -0.8658 
2023-10-26 15:29:37.813503: Pseudo dice [0.8602, 0.9137, 0.9672, 0.718, 0.9297] 
2023-10-26 15:29:37.813737: Epoch time: 4.05 s 
2023-10-26 15:29:38.902189:  
2023-10-26 15:29:38.902532: Epoch 77 
2023-10-26 15:29:38.902792: Current learning rate: 0.0093 
2023-10-26 15:29:42.910119: train_loss -0.8224 
2023-10-26 15:29:42.910589: val_loss -0.8397 
2023-10-26 15:29:42.911029: Pseudo dice [0.828, 0.908, 0.9641, 0.5919, 0.9268] 
2023-10-26 15:29:42.911351: Epoch time: 4.01 s 
2023-10-26 15:29:43.980303:  
2023-10-26 15:29:43.980597: Epoch 78 
2023-10-26 15:29:43.980838: Current learning rate: 0.0093 
2023-10-26 15:29:48.040658: train_loss -0.8262 
2023-10-26 15:29:48.041140: val_loss -0.8537 
2023-10-26 15:29:48.041414: Pseudo dice [0.8551, 0.9186, 0.9632, 0.6888, 0.9345] 
2023-10-26 15:29:48.041656: Epoch time: 4.06 s 
2023-10-26 15:29:49.108355:  
2023-10-26 15:29:49.108671: Epoch 79 
2023-10-26 15:29:49.108926: Current learning rate: 0.00929 
2023-10-26 15:29:53.138411: train_loss -0.8274 
2023-10-26 15:29:53.138761: val_loss -0.8503 
2023-10-26 15:29:53.139028: Pseudo dice [0.8527, 0.9189, 0.9671, 0.6253, 0.9264] 
2023-10-26 15:29:53.139258: Epoch time: 4.03 s 
2023-10-26 15:29:54.215325:  
2023-10-26 15:29:54.215609: Epoch 80 
2023-10-26 15:29:54.215842: Current learning rate: 0.00928 
2023-10-26 15:29:58.285253: train_loss -0.8112 
2023-10-26 15:29:58.285619: val_loss -0.8527 
2023-10-26 15:29:58.285909: Pseudo dice [0.855, 0.9092, 0.9675, 0.6222, 0.9262] 
2023-10-26 15:29:58.286145: Epoch time: 4.07 s 
2023-10-26 15:29:59.381245:  
2023-10-26 15:29:59.381574: Epoch 81 
2023-10-26 15:29:59.381880: Current learning rate: 0.00927 
2023-10-26 15:30:03.409745: train_loss -0.8038 
2023-10-26 15:30:03.410141: val_loss -0.8475 
2023-10-26 15:30:03.410402: Pseudo dice [0.8533, 0.91, 0.9651, 0.6501, 0.9134] 
2023-10-26 15:30:03.410633: Epoch time: 4.03 s 
2023-10-26 15:30:04.670990:  
2023-10-26 15:30:04.671286: Epoch 82 
2023-10-26 15:30:04.671534: Current learning rate: 0.00926 
2023-10-26 15:30:08.694927: train_loss -0.818 
2023-10-26 15:30:08.695551: val_loss -0.8343 
2023-10-26 15:30:08.696014: Pseudo dice [0.8516, 0.8899, 0.9657, 0.2113, 0.9125] 
2023-10-26 15:30:08.696469: Epoch time: 4.02 s 
2023-10-26 15:30:09.747099:  
2023-10-26 15:30:09.747427: Epoch 83 
2023-10-26 15:30:09.747681: Current learning rate: 0.00925 
2023-10-26 15:30:13.673818: train_loss -0.8242 
2023-10-26 15:30:13.674239: val_loss -0.8515 
2023-10-26 15:30:13.674498: Pseudo dice [0.846, 0.9183, 0.9667, 0.6547, 0.9273] 
2023-10-26 15:30:13.674731: Epoch time: 3.93 s 
2023-10-26 15:30:14.720509:  
2023-10-26 15:30:14.720804: Epoch 84 
2023-10-26 15:30:14.721089: Current learning rate: 0.00924 
2023-10-26 15:30:18.623487: train_loss -0.8204 
2023-10-26 15:30:18.623878: val_loss -0.8596 
2023-10-26 15:30:18.624140: Pseudo dice [0.8627, 0.912, 0.9665, 0.6469, 0.9212] 
2023-10-26 15:30:18.624381: Epoch time: 3.9 s 
2023-10-26 15:30:19.667070:  
2023-10-26 15:30:19.667381: Epoch 85 
2023-10-26 15:30:19.667663: Current learning rate: 0.00923 
2023-10-26 15:30:23.621846: train_loss -0.8307 
2023-10-26 15:30:23.622242: val_loss -0.8723 
2023-10-26 15:30:23.622502: Pseudo dice [0.8682, 0.9175, 0.9684, 0.6227, 0.9346] 
2023-10-26 15:30:23.622742: Epoch time: 3.96 s 
2023-10-26 15:30:24.708145:  
2023-10-26 15:30:24.708457: Epoch 86 
2023-10-26 15:30:24.708706: Current learning rate: 0.00922 
2023-10-26 15:30:28.525112: train_loss -0.8336 
2023-10-26 15:30:28.527622: val_loss -0.8649 
2023-10-26 15:30:28.527900: Pseudo dice [0.8655, 0.9182, 0.9688, 0.7679, 0.9148] 
2023-10-26 15:30:28.528153: Epoch time: 3.82 s 
2023-10-26 15:30:29.578575:  
2023-10-26 15:30:29.578902: Epoch 87 
2023-10-26 15:30:29.579203: Current learning rate: 0.00921 
2023-10-26 15:30:33.367590: train_loss -0.831 
2023-10-26 15:30:33.367961: val_loss -0.865 
2023-10-26 15:30:33.368253: Pseudo dice [0.8639, 0.9197, 0.9698, 0.6006, 0.9244] 
2023-10-26 15:30:33.368534: Epoch time: 3.79 s 
2023-10-26 15:30:34.453882:  
2023-10-26 15:30:34.454181: Epoch 88 
2023-10-26 15:30:34.454668: Current learning rate: 0.0092 
2023-10-26 15:30:38.459482: train_loss -0.8338 
2023-10-26 15:30:38.459903: val_loss -0.8538 
2023-10-26 15:30:38.460177: Pseudo dice [0.8519, 0.9078, 0.9635, 0.7985, 0.9136] 
2023-10-26 15:30:38.460431: Epoch time: 4.01 s 
2023-10-26 15:30:39.677848:  
2023-10-26 15:30:39.678177: Epoch 89 
2023-10-26 15:30:39.678424: Current learning rate: 0.0092 
2023-10-26 15:30:43.589344: train_loss -0.8217 
2023-10-26 15:30:43.589756: val_loss -0.8425 
2023-10-26 15:30:43.590037: Pseudo dice [0.8425, 0.9039, 0.9658, 0.6653, 0.9127] 
2023-10-26 15:30:43.590278: Epoch time: 3.91 s 
2023-10-26 15:30:44.636280:  
2023-10-26 15:30:44.636603: Epoch 90 
2023-10-26 15:30:44.636848: Current learning rate: 0.00919 
2023-10-26 15:30:48.646194: train_loss -0.8118 
2023-10-26 15:30:48.646664: val_loss -0.819 
2023-10-26 15:30:48.646940: Pseudo dice [0.846, 0.9075, 0.9635, 0.7796, 0.9181] 
2023-10-26 15:30:48.647260: Epoch time: 4.01 s 
2023-10-26 15:30:49.698939:  
2023-10-26 15:30:49.699281: Epoch 91 
2023-10-26 15:30:49.699530: Current learning rate: 0.00918 
2023-10-26 15:30:53.568382: train_loss -0.8177 
2023-10-26 15:30:53.568764: val_loss -0.8506 
2023-10-26 15:30:53.569035: Pseudo dice [0.8558, 0.9181, 0.9679, 0.7303, 0.9272] 
2023-10-26 15:30:53.569265: Epoch time: 3.87 s 
2023-10-26 15:30:54.588657:  
2023-10-26 15:30:54.588991: Epoch 92 
2023-10-26 15:30:54.589254: Current learning rate: 0.00917 
2023-10-26 15:30:58.590990: train_loss -0.8228 
2023-10-26 15:30:58.591362: val_loss -0.8493 
2023-10-26 15:30:58.591617: Pseudo dice [0.8486, 0.914, 0.9656, 0.6642, 0.9218] 
2023-10-26 15:30:58.591884: Epoch time: 4.0 s 
2023-10-26 15:30:59.608932:  
2023-10-26 15:30:59.609235: Epoch 93 
2023-10-26 15:30:59.609481: Current learning rate: 0.00916 
2023-10-26 15:31:03.652861: train_loss -0.8299 
2023-10-26 15:31:03.653284: val_loss -0.837 
2023-10-26 15:31:03.653571: Pseudo dice [0.8435, 0.898, 0.9662, 0.7696, 0.9144] 
2023-10-26 15:31:03.653801: Epoch time: 4.04 s 
2023-10-26 15:31:04.662063:  
2023-10-26 15:31:04.662399: Epoch 94 
2023-10-26 15:31:04.662703: Current learning rate: 0.00915 
2023-10-26 15:31:08.535380: train_loss -0.8244 
2023-10-26 15:31:08.535975: val_loss -0.8535 
2023-10-26 15:31:08.536491: Pseudo dice [0.8681, 0.9176, 0.9685, 0.6598, 0.9239] 
2023-10-26 15:31:08.536916: Epoch time: 3.87 s 
2023-10-26 15:31:09.736555:  
2023-10-26 15:31:09.736892: Epoch 95 
2023-10-26 15:31:09.737193: Current learning rate: 0.00914 
2023-10-26 15:31:13.784777: train_loss -0.8209 
2023-10-26 15:31:13.785199: val_loss -0.8412 
2023-10-26 15:31:13.785627: Pseudo dice [0.8312, 0.9167, 0.9651, 0.7184, 0.9069] 
2023-10-26 15:31:13.785936: Epoch time: 4.05 s 
2023-10-26 15:31:14.850183:  
2023-10-26 15:31:14.850499: Epoch 96 
2023-10-26 15:31:14.850743: Current learning rate: 0.00913 
2023-10-26 15:31:18.823446: train_loss -0.8094 
2023-10-26 15:31:18.823961: val_loss -0.8467 
2023-10-26 15:31:18.824467: Pseudo dice [0.8599, 0.912, 0.9666, 0.4004, 0.911] 
2023-10-26 15:31:18.824792: Epoch time: 3.97 s 
2023-10-26 15:31:19.892604:  
2023-10-26 15:31:19.892943: Epoch 97 
2023-10-26 15:31:19.893268: Current learning rate: 0.00912 
2023-10-26 15:31:23.834347: train_loss -0.8177 
2023-10-26 15:31:23.834724: val_loss -0.8448 
2023-10-26 15:31:23.835066: Pseudo dice [0.845, 0.91, 0.9673, 0.7536, 0.9309] 
2023-10-26 15:31:23.835313: Epoch time: 3.94 s 
2023-10-26 15:31:24.893752:  
2023-10-26 15:31:24.894064: Epoch 98 
2023-10-26 15:31:24.894311: Current learning rate: 0.00911 
2023-10-26 15:31:28.803406: train_loss -0.8227 
2023-10-26 15:31:28.803813: val_loss -0.8549 
2023-10-26 15:31:28.804099: Pseudo dice [0.8625, 0.9158, 0.9661, 0.6679, 0.9378] 
2023-10-26 15:31:28.804379: Epoch time: 3.91 s 
2023-10-26 15:31:29.847358:  
2023-10-26 15:31:29.847696: Epoch 99 
2023-10-26 15:31:29.847951: Current learning rate: 0.0091 
2023-10-26 15:31:33.798114: train_loss -0.8212 
2023-10-26 15:31:33.798565: val_loss -0.8299 
2023-10-26 15:31:33.798984: Pseudo dice [0.851, 0.9091, 0.9647, 0.3056, 0.9225] 
2023-10-26 15:31:33.799363: Epoch time: 3.95 s 
2023-10-26 15:31:34.909745:  
2023-10-26 15:31:34.910096: Epoch 100 
2023-10-26 15:31:34.910354: Current learning rate: 0.0091 
2023-10-26 15:31:38.760519: train_loss -0.8198 
2023-10-26 15:31:38.760913: val_loss -0.8609 
2023-10-26 15:31:38.761389: Pseudo dice [0.8555, 0.9144, 0.969, 0.7106, 0.9184] 
2023-10-26 15:31:38.761654: Epoch time: 3.85 s 
2023-10-26 15:31:39.861383:  
2023-10-26 15:31:39.861820: Epoch 101 
2023-10-26 15:31:39.862072: Current learning rate: 0.00909 
2023-10-26 15:31:43.783183: train_loss -0.8292 
2023-10-26 15:31:43.783558: val_loss -0.8642 
2023-10-26 15:31:43.783822: Pseudo dice [0.8636, 0.92, 0.9685, 0.7534, 0.9241] 
2023-10-26 15:31:43.784142: Epoch time: 3.92 s 
2023-10-26 15:31:44.990731:  
2023-10-26 15:31:44.991059: Epoch 102 
2023-10-26 15:31:44.991346: Current learning rate: 0.00908 
2023-10-26 15:31:48.923519: train_loss -0.833 
2023-10-26 15:31:48.924001: val_loss -0.86 
2023-10-26 15:31:48.924483: Pseudo dice [0.8698, 0.9141, 0.9664, 0.6081, 0.9158] 
2023-10-26 15:31:48.924838: Epoch time: 3.93 s 
2023-10-26 15:31:50.007381:  
2023-10-26 15:31:50.007738: Epoch 103 
2023-10-26 15:31:50.007999: Current learning rate: 0.00907 
2023-10-26 15:31:53.967780: train_loss -0.8318 
2023-10-26 15:31:53.968386: val_loss -0.8667 
2023-10-26 15:31:53.968722: Pseudo dice [0.8623, 0.9021, 0.9681, 0.6834, 0.9187] 
2023-10-26 15:31:53.969051: Epoch time: 3.96 s 
2023-10-26 15:31:55.050909:  
2023-10-26 15:31:55.051250: Epoch 104 
2023-10-26 15:31:55.051521: Current learning rate: 0.00906 
2023-10-26 15:31:58.966534: train_loss -0.8305 
2023-10-26 15:31:58.966959: val_loss -0.8626 
2023-10-26 15:31:58.967228: Pseudo dice [0.8583, 0.9109, 0.9671, 0.6296, 0.9224] 
2023-10-26 15:31:58.967487: Epoch time: 3.92 s 
2023-10-26 15:32:00.003243:  
2023-10-26 15:32:00.003556: Epoch 105 
2023-10-26 15:32:00.003819: Current learning rate: 0.00905 
2023-10-26 15:32:03.889330: train_loss -0.8272 
2023-10-26 15:32:03.889704: val_loss -0.8497 
2023-10-26 15:32:03.889966: Pseudo dice [0.8562, 0.9065, 0.968, 0.576, 0.9205] 
2023-10-26 15:32:03.890189: Epoch time: 3.89 s 
2023-10-26 15:32:04.953185:  
2023-10-26 15:32:04.953491: Epoch 106 
2023-10-26 15:32:04.953744: Current learning rate: 0.00904 
2023-10-26 15:32:09.000931: train_loss -0.8277 
2023-10-26 15:32:09.001308: val_loss -0.8657 
2023-10-26 15:32:09.001572: Pseudo dice [0.8591, 0.9085, 0.9667, 0.5366, 0.9297] 
2023-10-26 15:32:09.001864: Epoch time: 4.05 s 
2023-10-26 15:32:10.046616:  
2023-10-26 15:32:10.046945: Epoch 107 
2023-10-26 15:32:10.047195: Current learning rate: 0.00903 
2023-10-26 15:32:13.995203: train_loss -0.8241 
2023-10-26 15:32:13.995594: val_loss -0.8625 
2023-10-26 15:32:13.995847: Pseudo dice [0.853, 0.9217, 0.9692, 0.6522, 0.9288] 
2023-10-26 15:32:13.996091: Epoch time: 3.95 s 
2023-10-26 15:32:15.095637:  
2023-10-26 15:32:15.095957: Epoch 108 
2023-10-26 15:32:15.096196: Current learning rate: 0.00902 
2023-10-26 15:32:19.082607: train_loss -0.8313 
2023-10-26 15:32:19.083020: val_loss -0.8718 
2023-10-26 15:32:19.083547: Pseudo dice [0.8682, 0.9145, 0.9701, 0.6949, 0.9286] 
2023-10-26 15:32:19.083915: Epoch time: 3.99 s 
2023-10-26 15:32:20.324525:  
2023-10-26 15:32:20.324838: Epoch 109 
2023-10-26 15:32:20.325264: Current learning rate: 0.00901 
2023-10-26 15:32:24.406947: train_loss -0.8304 
2023-10-26 15:32:24.407350: val_loss -0.8494 
2023-10-26 15:32:24.407637: Pseudo dice [0.8575, 0.9158, 0.9652, 0.5693, 0.9117] 
2023-10-26 15:32:24.407879: Epoch time: 4.08 s 
2023-10-26 15:32:25.504762:  
2023-10-26 15:32:25.505139: Epoch 110 
2023-10-26 15:32:25.505444: Current learning rate: 0.009 
2023-10-26 15:32:29.434820: train_loss -0.839 
2023-10-26 15:32:29.435186: val_loss -0.8703 
2023-10-26 15:32:29.435434: Pseudo dice [0.8693, 0.9147, 0.9684, 0.7098, 0.9286] 
2023-10-26 15:32:29.435666: Epoch time: 3.93 s 
2023-10-26 15:32:30.474196:  
2023-10-26 15:32:30.474503: Epoch 111 
2023-10-26 15:32:30.474737: Current learning rate: 0.009 
2023-10-26 15:32:34.403785: train_loss -0.8334 
2023-10-26 15:32:34.404174: val_loss -0.8423 
2023-10-26 15:32:34.404478: Pseudo dice [0.8625, 0.9192, 0.9674, 0.4045, 0.9203] 
2023-10-26 15:32:34.404729: Epoch time: 3.93 s 
2023-10-26 15:32:35.446929:  
2023-10-26 15:32:35.447247: Epoch 112 
2023-10-26 15:32:35.447506: Current learning rate: 0.00899 
2023-10-26 15:32:39.427962: train_loss -0.8267 
2023-10-26 15:32:39.428382: val_loss -0.8657 
2023-10-26 15:32:39.428675: Pseudo dice [0.8699, 0.907, 0.9664, 0.7709, 0.9108] 
2023-10-26 15:32:39.428944: Epoch time: 3.98 s 
2023-10-26 15:32:40.487885:  
2023-10-26 15:32:40.488189: Epoch 113 
2023-10-26 15:32:40.488425: Current learning rate: 0.00898 
2023-10-26 15:32:44.327347: train_loss -0.8281 
2023-10-26 15:32:44.327966: val_loss -0.8557 
2023-10-26 15:32:44.328401: Pseudo dice [0.8711, 0.9131, 0.9655, 0.732, 0.9213] 
2023-10-26 15:32:44.328786: Epoch time: 3.84 s 
2023-10-26 15:32:45.394280:  
2023-10-26 15:32:45.394583: Epoch 114 
2023-10-26 15:32:45.394834: Current learning rate: 0.00897 
2023-10-26 15:32:49.275746: train_loss -0.8228 
2023-10-26 15:32:49.276147: val_loss -0.8604 
2023-10-26 15:32:49.276422: Pseudo dice [0.8472, 0.9158, 0.9688, 0.698, 0.9285] 
2023-10-26 15:32:49.276659: Epoch time: 3.88 s 
2023-10-26 15:32:50.533268:  
2023-10-26 15:32:50.535799: Epoch 115 
2023-10-26 15:32:50.536079: Current learning rate: 0.00896 
2023-10-26 15:32:54.493590: train_loss -0.8244 
2023-10-26 15:32:54.494008: val_loss -0.8505 
2023-10-26 15:32:54.494278: Pseudo dice [0.8479, 0.9151, 0.9702, 0.7499, 0.9222] 
2023-10-26 15:32:54.494518: Epoch time: 3.96 s 
2023-10-26 15:32:55.568377:  
2023-10-26 15:32:55.568702: Epoch 116 
2023-10-26 15:32:55.568954: Current learning rate: 0.00895 
2023-10-26 15:32:59.395465: train_loss -0.8309 
2023-10-26 15:32:59.395918: val_loss -0.8613 
2023-10-26 15:32:59.396180: Pseudo dice [0.8711, 0.9159, 0.9689, 0.7407, 0.921] 
2023-10-26 15:32:59.396406: Epoch time: 3.83 s 
2023-10-26 15:32:59.396612: Yayy! New best EMA pseudo Dice: 0.865 
2023-10-26 15:33:00.543495:  
2023-10-26 15:33:00.543824: Epoch 117 
2023-10-26 15:33:00.544134: Current learning rate: 0.00894 
2023-10-26 15:33:04.519117: train_loss -0.828 
2023-10-26 15:33:04.519663: val_loss -0.861 
2023-10-26 15:33:04.520082: Pseudo dice [0.8623, 0.9132, 0.9643, 0.6382, 0.9181] 
2023-10-26 15:33:04.520417: Epoch time: 3.98 s 
2023-10-26 15:33:05.631213:  
2023-10-26 15:33:05.631582: Epoch 118 
2023-10-26 15:33:05.631891: Current learning rate: 0.00893 
2023-10-26 15:33:09.605175: train_loss -0.8278 
2023-10-26 15:33:09.605573: val_loss -0.8537 
2023-10-26 15:33:09.605838: Pseudo dice [0.8636, 0.914, 0.9643, 0.5626, 0.9332] 
2023-10-26 15:33:09.606100: Epoch time: 3.97 s 
2023-10-26 15:33:10.667597:  
2023-10-26 15:33:10.667935: Epoch 119 
2023-10-26 15:33:10.668256: Current learning rate: 0.00892 
2023-10-26 15:33:14.549082: train_loss -0.8279 
2023-10-26 15:33:14.549461: val_loss -0.8652 
2023-10-26 15:33:14.549712: Pseudo dice [0.8629, 0.9134, 0.9679, 0.6916, 0.9289] 
2023-10-26 15:33:14.549937: Epoch time: 3.88 s 
2023-10-26 15:33:15.619471:  
2023-10-26 15:33:15.619770: Epoch 120 
2023-10-26 15:33:15.620023: Current learning rate: 0.00891 
2023-10-26 15:33:19.567949: train_loss -0.8408 
2023-10-26 15:33:19.568779: val_loss -0.8663 
2023-10-26 15:33:19.569411: Pseudo dice [0.8615, 0.9153, 0.9683, 0.7061, 0.9248] 
2023-10-26 15:33:19.570447: Epoch time: 3.95 s 
2023-10-26 15:33:20.648761:  
2023-10-26 15:33:20.649109: Epoch 121 
2023-10-26 15:33:20.649459: Current learning rate: 0.0089 
2023-10-26 15:33:24.689918: train_loss -0.8386 
2023-10-26 15:33:24.690341: val_loss -0.8634 
2023-10-26 15:33:24.690696: Pseudo dice [0.8585, 0.9204, 0.9695, 0.7182, 0.9278] 
2023-10-26 15:33:24.690976: Epoch time: 4.04 s 
2023-10-26 15:33:24.691248: Yayy! New best EMA pseudo Dice: 0.8663 
2023-10-26 15:33:25.984197:  
2023-10-26 15:33:25.984529: Epoch 122 
2023-10-26 15:33:25.984782: Current learning rate: 0.00889 
2023-10-26 15:33:29.940304: train_loss -0.8572 
2023-10-26 15:33:29.940706: val_loss -0.8722 
2023-10-26 15:33:29.940974: Pseudo dice [0.8636, 0.9083, 0.9684, 0.6743, 0.9259] 
2023-10-26 15:33:29.941220: Epoch time: 3.96 s 
2023-10-26 15:33:29.941493: Yayy! New best EMA pseudo Dice: 0.8665 
2023-10-26 15:33:31.075412:  
2023-10-26 15:33:31.075746: Epoch 123 
2023-10-26 15:33:31.076045: Current learning rate: 0.00889 
2023-10-26 15:33:35.066867: train_loss -0.858 
2023-10-26 15:33:35.067237: val_loss -0.8673 
2023-10-26 15:33:35.067485: Pseudo dice [0.8664, 0.9162, 0.9677, 0.6019, 0.9255] 
2023-10-26 15:33:35.067720: Epoch time: 3.99 s 
2023-10-26 15:33:36.118382:  
2023-10-26 15:33:36.118685: Epoch 124 
2023-10-26 15:33:36.118935: Current learning rate: 0.00888 
2023-10-26 15:33:40.182345: train_loss -0.8559 
2023-10-26 15:33:40.182725: val_loss -0.877 
2023-10-26 15:33:40.183011: Pseudo dice [0.8662, 0.9097, 0.9682, 0.692, 0.9339] 
2023-10-26 15:33:40.183244: Epoch time: 4.06 s 
2023-10-26 15:33:41.240558:  
2023-10-26 15:33:41.240877: Epoch 125 
2023-10-26 15:33:41.241144: Current learning rate: 0.00887 
2023-10-26 15:33:45.327888: train_loss -0.8546 
2023-10-26 15:33:45.328277: val_loss -0.8775 
2023-10-26 15:33:45.328536: Pseudo dice [0.8622, 0.9159, 0.9689, 0.6349, 0.9244] 
2023-10-26 15:33:45.328767: Epoch time: 4.09 s 
2023-10-26 15:33:46.387356:  
2023-10-26 15:33:46.387655: Epoch 126 
2023-10-26 15:33:46.387900: Current learning rate: 0.00886 
2023-10-26 15:33:50.398630: train_loss -0.8542 
2023-10-26 15:33:50.398996: val_loss -0.8746 
2023-10-26 15:33:50.399263: Pseudo dice [0.855, 0.9137, 0.9683, 0.7199, 0.9336] 
2023-10-26 15:33:50.399487: Epoch time: 4.01 s 
2023-10-26 15:33:50.399700: Yayy! New best EMA pseudo Dice: 0.867 
2023-10-26 15:33:51.536868:  
2023-10-26 15:33:51.537192: Epoch 127 
2023-10-26 15:33:51.537438: Current learning rate: 0.00885 
2023-10-26 15:33:55.405147: train_loss -0.8538 
2023-10-26 15:33:55.405526: val_loss -0.8723 
2023-10-26 15:33:55.405788: Pseudo dice [0.8521, 0.9155, 0.9645, 0.7461, 0.9317] 
2023-10-26 15:33:55.406033: Epoch time: 3.87 s 
2023-10-26 15:33:55.406236: Yayy! New best EMA pseudo Dice: 0.8685 
2023-10-26 15:33:56.704267:  
2023-10-26 15:33:56.704561: Epoch 128 
2023-10-26 15:33:56.704806: Current learning rate: 0.00884 
2023-10-26 15:34:00.640507: train_loss -0.8387 
2023-10-26 15:34:00.642854: val_loss -0.8264 
2023-10-26 15:34:00.643136: Pseudo dice [0.8662, 0.9149, 0.9662, 0.7345, 0.9216] 
2023-10-26 15:34:00.643380: Epoch time: 3.94 s 
2023-10-26 15:34:00.643630: Yayy! New best EMA pseudo Dice: 0.8697 
2023-10-26 15:34:01.834752:  
2023-10-26 15:34:01.835093: Epoch 129 
2023-10-26 15:34:01.835375: Current learning rate: 0.00883 
2023-10-26 15:34:05.736265: train_loss -0.8517 
2023-10-26 15:34:05.736676: val_loss -0.8714 
2023-10-26 15:34:05.736998: Pseudo dice [0.8542, 0.9151, 0.9684, 0.7659, 0.9322] 
2023-10-26 15:34:05.737245: Epoch time: 3.9 s 
2023-10-26 15:34:05.737456: Yayy! New best EMA pseudo Dice: 0.8714 
2023-10-26 15:34:06.947222:  
2023-10-26 15:34:06.947549: Epoch 130 
2023-10-26 15:34:06.947788: Current learning rate: 0.00882 
2023-10-26 15:34:10.984712: train_loss -0.8529 
2023-10-26 15:34:10.985110: val_loss -0.8835 
2023-10-26 15:34:10.985370: Pseudo dice [0.8804, 0.9179, 0.9714, 0.75, 0.9329] 
2023-10-26 15:34:10.985627: Epoch time: 4.04 s 
2023-10-26 15:34:10.985840: Yayy! New best EMA pseudo Dice: 0.8733 
2023-10-26 15:34:12.152600:  
2023-10-26 15:34:12.152933: Epoch 131 
2023-10-26 15:34:12.153250: Current learning rate: 0.00881 
2023-10-26 15:34:16.026206: train_loss -0.8528 
2023-10-26 15:34:16.026706: val_loss -0.8813 
2023-10-26 15:34:16.026995: Pseudo dice [0.8699, 0.9191, 0.9678, 0.7379, 0.9276] 
2023-10-26 15:34:16.027255: Epoch time: 3.87 s 
2023-10-26 15:34:16.027463: Yayy! New best EMA pseudo Dice: 0.8745 
2023-10-26 15:34:17.183412:  
2023-10-26 15:34:17.183694: Epoch 132 
2023-10-26 15:34:17.183942: Current learning rate: 0.0088 
2023-10-26 15:34:21.264690: train_loss -0.8591 
2023-10-26 15:34:21.265267: val_loss -0.8763 
2023-10-26 15:34:21.265628: Pseudo dice [0.8606, 0.9106, 0.968, 0.7688, 0.9215] 
2023-10-26 15:34:21.265965: Epoch time: 4.08 s 
2023-10-26 15:34:21.266271: Yayy! New best EMA pseudo Dice: 0.8756 
2023-10-26 15:34:22.502596:  
2023-10-26 15:34:22.502909: Epoch 133 
2023-10-26 15:34:22.503152: Current learning rate: 0.00879 
2023-10-26 15:34:26.433628: train_loss -0.8583 
2023-10-26 15:34:26.434027: val_loss -0.8735 
2023-10-26 15:34:26.434283: Pseudo dice [0.871, 0.9155, 0.968, 0.7735, 0.9178] 
2023-10-26 15:34:26.434643: Epoch time: 3.93 s 
2023-10-26 15:34:26.434861: Yayy! New best EMA pseudo Dice: 0.877 
2023-10-26 15:34:27.854728:  
2023-10-26 15:34:27.855062: Epoch 134 
2023-10-26 15:34:27.855317: Current learning rate: 0.00879 
2023-10-26 15:34:31.770237: train_loss -0.85 
2023-10-26 15:34:31.770634: val_loss -0.8742 
2023-10-26 15:34:31.770893: Pseudo dice [0.8717, 0.9188, 0.9639, 0.7399, 0.9246] 
2023-10-26 15:34:31.771133: Epoch time: 3.92 s 
2023-10-26 15:34:31.771334: Yayy! New best EMA pseudo Dice: 0.8776 
2023-10-26 15:34:32.924379:  
2023-10-26 15:34:32.924729: Epoch 135 
2023-10-26 15:34:32.924975: Current learning rate: 0.00878 
2023-10-26 15:34:36.923469: train_loss -0.8565 
2023-10-26 15:34:36.923848: val_loss -0.8702 
2023-10-26 15:34:36.924127: Pseudo dice [0.8633, 0.9185, 0.9698, 0.6712, 0.9401] 
2023-10-26 15:34:36.924359: Epoch time: 4.0 s 
2023-10-26 15:34:38.033639:  
2023-10-26 15:34:38.033972: Epoch 136 
2023-10-26 15:34:38.034242: Current learning rate: 0.00877 
2023-10-26 15:34:41.961395: train_loss -0.8562 
2023-10-26 15:34:41.961766: val_loss -0.8793 
2023-10-26 15:34:41.962047: Pseudo dice [0.8738, 0.9212, 0.97, 0.6888, 0.9221] 
2023-10-26 15:34:41.962287: Epoch time: 3.93 s 
2023-10-26 15:34:43.065655:  
2023-10-26 15:34:43.065987: Epoch 137 
2023-10-26 15:34:43.066242: Current learning rate: 0.00876 
2023-10-26 15:34:47.072231: train_loss -0.853 
2023-10-26 15:34:47.072726: val_loss -0.8625 
2023-10-26 15:34:47.073221: Pseudo dice [0.8547, 0.9135, 0.9669, 0.6286, 0.9265] 
2023-10-26 15:34:47.073538: Epoch time: 4.01 s 
2023-10-26 15:34:48.251176:  
2023-10-26 15:34:48.251500: Epoch 138 
2023-10-26 15:34:48.251758: Current learning rate: 0.00875 
2023-10-26 15:34:52.114274: train_loss -0.8563 
2023-10-26 15:34:52.114665: val_loss -0.8693 
2023-10-26 15:34:52.114936: Pseudo dice [0.8585, 0.9175, 0.9685, 0.6521, 0.9158] 
2023-10-26 15:34:52.115172: Epoch time: 3.86 s 
2023-10-26 15:34:53.209865:  
2023-10-26 15:34:53.210171: Epoch 139 
2023-10-26 15:34:53.210423: Current learning rate: 0.00874 
2023-10-26 15:34:57.339245: train_loss -0.8523 
2023-10-26 15:34:57.339747: val_loss -0.8733 
2023-10-26 15:34:57.340232: Pseudo dice [0.8697, 0.9181, 0.968, 0.7145, 0.911] 
2023-10-26 15:34:57.340527: Epoch time: 4.13 s 
2023-10-26 15:34:58.604582:  
2023-10-26 15:34:58.604897: Epoch 140 
2023-10-26 15:34:58.605137: Current learning rate: 0.00873 
2023-10-26 15:35:02.631845: train_loss -0.8615 
2023-10-26 15:35:02.632258: val_loss -0.873 
2023-10-26 15:35:02.632519: Pseudo dice [0.8638, 0.9189, 0.9681, 0.6882, 0.935] 
2023-10-26 15:35:02.632753: Epoch time: 4.03 s 
2023-10-26 15:35:03.708332:  
2023-10-26 15:35:03.708648: Epoch 141 
2023-10-26 15:35:03.708903: Current learning rate: 0.00872 
2023-10-26 15:35:07.596422: train_loss -0.8633 
2023-10-26 15:35:07.596781: val_loss -0.866 
2023-10-26 15:35:07.597043: Pseudo dice [0.8673, 0.9207, 0.9691, 0.5531, 0.9283] 
2023-10-26 15:35:07.597266: Epoch time: 3.89 s 
2023-10-26 15:35:08.684366:  
2023-10-26 15:35:08.684666: Epoch 142 
2023-10-26 15:35:08.684903: Current learning rate: 0.00871 
2023-10-26 15:35:12.645038: train_loss -0.8534 
2023-10-26 15:35:12.645396: val_loss -0.864 
2023-10-26 15:35:12.645649: Pseudo dice [0.8634, 0.9159, 0.9671, 0.6005, 0.9239] 
2023-10-26 15:35:12.645876: Epoch time: 3.96 s 
2023-10-26 15:35:13.815950:  
2023-10-26 15:35:13.816279: Epoch 143 
2023-10-26 15:35:13.816564: Current learning rate: 0.0087 
2023-10-26 15:35:17.747471: train_loss -0.8361 
2023-10-26 15:35:17.747847: val_loss -0.8708 
2023-10-26 15:35:17.748110: Pseudo dice [0.8736, 0.9175, 0.9684, 0.7907, 0.9171] 
2023-10-26 15:35:17.748351: Epoch time: 3.93 s 
2023-10-26 15:35:18.858030:  
2023-10-26 15:35:18.858329: Epoch 144 
2023-10-26 15:35:18.858566: Current learning rate: 0.00869 
2023-10-26 15:35:22.847521: train_loss -0.8518 
2023-10-26 15:35:22.847897: val_loss -0.863 
2023-10-26 15:35:22.848145: Pseudo dice [0.8603, 0.914, 0.9642, 0.7587, 0.9252] 
2023-10-26 15:35:22.848361: Epoch time: 3.99 s 
2023-10-26 15:35:23.915148:  
2023-10-26 15:35:23.915449: Epoch 145 
2023-10-26 15:35:23.915698: Current learning rate: 0.00868 
2023-10-26 15:35:27.969523: train_loss -0.8429 
2023-10-26 15:35:27.969949: val_loss -0.863 
2023-10-26 15:35:27.970439: Pseudo dice [0.8454, 0.9128, 0.968, 0.6414, 0.9112] 
2023-10-26 15:35:27.970714: Epoch time: 4.05 s 
2023-10-26 15:35:29.041331:  
2023-10-26 15:35:29.041608: Epoch 146 
2023-10-26 15:35:29.041847: Current learning rate: 0.00868 
2023-10-26 15:35:32.964699: train_loss -0.8507 
2023-10-26 15:35:32.965165: val_loss -0.8645 
2023-10-26 15:35:32.965482: Pseudo dice [0.8627, 0.9175, 0.9687, 0.6073, 0.9263] 
2023-10-26 15:35:32.965737: Epoch time: 3.92 s 
2023-10-26 15:35:34.200366:  
2023-10-26 15:35:34.200665: Epoch 147 
2023-10-26 15:35:34.200915: Current learning rate: 0.00867 
2023-10-26 15:35:38.347664: train_loss -0.8462 
2023-10-26 15:35:38.348063: val_loss -0.8765 
2023-10-26 15:35:38.348315: Pseudo dice [0.877, 0.9151, 0.9685, 0.6542, 0.9275] 
2023-10-26 15:35:38.348535: Epoch time: 4.15 s 
2023-10-26 15:35:39.437021:  
2023-10-26 15:35:39.437323: Epoch 148 
2023-10-26 15:35:39.437590: Current learning rate: 0.00866 
2023-10-26 15:35:43.329730: train_loss -0.847 
2023-10-26 15:35:43.330125: val_loss -0.8691 
2023-10-26 15:35:43.330400: Pseudo dice [0.8646, 0.9064, 0.9673, 0.7428, 0.9257] 
2023-10-26 15:35:43.330638: Epoch time: 3.89 s 
2023-10-26 15:35:44.460178:  
2023-10-26 15:35:44.460480: Epoch 149 
2023-10-26 15:35:44.460716: Current learning rate: 0.00865 
2023-10-26 15:35:48.359735: train_loss -0.8547 
2023-10-26 15:35:48.360124: val_loss -0.8838 
2023-10-26 15:35:48.360377: Pseudo dice [0.8786, 0.9172, 0.9702, 0.706, 0.9339] 
2023-10-26 15:35:48.360607: Epoch time: 3.9 s 
2023-10-26 15:35:49.557180:  
2023-10-26 15:35:49.557471: Epoch 150 
2023-10-26 15:35:49.557785: Current learning rate: 0.00864 
2023-10-26 15:35:53.494221: train_loss -0.8569 
2023-10-26 15:35:53.496858: val_loss -0.8716 
2023-10-26 15:35:53.497143: Pseudo dice [0.8697, 0.9178, 0.9687, 0.703, 0.9323] 
2023-10-26 15:35:53.497411: Epoch time: 3.94 s 
2023-10-26 15:35:54.612480:  
2023-10-26 15:35:54.612775: Epoch 151 
2023-10-26 15:35:54.613030: Current learning rate: 0.00863 
2023-10-26 15:35:58.527520: train_loss -0.8586 
2023-10-26 15:35:58.527943: val_loss -0.8696 
2023-10-26 15:35:58.528259: Pseudo dice [0.8632, 0.9162, 0.9666, 0.6569, 0.9232] 
2023-10-26 15:35:58.528553: Epoch time: 3.92 s 
2023-10-26 15:35:59.644618:  
2023-10-26 15:35:59.644908: Epoch 152 
2023-10-26 15:35:59.645161: Current learning rate: 0.00862 
2023-10-26 15:36:03.436440: train_loss -0.8549 
2023-10-26 15:36:03.436852: val_loss -0.8689 
2023-10-26 15:36:03.437121: Pseudo dice [0.8555, 0.9109, 0.9675, 0.6921, 0.9403] 
2023-10-26 15:36:03.437339: Epoch time: 3.79 s 
2023-10-26 15:36:04.715764:  
2023-10-26 15:36:04.716087: Epoch 153 
2023-10-26 15:36:04.716368: Current learning rate: 0.00861 
2023-10-26 15:36:08.712388: train_loss -0.8495 
2023-10-26 15:36:08.712804: val_loss -0.8678 
2023-10-26 15:36:08.713075: Pseudo dice [0.8381, 0.9107, 0.9668, 0.6953, 0.9355] 
2023-10-26 15:36:08.713346: Epoch time: 4.0 s 
2023-10-26 15:36:09.823921:  
2023-10-26 15:36:09.824218: Epoch 154 
2023-10-26 15:36:09.824463: Current learning rate: 0.0086 
2023-10-26 15:36:13.751550: train_loss -0.854 
2023-10-26 15:36:13.751956: val_loss -0.8762 
2023-10-26 15:36:13.752301: Pseudo dice [0.8744, 0.9059, 0.9694, 0.775, 0.9309] 
2023-10-26 15:36:13.752569: Epoch time: 3.93 s 
2023-10-26 15:36:14.857433:  
2023-10-26 15:36:14.857756: Epoch 155 
2023-10-26 15:36:14.858028: Current learning rate: 0.00859 
2023-10-26 15:36:18.627024: train_loss -0.859 
2023-10-26 15:36:18.627420: val_loss -0.8759 
2023-10-26 15:36:18.627685: Pseudo dice [0.863, 0.9167, 0.9672, 0.6823, 0.9315] 
2023-10-26 15:36:18.627933: Epoch time: 3.77 s 
2023-10-26 15:36:19.732629:  
2023-10-26 15:36:19.732950: Epoch 156 
2023-10-26 15:36:19.733207: Current learning rate: 0.00858 
2023-10-26 15:36:23.503643: train_loss -0.8537 
2023-10-26 15:36:23.504015: val_loss -0.8676 
2023-10-26 15:36:23.504269: Pseudo dice [0.876, 0.9072, 0.965, 0.6119, 0.9217] 
2023-10-26 15:36:23.504492: Epoch time: 3.77 s 
2023-10-26 15:36:24.633131:  
2023-10-26 15:36:24.633574: Epoch 157 
2023-10-26 15:36:24.633833: Current learning rate: 0.00858 
2023-10-26 15:36:28.578203: train_loss -0.8504 
2023-10-26 15:36:28.578552: val_loss -0.8606 
2023-10-26 15:36:28.578863: Pseudo dice [0.8734, 0.9201, 0.9689, 0.6109, 0.9241] 
2023-10-26 15:36:28.579095: Epoch time: 3.95 s 
2023-10-26 15:36:29.670232:  
2023-10-26 15:36:29.670525: Epoch 158 
2023-10-26 15:36:29.670764: Current learning rate: 0.00857 
2023-10-26 15:36:33.780516: train_loss -0.8563 
2023-10-26 15:36:33.780923: val_loss -0.8747 
2023-10-26 15:36:33.781235: Pseudo dice [0.873, 0.917, 0.9697, 0.6813, 0.9275] 
2023-10-26 15:36:33.781659: Epoch time: 4.11 s 
2023-10-26 15:36:35.107673:  
2023-10-26 15:36:35.107991: Epoch 159 
2023-10-26 15:36:35.108261: Current learning rate: 0.00856 
2023-10-26 15:36:39.192451: train_loss -0.8584 
2023-10-26 15:36:39.192814: val_loss -0.8591 
2023-10-26 15:36:39.193067: Pseudo dice [0.8587, 0.9179, 0.9631, 0.5765, 0.9255] 
2023-10-26 15:36:39.193298: Epoch time: 4.09 s 
2023-10-26 15:36:40.313176:  
2023-10-26 15:36:40.313539: Epoch 160 
2023-10-26 15:36:40.313938: Current learning rate: 0.00855 
2023-10-26 15:36:44.356018: train_loss -0.8603 
2023-10-26 15:36:44.356393: val_loss -0.8746 
2023-10-26 15:36:44.356677: Pseudo dice [0.872, 0.9134, 0.9686, 0.6581, 0.9349] 
2023-10-26 15:36:44.356925: Epoch time: 4.04 s 
2023-10-26 15:36:45.444713:  
2023-10-26 15:36:45.445081: Epoch 161 
2023-10-26 15:36:45.445332: Current learning rate: 0.00854 
2023-10-26 15:36:49.816430: train_loss -0.8606 
2023-10-26 15:36:49.816824: val_loss -0.8729 
2023-10-26 15:36:49.817091: Pseudo dice [0.8688, 0.9195, 0.9691, 0.687, 0.9327] 
2023-10-26 15:36:49.817328: Epoch time: 4.37 s 
2023-10-26 15:36:50.900618:  
2023-10-26 15:36:50.900957: Epoch 162 
2023-10-26 15:36:50.901212: Current learning rate: 0.00853 
2023-10-26 15:36:54.862542: train_loss -0.8567 
2023-10-26 15:36:54.862918: val_loss -0.8568 
2023-10-26 15:36:54.863195: Pseudo dice [0.8674, 0.9077, 0.9649, 0.6075, 0.9256] 
2023-10-26 15:36:54.863419: Epoch time: 3.96 s 
2023-10-26 15:36:55.965937:  
2023-10-26 15:36:55.966251: Epoch 163 
2023-10-26 15:36:55.966504: Current learning rate: 0.00852 
2023-10-26 15:36:59.880662: train_loss -0.8546 
2023-10-26 15:36:59.881154: val_loss -0.8505 
2023-10-26 15:36:59.881453: Pseudo dice [0.8669, 0.9164, 0.9683, 0.6964, 0.9288] 
2023-10-26 15:36:59.881697: Epoch time: 3.92 s 
2023-10-26 15:37:01.036084:  
2023-10-26 15:37:01.036380: Epoch 164 
2023-10-26 15:37:01.036623: Current learning rate: 0.00851 
2023-10-26 15:37:05.038790: train_loss -0.8636 
2023-10-26 15:37:05.039175: val_loss -0.8781 
2023-10-26 15:37:05.039431: Pseudo dice [0.8688, 0.9125, 0.9692, 0.7701, 0.9346] 
2023-10-26 15:37:05.039656: Epoch time: 4.0 s 
2023-10-26 15:37:06.280501:  
2023-10-26 15:37:06.280794: Epoch 165 
2023-10-26 15:37:06.281046: Current learning rate: 0.0085 
2023-10-26 15:37:10.254726: train_loss -0.8601 
2023-10-26 15:37:10.255105: val_loss -0.8708 
2023-10-26 15:37:10.255378: Pseudo dice [0.8704, 0.9177, 0.9701, 0.7504, 0.9372] 
2023-10-26 15:37:10.255599: Epoch time: 3.97 s 
2023-10-26 15:37:11.330531:  
2023-10-26 15:37:11.330870: Epoch 166 
2023-10-26 15:37:11.331146: Current learning rate: 0.00849 
2023-10-26 15:37:15.285765: train_loss -0.8569 
2023-10-26 15:37:15.286186: val_loss -0.869 
2023-10-26 15:37:15.286476: Pseudo dice [0.8609, 0.9177, 0.9683, 0.7119, 0.9069] 
2023-10-26 15:37:15.286752: Epoch time: 3.96 s 
2023-10-26 15:37:16.362462:  
2023-10-26 15:37:16.362765: Epoch 167 
2023-10-26 15:37:16.363014: Current learning rate: 0.00848 
2023-10-26 15:37:20.194773: train_loss -0.8633 
2023-10-26 15:37:20.195150: val_loss -0.8697 
2023-10-26 15:37:20.195428: Pseudo dice [0.8596, 0.918, 0.9673, 0.6691, 0.9284] 
2023-10-26 15:37:20.195745: Epoch time: 3.83 s 
2023-10-26 15:37:21.283404:  
2023-10-26 15:37:21.283718: Epoch 168 
2023-10-26 15:37:21.283968: Current learning rate: 0.00847 
2023-10-26 15:37:25.429243: train_loss -0.8594 
2023-10-26 15:37:25.429611: val_loss -0.8647 
2023-10-26 15:37:25.429889: Pseudo dice [0.8527, 0.9175, 0.9687, 0.78, 0.9278] 
2023-10-26 15:37:25.430116: Epoch time: 4.15 s 
2023-10-26 15:37:26.538952:  
2023-10-26 15:37:26.539481: Epoch 169 
2023-10-26 15:37:26.539728: Current learning rate: 0.00847 
2023-10-26 15:37:30.553109: train_loss -0.8544 
2023-10-26 15:37:30.553489: val_loss -0.8674 
2023-10-26 15:37:30.553740: Pseudo dice [0.8567, 0.9107, 0.9655, 0.6388, 0.9276] 
2023-10-26 15:37:30.553971: Epoch time: 4.01 s 
2023-10-26 15:37:31.657248:  
2023-10-26 15:37:31.657581: Epoch 170 
2023-10-26 15:37:31.657846: Current learning rate: 0.00846 
2023-10-26 15:37:35.981431: train_loss -0.8611 
2023-10-26 15:37:35.981850: val_loss -0.8737 
2023-10-26 15:37:35.982109: Pseudo dice [0.8702, 0.9198, 0.9676, 0.7621, 0.9109] 
2023-10-26 15:37:35.982337: Epoch time: 4.32 s 
2023-10-26 15:37:37.241827:  
2023-10-26 15:37:37.242137: Epoch 171 
2023-10-26 15:37:37.242388: Current learning rate: 0.00845 
2023-10-26 15:37:41.151910: train_loss -0.8551 
2023-10-26 15:37:41.152332: val_loss -0.8748 
2023-10-26 15:37:41.152585: Pseudo dice [0.8554, 0.9132, 0.9711, 0.7062, 0.9194] 
2023-10-26 15:37:41.152819: Epoch time: 3.91 s 
2023-10-26 15:37:42.247626:  
2023-10-26 15:37:42.248009: Epoch 172 
2023-10-26 15:37:42.248315: Current learning rate: 0.00844 
2023-10-26 15:37:46.051227: train_loss -0.8499 
2023-10-26 15:37:46.051623: val_loss -0.8686 
2023-10-26 15:37:46.051893: Pseudo dice [0.8729, 0.9204, 0.97, 0.6822, 0.9265] 
2023-10-26 15:37:46.052142: Epoch time: 3.8 s 
2023-10-26 15:37:47.172735:  
2023-10-26 15:37:47.173061: Epoch 173 
2023-10-26 15:37:47.173310: Current learning rate: 0.00843 
2023-10-26 15:37:50.918939: train_loss -0.8569 
2023-10-26 15:37:50.919425: val_loss -0.8746 
2023-10-26 15:37:50.919800: Pseudo dice [0.8703, 0.9218, 0.9671, 0.695, 0.9385] 
2023-10-26 15:37:50.920196: Epoch time: 3.75 s 
2023-10-26 15:37:52.065598:  
2023-10-26 15:37:52.065924: Epoch 174 
2023-10-26 15:37:52.066181: Current learning rate: 0.00842 
2023-10-26 15:37:55.925465: train_loss -0.8585 
2023-10-26 15:37:55.925813: val_loss -0.8684 
2023-10-26 15:37:55.926073: Pseudo dice [0.8729, 0.8993, 0.9645, 0.5758, 0.9324] 
2023-10-26 15:37:55.926297: Epoch time: 3.86 s 
2023-10-26 15:37:57.043693:  
2023-10-26 15:37:57.044043: Epoch 175 
2023-10-26 15:37:57.044355: Current learning rate: 0.00841 
2023-10-26 15:38:01.033364: train_loss -0.8437 
2023-10-26 15:38:01.033728: val_loss -0.8718 
2023-10-26 15:38:01.034005: Pseudo dice [0.8737, 0.9177, 0.9699, 0.7343, 0.9303] 
2023-10-26 15:38:01.034238: Epoch time: 3.99 s 
2023-10-26 15:38:02.125412:  
2023-10-26 15:38:02.125709: Epoch 176 
2023-10-26 15:38:02.125956: Current learning rate: 0.0084 
2023-10-26 15:38:06.070202: train_loss -0.8519 
2023-10-26 15:38:06.070580: val_loss -0.8774 
2023-10-26 15:38:06.070838: Pseudo dice [0.866, 0.9135, 0.9708, 0.7481, 0.9404] 
2023-10-26 15:38:06.071105: Epoch time: 3.95 s 
2023-10-26 15:38:07.304744:  
2023-10-26 15:38:07.305057: Epoch 177 
2023-10-26 15:38:07.305307: Current learning rate: 0.00839 
2023-10-26 15:38:11.532357: train_loss -0.864 
2023-10-26 15:38:11.532962: val_loss -0.8669 
2023-10-26 15:38:11.533362: Pseudo dice [0.8799, 0.9187, 0.9675, 0.6514, 0.929] 
2023-10-26 15:38:11.533769: Epoch time: 4.23 s 
2023-10-26 15:38:12.610564:  
2023-10-26 15:38:12.610885: Epoch 178 
2023-10-26 15:38:12.611126: Current learning rate: 0.00838 
2023-10-26 15:38:16.686258: train_loss -0.8544 
2023-10-26 15:38:16.686654: val_loss -0.8688 
2023-10-26 15:38:16.686939: Pseudo dice [0.8756, 0.9172, 0.9689, 0.5676, 0.9174] 
2023-10-26 15:38:16.687190: Epoch time: 4.08 s 
2023-10-26 15:38:17.787433:  
2023-10-26 15:38:17.787732: Epoch 179 
2023-10-26 15:38:17.787978: Current learning rate: 0.00837 
2023-10-26 15:38:21.852289: train_loss -0.8636 
2023-10-26 15:38:21.852660: val_loss -0.878 
2023-10-26 15:38:21.852943: Pseudo dice [0.8648, 0.9154, 0.9693, 0.7302, 0.9347] 
2023-10-26 15:38:21.853182: Epoch time: 4.07 s 
2023-10-26 15:38:22.937787:  
2023-10-26 15:38:22.938082: Epoch 180 
2023-10-26 15:38:22.938328: Current learning rate: 0.00836 
2023-10-26 15:38:26.884834: train_loss -0.8719 
2023-10-26 15:38:26.885280: val_loss -0.8627 
2023-10-26 15:38:26.885645: Pseudo dice [0.8774, 0.9187, 0.968, 0.7552, 0.9182] 
2023-10-26 15:38:26.886099: Epoch time: 3.95 s 
2023-10-26 15:38:28.014411:  
2023-10-26 15:38:28.014698: Epoch 181 
2023-10-26 15:38:28.014937: Current learning rate: 0.00836 
2023-10-26 15:38:31.960134: train_loss -0.8522 
2023-10-26 15:38:31.960514: val_loss -0.8744 
2023-10-26 15:38:31.960768: Pseudo dice [0.8674, 0.9095, 0.966, 0.6804, 0.9374] 
2023-10-26 15:38:31.961014: Epoch time: 3.95 s 
2023-10-26 15:38:33.053128:  
2023-10-26 15:38:33.053419: Epoch 182 
2023-10-26 15:38:33.053658: Current learning rate: 0.00835 
2023-10-26 15:38:36.963538: train_loss -0.8596 
2023-10-26 15:38:36.964103: val_loss -0.8592 
2023-10-26 15:38:36.964423: Pseudo dice [0.861, 0.9144, 0.9688, 0.5535, 0.9434] 
2023-10-26 15:38:36.964895: Epoch time: 3.91 s 
2023-10-26 15:38:38.257587:  
2023-10-26 15:38:38.257887: Epoch 183 
2023-10-26 15:38:38.258133: Current learning rate: 0.00834 
2023-10-26 15:38:42.221243: train_loss -0.8462 
2023-10-26 15:38:42.221618: val_loss -0.8714 
2023-10-26 15:38:42.221900: Pseudo dice [0.8566, 0.9171, 0.9651, 0.708, 0.9245] 
2023-10-26 15:38:42.222123: Epoch time: 3.96 s 
2023-10-26 15:38:43.313264:  
2023-10-26 15:38:43.313547: Epoch 184 
2023-10-26 15:38:43.313788: Current learning rate: 0.00833 
2023-10-26 15:38:47.246727: train_loss -0.8609 
2023-10-26 15:38:47.247148: val_loss -0.8744 
2023-10-26 15:38:47.247421: Pseudo dice [0.8759, 0.9177, 0.9678, 0.6476, 0.9231] 
2023-10-26 15:38:47.247650: Epoch time: 3.93 s 
2023-10-26 15:38:48.380068:  
2023-10-26 15:38:48.380391: Epoch 185 
2023-10-26 15:38:48.380650: Current learning rate: 0.00832 
2023-10-26 15:38:52.291155: train_loss -0.8579 
2023-10-26 15:38:52.291565: val_loss -0.8738 
2023-10-26 15:38:52.291822: Pseudo dice [0.8813, 0.915, 0.9688, 0.6677, 0.918] 
2023-10-26 15:38:52.292054: Epoch time: 3.91 s 
2023-10-26 15:38:53.369216:  
2023-10-26 15:38:53.369570: Epoch 186 
2023-10-26 15:38:53.369834: Current learning rate: 0.00831 
2023-10-26 15:38:57.200401: train_loss -0.8576 
2023-10-26 15:38:57.200762: val_loss -0.8657 
2023-10-26 15:38:57.201020: Pseudo dice [0.88, 0.9166, 0.9646, 0.6941, 0.9248] 
2023-10-26 15:38:57.201241: Epoch time: 3.83 s 
2023-10-26 15:38:58.297755:  
2023-10-26 15:38:58.298078: Epoch 187 
2023-10-26 15:38:58.298338: Current learning rate: 0.0083 
2023-10-26 15:39:02.186073: train_loss -0.8545 
2023-10-26 15:39:02.186455: val_loss -0.8741 
2023-10-26 15:39:02.186722: Pseudo dice [0.8728, 0.9109, 0.9687, 0.7219, 0.9335] 
2023-10-26 15:39:02.187212: Epoch time: 3.89 s 
2023-10-26 15:39:03.282862:  
2023-10-26 15:39:03.283201: Epoch 188 
2023-10-26 15:39:03.283495: Current learning rate: 0.00829 
2023-10-26 15:39:07.106333: train_loss -0.8693 
2023-10-26 15:39:07.106741: val_loss -0.8786 
2023-10-26 15:39:07.107036: Pseudo dice [0.8716, 0.9231, 0.9695, 0.6855, 0.9266] 
2023-10-26 15:39:07.107292: Epoch time: 3.82 s 
2023-10-26 15:39:08.477556:  
2023-10-26 15:39:08.477896: Epoch 189 
2023-10-26 15:39:08.478179: Current learning rate: 0.00828 
2023-10-26 15:39:12.473491: train_loss -0.8634 
2023-10-26 15:39:12.473855: val_loss -0.8783 
2023-10-26 15:39:12.474101: Pseudo dice [0.8734, 0.9217, 0.9678, 0.7391, 0.9252] 
2023-10-26 15:39:12.474327: Epoch time: 4.0 s 
2023-10-26 15:39:13.563034:  
2023-10-26 15:39:13.563336: Epoch 190 
2023-10-26 15:39:13.563617: Current learning rate: 0.00827 
2023-10-26 15:39:17.539107: train_loss -0.8675 
2023-10-26 15:39:17.539507: val_loss -0.8777 
2023-10-26 15:39:17.539796: Pseudo dice [0.8645, 0.9166, 0.9694, 0.7203, 0.9307] 
2023-10-26 15:39:17.540047: Epoch time: 3.98 s 
2023-10-26 15:39:18.673814:  
2023-10-26 15:39:18.674135: Epoch 191 
2023-10-26 15:39:18.674391: Current learning rate: 0.00826 
2023-10-26 15:39:22.551562: train_loss -0.8627 
2023-10-26 15:39:22.551919: val_loss -0.8724 
2023-10-26 15:39:22.552174: Pseudo dice [0.8675, 0.9088, 0.9674, 0.7635, 0.9268] 
2023-10-26 15:39:22.552414: Epoch time: 3.88 s 
2023-10-26 15:39:23.677381:  
2023-10-26 15:39:23.677685: Epoch 192 
2023-10-26 15:39:23.677943: Current learning rate: 0.00825 
2023-10-26 15:39:27.531208: train_loss -0.8574 
2023-10-26 15:39:27.531564: val_loss -0.8783 
2023-10-26 15:39:27.531819: Pseudo dice [0.8778, 0.9182, 0.9698, 0.7053, 0.9328] 
2023-10-26 15:39:27.532056: Epoch time: 3.85 s 
2023-10-26 15:39:28.667471:  
2023-10-26 15:39:28.667809: Epoch 193 
2023-10-26 15:39:28.668048: Current learning rate: 0.00824 
2023-10-26 15:39:32.698903: train_loss -0.8503 
2023-10-26 15:39:32.699290: val_loss -0.8723 
2023-10-26 15:39:32.699563: Pseudo dice [0.8828, 0.9197, 0.9671, 0.7154, 0.9262] 
2023-10-26 15:39:32.699793: Epoch time: 4.03 s 
2023-10-26 15:39:33.793736:  
2023-10-26 15:39:33.794074: Epoch 194 
2023-10-26 15:39:33.794360: Current learning rate: 0.00824 
2023-10-26 15:39:37.778339: train_loss -0.8573 
2023-10-26 15:39:37.778698: val_loss -0.8701 
2023-10-26 15:39:37.778947: Pseudo dice [0.877, 0.9189, 0.9664, 0.77, 0.9215] 
2023-10-26 15:39:37.779160: Epoch time: 3.99 s 
2023-10-26 15:39:37.779354: Yayy! New best EMA pseudo Dice: 0.8784 
2023-10-26 15:39:39.148685:  
2023-10-26 15:39:39.149032: Epoch 195 
2023-10-26 15:39:39.149292: Current learning rate: 0.00823 
2023-10-26 15:39:43.136751: train_loss -0.8574 
2023-10-26 15:39:43.137120: val_loss -0.877 
2023-10-26 15:39:43.137379: Pseudo dice [0.8805, 0.9176, 0.9695, 0.7483, 0.93] 
2023-10-26 15:39:43.137624: Epoch time: 3.99 s 
2023-10-26 15:39:43.137836: Yayy! New best EMA pseudo Dice: 0.8795 
2023-10-26 15:39:44.312103:  
2023-10-26 15:39:44.312428: Epoch 196 
2023-10-26 15:39:44.312719: Current learning rate: 0.00822 
2023-10-26 15:39:48.309399: train_loss -0.8615 
2023-10-26 15:39:48.309754: val_loss -0.8799 
2023-10-26 15:39:48.310032: Pseudo dice [0.8779, 0.9203, 0.9695, 0.7457, 0.9326] 
2023-10-26 15:39:48.310276: Epoch time: 4.0 s 
2023-10-26 15:39:48.310491: Yayy! New best EMA pseudo Dice: 0.8804 
2023-10-26 15:39:49.473218:  
2023-10-26 15:39:49.473604: Epoch 197 
2023-10-26 15:39:49.473854: Current learning rate: 0.00821 
2023-10-26 15:39:53.498587: train_loss -0.8615 
2023-10-26 15:39:53.498963: val_loss -0.8749 
2023-10-26 15:39:53.499219: Pseudo dice [0.869, 0.9113, 0.9702, 0.6484, 0.9389] 
2023-10-26 15:39:53.499446: Epoch time: 4.03 s 
2023-10-26 15:39:54.582638:  
2023-10-26 15:39:54.582927: Epoch 198 
2023-10-26 15:39:54.583166: Current learning rate: 0.0082 
2023-10-26 15:39:58.608949: train_loss -0.8653 
2023-10-26 15:39:58.609332: val_loss -0.8792 
2023-10-26 15:39:58.609596: Pseudo dice [0.8801, 0.919, 0.9704, 0.7649, 0.935] 
2023-10-26 15:39:58.609830: Epoch time: 4.03 s 
2023-10-26 15:39:58.610047: Yayy! New best EMA pseudo Dice: 0.8806 
2023-10-26 15:39:59.771206:  
2023-10-26 15:39:59.771491: Epoch 199 
2023-10-26 15:39:59.771722: Current learning rate: 0.00819 
2023-10-26 15:40:03.774527: train_loss -0.8555 
2023-10-26 15:40:03.774928: val_loss -0.8727 
2023-10-26 15:40:03.775185: Pseudo dice [0.8787, 0.916, 0.9679, 0.7, 0.935] 
2023-10-26 15:40:03.775416: Epoch time: 4.0 s 
2023-10-26 15:40:04.942553:  
2023-10-26 15:40:04.942845: Epoch 200 
2023-10-26 15:40:04.943077: Current learning rate: 0.00818 
2023-10-26 15:40:08.962278: train_loss -0.861 
2023-10-26 15:40:08.962677: val_loss -0.8786 
2023-10-26 15:40:08.962948: Pseudo dice [0.8788, 0.9128, 0.9691, 0.6444, 0.9328] 
2023-10-26 15:40:08.963173: Epoch time: 4.02 s 
2023-10-26 15:40:10.238365:  
2023-10-26 15:40:10.238660: Epoch 201 
2023-10-26 15:40:10.238909: Current learning rate: 0.00817 
2023-10-26 15:40:14.186915: train_loss -0.8697 
2023-10-26 15:40:14.187280: val_loss -0.8752 
2023-10-26 15:40:14.187541: Pseudo dice [0.8807, 0.9074, 0.9694, 0.6718, 0.9284] 
2023-10-26 15:40:14.187807: Epoch time: 3.95 s 
2023-10-26 15:40:15.317897:  
2023-10-26 15:40:15.318172: Epoch 202 
2023-10-26 15:40:15.318406: Current learning rate: 0.00816 
2023-10-26 15:40:19.213652: train_loss -0.8568 
2023-10-26 15:40:19.214169: val_loss -0.8721 
2023-10-26 15:40:19.214684: Pseudo dice [0.8636, 0.911, 0.9679, 0.6701, 0.9268] 
2023-10-26 15:40:19.215103: Epoch time: 3.9 s 
2023-10-26 15:40:20.364210:  
2023-10-26 15:40:20.364502: Epoch 203 
2023-10-26 15:40:20.364737: Current learning rate: 0.00815 
2023-10-26 15:40:24.291660: train_loss -0.8547 
2023-10-26 15:40:24.292129: val_loss -0.8779 
2023-10-26 15:40:24.292475: Pseudo dice [0.8712, 0.9159, 0.9652, 0.7633, 0.9254] 
2023-10-26 15:40:24.293169: Epoch time: 3.93 s 
2023-10-26 15:40:25.451254:  
2023-10-26 15:40:25.451549: Epoch 204 
2023-10-26 15:40:25.451799: Current learning rate: 0.00814 
2023-10-26 15:40:29.292515: train_loss -0.8552 
2023-10-26 15:40:29.292895: val_loss -0.8727 
2023-10-26 15:40:29.293161: Pseudo dice [0.882, 0.9155, 0.9692, 0.6897, 0.9313] 
2023-10-26 15:40:29.293467: Epoch time: 3.84 s 
2023-10-26 15:40:30.389365:  
2023-10-26 15:40:30.389700: Epoch 205 
2023-10-26 15:40:30.389993: Current learning rate: 0.00813 
2023-10-26 15:40:34.300805: train_loss -0.8578 
2023-10-26 15:40:34.301264: val_loss -0.8798 
2023-10-26 15:40:34.301701: Pseudo dice [0.8815, 0.9145, 0.9702, 0.7621, 0.9326] 
2023-10-26 15:40:34.302001: Epoch time: 3.91 s 
2023-10-26 15:40:35.368604:  
2023-10-26 15:40:35.368926: Epoch 206 
2023-10-26 15:40:35.369235: Current learning rate: 0.00813 
2023-10-26 15:40:39.316159: train_loss -0.86 
2023-10-26 15:40:39.316589: val_loss -0.8692 
2023-10-26 15:40:39.316856: Pseudo dice [0.8702, 0.9216, 0.9672, 0.5559, 0.9276] 
2023-10-26 15:40:39.317090: Epoch time: 3.95 s 
2023-10-26 15:40:40.613828:  
2023-10-26 15:40:40.614139: Epoch 207 
2023-10-26 15:40:40.614380: Current learning rate: 0.00812 
2023-10-26 15:40:44.463532: train_loss -0.8625 
2023-10-26 15:40:44.463927: val_loss -0.8707 
2023-10-26 15:40:44.464201: Pseudo dice [0.8709, 0.9202, 0.9693, 0.7026, 0.9343] 
2023-10-26 15:40:44.464443: Epoch time: 3.85 s 
2023-10-26 15:40:45.529284:  
2023-10-26 15:40:45.529585: Epoch 208 
2023-10-26 15:40:45.529822: Current learning rate: 0.00811 
2023-10-26 15:40:49.400056: train_loss -0.8682 
2023-10-26 15:40:49.400507: val_loss -0.8614 
2023-10-26 15:40:49.400862: Pseudo dice [0.8741, 0.9143, 0.9686, 0.6093, 0.9341] 
2023-10-26 15:40:49.401115: Epoch time: 3.87 s 
2023-10-26 15:40:50.502229:  
2023-10-26 15:40:50.502545: Epoch 209 
2023-10-26 15:40:50.502785: Current learning rate: 0.0081 
2023-10-26 15:40:54.402006: train_loss -0.8579 
2023-10-26 15:40:54.402353: val_loss -0.8783 
2023-10-26 15:40:54.402634: Pseudo dice [0.8738, 0.9214, 0.9686, 0.772, 0.9258] 
2023-10-26 15:40:54.402866: Epoch time: 3.9 s 
2023-10-26 15:40:55.459807:  
2023-10-26 15:40:55.460121: Epoch 210 
2023-10-26 15:40:55.460366: Current learning rate: 0.00809 
2023-10-26 15:40:59.182320: train_loss -0.8507 
2023-10-26 15:40:59.182672: val_loss -0.876 
2023-10-26 15:40:59.182943: Pseudo dice [0.8707, 0.921, 0.9656, 0.7274, 0.9288] 
2023-10-26 15:40:59.183176: Epoch time: 3.72 s 
2023-10-26 15:41:00.240619:  
2023-10-26 15:41:00.240913: Epoch 211 
2023-10-26 15:41:00.241151: Current learning rate: 0.00808 
2023-10-26 15:41:04.320287: train_loss -0.8608 
2023-10-26 15:41:04.320665: val_loss -0.8644 
2023-10-26 15:41:04.320947: Pseudo dice [0.8671, 0.9117, 0.9663, 0.502, 0.9102] 
2023-10-26 15:41:04.321181: Epoch time: 4.08 s 
2023-10-26 15:41:05.403101:  
2023-10-26 15:41:05.403444: Epoch 212 
2023-10-26 15:41:05.403752: Current learning rate: 0.00807 
2023-10-26 15:41:09.378164: train_loss -0.8635 
2023-10-26 15:41:09.378551: val_loss -0.873 
2023-10-26 15:41:09.378812: Pseudo dice [0.8716, 0.8998, 0.966, 0.742, 0.9294] 
2023-10-26 15:41:09.379063: Epoch time: 3.98 s 
2023-10-26 15:41:10.501326:  
2023-10-26 15:41:10.501642: Epoch 213 
2023-10-26 15:41:10.501898: Current learning rate: 0.00806 
2023-10-26 15:41:14.489489: train_loss -0.8638 
2023-10-26 15:41:14.489928: val_loss -0.8856 
2023-10-26 15:41:14.490199: Pseudo dice [0.8801, 0.9156, 0.9708, 0.6907, 0.933] 
2023-10-26 15:41:14.490534: Epoch time: 3.99 s 
2023-10-26 15:41:15.750710:  
2023-10-26 15:41:15.751089: Epoch 214 
2023-10-26 15:41:15.751361: Current learning rate: 0.00805 
2023-10-26 15:41:19.577360: train_loss -0.8652 
2023-10-26 15:41:19.577749: val_loss -0.8753 
2023-10-26 15:41:19.578023: Pseudo dice [0.8688, 0.9061, 0.9665, 0.6593, 0.9337] 
2023-10-26 15:41:19.578262: Epoch time: 3.83 s 
2023-10-26 15:41:20.645682:  
2023-10-26 15:41:20.646008: Epoch 215 
2023-10-26 15:41:20.646331: Current learning rate: 0.00804 
2023-10-26 15:41:24.637175: train_loss -0.8674 
2023-10-26 15:41:24.637648: val_loss -0.8708 
2023-10-26 15:41:24.637924: Pseudo dice [0.8858, 0.9137, 0.9692, 0.6689, 0.931] 
2023-10-26 15:41:24.638200: Epoch time: 3.99 s 
2023-10-26 15:41:25.720230:  
2023-10-26 15:41:25.720525: Epoch 216 
2023-10-26 15:41:25.720778: Current learning rate: 0.00803 
2023-10-26 15:41:29.630480: train_loss -0.8613 
2023-10-26 15:41:29.630902: val_loss -0.8733 
2023-10-26 15:41:29.631207: Pseudo dice [0.8737, 0.9155, 0.9656, 0.493, 0.9199] 
2023-10-26 15:41:29.631569: Epoch time: 3.91 s 
2023-10-26 15:41:30.703853:  
2023-10-26 15:41:30.704176: Epoch 217 
2023-10-26 15:41:30.704431: Current learning rate: 0.00802 
2023-10-26 15:41:34.483203: train_loss -0.8514 
2023-10-26 15:41:34.483609: val_loss -0.8516 
2023-10-26 15:41:34.483883: Pseudo dice [0.8767, 0.908, 0.9633, 0.709, 0.9231] 
2023-10-26 15:41:34.484106: Epoch time: 3.78 s 
2023-10-26 15:41:35.547649:  
2023-10-26 15:41:35.547964: Epoch 218 
2023-10-26 15:41:35.548229: Current learning rate: 0.00801 
2023-10-26 15:41:39.363637: train_loss -0.8504 
2023-10-26 15:41:39.364039: val_loss -0.8583 
2023-10-26 15:41:39.364313: Pseudo dice [0.8716, 0.9144, 0.9672, 0.7418, 0.9094] 
2023-10-26 15:41:39.364555: Epoch time: 3.82 s 
2023-10-26 15:41:40.452781:  
2023-10-26 15:41:40.453110: Epoch 219 
2023-10-26 15:41:40.453380: Current learning rate: 0.00801 
2023-10-26 15:41:44.421367: train_loss -0.8528 
2023-10-26 15:41:44.421834: val_loss -0.8754 
2023-10-26 15:41:44.422114: Pseudo dice [0.8706, 0.9192, 0.9681, 0.717, 0.9303] 
2023-10-26 15:41:44.422382: Epoch time: 3.97 s 
2023-10-26 15:41:45.507747:  
2023-10-26 15:41:45.508074: Epoch 220 
2023-10-26 15:41:45.508322: Current learning rate: 0.008 
2023-10-26 15:41:49.319975: train_loss -0.8567 
2023-10-26 15:41:49.320361: val_loss -0.8689 
2023-10-26 15:41:49.320630: Pseudo dice [0.8707, 0.9192, 0.9685, 0.6667, 0.9201] 
2023-10-26 15:41:49.320862: Epoch time: 3.81 s 
2023-10-26 15:41:50.538363:  
2023-10-26 15:41:50.538674: Epoch 221 
2023-10-26 15:41:50.538955: Current learning rate: 0.00799 
2023-10-26 15:41:54.328332: train_loss -0.8444 
2023-10-26 15:41:54.328715: val_loss -0.8656 
2023-10-26 15:41:54.328971: Pseudo dice [0.8835, 0.9179, 0.9689, 0.6419, 0.9307] 
2023-10-26 15:41:54.329196: Epoch time: 3.79 s 
2023-10-26 15:41:55.453773:  
2023-10-26 15:41:55.454080: Epoch 222 
2023-10-26 15:41:55.454322: Current learning rate: 0.00798 
2023-10-26 15:41:59.333008: train_loss -0.847 
2023-10-26 15:41:59.333361: val_loss -0.8511 
2023-10-26 15:41:59.333603: Pseudo dice [0.8784, 0.9228, 0.9685, 0.7866, 0.915] 
2023-10-26 15:41:59.333814: Epoch time: 3.88 s 
2023-10-26 15:42:00.393462:  
2023-10-26 15:42:00.393807: Epoch 223 
2023-10-26 15:42:00.394123: Current learning rate: 0.00797 
2023-10-26 15:42:04.254831: train_loss -0.8552 
2023-10-26 15:42:04.255205: val_loss -0.8723 
2023-10-26 15:42:04.255466: Pseudo dice [0.8675, 0.9148, 0.9683, 0.6364, 0.9246] 
2023-10-26 15:42:04.255685: Epoch time: 3.86 s 
2023-10-26 15:42:05.315715:  
2023-10-26 15:42:05.316037: Epoch 224 
2023-10-26 15:42:05.316289: Current learning rate: 0.00796 
2023-10-26 15:42:09.131372: train_loss -0.8557 
2023-10-26 15:42:09.131926: val_loss -0.8707 
2023-10-26 15:42:09.132366: Pseudo dice [0.8795, 0.913, 0.9689, 0.7525, 0.927] 
2023-10-26 15:42:09.132764: Epoch time: 3.82 s 
2023-10-26 15:42:10.182154:  
2023-10-26 15:42:10.182456: Epoch 225 
2023-10-26 15:42:10.182704: Current learning rate: 0.00795 
2023-10-26 15:42:14.051035: train_loss -0.8614 
2023-10-26 15:42:14.051476: val_loss -0.8716 
2023-10-26 15:42:14.051898: Pseudo dice [0.8823, 0.9175, 0.9682, 0.7125, 0.9272] 
2023-10-26 15:42:14.052222: Epoch time: 3.87 s 
2023-10-26 15:42:15.167800:  
2023-10-26 15:42:15.168118: Epoch 226 
2023-10-26 15:42:15.168361: Current learning rate: 0.00794 
2023-10-26 15:42:19.128081: train_loss -0.857 
2023-10-26 15:42:19.128477: val_loss -0.8681 
2023-10-26 15:42:19.128739: Pseudo dice [0.878, 0.9175, 0.9671, 0.6559, 0.928] 
2023-10-26 15:42:19.128973: Epoch time: 3.96 s 
2023-10-26 15:42:20.355532:  
2023-10-26 15:42:20.355847: Epoch 227 
2023-10-26 15:42:20.356088: Current learning rate: 0.00793 
2023-10-26 15:42:24.253810: train_loss -0.8571 
2023-10-26 15:42:24.254219: val_loss -0.882 
2023-10-26 15:42:24.254581: Pseudo dice [0.8901, 0.9233, 0.9701, 0.7532, 0.9313] 
2023-10-26 15:42:24.254816: Epoch time: 3.9 s 
2023-10-26 15:42:25.321882:  
2023-10-26 15:42:25.322181: Epoch 228 
2023-10-26 15:42:25.322416: Current learning rate: 0.00792 
2023-10-26 15:42:29.196068: train_loss -0.8728 
2023-10-26 15:42:29.196431: val_loss -0.8798 
2023-10-26 15:42:29.196674: Pseudo dice [0.8759, 0.9138, 0.9686, 0.7743, 0.9297] 
2023-10-26 15:42:29.196890: Epoch time: 3.87 s 
2023-10-26 15:42:30.314574:  
2023-10-26 15:42:30.314882: Epoch 229 
2023-10-26 15:42:30.315170: Current learning rate: 0.00791 
2023-10-26 15:42:34.288213: train_loss -0.862 
2023-10-26 15:42:34.288585: val_loss -0.8803 
2023-10-26 15:42:34.288867: Pseudo dice [0.8847, 0.9185, 0.9673, 0.6624, 0.9264] 
2023-10-26 15:42:34.289106: Epoch time: 3.97 s 
2023-10-26 15:42:35.335410:  
2023-10-26 15:42:35.335742: Epoch 230 
2023-10-26 15:42:35.335992: Current learning rate: 0.0079 
2023-10-26 15:42:39.290431: train_loss -0.8602 
2023-10-26 15:42:39.290851: val_loss -0.8717 
2023-10-26 15:42:39.291125: Pseudo dice [0.8737, 0.913, 0.9684, 0.6246, 0.9247] 
2023-10-26 15:42:39.291370: Epoch time: 3.96 s 
2023-10-26 15:42:40.323946:  
2023-10-26 15:42:40.324237: Epoch 231 
2023-10-26 15:42:40.324477: Current learning rate: 0.00789 
2023-10-26 15:42:44.276748: train_loss -0.8675 
2023-10-26 15:42:44.277194: val_loss -0.8839 
2023-10-26 15:42:44.277543: Pseudo dice [0.8829, 0.9185, 0.9701, 0.7539, 0.9302] 
2023-10-26 15:42:44.277813: Epoch time: 3.95 s 
2023-10-26 15:42:45.316143:  
2023-10-26 15:42:45.316424: Epoch 232 
2023-10-26 15:42:45.316660: Current learning rate: 0.00789 
2023-10-26 15:42:49.315586: train_loss -0.8649 
2023-10-26 15:42:49.316353: val_loss -0.8736 
2023-10-26 15:42:49.316695: Pseudo dice [0.8757, 0.9094, 0.9686, 0.8078, 0.9371] 
2023-10-26 15:42:49.317298: Epoch time: 4.0 s 
2023-10-26 15:42:50.402155:  
2023-10-26 15:42:50.402462: Epoch 233 
2023-10-26 15:42:50.402733: Current learning rate: 0.00788 
2023-10-26 15:42:54.470605: train_loss -0.8681 
2023-10-26 15:42:54.471077: val_loss -0.8782 
2023-10-26 15:42:54.471378: Pseudo dice [0.8832, 0.9206, 0.9697, 0.6287, 0.9252] 
2023-10-26 15:42:54.471667: Epoch time: 4.07 s 
2023-10-26 15:42:55.709355:  
2023-10-26 15:42:55.709659: Epoch 234 
2023-10-26 15:42:55.709929: Current learning rate: 0.00787 
2023-10-26 15:42:59.598104: train_loss -0.8619 
2023-10-26 15:42:59.598523: val_loss -0.8703 
2023-10-26 15:42:59.598812: Pseudo dice [0.8726, 0.9197, 0.9687, 0.6968, 0.9165] 
2023-10-26 15:42:59.599079: Epoch time: 3.89 s 
2023-10-26 15:43:00.715325:  
2023-10-26 15:43:00.715616: Epoch 235 
2023-10-26 15:43:00.715863: Current learning rate: 0.00786 
2023-10-26 15:43:04.643184: train_loss -0.8577 
2023-10-26 15:43:04.643707: val_loss -0.8712 
2023-10-26 15:43:04.644084: Pseudo dice [0.8799, 0.9173, 0.9706, 0.5298, 0.925] 
2023-10-26 15:43:04.644389: Epoch time: 3.93 s 
2023-10-26 15:43:05.688393:  
2023-10-26 15:43:05.688684: Epoch 236 
2023-10-26 15:43:05.688923: Current learning rate: 0.00785 
2023-10-26 15:43:09.556479: train_loss -0.8585 
2023-10-26 15:43:09.556862: val_loss -0.879 
2023-10-26 15:43:09.557162: Pseudo dice [0.878, 0.9168, 0.9711, 0.7662, 0.934] 
2023-10-26 15:43:09.557398: Epoch time: 3.87 s 
2023-10-26 15:43:10.605924:  
2023-10-26 15:43:10.606244: Epoch 237 
2023-10-26 15:43:10.606496: Current learning rate: 0.00784 
2023-10-26 15:43:14.436926: train_loss -0.8686 
2023-10-26 15:43:14.437303: val_loss -0.8739 
2023-10-26 15:43:14.437550: Pseudo dice [0.88, 0.9172, 0.9677, 0.7679, 0.9234] 
2023-10-26 15:43:14.437792: Epoch time: 3.83 s 
2023-10-26 15:43:15.508784:  
2023-10-26 15:43:15.509075: Epoch 238 
2023-10-26 15:43:15.509304: Current learning rate: 0.00783 
2023-10-26 15:43:19.301358: train_loss -0.8651 
2023-10-26 15:43:19.302039: val_loss -0.8763 
2023-10-26 15:43:19.302391: Pseudo dice [0.8798, 0.9222, 0.9684, 0.6586, 0.9289] 
2023-10-26 15:43:19.302692: Epoch time: 3.79 s 
2023-10-26 15:43:20.349929:  
2023-10-26 15:43:20.350278: Epoch 239 
2023-10-26 15:43:20.350596: Current learning rate: 0.00782 
2023-10-26 15:43:24.272094: train_loss -0.8679 
2023-10-26 15:43:24.272464: val_loss -0.8667 
2023-10-26 15:43:24.272824: Pseudo dice [0.8774, 0.9082, 0.9674, 0.291, 0.9333] 
2023-10-26 15:43:24.273179: Epoch time: 3.92 s 
2023-10-26 15:43:25.326972:  
2023-10-26 15:43:25.327271: Epoch 240 
2023-10-26 15:43:25.327510: Current learning rate: 0.00781 
2023-10-26 15:43:29.234046: train_loss -0.8508 
2023-10-26 15:43:29.234395: val_loss -0.8633 
2023-10-26 15:43:29.234643: Pseudo dice [0.8794, 0.9147, 0.9672, 0.535, 0.92] 
2023-10-26 15:43:29.234867: Epoch time: 3.91 s 
2023-10-26 15:43:30.545901:  
2023-10-26 15:43:30.546214: Epoch 241 
2023-10-26 15:43:30.546461: Current learning rate: 0.0078 
2023-10-26 15:43:34.495950: train_loss -0.8624 
2023-10-26 15:43:34.496520: val_loss -0.8778 
2023-10-26 15:43:34.496780: Pseudo dice [0.8801, 0.9142, 0.9682, 0.6906, 0.9381] 
2023-10-26 15:43:34.497032: Epoch time: 3.95 s 
2023-10-26 15:43:35.557744:  
2023-10-26 15:43:35.558052: Epoch 242 
2023-10-26 15:43:35.558305: Current learning rate: 0.00779 
2023-10-26 15:43:39.485261: train_loss -0.8679 
2023-10-26 15:43:39.485816: val_loss -0.879 
2023-10-26 15:43:39.486101: Pseudo dice [0.8857, 0.9158, 0.9673, 0.6942, 0.9195] 
2023-10-26 15:43:39.486357: Epoch time: 3.93 s 
2023-10-26 15:43:40.579051:  
2023-10-26 15:43:40.579365: Epoch 243 
2023-10-26 15:43:40.579693: Current learning rate: 0.00778 
2023-10-26 15:43:44.591552: train_loss -0.8656 
2023-10-26 15:43:44.591958: val_loss -0.8776 
2023-10-26 15:43:44.592414: Pseudo dice [0.8843, 0.9205, 0.9693, 0.7533, 0.9362] 
2023-10-26 15:43:44.592866: Epoch time: 4.01 s 
2023-10-26 15:43:45.685441:  
2023-10-26 15:43:45.685776: Epoch 244 
2023-10-26 15:43:45.686073: Current learning rate: 0.00777 
2023-10-26 15:43:49.880494: train_loss -0.8566 
2023-10-26 15:43:49.880853: val_loss -0.8723 
2023-10-26 15:43:49.881113: Pseudo dice [0.8784, 0.9191, 0.969, 0.5842, 0.9263] 
2023-10-26 15:43:49.881335: Epoch time: 4.2 s 
2023-10-26 15:43:50.948847:  
2023-10-26 15:43:50.949165: Epoch 245 
2023-10-26 15:43:50.949413: Current learning rate: 0.00777 
2023-10-26 15:43:54.868147: train_loss -0.862 
2023-10-26 15:43:54.868498: val_loss -0.8834 
2023-10-26 15:43:54.868746: Pseudo dice [0.8839, 0.9194, 0.9709, 0.7634, 0.9329] 
2023-10-26 15:43:54.868983: Epoch time: 3.92 s 
2023-10-26 15:43:55.954360:  
2023-10-26 15:43:55.954657: Epoch 246 
2023-10-26 15:43:55.954912: Current learning rate: 0.00776 
2023-10-26 15:43:59.863308: train_loss -0.8583 
2023-10-26 15:43:59.863682: val_loss -0.8752 
2023-10-26 15:43:59.863949: Pseudo dice [0.8708, 0.9084, 0.9681, 0.7646, 0.9352] 
2023-10-26 15:43:59.864195: Epoch time: 3.91 s 
2023-10-26 15:44:00.953189:  
2023-10-26 15:44:00.953485: Epoch 247 
2023-10-26 15:44:00.953729: Current learning rate: 0.00775 
2023-10-26 15:44:04.928079: train_loss -0.8542 
2023-10-26 15:44:04.928416: val_loss -0.8715 
2023-10-26 15:44:04.928677: Pseudo dice [0.8795, 0.9128, 0.9675, 0.7127, 0.9201] 
2023-10-26 15:44:04.928923: Epoch time: 3.98 s 
2023-10-26 15:44:06.180598:  
2023-10-26 15:44:06.180908: Epoch 248 
2023-10-26 15:44:06.181161: Current learning rate: 0.00774 
2023-10-26 15:44:10.300127: train_loss -0.862 
2023-10-26 15:44:10.300485: val_loss -0.8732 
2023-10-26 15:44:10.300729: Pseudo dice [0.8784, 0.9174, 0.9669, 0.6093, 0.933] 
2023-10-26 15:44:10.300944: Epoch time: 4.12 s 
2023-10-26 15:44:11.357236:  
2023-10-26 15:44:11.357538: Epoch 249 
2023-10-26 15:44:11.357766: Current learning rate: 0.00773 
2023-10-26 15:44:15.300713: train_loss -0.8634 
2023-10-26 15:44:15.301152: val_loss -0.8735 
2023-10-26 15:44:15.301457: Pseudo dice [0.8804, 0.9174, 0.9681, 0.6949, 0.9198] 
2023-10-26 15:44:15.301691: Epoch time: 3.94 s 
2023-10-26 15:44:16.410618:  
2023-10-26 15:44:16.410920: Epoch 250 
2023-10-26 15:44:16.411155: Current learning rate: 0.00772 
2023-10-26 15:44:20.279449: train_loss -0.8681 
2023-10-26 15:44:20.279808: val_loss -0.8753 
2023-10-26 15:44:20.280068: Pseudo dice [0.8713, 0.9128, 0.9663, 0.7085, 0.9314] 
2023-10-26 15:44:20.280294: Epoch time: 3.87 s 
2023-10-26 15:44:21.376838:  
2023-10-26 15:44:21.377134: Epoch 251 
2023-10-26 15:44:21.377521: Current learning rate: 0.00771 
2023-10-26 15:44:25.281921: train_loss -0.8673 
2023-10-26 15:44:25.282454: val_loss -0.8813 
2023-10-26 15:44:25.282862: Pseudo dice [0.8848, 0.9127, 0.9675, 0.7646, 0.9444] 
2023-10-26 15:44:25.283116: Epoch time: 3.91 s 
2023-10-26 15:44:26.342080:  
2023-10-26 15:44:26.342376: Epoch 252 
2023-10-26 15:44:26.342613: Current learning rate: 0.0077 
2023-10-26 15:44:30.269756: train_loss -0.8625 
2023-10-26 15:44:30.270158: val_loss -0.8783 
2023-10-26 15:44:30.270426: Pseudo dice [0.8762, 0.9139, 0.9697, 0.7349, 0.9275] 
2023-10-26 15:44:30.270646: Epoch time: 3.93 s 
2023-10-26 15:44:31.330284:  
2023-10-26 15:44:31.330574: Epoch 253 
2023-10-26 15:44:31.330813: Current learning rate: 0.00769 
2023-10-26 15:44:35.036477: train_loss -0.873 
2023-10-26 15:44:35.036890: val_loss -0.8819 
2023-10-26 15:44:35.037150: Pseudo dice [0.8827, 0.9184, 0.9687, 0.6999, 0.9316] 
2023-10-26 15:44:35.037381: Epoch time: 3.71 s 
2023-10-26 15:44:36.261840:  
2023-10-26 15:44:36.262156: Epoch 254 
2023-10-26 15:44:36.262396: Current learning rate: 0.00768 
2023-10-26 15:44:40.170835: train_loss -0.8634 
2023-10-26 15:44:40.171209: val_loss -0.8768 
2023-10-26 15:44:40.171461: Pseudo dice [0.8832, 0.9155, 0.969, 0.7406, 0.9269] 
2023-10-26 15:44:40.171685: Epoch time: 3.91 s 
2023-10-26 15:44:41.232519:  
2023-10-26 15:44:41.232816: Epoch 255 
2023-10-26 15:44:41.233062: Current learning rate: 0.00767 
2023-10-26 15:44:45.146669: train_loss -0.8584 
2023-10-26 15:44:45.147120: val_loss -0.8606 
2023-10-26 15:44:45.147535: Pseudo dice [0.8643, 0.9144, 0.9661, 0.6423, 0.927] 
2023-10-26 15:44:45.147861: Epoch time: 3.91 s 
2023-10-26 15:44:46.214544:  
2023-10-26 15:44:46.214835: Epoch 256 
2023-10-26 15:44:46.215070: Current learning rate: 0.00766 
2023-10-26 15:44:50.150344: train_loss -0.8648 
2023-10-26 15:44:50.150729: val_loss -0.8807 
2023-10-26 15:44:50.151015: Pseudo dice [0.8735, 0.9124, 0.9691, 0.7305, 0.9275] 
2023-10-26 15:44:50.151250: Epoch time: 3.94 s 
2023-10-26 15:44:51.254399:  
2023-10-26 15:44:51.254695: Epoch 257 
2023-10-26 15:44:51.254955: Current learning rate: 0.00765 
2023-10-26 15:44:55.183228: train_loss -0.8313 
2023-10-26 15:44:55.183621: val_loss -0.8534 
2023-10-26 15:44:55.183887: Pseudo dice [0.868, 0.9167, 0.9638, 0.5928, 0.9294] 
2023-10-26 15:44:55.184120: Epoch time: 3.93 s 
2023-10-26 15:44:56.242346:  
2023-10-26 15:44:56.242658: Epoch 258 
2023-10-26 15:44:56.242914: Current learning rate: 0.00764 
2023-10-26 15:45:00.085822: train_loss -0.8582 
2023-10-26 15:45:00.086222: val_loss -0.8784 
2023-10-26 15:45:00.086471: Pseudo dice [0.871, 0.9197, 0.9685, 0.6708, 0.9395] 
2023-10-26 15:45:00.086700: Epoch time: 3.84 s 
2023-10-26 15:45:01.164670:  
2023-10-26 15:45:01.165024: Epoch 259 
2023-10-26 15:45:01.165286: Current learning rate: 0.00764 
2023-10-26 15:45:04.931852: train_loss -0.8623 
2023-10-26 15:45:04.932353: val_loss -0.8672 
2023-10-26 15:45:04.932651: Pseudo dice [0.8647, 0.9173, 0.9676, 0.6049, 0.9309] 
2023-10-26 15:45:04.933009: Epoch time: 3.77 s 
2023-10-26 15:45:05.993332:  
2023-10-26 15:45:05.993638: Epoch 260 
2023-10-26 15:45:05.993900: Current learning rate: 0.00763 
2023-10-26 15:45:09.919842: train_loss -0.8642 
2023-10-26 15:45:09.920229: val_loss -0.8703 
2023-10-26 15:45:09.920490: Pseudo dice [0.8862, 0.9143, 0.9694, 0.6843, 0.9315] 
2023-10-26 15:45:09.920715: Epoch time: 3.93 s 
2023-10-26 15:45:11.161859:  
2023-10-26 15:45:11.162275: Epoch 261 
2023-10-26 15:45:11.162519: Current learning rate: 0.00762 
2023-10-26 15:45:15.125037: train_loss -0.8683 
2023-10-26 15:45:15.125407: val_loss -0.8426 
2023-10-26 15:45:15.125666: Pseudo dice [0.8845, 0.9197, 0.9666, 0.5939, 0.9192] 
2023-10-26 15:45:15.125904: Epoch time: 3.96 s 
2023-10-26 15:45:16.178030:  
2023-10-26 15:45:16.178348: Epoch 262 
2023-10-26 15:45:16.178600: Current learning rate: 0.00761 
2023-10-26 15:45:19.986931: train_loss -0.8657 
2023-10-26 15:45:19.987359: val_loss -0.867 
2023-10-26 15:45:19.987683: Pseudo dice [0.8715, 0.9167, 0.968, 0.4148, 0.9257] 
2023-10-26 15:45:19.987942: Epoch time: 3.81 s 
2023-10-26 15:45:21.032242:  
2023-10-26 15:45:21.032530: Epoch 263 
2023-10-26 15:45:21.032776: Current learning rate: 0.0076 
2023-10-26 15:45:24.812125: train_loss -0.8666 
2023-10-26 15:45:24.812552: val_loss -0.8702 
2023-10-26 15:45:24.812895: Pseudo dice [0.8795, 0.9193, 0.9685, 0.6752, 0.9298] 
2023-10-26 15:45:24.813174: Epoch time: 3.78 s 
2023-10-26 15:45:25.875541:  
2023-10-26 15:45:25.875900: Epoch 264 
2023-10-26 15:45:25.876193: Current learning rate: 0.00759 
2023-10-26 15:45:29.824657: train_loss -0.867 
2023-10-26 15:45:29.825050: val_loss -0.8692 
2023-10-26 15:45:29.825313: Pseudo dice [0.8736, 0.9164, 0.9637, 0.6786, 0.9215] 
2023-10-26 15:45:29.825559: Epoch time: 3.95 s 
2023-10-26 15:45:30.891483:  
2023-10-26 15:45:30.891777: Epoch 265 
2023-10-26 15:45:30.892023: Current learning rate: 0.00758 
2023-10-26 15:45:34.827123: train_loss -0.8626 
2023-10-26 15:45:34.827498: val_loss -0.8781 
2023-10-26 15:45:34.827752: Pseudo dice [0.8768, 0.9144, 0.9683, 0.6629, 0.9318] 
2023-10-26 15:45:34.828020: Epoch time: 3.94 s 
2023-10-26 15:45:35.915669:  
2023-10-26 15:45:35.915978: Epoch 266 
2023-10-26 15:45:35.916223: Current learning rate: 0.00757 
2023-10-26 15:45:39.716269: train_loss -0.8521 
2023-10-26 15:45:39.716961: val_loss -0.8656 
2023-10-26 15:45:39.717254: Pseudo dice [0.8735, 0.9164, 0.9682, 0.6179, 0.9226] 
2023-10-26 15:45:39.717494: Epoch time: 3.8 s 
2023-10-26 15:45:40.971914:  
2023-10-26 15:45:40.972220: Epoch 267 
2023-10-26 15:45:40.972474: Current learning rate: 0.00756 
2023-10-26 15:45:44.833966: train_loss -0.8601 
2023-10-26 15:45:44.834336: val_loss -0.871 
2023-10-26 15:45:44.834588: Pseudo dice [0.8805, 0.9133, 0.9692, 0.7369, 0.9376] 
2023-10-26 15:45:44.834811: Epoch time: 3.86 s 
2023-10-26 15:45:45.906525:  
2023-10-26 15:45:45.906822: Epoch 268 
2023-10-26 15:45:45.907084: Current learning rate: 0.00755 
2023-10-26 15:45:49.776314: train_loss -0.8646 
2023-10-26 15:45:49.776675: val_loss -0.8678 
2023-10-26 15:45:49.776938: Pseudo dice [0.8827, 0.9094, 0.9696, 0.6312, 0.9323] 
2023-10-26 15:45:49.777174: Epoch time: 3.87 s 
2023-10-26 15:45:50.842654:  
2023-10-26 15:45:50.842954: Epoch 269 
2023-10-26 15:45:50.843197: Current learning rate: 0.00754 
2023-10-26 15:45:54.786391: train_loss -0.8708 
2023-10-26 15:45:54.786768: val_loss -0.87 
2023-10-26 15:45:54.787028: Pseudo dice [0.8812, 0.911, 0.9681, 0.647, 0.9239] 
2023-10-26 15:45:54.787244: Epoch time: 3.94 s 
2023-10-26 15:45:55.847677:  
2023-10-26 15:45:55.847993: Epoch 270 
2023-10-26 15:45:55.848242: Current learning rate: 0.00753 
2023-10-26 15:45:59.789352: train_loss -0.8661 
2023-10-26 15:45:59.789767: val_loss -0.8767 
2023-10-26 15:45:59.790039: Pseudo dice [0.8867, 0.924, 0.9691, 0.7206, 0.9202] 
2023-10-26 15:45:59.790269: Epoch time: 3.94 s 
2023-10-26 15:46:00.898844:  
2023-10-26 15:46:00.899164: Epoch 271 
2023-10-26 15:46:00.899408: Current learning rate: 0.00752 
2023-10-26 15:46:04.752921: train_loss -0.8645 
2023-10-26 15:46:04.753295: val_loss -0.8757 
2023-10-26 15:46:04.753545: Pseudo dice [0.8835, 0.9194, 0.9674, 0.6815, 0.9349] 
2023-10-26 15:46:04.753774: Epoch time: 3.85 s 
2023-10-26 15:46:05.814769:  
2023-10-26 15:46:05.815070: Epoch 272 
2023-10-26 15:46:05.815317: Current learning rate: 0.00751 
2023-10-26 15:46:09.759311: train_loss -0.8672 
2023-10-26 15:46:09.760046: val_loss -0.8797 
2023-10-26 15:46:09.760374: Pseudo dice [0.8823, 0.9163, 0.9676, 0.6692, 0.9385] 
2023-10-26 15:46:09.760635: Epoch time: 3.95 s 
2023-10-26 15:46:10.845702:  
2023-10-26 15:46:10.845996: Epoch 273 
2023-10-26 15:46:10.846258: Current learning rate: 0.00751 
2023-10-26 15:46:14.854540: train_loss -0.8531 
2023-10-26 15:46:14.855010: val_loss -0.869 
2023-10-26 15:46:14.855287: Pseudo dice [0.8808, 0.9127, 0.9674, 0.7357, 0.9334] 
2023-10-26 15:46:14.855517: Epoch time: 4.01 s 
2023-10-26 15:46:16.109111:  
2023-10-26 15:46:16.109418: Epoch 274 
2023-10-26 15:46:16.109657: Current learning rate: 0.0075 
2023-10-26 15:46:20.137370: train_loss -0.8439 
2023-10-26 15:46:20.137844: val_loss -0.8677 
2023-10-26 15:46:20.138390: Pseudo dice [0.8737, 0.9162, 0.9618, 0.7411, 0.9248] 
2023-10-26 15:46:20.139013: Epoch time: 4.03 s 
2023-10-26 15:46:21.218678:  
2023-10-26 15:46:21.218989: Epoch 275 
2023-10-26 15:46:21.219237: Current learning rate: 0.00749 
2023-10-26 15:46:25.217035: train_loss -0.8497 
2023-10-26 15:46:25.217432: val_loss -0.8683 
2023-10-26 15:46:25.217759: Pseudo dice [0.8615, 0.912, 0.9677, 0.7143, 0.9173] 
2023-10-26 15:46:25.218037: Epoch time: 4.0 s 
2023-10-26 15:46:26.335287:  
2023-10-26 15:46:26.335592: Epoch 276 
2023-10-26 15:46:26.335838: Current learning rate: 0.00748 
2023-10-26 15:46:30.391766: train_loss -0.8525 
2023-10-26 15:46:30.392143: val_loss -0.875 
2023-10-26 15:46:30.392410: Pseudo dice [0.8785, 0.9167, 0.9679, 0.6765, 0.9291] 
2023-10-26 15:46:30.392636: Epoch time: 4.06 s 
2023-10-26 15:46:31.495855:  
2023-10-26 15:46:31.496178: Epoch 277 
2023-10-26 15:46:31.496421: Current learning rate: 0.00747 
2023-10-26 15:46:35.564698: train_loss -0.8555 
2023-10-26 15:46:35.565113: val_loss -0.8673 
2023-10-26 15:46:35.565391: Pseudo dice [0.8675, 0.9054, 0.9663, 0.7945, 0.9151] 
2023-10-26 15:46:35.565638: Epoch time: 4.07 s 
2023-10-26 15:46:36.649124:  
2023-10-26 15:46:36.649442: Epoch 278 
2023-10-26 15:46:36.649691: Current learning rate: 0.00746 
2023-10-26 15:46:40.739050: train_loss -0.8529 
2023-10-26 15:46:40.739433: val_loss -0.8748 
2023-10-26 15:46:40.739696: Pseudo dice [0.8797, 0.9166, 0.9669, 0.6495, 0.9257] 
2023-10-26 15:46:40.739923: Epoch time: 4.09 s 
2023-10-26 15:46:41.793506:  
2023-10-26 15:46:41.793809: Epoch 279 
2023-10-26 15:46:41.794056: Current learning rate: 0.00745 
2023-10-26 15:46:45.885082: train_loss -0.8605 
2023-10-26 15:46:45.885468: val_loss -0.8743 
2023-10-26 15:46:45.885713: Pseudo dice [0.8822, 0.9121, 0.9676, 0.7528, 0.9306] 
2023-10-26 15:46:45.885944: Epoch time: 4.09 s 
2023-10-26 15:46:46.931281:  
2023-10-26 15:46:46.931577: Epoch 280 
2023-10-26 15:46:46.931818: Current learning rate: 0.00744 
2023-10-26 15:46:50.994941: train_loss -0.865 
2023-10-26 15:46:50.995357: val_loss -0.878 
2023-10-26 15:46:50.995626: Pseudo dice [0.875, 0.9157, 0.9686, 0.6627, 0.9231] 
2023-10-26 15:46:50.995855: Epoch time: 4.06 s 
2023-10-26 15:46:52.044552:  
2023-10-26 15:46:52.044840: Epoch 281 
2023-10-26 15:46:52.045068: Current learning rate: 0.00743 
2023-10-26 15:46:55.994349: train_loss -0.8589 
2023-10-26 15:46:55.994751: val_loss -0.8782 
2023-10-26 15:46:55.995024: Pseudo dice [0.886, 0.9117, 0.9696, 0.7048, 0.9344] 
2023-10-26 15:46:55.995272: Epoch time: 3.95 s 
2023-10-26 15:46:57.093314:  
2023-10-26 15:46:57.093601: Epoch 282 
2023-10-26 15:46:57.093831: Current learning rate: 0.00742 
2023-10-26 15:47:00.948332: train_loss -0.8662 
2023-10-26 15:47:00.948726: val_loss -0.8803 
2023-10-26 15:47:00.949004: Pseudo dice [0.8777, 0.9204, 0.9679, 0.7106, 0.922] 
2023-10-26 15:47:00.949250: Epoch time: 3.86 s 
2023-10-26 15:47:02.008760:  
2023-10-26 15:47:02.009064: Epoch 283 
2023-10-26 15:47:02.009315: Current learning rate: 0.00741 
2023-10-26 15:47:06.027297: train_loss -0.8623 
2023-10-26 15:47:06.027707: val_loss -0.8708 
2023-10-26 15:47:06.027962: Pseudo dice [0.8819, 0.9084, 0.9648, 0.8006, 0.9304] 
2023-10-26 15:47:06.028199: Epoch time: 4.02 s 
2023-10-26 15:47:07.087421:  
2023-10-26 15:47:07.087711: Epoch 284 
2023-10-26 15:47:07.087959: Current learning rate: 0.0074 
2023-10-26 15:47:11.028357: train_loss -0.8696 
2023-10-26 15:47:11.028750: val_loss -0.8774 
2023-10-26 15:47:11.029024: Pseudo dice [0.8825, 0.9218, 0.9681, 0.699, 0.9351] 
2023-10-26 15:47:11.029252: Epoch time: 3.94 s 
2023-10-26 15:47:12.126348:  
2023-10-26 15:47:12.126644: Epoch 285 
2023-10-26 15:47:12.126892: Current learning rate: 0.00739 
2023-10-26 15:47:15.939927: train_loss -0.8664 
2023-10-26 15:47:15.940305: val_loss -0.8809 
2023-10-26 15:47:15.940562: Pseudo dice [0.8788, 0.9148, 0.9671, 0.7484, 0.9345] 
2023-10-26 15:47:15.940819: Epoch time: 3.81 s 
2023-10-26 15:47:17.014057:  
2023-10-26 15:47:17.014360: Epoch 286 
2023-10-26 15:47:17.014599: Current learning rate: 0.00738 
2023-10-26 15:47:21.059952: train_loss -0.8585 
2023-10-26 15:47:21.060353: val_loss -0.8828 
2023-10-26 15:47:21.060623: Pseudo dice [0.8794, 0.9188, 0.9682, 0.74, 0.9284] 
2023-10-26 15:47:21.060862: Epoch time: 4.05 s 
2023-10-26 15:47:22.354923:  
2023-10-26 15:47:22.355304: Epoch 287 
2023-10-26 15:47:22.355642: Current learning rate: 0.00738 
2023-10-26 15:47:26.332511: train_loss -0.8688 
2023-10-26 15:47:26.332892: val_loss -0.8801 
2023-10-26 15:47:26.333164: Pseudo dice [0.8796, 0.9208, 0.968, 0.7647, 0.9302] 
2023-10-26 15:47:26.333398: Epoch time: 3.98 s 
2023-10-26 15:47:26.333608: Yayy! New best EMA pseudo Dice: 0.8817 
2023-10-26 15:47:27.512178:  
2023-10-26 15:47:27.512482: Epoch 288 
2023-10-26 15:47:27.512738: Current learning rate: 0.00737 
2023-10-26 15:47:31.416885: train_loss -0.8642 
2023-10-26 15:47:31.417249: val_loss -0.8723 
2023-10-26 15:47:31.417503: Pseudo dice [0.8807, 0.9163, 0.97, 0.6252, 0.9386] 
2023-10-26 15:47:31.417727: Epoch time: 3.91 s 
2023-10-26 15:47:32.534380:  
2023-10-26 15:47:32.534678: Epoch 289 
2023-10-26 15:47:32.534925: Current learning rate: 0.00736 
2023-10-26 15:47:36.321120: train_loss -0.8693 
2023-10-26 15:47:36.321481: val_loss -0.8739 
2023-10-26 15:47:36.321727: Pseudo dice [0.877, 0.9118, 0.9699, 0.6964, 0.9209] 
2023-10-26 15:47:36.321947: Epoch time: 3.79 s 
2023-10-26 15:47:37.423123:  
2023-10-26 15:47:37.423440: Epoch 290 
2023-10-26 15:47:37.423694: Current learning rate: 0.00735 
2023-10-26 15:47:41.359859: train_loss -0.8656 
2023-10-26 15:47:41.360261: val_loss -0.8636 
2023-10-26 15:47:41.360544: Pseudo dice [0.8841, 0.9133, 0.9684, 0.6798, 0.9341] 
2023-10-26 15:47:41.360770: Epoch time: 3.94 s 
2023-10-26 15:47:42.444067:  
2023-10-26 15:47:42.444408: Epoch 291 
2023-10-26 15:47:42.444668: Current learning rate: 0.00734 
2023-10-26 15:47:46.407206: train_loss -0.8626 
2023-10-26 15:47:46.407635: val_loss -0.8754 
2023-10-26 15:47:46.407960: Pseudo dice [0.8824, 0.9112, 0.9669, 0.7263, 0.927] 
2023-10-26 15:47:46.408242: Epoch time: 3.96 s 
2023-10-26 15:47:47.506540:  
2023-10-26 15:47:47.506830: Epoch 292 
2023-10-26 15:47:47.507078: Current learning rate: 0.00733 
2023-10-26 15:47:51.539249: train_loss -0.8647 
2023-10-26 15:47:51.539614: val_loss -0.8758 
2023-10-26 15:47:51.539868: Pseudo dice [0.8863, 0.9141, 0.966, 0.7184, 0.9406] 
2023-10-26 15:47:51.540108: Epoch time: 4.03 s 
2023-10-26 15:47:52.816381:  
2023-10-26 15:47:52.816685: Epoch 293 
2023-10-26 15:47:52.816941: Current learning rate: 0.00732 
2023-10-26 15:47:56.839683: train_loss -0.8546 
2023-10-26 15:47:56.840047: val_loss -0.8484 
2023-10-26 15:47:56.840300: Pseudo dice [0.8761, 0.9126, 0.9611, 0.5217, 0.9101] 
2023-10-26 15:47:56.840527: Epoch time: 4.02 s 
2023-10-26 15:47:57.926356:  
2023-10-26 15:47:57.926664: Epoch 294 
2023-10-26 15:47:57.926919: Current learning rate: 0.00731 
2023-10-26 15:48:01.841304: train_loss -0.8428 
2023-10-26 15:48:01.841660: val_loss -0.8541 
2023-10-26 15:48:01.841914: Pseudo dice [0.8781, 0.9162, 0.9682, 0.7034, 0.9195] 
2023-10-26 15:48:01.842136: Epoch time: 3.92 s 
2023-10-26 15:48:02.903838:  
2023-10-26 15:48:02.904140: Epoch 295 
2023-10-26 15:48:02.904374: Current learning rate: 0.0073 
2023-10-26 15:48:06.871786: train_loss -0.8488 
2023-10-26 15:48:06.872154: val_loss -0.8714 
2023-10-26 15:48:06.872443: Pseudo dice [0.8684, 0.9146, 0.9685, 0.5667, 0.9282] 
2023-10-26 15:48:06.872674: Epoch time: 3.97 s 
2023-10-26 15:48:07.953107:  
2023-10-26 15:48:07.953406: Epoch 296 
2023-10-26 15:48:07.953650: Current learning rate: 0.00729 
2023-10-26 15:48:12.043571: train_loss -0.8641 
2023-10-26 15:48:12.043992: val_loss -0.8819 
2023-10-26 15:48:12.044267: Pseudo dice [0.8823, 0.9117, 0.9707, 0.7026, 0.9215] 
2023-10-26 15:48:12.044526: Epoch time: 4.09 s 
2023-10-26 15:48:13.135556:  
2023-10-26 15:48:13.135849: Epoch 297 
2023-10-26 15:48:13.136109: Current learning rate: 0.00728 
2023-10-26 15:48:17.164821: train_loss -0.8668 
2023-10-26 15:48:17.165201: val_loss -0.879 
2023-10-26 15:48:17.165452: Pseudo dice [0.883, 0.9107, 0.9685, 0.6815, 0.9293] 
2023-10-26 15:48:17.165673: Epoch time: 4.03 s 
2023-10-26 15:48:18.236295:  
2023-10-26 15:48:18.236577: Epoch 298 
2023-10-26 15:48:18.236818: Current learning rate: 0.00727 
2023-10-26 15:48:22.248474: train_loss -0.8527 
2023-10-26 15:48:22.248835: val_loss -0.851 
2023-10-26 15:48:22.249091: Pseudo dice [0.8741, 0.9018, 0.9672, 0.3457, 0.9288] 
2023-10-26 15:48:22.249318: Epoch time: 4.01 s 
2023-10-26 15:48:23.353273:  
2023-10-26 15:48:23.353570: Epoch 299 
2023-10-26 15:48:23.353809: Current learning rate: 0.00726 
2023-10-26 15:48:27.401096: train_loss -0.855 
2023-10-26 15:48:27.401484: val_loss -0.8654 
2023-10-26 15:48:27.401746: Pseudo dice [0.8547, 0.9162, 0.9689, 0.7611, 0.9239] 
2023-10-26 15:48:27.401984: Epoch time: 4.05 s 
2023-10-26 15:48:28.797615:  
2023-10-26 15:48:28.797926: Epoch 300 
2023-10-26 15:48:28.798209: Current learning rate: 0.00725 
2023-10-26 15:48:32.870899: train_loss -0.8627 
2023-10-26 15:48:32.871350: val_loss -0.8715 
2023-10-26 15:48:32.871858: Pseudo dice [0.885, 0.9185, 0.9694, 0.6909, 0.9235] 
2023-10-26 15:48:32.872171: Epoch time: 4.07 s 
2023-10-26 15:48:34.019953:  
2023-10-26 15:48:34.020236: Epoch 301 
2023-10-26 15:48:34.020473: Current learning rate: 0.00724 
2023-10-26 15:48:38.035020: train_loss -0.8635 
2023-10-26 15:48:38.035494: val_loss -0.8744 
2023-10-26 15:48:38.035802: Pseudo dice [0.8621, 0.9083, 0.9675, 0.7043, 0.944] 
2023-10-26 15:48:38.036041: Epoch time: 4.02 s 
2023-10-26 15:48:39.161274:  
2023-10-26 15:48:39.161567: Epoch 302 
2023-10-26 15:48:39.161805: Current learning rate: 0.00724 
2023-10-26 15:48:43.082187: train_loss -0.8594 
2023-10-26 15:48:43.082669: val_loss -0.8766 
2023-10-26 15:48:43.082975: Pseudo dice [0.8789, 0.9168, 0.9645, 0.8045, 0.9282] 
2023-10-26 15:48:43.083244: Epoch time: 3.92 s 
2023-10-26 15:48:44.220328:  
2023-10-26 15:48:44.220634: Epoch 303 
2023-10-26 15:48:44.220900: Current learning rate: 0.00723 
2023-10-26 15:48:48.126413: train_loss -0.8562 
2023-10-26 15:48:48.126776: val_loss -0.8714 
2023-10-26 15:48:48.127037: Pseudo dice [0.8738, 0.9132, 0.9677, 0.7337, 0.922] 
2023-10-26 15:48:48.127549: Epoch time: 3.91 s 
2023-10-26 15:48:49.204890:  
2023-10-26 15:48:49.205186: Epoch 304 
2023-10-26 15:48:49.205460: Current learning rate: 0.00722 
2023-10-26 15:48:53.169045: train_loss -0.8604 
2023-10-26 15:48:53.169414: val_loss -0.8758 
2023-10-26 15:48:53.169677: Pseudo dice [0.8794, 0.915, 0.9674, 0.7111, 0.93] 
2023-10-26 15:48:53.169912: Epoch time: 3.96 s 
2023-10-26 15:48:54.259337:  
2023-10-26 15:48:54.273053: Epoch 305 
2023-10-26 15:48:54.273295: Current learning rate: 0.00721 
2023-10-26 15:48:58.146408: train_loss -0.8621 
2023-10-26 15:48:58.146788: val_loss -0.8599 
2023-10-26 15:48:58.147080: Pseudo dice [0.8663, 0.9021, 0.9671, 0.6101, 0.9273] 
2023-10-26 15:48:58.147324: Epoch time: 3.89 s 
2023-10-26 15:48:59.393400:  
2023-10-26 15:48:59.393703: Epoch 306 
2023-10-26 15:48:59.393947: Current learning rate: 0.0072 
2023-10-26 15:49:03.353770: train_loss -0.8681 
2023-10-26 15:49:03.354336: val_loss -0.8771 
2023-10-26 15:49:03.354735: Pseudo dice [0.8774, 0.9163, 0.9653, 0.7423, 0.9221] 
2023-10-26 15:49:03.355045: Epoch time: 3.96 s 
2023-10-26 15:49:04.437047:  
2023-10-26 15:49:04.437338: Epoch 307 
2023-10-26 15:49:04.437584: Current learning rate: 0.00719 
2023-10-26 15:49:08.335794: train_loss -0.8716 
2023-10-26 15:49:08.336178: val_loss -0.8748 
2023-10-26 15:49:08.336441: Pseudo dice [0.8788, 0.9157, 0.969, 0.7169, 0.9314] 
2023-10-26 15:49:08.336686: Epoch time: 3.9 s 
2023-10-26 15:49:09.463709:  
2023-10-26 15:49:09.464011: Epoch 308 
2023-10-26 15:49:09.464259: Current learning rate: 0.00718 
2023-10-26 15:49:13.393197: train_loss -0.8671 
2023-10-26 15:49:13.393563: val_loss -0.8749 
2023-10-26 15:49:13.393824: Pseudo dice [0.8754, 0.9167, 0.9696, 0.6372, 0.9298] 
2023-10-26 15:49:13.394037: Epoch time: 3.93 s 
2023-10-26 15:49:14.467889:  
2023-10-26 15:49:14.468183: Epoch 309 
2023-10-26 15:49:14.468421: Current learning rate: 0.00717 
2023-10-26 15:49:18.396198: train_loss -0.8635 
2023-10-26 15:49:18.396580: val_loss -0.8738 
2023-10-26 15:49:18.396928: Pseudo dice [0.8808, 0.9151, 0.9681, 0.7342, 0.9292] 
2023-10-26 15:49:18.397235: Epoch time: 3.93 s 
2023-10-26 15:49:19.506816:  
2023-10-26 15:49:19.507128: Epoch 310 
2023-10-26 15:49:19.507383: Current learning rate: 0.00716 
2023-10-26 15:49:23.376109: train_loss -0.864 
2023-10-26 15:49:23.376475: val_loss -0.8532 
2023-10-26 15:49:23.376729: Pseudo dice [0.8754, 0.9159, 0.969, 0.6382, 0.9331] 
2023-10-26 15:49:23.376958: Epoch time: 3.87 s 
2023-10-26 15:49:24.493469:  
2023-10-26 15:49:24.493761: Epoch 311 
2023-10-26 15:49:24.494003: Current learning rate: 0.00715 
2023-10-26 15:49:28.476520: train_loss -0.8381 
2023-10-26 15:49:28.477123: val_loss -0.8575 
2023-10-26 15:49:28.477476: Pseudo dice [0.8703, 0.905, 0.964, 0.7084, 0.9252] 
2023-10-26 15:49:28.477727: Epoch time: 3.98 s 
2023-10-26 15:49:29.721487:  
2023-10-26 15:49:29.721789: Epoch 312 
2023-10-26 15:49:29.722034: Current learning rate: 0.00714 
2023-10-26 15:49:33.818146: train_loss -0.8562 
2023-10-26 15:49:33.818498: val_loss -0.8608 
2023-10-26 15:49:33.818775: Pseudo dice [0.8611, 0.9019, 0.9691, 0.6878, 0.9193] 
2023-10-26 15:49:33.819031: Epoch time: 4.1 s 
2023-10-26 15:49:34.944147:  
2023-10-26 15:49:34.944451: Epoch 313 
2023-10-26 15:49:34.944691: Current learning rate: 0.00713 
2023-10-26 15:49:38.973222: train_loss -0.8558 
2023-10-26 15:49:38.973566: val_loss -0.8773 
2023-10-26 15:49:38.973814: Pseudo dice [0.8703, 0.9163, 0.9688, 0.8025, 0.9291] 
2023-10-26 15:49:38.974039: Epoch time: 4.03 s 
2023-10-26 15:49:40.037600:  
2023-10-26 15:49:40.037910: Epoch 314 
2023-10-26 15:49:40.038147: Current learning rate: 0.00712 
2023-10-26 15:49:44.104473: train_loss -0.8608 
2023-10-26 15:49:44.104828: val_loss -0.8711 
2023-10-26 15:49:44.105079: Pseudo dice [0.8685, 0.9149, 0.9689, 0.6904, 0.9317] 
2023-10-26 15:49:44.105316: Epoch time: 4.07 s 
2023-10-26 15:49:45.171553:  
2023-10-26 15:49:45.171861: Epoch 315 
2023-10-26 15:49:45.172114: Current learning rate: 0.00711 
2023-10-26 15:49:49.217583: train_loss -0.8641 
2023-10-26 15:49:49.217954: val_loss -0.8806 
2023-10-26 15:49:49.218227: Pseudo dice [0.8817, 0.9183, 0.9697, 0.7704, 0.9285] 
2023-10-26 15:49:49.218498: Epoch time: 4.05 s 
2023-10-26 15:49:50.287525:  
2023-10-26 15:49:50.287801: Epoch 316 
2023-10-26 15:49:50.288063: Current learning rate: 0.0071 
2023-10-26 15:49:54.332358: train_loss -0.8572 
2023-10-26 15:49:54.332726: val_loss -0.8705 
2023-10-26 15:49:54.332981: Pseudo dice [0.8745, 0.9197, 0.9688, 0.745, 0.9251] 
2023-10-26 15:49:54.333212: Epoch time: 4.05 s 
2023-10-26 15:49:55.400210:  
2023-10-26 15:49:55.400506: Epoch 317 
2023-10-26 15:49:55.400743: Current learning rate: 0.0071 
2023-10-26 15:49:59.385588: train_loss -0.867 
2023-10-26 15:49:59.386014: val_loss -0.8745 
2023-10-26 15:49:59.386279: Pseudo dice [0.8808, 0.9146, 0.9667, 0.683, 0.9305] 
2023-10-26 15:49:59.386541: Epoch time: 3.99 s 
2023-10-26 15:50:00.507600:  
2023-10-26 15:50:00.507913: Epoch 318 
2023-10-26 15:50:00.508165: Current learning rate: 0.00709 
2023-10-26 15:50:04.514241: train_loss -0.8746 
2023-10-26 15:50:04.514623: val_loss -0.8834 
2023-10-26 15:50:04.514888: Pseudo dice [0.8893, 0.9172, 0.9693, 0.766, 0.9301] 
2023-10-26 15:50:04.515109: Epoch time: 4.01 s 
2023-10-26 15:50:05.764294:  
2023-10-26 15:50:05.764593: Epoch 319 
2023-10-26 15:50:05.764827: Current learning rate: 0.00708 
2023-10-26 15:50:09.751311: train_loss -0.864 
2023-10-26 15:50:09.751691: val_loss -0.886 
2023-10-26 15:50:09.751949: Pseudo dice [0.8891, 0.9156, 0.9706, 0.7624, 0.9355] 
2023-10-26 15:50:09.752174: Epoch time: 3.99 s 
2023-10-26 15:50:10.847581:  
2023-10-26 15:50:10.847886: Epoch 320 
2023-10-26 15:50:10.848126: Current learning rate: 0.00707 
2023-10-26 15:50:14.816138: train_loss -0.87 
2023-10-26 15:50:14.816603: val_loss -0.8755 
2023-10-26 15:50:14.816943: Pseudo dice [0.8765, 0.9115, 0.9675, 0.7548, 0.9313] 
2023-10-26 15:50:14.817256: Epoch time: 3.97 s 
2023-10-26 15:50:14.817495: Yayy! New best EMA pseudo Dice: 0.882 
2023-10-26 15:50:15.985128:  
2023-10-26 15:50:15.985416: Epoch 321 
2023-10-26 15:50:15.985651: Current learning rate: 0.00706 
2023-10-26 15:50:19.650657: train_loss -0.8645 
2023-10-26 15:50:19.651028: val_loss -0.8715 
2023-10-26 15:50:19.651287: Pseudo dice [0.8808, 0.9224, 0.9696, 0.7534, 0.9206] 
2023-10-26 15:50:19.651660: Epoch time: 3.67 s 
2023-10-26 15:50:19.651877: Yayy! New best EMA pseudo Dice: 0.8827 
2023-10-26 15:50:20.902507:  
2023-10-26 15:50:20.902809: Epoch 322 
2023-10-26 15:50:20.903084: Current learning rate: 0.00705 
2023-10-26 15:50:24.819880: train_loss -0.8655 
2023-10-26 15:50:24.820256: val_loss -0.8835 
2023-10-26 15:50:24.820519: Pseudo dice [0.8902, 0.9226, 0.9709, 0.6984, 0.9405] 
2023-10-26 15:50:24.820758: Epoch time: 3.92 s 
2023-10-26 15:50:24.820981: Yayy! New best EMA pseudo Dice: 0.8829 
2023-10-26 15:50:26.008308:  
2023-10-26 15:50:26.008613: Epoch 323 
2023-10-26 15:50:26.008865: Current learning rate: 0.00704 
2023-10-26 15:50:29.972107: train_loss -0.8664 
2023-10-26 15:50:29.972500: val_loss -0.8764 
2023-10-26 15:50:29.972766: Pseudo dice [0.8779, 0.916, 0.9686, 0.6971, 0.9297] 
2023-10-26 15:50:29.973001: Epoch time: 3.96 s 
2023-10-26 15:50:31.091552:  
2023-10-26 15:50:31.091847: Epoch 324 
2023-10-26 15:50:31.092104: Current learning rate: 0.00703 
2023-10-26 15:50:34.944773: train_loss -0.8706 
2023-10-26 15:50:34.945129: val_loss -0.878 
2023-10-26 15:50:34.945384: Pseudo dice [0.8791, 0.9212, 0.969, 0.788, 0.9267] 
2023-10-26 15:50:34.945602: Epoch time: 3.85 s 
2023-10-26 15:50:34.945804: Yayy! New best EMA pseudo Dice: 0.8838 
2023-10-26 15:50:36.282879:  
2023-10-26 15:50:36.283165: Epoch 325 
2023-10-26 15:50:36.283399: Current learning rate: 0.00702 
2023-10-26 15:50:39.971438: train_loss -0.8718 
2023-10-26 15:50:39.971867: val_loss -0.8865 
2023-10-26 15:50:39.972151: Pseudo dice [0.8818, 0.9127, 0.9678, 0.8047, 0.9342] 
2023-10-26 15:50:39.972531: Epoch time: 3.69 s 
2023-10-26 15:50:39.973016: Yayy! New best EMA pseudo Dice: 0.8855 
2023-10-26 15:50:41.143304:  
2023-10-26 15:50:41.143597: Epoch 326 
2023-10-26 15:50:41.143837: Current learning rate: 0.00701 
2023-10-26 15:50:45.037296: train_loss -0.8752 
2023-10-26 15:50:45.037771: val_loss -0.8863 
2023-10-26 15:50:45.038087: Pseudo dice [0.8812, 0.9214, 0.9693, 0.7674, 0.9359] 
2023-10-26 15:50:45.038375: Epoch time: 3.89 s 
2023-10-26 15:50:45.038610: Yayy! New best EMA pseudo Dice: 0.8864 
2023-10-26 15:50:46.200055:  
2023-10-26 15:50:46.200369: Epoch 327 
2023-10-26 15:50:46.200612: Current learning rate: 0.007 
2023-10-26 15:50:50.140316: train_loss -0.8692 
2023-10-26 15:50:50.140696: val_loss -0.8464 
2023-10-26 15:50:50.141041: Pseudo dice [0.8616, 0.9103, 0.9693, 0.2824, 0.93] 
2023-10-26 15:50:50.141309: Epoch time: 3.94 s 
2023-10-26 15:50:51.251223:  
2023-10-26 15:50:51.251529: Epoch 328 
2023-10-26 15:50:51.251797: Current learning rate: 0.00699 
2023-10-26 15:50:55.233395: train_loss -0.8697 
2023-10-26 15:50:55.233818: val_loss -0.8788 
2023-10-26 15:50:55.234120: Pseudo dice [0.8875, 0.9131, 0.9694, 0.7066, 0.9337] 
2023-10-26 15:50:55.234381: Epoch time: 3.98 s 
2023-10-26 15:50:56.358326:  
2023-10-26 15:50:56.358651: Epoch 329 
2023-10-26 15:50:56.358932: Current learning rate: 0.00698 
2023-10-26 15:51:00.152162: train_loss -0.8663 
2023-10-26 15:51:00.152541: val_loss -0.8767 
2023-10-26 15:51:00.152806: Pseudo dice [0.8807, 0.9257, 0.9695, 0.7677, 0.9275] 
2023-10-26 15:51:00.153039: Epoch time: 3.79 s 
2023-10-26 15:51:01.268084:  
2023-10-26 15:51:01.268386: Epoch 330 
2023-10-26 15:51:01.268631: Current learning rate: 0.00697 
2023-10-26 15:51:05.429240: train_loss -0.8695 
2023-10-26 15:51:05.429607: val_loss -0.8805 
2023-10-26 15:51:05.429857: Pseudo dice [0.8782, 0.9201, 0.9699, 0.7572, 0.9286] 
2023-10-26 15:51:05.430099: Epoch time: 4.16 s 
2023-10-26 15:51:06.502636:  
2023-10-26 15:51:06.502943: Epoch 331 
2023-10-26 15:51:06.503223: Current learning rate: 0.00696 
2023-10-26 15:51:10.534299: train_loss -0.802 
2023-10-26 15:51:10.534700: val_loss -0.8478 
2023-10-26 15:51:10.534949: Pseudo dice [0.8813, 0.9104, 0.9689, 0.0, 0.9279] 
2023-10-26 15:51:10.535176: Epoch time: 4.03 s 
2023-10-26 15:51:11.634862:  
2023-10-26 15:51:11.635180: Epoch 332 
2023-10-26 15:51:11.635432: Current learning rate: 0.00696 
2023-10-26 15:51:15.707811: train_loss -0.804 
2023-10-26 15:51:15.708230: val_loss -0.8077 
2023-10-26 15:51:15.708493: Pseudo dice [0.8829, 0.9104, 0.9665, 0.0, 0.929] 
2023-10-26 15:51:15.708724: Epoch time: 4.07 s 
2023-10-26 15:51:16.777215:  
2023-10-26 15:51:16.777517: Epoch 333 
2023-10-26 15:51:16.777760: Current learning rate: 0.00695 
2023-10-26 15:51:20.814382: train_loss -0.8337 
2023-10-26 15:51:20.814782: val_loss -0.8583 
2023-10-26 15:51:20.815065: Pseudo dice [0.8805, 0.8935, 0.9676, 0.7369, 0.9266] 
2023-10-26 15:51:20.815296: Epoch time: 4.04 s 
2023-10-26 15:51:21.887495:  
2023-10-26 15:51:21.887803: Epoch 334 
2023-10-26 15:51:21.888051: Current learning rate: 0.00694 
2023-10-26 15:51:25.827994: train_loss -0.8564 
2023-10-26 15:51:25.828410: val_loss -0.8741 
2023-10-26 15:51:25.828685: Pseudo dice [0.8702, 0.9137, 0.9665, 0.7829, 0.9287] 
2023-10-26 15:51:25.829038: Epoch time: 3.94 s 
2023-10-26 15:51:27.034065:  
2023-10-26 15:51:27.034392: Epoch 335 
2023-10-26 15:51:27.034659: Current learning rate: 0.00693 
2023-10-26 15:51:31.031140: train_loss -0.8654 
2023-10-26 15:51:31.031550: val_loss -0.8768 
2023-10-26 15:51:31.031823: Pseudo dice [0.8745, 0.9138, 0.9694, 0.6099, 0.9292] 
2023-10-26 15:51:31.032084: Epoch time: 4.0 s 
2023-10-26 15:51:32.180899:  
2023-10-26 15:51:32.181218: Epoch 336 
2023-10-26 15:51:32.181471: Current learning rate: 0.00692 
2023-10-26 15:51:36.108267: train_loss -0.8633 
2023-10-26 15:51:36.108673: val_loss -0.8728 
2023-10-26 15:51:36.108936: Pseudo dice [0.8691, 0.9062, 0.9683, 0.7793, 0.92] 
2023-10-26 15:51:36.109169: Epoch time: 3.93 s 
2023-10-26 15:51:37.439146:  
2023-10-26 15:51:37.439474: Epoch 337 
2023-10-26 15:51:37.439747: Current learning rate: 0.00691 
2023-10-26 15:51:41.462605: train_loss -0.8679 
2023-10-26 15:51:41.463023: val_loss -0.8821 
2023-10-26 15:51:41.463284: Pseudo dice [0.8829, 0.9175, 0.968, 0.7291, 0.9384] 
2023-10-26 15:51:41.463545: Epoch time: 4.02 s 
2023-10-26 15:51:42.615940:  
2023-10-26 15:51:42.616235: Epoch 338 
2023-10-26 15:51:42.616481: Current learning rate: 0.0069 
2023-10-26 15:51:46.676902: train_loss -0.8561 
2023-10-26 15:51:46.677332: val_loss -0.8764 
2023-10-26 15:51:46.677615: Pseudo dice [0.8796, 0.9178, 0.9671, 0.7016, 0.9259] 
2023-10-26 15:51:46.677861: Epoch time: 4.06 s 
2023-10-26 15:51:47.800252:  
2023-10-26 15:51:47.800565: Epoch 339 
2023-10-26 15:51:47.800816: Current learning rate: 0.00689 
2023-10-26 15:51:51.690799: train_loss -0.862 
2023-10-26 15:51:51.691271: val_loss -0.8771 
2023-10-26 15:51:51.691697: Pseudo dice [0.8744, 0.9113, 0.9672, 0.7018, 0.9232] 
2023-10-26 15:51:51.691979: Epoch time: 3.89 s 
2023-10-26 15:51:52.804835:  
2023-10-26 15:51:52.805151: Epoch 340 
2023-10-26 15:51:52.805384: Current learning rate: 0.00688 
2023-10-26 15:51:56.817996: train_loss -0.8613 
2023-10-26 15:51:56.818462: val_loss -0.8738 
2023-10-26 15:51:56.818762: Pseudo dice [0.8795, 0.9155, 0.9628, 0.7509, 0.9279] 
2023-10-26 15:51:56.819028: Epoch time: 4.01 s 
2023-10-26 15:51:57.931933:  
2023-10-26 15:51:57.932252: Epoch 341 
2023-10-26 15:51:57.932496: Current learning rate: 0.00687 
2023-10-26 15:52:01.819051: train_loss -0.8651 
2023-10-26 15:52:01.819425: val_loss -0.8824 
2023-10-26 15:52:01.819717: Pseudo dice [0.8847, 0.9067, 0.969, 0.6312, 0.9422] 
2023-10-26 15:52:01.819950: Epoch time: 3.89 s 
2023-10-26 15:52:02.926384:  
2023-10-26 15:52:02.926686: Epoch 342 
2023-10-26 15:52:02.926933: Current learning rate: 0.00686 
2023-10-26 15:52:06.795917: train_loss -0.8687 
2023-10-26 15:52:06.796327: val_loss -0.8786 
2023-10-26 15:52:06.796576: Pseudo dice [0.8885, 0.9208, 0.9678, 0.615, 0.9365] 
2023-10-26 15:52:06.796801: Epoch time: 3.87 s 
2023-10-26 15:52:08.077280:  
2023-10-26 15:52:08.077603: Epoch 343 
2023-10-26 15:52:08.077843: Current learning rate: 0.00685 
2023-10-26 15:52:12.021912: train_loss -0.8591 
2023-10-26 15:52:12.022511: val_loss -0.8801 
2023-10-26 15:52:12.022766: Pseudo dice [0.8826, 0.9221, 0.9675, 0.7051, 0.9297] 
2023-10-26 15:52:12.022990: Epoch time: 3.95 s 
2023-10-26 15:52:13.148643:  
2023-10-26 15:52:13.148967: Epoch 344 
2023-10-26 15:52:13.149217: Current learning rate: 0.00684 
2023-10-26 15:52:17.128878: train_loss -0.8622 
2023-10-26 15:52:17.129258: val_loss -0.872 
2023-10-26 15:52:17.129561: Pseudo dice [0.878, 0.9222, 0.9691, 0.7577, 0.9139] 
2023-10-26 15:52:17.129789: Epoch time: 3.98 s 
2023-10-26 15:52:18.256619:  
2023-10-26 15:52:18.256914: Epoch 345 
2023-10-26 15:52:18.257160: Current learning rate: 0.00683 
2023-10-26 15:52:22.168430: train_loss -0.8609 
2023-10-26 15:52:22.168854: val_loss -0.872 
2023-10-26 15:52:22.169259: Pseudo dice [0.8756, 0.9208, 0.967, 0.7179, 0.9243] 
2023-10-26 15:52:22.169549: Epoch time: 3.91 s 
2023-10-26 15:52:23.274844:  
2023-10-26 15:52:23.275151: Epoch 346 
2023-10-26 15:52:23.275402: Current learning rate: 0.00682 
2023-10-26 15:52:27.108166: train_loss -0.8728 
2023-10-26 15:52:27.108550: val_loss -0.8756 
2023-10-26 15:52:27.108808: Pseudo dice [0.8817, 0.9125, 0.9683, 0.7062, 0.9378] 
2023-10-26 15:52:27.109039: Epoch time: 3.83 s 
2023-10-26 15:52:28.198557:  
2023-10-26 15:52:28.198862: Epoch 347 
2023-10-26 15:52:28.199112: Current learning rate: 0.00681 
2023-10-26 15:52:32.009207: train_loss -0.8673 
2023-10-26 15:52:32.009581: val_loss -0.878 
2023-10-26 15:52:32.009832: Pseudo dice [0.882, 0.9099, 0.9679, 0.76, 0.9186] 
2023-10-26 15:52:32.010054: Epoch time: 3.81 s 
2023-10-26 15:52:33.103569:  
2023-10-26 15:52:33.103868: Epoch 348 
2023-10-26 15:52:33.104115: Current learning rate: 0.0068 
2023-10-26 15:52:37.279047: train_loss -0.8633 
2023-10-26 15:52:37.279433: val_loss -0.8872 
2023-10-26 15:52:37.279686: Pseudo dice [0.887, 0.9062, 0.9704, 0.8381, 0.9349] 
2023-10-26 15:52:37.279917: Epoch time: 4.18 s 
2023-10-26 15:52:38.563747:  
2023-10-26 15:52:38.564050: Epoch 349 
2023-10-26 15:52:38.564283: Current learning rate: 0.0068 
2023-10-26 15:52:42.565618: train_loss -0.8572 
2023-10-26 15:52:42.566022: val_loss -0.8744 
2023-10-26 15:52:42.566277: Pseudo dice [0.8773, 0.9036, 0.9679, 0.7138, 0.9329] 
2023-10-26 15:52:42.566510: Epoch time: 4.0 s 
2023-10-26 15:52:43.757204:  
2023-10-26 15:52:43.757502: Epoch 350 
2023-10-26 15:52:43.757741: Current learning rate: 0.00679 
2023-10-26 15:52:47.818451: train_loss -0.8645 
2023-10-26 15:52:47.818826: val_loss -0.8795 
2023-10-26 15:52:47.819117: Pseudo dice [0.8855, 0.9214, 0.9688, 0.7453, 0.9333] 
2023-10-26 15:52:47.819349: Epoch time: 4.06 s 
2023-10-26 15:52:48.904258:  
2023-10-26 15:52:48.904548: Epoch 351 
2023-10-26 15:52:48.904795: Current learning rate: 0.00678 
2023-10-26 15:52:52.907979: train_loss -0.8667 
2023-10-26 15:52:52.908364: val_loss -0.8723 
2023-10-26 15:52:52.908624: Pseudo dice [0.8758, 0.9104, 0.9687, 0.6343, 0.9289] 
2023-10-26 15:52:52.908846: Epoch time: 4.0 s 
2023-10-26 15:52:54.000810:  
2023-10-26 15:52:54.001100: Epoch 352 
2023-10-26 15:52:54.001339: Current learning rate: 0.00677 
2023-10-26 15:52:57.944516: train_loss -0.8566 
2023-10-26 15:52:57.944916: val_loss -0.8766 
2023-10-26 15:52:57.945180: Pseudo dice [0.8836, 0.9201, 0.9673, 0.7088, 0.9294] 
2023-10-26 15:52:57.945425: Epoch time: 3.94 s 
2023-10-26 15:52:59.102271:  
2023-10-26 15:52:59.102591: Epoch 353 
2023-10-26 15:52:59.102846: Current learning rate: 0.00676 
2023-10-26 15:53:03.187526: train_loss -0.862 
2023-10-26 15:53:03.187878: val_loss -0.8757 
2023-10-26 15:53:03.188129: Pseudo dice [0.8798, 0.9074, 0.9669, 0.6424, 0.9377] 
2023-10-26 15:53:03.188346: Epoch time: 4.09 s 
2023-10-26 15:53:04.279860:  
2023-10-26 15:53:04.280191: Epoch 354 
2023-10-26 15:53:04.280427: Current learning rate: 0.00675 
2023-10-26 15:53:08.268034: train_loss -0.8658 
2023-10-26 15:53:08.268418: val_loss -0.8837 
2023-10-26 15:53:08.268671: Pseudo dice [0.8814, 0.9135, 0.9701, 0.7347, 0.927] 
2023-10-26 15:53:08.268914: Epoch time: 3.99 s 
2023-10-26 15:53:09.371319:  
2023-10-26 15:53:09.371598: Epoch 355 
2023-10-26 15:53:09.371826: Current learning rate: 0.00674 
2023-10-26 15:53:13.588240: train_loss -0.8685 
2023-10-26 15:53:13.588621: val_loss -0.8513 
2023-10-26 15:53:13.588912: Pseudo dice [0.8857, 0.9137, 0.9691, 0.4242, 0.9434] 
2023-10-26 15:53:13.589139: Epoch time: 4.22 s 
2023-10-26 15:53:14.703861:  
2023-10-26 15:53:14.704163: Epoch 356 
2023-10-26 15:53:14.704404: Current learning rate: 0.00673 
2023-10-26 15:53:18.521468: train_loss -0.8623 
2023-10-26 15:53:18.521893: val_loss -0.8743 
2023-10-26 15:53:18.522153: Pseudo dice [0.8876, 0.9122, 0.9679, 0.7332, 0.9249] 
2023-10-26 15:53:18.522384: Epoch time: 3.82 s 
2023-10-26 15:53:19.623254:  
2023-10-26 15:53:19.623543: Epoch 357 
2023-10-26 15:53:19.623786: Current learning rate: 0.00672 
2023-10-26 15:53:23.485273: train_loss -0.8561 
2023-10-26 15:53:23.485631: val_loss -0.8714 
2023-10-26 15:53:23.485896: Pseudo dice [0.876, 0.9141, 0.9679, 0.7106, 0.927] 
2023-10-26 15:53:23.486179: Epoch time: 3.86 s 
2023-10-26 15:53:24.592853:  
2023-10-26 15:53:24.593163: Epoch 358 
2023-10-26 15:53:24.593410: Current learning rate: 0.00671 
2023-10-26 15:53:28.516052: train_loss -0.8638 
2023-10-26 15:53:28.516451: val_loss -0.8748 
2023-10-26 15:53:28.516713: Pseudo dice [0.8829, 0.9181, 0.9686, 0.6681, 0.9236] 
2023-10-26 15:53:28.516966: Epoch time: 3.92 s 
2023-10-26 15:53:29.620052:  
2023-10-26 15:53:29.620356: Epoch 359 
2023-10-26 15:53:29.620599: Current learning rate: 0.0067 
2023-10-26 15:53:33.531816: train_loss -0.8726 
2023-10-26 15:53:33.532184: val_loss -0.8773 
2023-10-26 15:53:33.532437: Pseudo dice [0.8903, 0.9191, 0.9691, 0.6877, 0.9272] 
2023-10-26 15:53:33.532662: Epoch time: 3.91 s 
2023-10-26 15:53:34.631538:  
2023-10-26 15:53:34.631839: Epoch 360 
2023-10-26 15:53:34.632077: Current learning rate: 0.00669 
2023-10-26 15:53:38.354761: train_loss -0.8705 
2023-10-26 15:53:38.355166: val_loss -0.8771 
2023-10-26 15:53:38.355425: Pseudo dice [0.8793, 0.916, 0.9664, 0.729, 0.9306] 
2023-10-26 15:53:38.355658: Epoch time: 3.72 s 
2023-10-26 15:53:39.466514:  
2023-10-26 15:53:39.466830: Epoch 361 
2023-10-26 15:53:39.467108: Current learning rate: 0.00668 
2023-10-26 15:53:43.348286: train_loss -0.8705 
2023-10-26 15:53:43.348708: val_loss -0.8755 
2023-10-26 15:53:43.349087: Pseudo dice [0.8822, 0.9154, 0.9692, 0.7611, 0.9282] 
2023-10-26 15:53:43.349358: Epoch time: 3.88 s 
2023-10-26 15:53:44.633916:  
2023-10-26 15:53:44.634213: Epoch 362 
2023-10-26 15:53:44.634458: Current learning rate: 0.00667 
2023-10-26 15:53:48.474879: train_loss -0.8678 
2023-10-26 15:53:48.475350: val_loss -0.8789 
2023-10-26 15:53:48.475657: Pseudo dice [0.8822, 0.9127, 0.9694, 0.7188, 0.9311] 
2023-10-26 15:53:48.475924: Epoch time: 3.84 s 
2023-10-26 15:53:49.640598:  
2023-10-26 15:53:49.640904: Epoch 363 
2023-10-26 15:53:49.641160: Current learning rate: 0.00666 
2023-10-26 15:53:53.622948: train_loss -0.873 
2023-10-26 15:53:53.623338: val_loss -0.8728 
2023-10-26 15:53:53.623756: Pseudo dice [0.8776, 0.9157, 0.9683, 0.6832, 0.9211] 
2023-10-26 15:53:53.624043: Epoch time: 3.98 s 
2023-10-26 15:53:54.761625:  
2023-10-26 15:53:54.761947: Epoch 364 
2023-10-26 15:53:54.762209: Current learning rate: 0.00665 
2023-10-26 15:53:58.906396: train_loss -0.8716 
2023-10-26 15:53:58.906790: val_loss -0.8799 
2023-10-26 15:53:58.907051: Pseudo dice [0.8851, 0.9184, 0.9701, 0.7889, 0.9269] 
2023-10-26 15:53:58.907279: Epoch time: 4.15 s 
2023-10-26 15:54:00.068581:  
2023-10-26 15:54:00.068920: Epoch 365 
2023-10-26 15:54:00.069203: Current learning rate: 0.00665 
2023-10-26 15:54:04.210353: train_loss -0.8779 
2023-10-26 15:54:04.210835: val_loss -0.8776 
2023-10-26 15:54:04.211254: Pseudo dice [0.8798, 0.9211, 0.9714, 0.7256, 0.9209] 
2023-10-26 15:54:04.211559: Epoch time: 4.14 s 
2023-10-26 15:54:05.319722:  
2023-10-26 15:54:05.320034: Epoch 366 
2023-10-26 15:54:05.320271: Current learning rate: 0.00664 
2023-10-26 15:54:09.322808: train_loss -0.867 
2023-10-26 15:54:09.323199: val_loss -0.875 
2023-10-26 15:54:09.323452: Pseudo dice [0.8751, 0.9164, 0.968, 0.8021, 0.9222] 
2023-10-26 15:54:09.323687: Epoch time: 4.0 s 
2023-10-26 15:54:10.432791:  
2023-10-26 15:54:10.433100: Epoch 367 
2023-10-26 15:54:10.433363: Current learning rate: 0.00663 
2023-10-26 15:54:14.406931: train_loss -0.8697 
2023-10-26 15:54:14.407284: val_loss -0.8766 
2023-10-26 15:54:14.407556: Pseudo dice [0.8803, 0.9088, 0.9687, 0.7214, 0.92] 
2023-10-26 15:54:14.407791: Epoch time: 3.97 s 
2023-10-26 15:54:15.663136:  
2023-10-26 15:54:15.663430: Epoch 368 
2023-10-26 15:54:15.663669: Current learning rate: 0.00662 
2023-10-26 15:54:19.667840: train_loss -0.8516 
2023-10-26 15:54:19.668275: val_loss -0.8694 
2023-10-26 15:54:19.668586: Pseudo dice [0.875, 0.9106, 0.9703, 0.6542, 0.9271] 
2023-10-26 15:54:19.668843: Epoch time: 4.01 s 
2023-10-26 15:54:20.764907:  
2023-10-26 15:54:20.765207: Epoch 369 
2023-10-26 15:54:20.765450: Current learning rate: 0.00661 
2023-10-26 15:54:24.760131: train_loss -0.8632 
2023-10-26 15:54:24.760514: val_loss -0.8775 
2023-10-26 15:54:24.760797: Pseudo dice [0.8754, 0.915, 0.9692, 0.7372, 0.9338] 
2023-10-26 15:54:24.761038: Epoch time: 4.0 s 
2023-10-26 15:54:25.859499:  
2023-10-26 15:54:25.859799: Epoch 370 
2023-10-26 15:54:25.860061: Current learning rate: 0.0066 
2023-10-26 15:54:29.768272: train_loss -0.8655 
2023-10-26 15:54:29.768680: val_loss -0.8844 
2023-10-26 15:54:29.768950: Pseudo dice [0.8847, 0.9268, 0.9693, 0.6956, 0.9272] 
2023-10-26 15:54:29.769194: Epoch time: 3.91 s 
2023-10-26 15:54:30.874195:  
2023-10-26 15:54:30.874503: Epoch 371 
2023-10-26 15:54:30.874749: Current learning rate: 0.00659 
2023-10-26 15:54:34.862795: train_loss -0.8698 
2023-10-26 15:54:34.863174: val_loss -0.8633 
2023-10-26 15:54:34.863436: Pseudo dice [0.8727, 0.9075, 0.9683, 0.5842, 0.9298] 
2023-10-26 15:54:34.863664: Epoch time: 3.99 s 
2023-10-26 15:54:36.001213:  
2023-10-26 15:54:36.001503: Epoch 372 
2023-10-26 15:54:36.001741: Current learning rate: 0.00658 
2023-10-26 15:54:39.891635: train_loss -0.8574 
2023-10-26 15:54:39.892066: val_loss -0.8714 
2023-10-26 15:54:39.892336: Pseudo dice [0.8626, 0.9107, 0.9681, 0.6713, 0.9329] 
2023-10-26 15:54:39.892588: Epoch time: 3.89 s 
2023-10-26 15:54:41.058075:  
2023-10-26 15:54:41.058385: Epoch 373 
2023-10-26 15:54:41.058635: Current learning rate: 0.00657 
2023-10-26 15:54:45.035901: train_loss -0.8648 
2023-10-26 15:54:45.036268: val_loss -0.871 
2023-10-26 15:54:45.036527: Pseudo dice [0.8757, 0.9149, 0.9646, 0.7465, 0.904] 
2023-10-26 15:54:45.036743: Epoch time: 3.98 s 
2023-10-26 15:54:46.331569:  
2023-10-26 15:54:46.331853: Epoch 374 
2023-10-26 15:54:46.332100: Current learning rate: 0.00656 
2023-10-26 15:54:50.222997: train_loss -0.8554 
2023-10-26 15:54:50.223415: val_loss -0.873 
2023-10-26 15:54:50.223666: Pseudo dice [0.8816, 0.9126, 0.9694, 0.792, 0.924] 
2023-10-26 15:54:50.223892: Epoch time: 3.89 s 
2023-10-26 15:54:51.342278:  
2023-10-26 15:54:51.342584: Epoch 375 
2023-10-26 15:54:51.342835: Current learning rate: 0.00655 
2023-10-26 15:54:55.192556: train_loss -0.8614 
2023-10-26 15:54:55.192929: val_loss -0.8749 
2023-10-26 15:54:55.193197: Pseudo dice [0.8716, 0.9185, 0.9687, 0.7474, 0.9403] 
2023-10-26 15:54:55.193510: Epoch time: 3.85 s 
2023-10-26 15:54:56.303017:  
2023-10-26 15:54:56.303313: Epoch 376 
2023-10-26 15:54:56.303562: Current learning rate: 0.00654 
2023-10-26 15:55:00.136150: train_loss -0.8717 
2023-10-26 15:55:00.136633: val_loss -0.8703 
2023-10-26 15:55:00.137171: Pseudo dice [0.8851, 0.9119, 0.9682, 0.6142, 0.9267] 
2023-10-26 15:55:00.137498: Epoch time: 3.83 s 
2023-10-26 15:55:01.249364:  
2023-10-26 15:55:01.249687: Epoch 377 
2023-10-26 15:55:01.249943: Current learning rate: 0.00653 
2023-10-26 15:55:05.151396: train_loss -0.8558 
2023-10-26 15:55:05.151766: val_loss -0.8837 
2023-10-26 15:55:05.152018: Pseudo dice [0.8854, 0.9154, 0.9709, 0.7882, 0.924] 
2023-10-26 15:55:05.152244: Epoch time: 3.9 s 
2023-10-26 15:55:06.256477:  
2023-10-26 15:55:06.256778: Epoch 378 
2023-10-26 15:55:06.257040: Current learning rate: 0.00652 
2023-10-26 15:55:10.278824: train_loss -0.8652 
2023-10-26 15:55:10.279287: val_loss -0.8739 
2023-10-26 15:55:10.279582: Pseudo dice [0.8758, 0.9113, 0.9663, 0.7838, 0.9281] 
2023-10-26 15:55:10.279829: Epoch time: 4.02 s 
2023-10-26 15:55:11.406383:  
2023-10-26 15:55:11.406716: Epoch 379 
2023-10-26 15:55:11.406984: Current learning rate: 0.00651 
2023-10-26 15:55:15.347316: train_loss -0.8584 
2023-10-26 15:55:15.347707: val_loss -0.8738 
2023-10-26 15:55:15.347977: Pseudo dice [0.8796, 0.9091, 0.9683, 0.7605, 0.9289] 
2023-10-26 15:55:15.348212: Epoch time: 3.94 s 
2023-10-26 15:55:16.611455:  
2023-10-26 15:55:16.611917: Epoch 380 
2023-10-26 15:55:16.612255: Current learning rate: 0.0065 
2023-10-26 15:55:20.589841: train_loss -0.8594 
2023-10-26 15:55:20.590255: val_loss -0.8816 
2023-10-26 15:55:20.590519: Pseudo dice [0.8818, 0.9181, 0.9663, 0.8107, 0.9132] 
2023-10-26 15:55:20.590750: Epoch time: 3.98 s 
2023-10-26 15:55:21.742618:  
2023-10-26 15:55:21.742947: Epoch 381 
2023-10-26 15:55:21.743242: Current learning rate: 0.00649 
2023-10-26 15:55:25.754281: train_loss -0.8599 
2023-10-26 15:55:25.754647: val_loss -0.8699 
2023-10-26 15:55:25.754924: Pseudo dice [0.8815, 0.9136, 0.9664, 0.7706, 0.9283] 
2023-10-26 15:55:25.755150: Epoch time: 4.01 s 
2023-10-26 15:55:26.871223:  
2023-10-26 15:55:26.871525: Epoch 382 
2023-10-26 15:55:26.871760: Current learning rate: 0.00648 
2023-10-26 15:55:30.965413: train_loss -0.862 
2023-10-26 15:55:30.965825: val_loss -0.8703 
2023-10-26 15:55:30.966215: Pseudo dice [0.8777, 0.9154, 0.9681, 0.7169, 0.9105] 
2023-10-26 15:55:30.966480: Epoch time: 4.09 s 
2023-10-26 15:55:32.072781:  
2023-10-26 15:55:32.073071: Epoch 383 
2023-10-26 15:55:32.073304: Current learning rate: 0.00648 
2023-10-26 15:55:36.052033: train_loss -0.8584 
2023-10-26 15:55:36.052407: val_loss -0.8795 
2023-10-26 15:55:36.052662: Pseudo dice [0.8835, 0.9125, 0.969, 0.7032, 0.9331] 
2023-10-26 15:55:36.052900: Epoch time: 3.98 s 
2023-10-26 15:55:37.155800:  
2023-10-26 15:55:37.156079: Epoch 384 
2023-10-26 15:55:37.156318: Current learning rate: 0.00647 
2023-10-26 15:55:41.241439: train_loss -0.8672 
2023-10-26 15:55:41.242039: val_loss -0.8722 
2023-10-26 15:55:41.242414: Pseudo dice [0.8739, 0.9079, 0.9672, 0.77, 0.9132] 
2023-10-26 15:55:41.242685: Epoch time: 4.09 s 
2023-10-26 15:55:42.339988:  
2023-10-26 15:55:42.340333: Epoch 385 
2023-10-26 15:55:42.340647: Current learning rate: 0.00646 
2023-10-26 15:55:46.301705: train_loss -0.8678 
2023-10-26 15:55:46.302203: val_loss -0.8686 
2023-10-26 15:55:46.302754: Pseudo dice [0.8724, 0.9173, 0.9681, 0.701, 0.9262] 
2023-10-26 15:55:46.303131: Epoch time: 3.96 s 
2023-10-26 15:55:47.573955:  
2023-10-26 15:55:47.574257: Epoch 386 
2023-10-26 15:55:47.574506: Current learning rate: 0.00645 
2023-10-26 15:55:51.770526: train_loss -0.8684 
2023-10-26 15:55:51.770903: val_loss -0.878 
2023-10-26 15:55:51.771150: Pseudo dice [0.8817, 0.9064, 0.967, 0.7629, 0.9316] 
2023-10-26 15:55:51.771370: Epoch time: 4.2 s 
2023-10-26 15:55:52.881951:  
2023-10-26 15:55:52.882242: Epoch 387 
2023-10-26 15:55:52.882471: Current learning rate: 0.00644 
2023-10-26 15:55:56.770371: train_loss -0.8703 
2023-10-26 15:55:56.770766: val_loss -0.8777 
2023-10-26 15:55:56.771333: Pseudo dice [0.8851, 0.9146, 0.9692, 0.774, 0.9234] 
2023-10-26 15:55:56.771611: Epoch time: 3.89 s 
2023-10-26 15:55:57.874972:  
2023-10-26 15:55:57.875264: Epoch 388 
2023-10-26 15:55:57.875503: Current learning rate: 0.00643 
2023-10-26 15:56:01.926246: train_loss -0.8697 
2023-10-26 15:56:01.926639: val_loss -0.8692 
2023-10-26 15:56:01.926903: Pseudo dice [0.8872, 0.8906, 0.9646, 0.7807, 0.9361] 
2023-10-26 15:56:01.927131: Epoch time: 4.05 s 
2023-10-26 15:56:03.030811:  
2023-10-26 15:56:03.031098: Epoch 389 
2023-10-26 15:56:03.031329: Current learning rate: 0.00642 
2023-10-26 15:56:07.034028: train_loss -0.8607 
2023-10-26 15:56:07.034407: val_loss -0.8697 
2023-10-26 15:56:07.034654: Pseudo dice [0.8683, 0.9079, 0.9675, 0.6154, 0.9319] 
2023-10-26 15:56:07.034886: Epoch time: 4.0 s 
2023-10-26 15:56:08.146162:  
2023-10-26 15:56:08.146442: Epoch 390 
2023-10-26 15:56:08.146667: Current learning rate: 0.00641 
2023-10-26 15:56:12.176275: train_loss -0.8663 
2023-10-26 15:56:12.176643: val_loss -0.8785 
2023-10-26 15:56:12.176959: Pseudo dice [0.8751, 0.9098, 0.9705, 0.7821, 0.9383] 
2023-10-26 15:56:12.177202: Epoch time: 4.03 s 
2023-10-26 15:56:13.324142:  
2023-10-26 15:56:13.324421: Epoch 391 
2023-10-26 15:56:13.324648: Current learning rate: 0.0064 
2023-10-26 15:56:17.279295: train_loss -0.8658 
2023-10-26 15:56:17.279719: val_loss -0.8717 
2023-10-26 15:56:17.280020: Pseudo dice [0.8748, 0.9111, 0.9675, 0.7128, 0.9266] 
2023-10-26 15:56:17.280270: Epoch time: 3.96 s 
2023-10-26 15:56:18.627750:  
2023-10-26 15:56:18.628050: Epoch 392 
2023-10-26 15:56:18.628299: Current learning rate: 0.00639 
2023-10-26 15:56:22.621718: train_loss -0.8694 
2023-10-26 15:56:22.622343: val_loss -0.8829 
2023-10-26 15:56:22.622655: Pseudo dice [0.8831, 0.9179, 0.9696, 0.7752, 0.9331] 
2023-10-26 15:56:22.622903: Epoch time: 3.99 s 
2023-10-26 15:56:23.817155:  
2023-10-26 15:56:23.817448: Epoch 393 
2023-10-26 15:56:23.817689: Current learning rate: 0.00638 
2023-10-26 15:56:27.684506: train_loss -0.8781 
2023-10-26 15:56:27.684921: val_loss -0.8847 
2023-10-26 15:56:27.685639: Pseudo dice [0.8829, 0.9197, 0.97, 0.7465, 0.9287] 
2023-10-26 15:56:27.685893: Epoch time: 3.87 s 
2023-10-26 15:56:28.805416:  
2023-10-26 15:56:28.805717: Epoch 394 
2023-10-26 15:56:28.805994: Current learning rate: 0.00637 
2023-10-26 15:56:32.722656: train_loss -0.8695 
2023-10-26 15:56:32.723059: val_loss -0.8781 
2023-10-26 15:56:32.723318: Pseudo dice [0.8798, 0.9161, 0.9687, 0.7774, 0.9242] 
2023-10-26 15:56:32.723550: Epoch time: 3.92 s 
2023-10-26 15:56:33.838930:  
2023-10-26 15:56:33.839231: Epoch 395 
2023-10-26 15:56:33.839470: Current learning rate: 0.00636 
2023-10-26 15:56:37.886719: train_loss -0.8654 
2023-10-26 15:56:37.887155: val_loss -0.8749 
2023-10-26 15:56:37.887427: Pseudo dice [0.8815, 0.9124, 0.9663, 0.7721, 0.9322] 
2023-10-26 15:56:37.887689: Epoch time: 4.05 s 
2023-10-26 15:56:37.887914: Yayy! New best EMA pseudo Dice: 0.8867 
2023-10-26 15:56:39.100026:  
2023-10-26 15:56:39.100332: Epoch 396 
2023-10-26 15:56:39.100583: Current learning rate: 0.00635 
2023-10-26 15:56:42.971967: train_loss -0.8603 
2023-10-26 15:56:42.972325: val_loss -0.8762 
2023-10-26 15:56:42.972589: Pseudo dice [0.8786, 0.9192, 0.9674, 0.8136, 0.9272] 
2023-10-26 15:56:42.972808: Epoch time: 3.87 s 
2023-10-26 15:56:42.973104: Yayy! New best EMA pseudo Dice: 0.8881 
2023-10-26 15:56:44.202063:  
2023-10-26 15:56:44.202369: Epoch 397 
2023-10-26 15:56:44.202613: Current learning rate: 0.00634 
2023-10-26 15:56:48.151003: train_loss -0.8674 
2023-10-26 15:56:48.151398: val_loss -0.8806 
2023-10-26 15:56:48.151650: Pseudo dice [0.883, 0.9125, 0.9691, 0.6985, 0.937] 
2023-10-26 15:56:48.151895: Epoch time: 3.95 s 
2023-10-26 15:56:49.513028:  
2023-10-26 15:56:49.513333: Epoch 398 
2023-10-26 15:56:49.513568: Current learning rate: 0.00633 
2023-10-26 15:56:53.447845: train_loss -0.8631 
2023-10-26 15:56:53.448238: val_loss -0.8595 
2023-10-26 15:56:53.448503: Pseudo dice [0.8706, 0.8995, 0.9677, 0.5361, 0.9277] 
2023-10-26 15:56:53.448736: Epoch time: 3.94 s 
2023-10-26 15:56:54.594975:  
2023-10-26 15:56:54.595263: Epoch 399 
2023-10-26 15:56:54.595506: Current learning rate: 0.00632 
2023-10-26 15:56:58.544563: train_loss -0.8539 
2023-10-26 15:56:58.545154: val_loss -0.8749 
2023-10-26 15:56:58.545483: Pseudo dice [0.8766, 0.918, 0.9687, 0.7302, 0.9269] 
2023-10-26 15:56:58.545914: Epoch time: 3.95 s 
2023-10-26 15:56:59.777024:  
2023-10-26 15:56:59.777328: Epoch 400 
2023-10-26 15:56:59.777591: Current learning rate: 0.00631 
2023-10-26 15:57:03.636901: train_loss -0.8677 
2023-10-26 15:57:03.637259: val_loss -0.8693 
2023-10-26 15:57:03.637520: Pseudo dice [0.8766, 0.9125, 0.9688, 0.75, 0.9166] 
2023-10-26 15:57:03.637748: Epoch time: 3.86 s 
2023-10-26 15:57:04.771280:  
2023-10-26 15:57:04.771582: Epoch 401 
2023-10-26 15:57:04.771827: Current learning rate: 0.0063 
2023-10-26 15:57:08.652618: train_loss -0.8654 
2023-10-26 15:57:08.653009: val_loss -0.8708 
2023-10-26 15:57:08.653346: Pseudo dice [0.8862, 0.9056, 0.9682, 0.7418, 0.9383] 
2023-10-26 15:57:08.653772: Epoch time: 3.88 s 
2023-10-26 15:57:09.757645:  
2023-10-26 15:57:09.757957: Epoch 402 
2023-10-26 15:57:09.758209: Current learning rate: 0.0063 
2023-10-26 15:57:13.858645: train_loss -0.8586 
2023-10-26 15:57:13.859019: val_loss -0.8642 
2023-10-26 15:57:13.859286: Pseudo dice [0.8717, 0.9021, 0.9636, 0.6137, 0.9314] 
2023-10-26 15:57:13.859525: Epoch time: 4.1 s 
2023-10-26 15:57:15.054708:  
2023-10-26 15:57:15.055018: Epoch 403 
2023-10-26 15:57:15.055258: Current learning rate: 0.00629 
2023-10-26 15:57:19.049775: train_loss -0.861 
2023-10-26 15:57:19.050162: val_loss -0.8695 
2023-10-26 15:57:19.050429: Pseudo dice [0.8474, 0.9123, 0.9671, 0.7213, 0.9316] 
2023-10-26 15:57:19.050680: Epoch time: 4.0 s 
2023-10-26 15:57:20.296894:  
2023-10-26 15:57:20.297195: Epoch 404 
2023-10-26 15:57:20.297441: Current learning rate: 0.00628 
2023-10-26 15:57:24.374774: train_loss -0.8681 
2023-10-26 15:57:24.375284: val_loss -0.8843 
2023-10-26 15:57:24.375628: Pseudo dice [0.8872, 0.9176, 0.9697, 0.8132, 0.9404] 
2023-10-26 15:57:24.375904: Epoch time: 4.08 s 
2023-10-26 15:57:25.484358:  
2023-10-26 15:57:25.484708: Epoch 405 
2023-10-26 15:57:25.485018: Current learning rate: 0.00627 
2023-10-26 15:57:29.534479: train_loss -0.8735 
2023-10-26 15:57:29.534842: val_loss -0.8737 
2023-10-26 15:57:29.535105: Pseudo dice [0.866, 0.9041, 0.9691, 0.6826, 0.9326] 
2023-10-26 15:57:29.535336: Epoch time: 4.05 s 
2023-10-26 15:57:30.633608:  
2023-10-26 15:57:30.633975: Epoch 406 
2023-10-26 15:57:30.634285: Current learning rate: 0.00626 
2023-10-26 15:57:34.737138: train_loss -0.8645 
2023-10-26 15:57:34.737597: val_loss -0.8789 
2023-10-26 15:57:34.737881: Pseudo dice [0.8868, 0.9144, 0.9691, 0.7504, 0.9356] 
2023-10-26 15:57:34.738174: Epoch time: 4.1 s 
2023-10-26 15:57:35.848155:  
2023-10-26 15:57:35.848465: Epoch 407 
2023-10-26 15:57:35.848717: Current learning rate: 0.00625 
2023-10-26 15:57:39.854645: train_loss -0.8736 
2023-10-26 15:57:39.855093: val_loss -0.882 
2023-10-26 15:57:39.855361: Pseudo dice [0.8804, 0.9197, 0.9701, 0.7673, 0.939] 
2023-10-26 15:57:39.855635: Epoch time: 4.01 s 
2023-10-26 15:57:40.978243:  
2023-10-26 15:57:40.978545: Epoch 408 
2023-10-26 15:57:40.978791: Current learning rate: 0.00624 
2023-10-26 15:57:44.908479: train_loss -0.8612 
2023-10-26 15:57:44.908835: val_loss -0.8727 
2023-10-26 15:57:44.909098: Pseudo dice [0.8797, 0.9064, 0.9696, 0.731, 0.9249] 
2023-10-26 15:57:44.909325: Epoch time: 3.93 s 
2023-10-26 15:57:46.030890:  
2023-10-26 15:57:46.031195: Epoch 409 
2023-10-26 15:57:46.031432: Current learning rate: 0.00623 
2023-10-26 15:57:49.883031: train_loss -0.8677 
2023-10-26 15:57:49.883501: val_loss -0.8745 
2023-10-26 15:57:49.884036: Pseudo dice [0.8799, 0.9157, 0.9649, 0.7958, 0.9258] 
2023-10-26 15:57:49.884664: Epoch time: 3.85 s 
2023-10-26 15:57:51.189910:  
2023-10-26 15:57:51.190203: Epoch 410 
2023-10-26 15:57:51.190440: Current learning rate: 0.00622 
2023-10-26 15:57:55.131256: train_loss -0.862 
2023-10-26 15:57:55.131661: val_loss -0.8751 
2023-10-26 15:57:55.131929: Pseudo dice [0.8783, 0.9137, 0.9695, 0.6998, 0.9412] 
2023-10-26 15:57:55.132159: Epoch time: 3.94 s 
2023-10-26 15:57:56.199161:  
2023-10-26 15:57:56.199454: Epoch 411 
2023-10-26 15:57:56.199689: Current learning rate: 0.00621 
2023-10-26 15:57:59.958248: train_loss -0.8652 
2023-10-26 15:57:59.958626: val_loss -0.8845 
2023-10-26 15:57:59.958891: Pseudo dice [0.8772, 0.9231, 0.9697, 0.7858, 0.9279] 
2023-10-26 15:57:59.959119: Epoch time: 3.76 s 
2023-10-26 15:58:01.045962:  
2023-10-26 15:58:01.046268: Epoch 412 
2023-10-26 15:58:01.046516: Current learning rate: 0.0062 
2023-10-26 15:58:04.901038: train_loss -0.8784 
2023-10-26 15:58:04.901447: val_loss -0.8807 
2023-10-26 15:58:04.901717: Pseudo dice [0.8846, 0.9165, 0.9665, 0.7832, 0.9421] 
2023-10-26 15:58:04.901972: Epoch time: 3.86 s 
2023-10-26 15:58:05.987519:  
2023-10-26 15:58:05.987859: Epoch 413 
2023-10-26 15:58:05.988125: Current learning rate: 0.00619 
2023-10-26 15:58:09.846624: train_loss -0.8686 
2023-10-26 15:58:09.847025: val_loss -0.8765 
2023-10-26 15:58:09.847283: Pseudo dice [0.8784, 0.91, 0.9688, 0.73, 0.9329] 
2023-10-26 15:58:09.847523: Epoch time: 3.86 s 
2023-10-26 15:58:10.925852:  
2023-10-26 15:58:10.926166: Epoch 414 
2023-10-26 15:58:10.926409: Current learning rate: 0.00618 
2023-10-26 15:58:14.828681: train_loss -0.8639 
2023-10-26 15:58:14.829153: val_loss -0.8772 
2023-10-26 15:58:14.829455: Pseudo dice [0.879, 0.9074, 0.9692, 0.8091, 0.9285] 
2023-10-26 15:58:14.829726: Epoch time: 3.9 s 
2023-10-26 15:58:15.933273:  
2023-10-26 15:58:15.933589: Epoch 415 
2023-10-26 15:58:15.933841: Current learning rate: 0.00617 
2023-10-26 15:58:19.766608: train_loss -0.8626 
2023-10-26 15:58:19.766972: val_loss -0.88 
2023-10-26 15:58:19.767216: Pseudo dice [0.8844, 0.9054, 0.9684, 0.7348, 0.9286] 
2023-10-26 15:58:19.767441: Epoch time: 3.83 s 
2023-10-26 15:58:21.017942:  
2023-10-26 15:58:21.018234: Epoch 416 
2023-10-26 15:58:21.018490: Current learning rate: 0.00616 
2023-10-26 15:58:25.035399: train_loss -0.8682 
2023-10-26 15:58:25.035796: val_loss -0.8893 
2023-10-26 15:58:25.036056: Pseudo dice [0.8855, 0.9257, 0.9687, 0.814, 0.92] 
2023-10-26 15:58:25.036288: Epoch time: 4.02 s 
2023-10-26 15:58:25.036494: Yayy! New best EMA pseudo Dice: 0.8891 
2023-10-26 15:58:26.228793:  
2023-10-26 15:58:26.229096: Epoch 417 
2023-10-26 15:58:26.229332: Current learning rate: 0.00615 
2023-10-26 15:58:30.148406: train_loss -0.8599 
2023-10-26 15:58:30.148770: val_loss -0.8638 
2023-10-26 15:58:30.149074: Pseudo dice [0.8684, 0.9027, 0.9636, 0.5613, 0.9238] 
2023-10-26 15:58:30.149298: Epoch time: 3.92 s 
2023-10-26 15:58:31.249489:  
2023-10-26 15:58:31.249788: Epoch 418 
2023-10-26 15:58:31.250063: Current learning rate: 0.00614 
2023-10-26 15:58:35.205441: train_loss -0.8508 
2023-10-26 15:58:35.205798: val_loss -0.8756 
2023-10-26 15:58:35.206053: Pseudo dice [0.878, 0.9134, 0.9676, 0.8029, 0.9202] 
2023-10-26 15:58:35.206273: Epoch time: 3.96 s 
2023-10-26 15:58:36.313858:  
2023-10-26 15:58:36.314192: Epoch 419 
2023-10-26 15:58:36.314434: Current learning rate: 0.00613 
2023-10-26 15:58:40.316606: train_loss -0.8612 
2023-10-26 15:58:40.316999: val_loss -0.8575 
2023-10-26 15:58:40.317257: Pseudo dice [0.8744, 0.9186, 0.968, 0.3607, 0.9328] 
2023-10-26 15:58:40.317484: Epoch time: 4.0 s 
2023-10-26 15:58:41.403772:  
2023-10-26 15:58:41.404147: Epoch 420 
2023-10-26 15:58:41.404469: Current learning rate: 0.00612 
2023-10-26 15:58:45.434416: train_loss -0.8648 
2023-10-26 15:58:45.434758: val_loss -0.8735 
2023-10-26 15:58:45.435009: Pseudo dice [0.8823, 0.9157, 0.9666, 0.6592, 0.9249] 
2023-10-26 15:58:45.435220: Epoch time: 4.03 s 
2023-10-26 15:58:46.491380:  
2023-10-26 15:58:46.491683: Epoch 421 
2023-10-26 15:58:46.491966: Current learning rate: 0.00612 
2023-10-26 15:58:50.550149: train_loss -0.8648 
2023-10-26 15:58:50.550520: val_loss -0.8816 
2023-10-26 15:58:50.550776: Pseudo dice [0.8822, 0.9179, 0.9687, 0.7567, 0.93] 
2023-10-26 15:58:50.551021: Epoch time: 4.06 s 
2023-10-26 15:58:51.606200:  
2023-10-26 15:58:51.606496: Epoch 422 
2023-10-26 15:58:51.606733: Current learning rate: 0.00611 
2023-10-26 15:58:55.667574: train_loss -0.8685 
2023-10-26 15:58:55.668023: val_loss -0.8802 
2023-10-26 15:58:55.668480: Pseudo dice [0.8829, 0.9162, 0.9674, 0.713, 0.9335] 
2023-10-26 15:58:55.669013: Epoch time: 4.06 s 
2023-10-26 15:58:56.890202:  
2023-10-26 15:58:56.890490: Epoch 423 
2023-10-26 15:58:56.890728: Current learning rate: 0.0061 
2023-10-26 15:59:00.947735: train_loss -0.8673 
2023-10-26 15:59:00.948159: val_loss -0.879 
2023-10-26 15:59:00.948424: Pseudo dice [0.8841, 0.9216, 0.9686, 0.7546, 0.9218] 
2023-10-26 15:59:00.948649: Epoch time: 4.06 s 
2023-10-26 15:59:02.051311:  
2023-10-26 15:59:02.051597: Epoch 424 
2023-10-26 15:59:02.051833: Current learning rate: 0.00609 
2023-10-26 15:59:06.125571: train_loss -0.8729 
2023-10-26 15:59:06.125947: val_loss -0.883 
2023-10-26 15:59:06.126210: Pseudo dice [0.8781, 0.9158, 0.9691, 0.7573, 0.9295] 
2023-10-26 15:59:06.126438: Epoch time: 4.07 s 
2023-10-26 15:59:07.194836:  
2023-10-26 15:59:07.195138: Epoch 425 
2023-10-26 15:59:07.195377: Current learning rate: 0.00608 
2023-10-26 15:59:11.250859: train_loss -0.8721 
2023-10-26 15:59:11.251396: val_loss -0.8746 
2023-10-26 15:59:11.251909: Pseudo dice [0.8846, 0.9296, 0.9662, 0.6662, 0.9275] 
2023-10-26 15:59:11.252263: Epoch time: 4.06 s 
2023-10-26 15:59:12.331902:  
2023-10-26 15:59:12.332192: Epoch 426 
2023-10-26 15:59:12.332428: Current learning rate: 0.00607 
2023-10-26 15:59:16.164844: train_loss -0.856 
2023-10-26 15:59:16.165222: val_loss -0.8788 
2023-10-26 15:59:16.165491: Pseudo dice [0.8822, 0.9061, 0.97, 0.7241, 0.9368] 
2023-10-26 15:59:16.165733: Epoch time: 3.83 s 
2023-10-26 15:59:17.265038:  
2023-10-26 15:59:17.265339: Epoch 427 
2023-10-26 15:59:17.265588: Current learning rate: 0.00606 
2023-10-26 15:59:21.255511: train_loss -0.8666 
2023-10-26 15:59:21.255922: val_loss -0.8786 
2023-10-26 15:59:21.256185: Pseudo dice [0.8864, 0.9159, 0.9705, 0.7164, 0.9345] 
2023-10-26 15:59:21.256421: Epoch time: 3.99 s 
2023-10-26 15:59:22.340602:  
2023-10-26 15:59:22.340900: Epoch 428 
2023-10-26 15:59:22.341144: Current learning rate: 0.00605 
2023-10-26 15:59:26.269841: train_loss -0.8675 
2023-10-26 15:59:26.270237: val_loss -0.8785 
2023-10-26 15:59:26.270606: Pseudo dice [0.8859, 0.9153, 0.9693, 0.6955, 0.9302] 
2023-10-26 15:59:26.270840: Epoch time: 3.93 s 
2023-10-26 15:59:27.379373:  
2023-10-26 15:59:27.379664: Epoch 429 
2023-10-26 15:59:27.379959: Current learning rate: 0.00604 
2023-10-26 15:59:31.346739: train_loss -0.8644 
2023-10-26 15:59:31.347155: val_loss -0.8449 
2023-10-26 15:59:31.347416: Pseudo dice [0.8547, 0.911, 0.967, 0.0352, 0.9289] 
2023-10-26 15:59:31.347649: Epoch time: 3.97 s 
2023-10-26 15:59:32.591408:  
2023-10-26 15:59:32.591720: Epoch 430 
2023-10-26 15:59:32.591974: Current learning rate: 0.00603 
2023-10-26 15:59:36.316655: train_loss -0.8658 
2023-10-26 15:59:36.317040: val_loss -0.8762 
2023-10-26 15:59:36.317399: Pseudo dice [0.8855, 0.912, 0.9683, 0.7153, 0.9306] 
2023-10-26 15:59:36.317647: Epoch time: 3.73 s 
2023-10-26 15:59:37.448460:  
2023-10-26 15:59:37.448777: Epoch 431 
2023-10-26 15:59:37.449042: Current learning rate: 0.00602 
2023-10-26 15:59:41.313848: train_loss -0.8598 
2023-10-26 15:59:41.314231: val_loss -0.8682 
2023-10-26 15:59:41.314481: Pseudo dice [0.8757, 0.9144, 0.9688, 0.7069, 0.9137] 
2023-10-26 15:59:41.314707: Epoch time: 3.87 s 
2023-10-26 15:59:42.387066:  
2023-10-26 15:59:42.387355: Epoch 432 
2023-10-26 15:59:42.387593: Current learning rate: 0.00601 
2023-10-26 15:59:46.346745: train_loss -0.8646 
2023-10-26 15:59:46.347139: val_loss -0.8748 
2023-10-26 15:59:46.347394: Pseudo dice [0.8784, 0.9189, 0.968, 0.7212, 0.9199] 
2023-10-26 15:59:46.347628: Epoch time: 3.96 s 
2023-10-26 15:59:47.451145:  
2023-10-26 15:59:47.451457: Epoch 433 
2023-10-26 15:59:47.451731: Current learning rate: 0.006 
2023-10-26 15:59:51.422590: train_loss -0.8643 
2023-10-26 15:59:51.422940: val_loss -0.8804 
2023-10-26 15:59:51.423200: Pseudo dice [0.879, 0.9208, 0.9678, 0.7533, 0.9289] 
2023-10-26 15:59:51.423417: Epoch time: 3.97 s 
2023-10-26 15:59:52.504961:  
2023-10-26 15:59:52.505308: Epoch 434 
2023-10-26 15:59:52.505567: Current learning rate: 0.00599 
2023-10-26 15:59:56.455706: train_loss -0.8753 
2023-10-26 15:59:56.456099: val_loss -0.8792 
2023-10-26 15:59:56.456356: Pseudo dice [0.8803, 0.9076, 0.9692, 0.7385, 0.9312] 
2023-10-26 15:59:56.456582: Epoch time: 3.95 s 
2023-10-26 15:59:57.562611:  
2023-10-26 15:59:57.562917: Epoch 435 
2023-10-26 15:59:57.563180: Current learning rate: 0.00598 
2023-10-26 16:00:01.364578: train_loss -0.8591 
2023-10-26 16:00:01.365125: val_loss -0.8532 
2023-10-26 16:00:01.365474: Pseudo dice [0.8733, 0.9106, 0.9655, 0.5813, 0.9215] 
2023-10-26 16:00:01.365807: Epoch time: 3.8 s 
2023-10-26 16:00:02.637738:  
2023-10-26 16:00:02.638058: Epoch 436 
2023-10-26 16:00:02.638321: Current learning rate: 0.00597 
2023-10-26 16:00:06.550255: train_loss -0.8621 
2023-10-26 16:00:06.550672: val_loss -0.876 
2023-10-26 16:00:06.550953: Pseudo dice [0.8771, 0.9123, 0.9685, 0.6785, 0.9354] 
2023-10-26 16:00:06.551216: Epoch time: 3.91 s 
2023-10-26 16:00:07.633371:  
2023-10-26 16:00:07.633698: Epoch 437 
2023-10-26 16:00:07.633948: Current learning rate: 0.00596 
2023-10-26 16:00:11.452258: train_loss -0.8637 
2023-10-26 16:00:11.452651: val_loss -0.8826 
2023-10-26 16:00:11.452929: Pseudo dice [0.8786, 0.916, 0.9705, 0.7453, 0.919] 
2023-10-26 16:00:11.453153: Epoch time: 3.82 s 
2023-10-26 16:00:12.543507:  
2023-10-26 16:00:12.543797: Epoch 438 
2023-10-26 16:00:12.544043: Current learning rate: 0.00595 
2023-10-26 16:00:16.522133: train_loss -0.8677 
2023-10-26 16:00:16.522541: val_loss -0.8775 
2023-10-26 16:00:16.522799: Pseudo dice [0.8845, 0.9166, 0.9699, 0.6005, 0.9292] 
2023-10-26 16:00:16.523274: Epoch time: 3.98 s 
2023-10-26 16:00:17.663399:  
2023-10-26 16:00:17.663737: Epoch 439 
2023-10-26 16:00:17.664046: Current learning rate: 0.00594 
2023-10-26 16:00:21.645095: train_loss -0.8552 
2023-10-26 16:00:21.645501: val_loss -0.8598 
2023-10-26 16:00:21.645768: Pseudo dice [0.8676, 0.8982, 0.9632, 0.6048, 0.9309] 
2023-10-26 16:00:21.646037: Epoch time: 3.98 s 
2023-10-26 16:00:22.747910:  
2023-10-26 16:00:22.748223: Epoch 440 
2023-10-26 16:00:22.748546: Current learning rate: 0.00593 
2023-10-26 16:00:26.616409: train_loss -0.8619 
2023-10-26 16:00:26.616797: val_loss -0.8724 
2023-10-26 16:00:26.617059: Pseudo dice [0.878, 0.9169, 0.969, 0.6532, 0.9268] 
2023-10-26 16:00:26.617304: Epoch time: 3.87 s 
2023-10-26 16:00:27.696697:  
2023-10-26 16:00:27.697039: Epoch 441 
2023-10-26 16:00:27.697353: Current learning rate: 0.00592 
2023-10-26 16:00:31.632354: train_loss -0.8634 
2023-10-26 16:00:31.632745: val_loss -0.8757 
2023-10-26 16:00:31.633016: Pseudo dice [0.8862, 0.9155, 0.9696, 0.664, 0.9373] 
2023-10-26 16:00:31.633256: Epoch time: 3.94 s 
2023-10-26 16:00:32.707341:  
2023-10-26 16:00:32.707644: Epoch 442 
2023-10-26 16:00:32.707909: Current learning rate: 0.00592 
2023-10-26 16:00:36.539316: train_loss -0.8689 
2023-10-26 16:00:36.539729: val_loss -0.8733 
2023-10-26 16:00:36.539996: Pseudo dice [0.8777, 0.9121, 0.9705, 0.6136, 0.9336] 
2023-10-26 16:00:36.540233: Epoch time: 3.83 s 
2023-10-26 16:00:37.789351:  
2023-10-26 16:00:37.789664: Epoch 443 
2023-10-26 16:00:37.789914: Current learning rate: 0.00591 
2023-10-26 16:00:41.708174: train_loss -0.8695 
2023-10-26 16:00:41.708536: val_loss -0.866 
2023-10-26 16:00:41.708805: Pseudo dice [0.8784, 0.91, 0.9673, 0.7128, 0.9359] 
2023-10-26 16:00:41.709030: Epoch time: 3.92 s 
2023-10-26 16:00:42.844912:  
2023-10-26 16:00:42.845210: Epoch 444 
2023-10-26 16:00:42.845468: Current learning rate: 0.0059 
2023-10-26 16:00:46.833539: train_loss -0.8713 
2023-10-26 16:00:46.833977: val_loss -0.8701 
2023-10-26 16:00:46.834315: Pseudo dice [0.8863, 0.9127, 0.9703, 0.6636, 0.9222] 
2023-10-26 16:00:46.834570: Epoch time: 3.99 s 
2023-10-26 16:00:47.893839:  
2023-10-26 16:00:47.894142: Epoch 445 
2023-10-26 16:00:47.894396: Current learning rate: 0.00589 
2023-10-26 16:00:51.912905: train_loss -0.8733 
2023-10-26 16:00:51.913386: val_loss -0.8795 
2023-10-26 16:00:51.913667: Pseudo dice [0.887, 0.9141, 0.9698, 0.7824, 0.925] 
2023-10-26 16:00:51.913909: Epoch time: 4.02 s 
2023-10-26 16:00:53.041705:  
2023-10-26 16:00:53.042012: Epoch 446 
2023-10-26 16:00:53.042254: Current learning rate: 0.00588 
2023-10-26 16:00:57.037506: train_loss -0.8718 
2023-10-26 16:00:57.037920: val_loss -0.8731 
2023-10-26 16:00:57.038190: Pseudo dice [0.8842, 0.9107, 0.9684, 0.5985, 0.9221] 
2023-10-26 16:00:57.038424: Epoch time: 4.0 s 
2023-10-26 16:00:58.092384:  
2023-10-26 16:00:58.092677: Epoch 447 
2023-10-26 16:00:58.092920: Current learning rate: 0.00587 
2023-10-26 16:01:01.875857: train_loss -0.8616 
2023-10-26 16:01:01.876229: val_loss -0.8747 
2023-10-26 16:01:01.876481: Pseudo dice [0.8836, 0.9119, 0.9686, 0.7392, 0.9292] 
2023-10-26 16:01:01.876711: Epoch time: 3.78 s 
2023-10-26 16:01:02.981518:  
2023-10-26 16:01:02.981823: Epoch 448 
2023-10-26 16:01:02.982102: Current learning rate: 0.00586 
2023-10-26 16:01:07.030769: train_loss -0.8682 
2023-10-26 16:01:07.031247: val_loss -0.877 
2023-10-26 16:01:07.031560: Pseudo dice [0.8832, 0.9181, 0.971, 0.6297, 0.9237] 
2023-10-26 16:01:07.031802: Epoch time: 4.05 s 
2023-10-26 16:01:08.130327:  
2023-10-26 16:01:08.130626: Epoch 449 
2023-10-26 16:01:08.130863: Current learning rate: 0.00585 
2023-10-26 16:01:12.048421: train_loss -0.8727 
2023-10-26 16:01:12.048777: val_loss -0.8774 
2023-10-26 16:01:12.049040: Pseudo dice [0.8864, 0.917, 0.9678, 0.7331, 0.9454] 
2023-10-26 16:01:12.049256: Epoch time: 3.92 s 
2023-10-26 16:01:13.376633:  
2023-10-26 16:01:13.376930: Epoch 450 
2023-10-26 16:01:13.377173: Current learning rate: 0.00584 
2023-10-26 16:01:17.301525: train_loss -0.8782 
2023-10-26 16:01:17.301892: val_loss -0.8794 
2023-10-26 16:01:17.302149: Pseudo dice [0.8829, 0.9177, 0.97, 0.7131, 0.9331] 
2023-10-26 16:01:17.302374: Epoch time: 3.93 s 
2023-10-26 16:01:18.360157:  
2023-10-26 16:01:18.360453: Epoch 451 
2023-10-26 16:01:18.360698: Current learning rate: 0.00583 
2023-10-26 16:01:22.346737: train_loss -0.8764 
2023-10-26 16:01:22.347121: val_loss -0.8804 
2023-10-26 16:01:22.347382: Pseudo dice [0.8808, 0.9067, 0.9666, 0.7941, 0.9257] 
2023-10-26 16:01:22.347669: Epoch time: 3.99 s 
2023-10-26 16:01:23.409237:  
2023-10-26 16:01:23.409537: Epoch 452 
2023-10-26 16:01:23.409779: Current learning rate: 0.00582 
2023-10-26 16:01:27.442532: train_loss -0.876 
2023-10-26 16:01:27.442916: val_loss -0.8785 
2023-10-26 16:01:27.443184: Pseudo dice [0.8847, 0.9131, 0.9689, 0.7336, 0.9368] 
2023-10-26 16:01:27.443401: Epoch time: 4.03 s 
2023-10-26 16:01:28.494632:  
2023-10-26 16:01:28.494929: Epoch 453 
2023-10-26 16:01:28.495178: Current learning rate: 0.00581 
2023-10-26 16:01:32.441272: train_loss -0.8753 
2023-10-26 16:01:32.441668: val_loss -0.8794 
2023-10-26 16:01:32.441930: Pseudo dice [0.8774, 0.9144, 0.9656, 0.7457, 0.9328] 
2023-10-26 16:01:32.442170: Epoch time: 3.95 s 
2023-10-26 16:01:33.489883:  
2023-10-26 16:01:33.490177: Epoch 454 
2023-10-26 16:01:33.490425: Current learning rate: 0.0058 
2023-10-26 16:01:37.508514: train_loss -0.8683 
2023-10-26 16:01:37.508899: val_loss -0.8728 
2023-10-26 16:01:37.509179: Pseudo dice [0.8749, 0.906, 0.9694, 0.676, 0.9267] 
2023-10-26 16:01:37.509413: Epoch time: 4.02 s 
2023-10-26 16:01:38.861901:  
2023-10-26 16:01:38.862195: Epoch 455 
2023-10-26 16:01:38.862435: Current learning rate: 0.00579 
2023-10-26 16:01:42.911197: train_loss -0.8791 
2023-10-26 16:01:42.911586: val_loss -0.8784 
2023-10-26 16:01:42.911857: Pseudo dice [0.8843, 0.9141, 0.9703, 0.7767, 0.9287] 
2023-10-26 16:01:42.912125: Epoch time: 4.05 s 
2023-10-26 16:01:44.133101:  
2023-10-26 16:01:44.133384: Epoch 456 
2023-10-26 16:01:44.133628: Current learning rate: 0.00578 
2023-10-26 16:01:48.144384: train_loss -0.8718 
2023-10-26 16:01:48.145018: val_loss -0.8713 
2023-10-26 16:01:48.145334: Pseudo dice [0.8852, 0.9097, 0.9675, 0.7153, 0.9294] 
2023-10-26 16:01:48.145585: Epoch time: 4.01 s 
2023-10-26 16:01:49.239174:  
2023-10-26 16:01:49.239472: Epoch 457 
2023-10-26 16:01:49.239716: Current learning rate: 0.00577 
2023-10-26 16:01:53.199641: train_loss -0.8692 
2023-10-26 16:01:53.200010: val_loss -0.8745 
2023-10-26 16:01:53.200337: Pseudo dice [0.8832, 0.9111, 0.9621, 0.7433, 0.9184] 
2023-10-26 16:01:53.200560: Epoch time: 3.96 s 
2023-10-26 16:01:54.263208:  
2023-10-26 16:01:54.263504: Epoch 458 
2023-10-26 16:01:54.263749: Current learning rate: 0.00576 
2023-10-26 16:01:58.126450: train_loss -0.8696 
2023-10-26 16:01:58.126846: val_loss -0.8793 
2023-10-26 16:01:58.127115: Pseudo dice [0.8877, 0.9114, 0.9702, 0.7411, 0.9249] 
2023-10-26 16:01:58.127355: Epoch time: 3.86 s 
2023-10-26 16:01:59.262270:  
2023-10-26 16:01:59.262578: Epoch 459 
2023-10-26 16:01:59.262822: Current learning rate: 0.00575 
2023-10-26 16:02:03.096392: train_loss -0.8693 
2023-10-26 16:02:03.096767: val_loss -0.8849 
2023-10-26 16:02:03.097036: Pseudo dice [0.8805, 0.9114, 0.9695, 0.7657, 0.9366] 
2023-10-26 16:02:03.097256: Epoch time: 3.83 s 
2023-10-26 16:02:04.158037:  
2023-10-26 16:02:04.158342: Epoch 460 
2023-10-26 16:02:04.158582: Current learning rate: 0.00574 
2023-10-26 16:02:08.063405: train_loss -0.878 
2023-10-26 16:02:08.063777: val_loss -0.8739 
2023-10-26 16:02:08.064033: Pseudo dice [0.8824, 0.9111, 0.9689, 0.7449, 0.9278] 
2023-10-26 16:02:08.064250: Epoch time: 3.91 s 
2023-10-26 16:02:09.133556:  
2023-10-26 16:02:09.133865: Epoch 461 
2023-10-26 16:02:09.134112: Current learning rate: 0.00573 
2023-10-26 16:02:12.975088: train_loss -0.866 
2023-10-26 16:02:12.975495: val_loss -0.8823 
2023-10-26 16:02:12.975967: Pseudo dice [0.8849, 0.9112, 0.9713, 0.6448, 0.9306] 
2023-10-26 16:02:12.976259: Epoch time: 3.84 s 
2023-10-26 16:02:14.038936:  
2023-10-26 16:02:14.039237: Epoch 462 
2023-10-26 16:02:14.039480: Current learning rate: 0.00572 
2023-10-26 16:02:17.907238: train_loss -0.8734 
2023-10-26 16:02:17.907665: val_loss -0.8776 
2023-10-26 16:02:17.907938: Pseudo dice [0.8825, 0.918, 0.9709, 0.5856, 0.9321] 
2023-10-26 16:02:17.908162: Epoch time: 3.87 s 
2023-10-26 16:02:19.142542:  
2023-10-26 16:02:19.142837: Epoch 463 
2023-10-26 16:02:19.143068: Current learning rate: 0.00571 
2023-10-26 16:02:23.034171: train_loss -0.8756 
2023-10-26 16:02:23.034601: val_loss -0.8639 
2023-10-26 16:02:23.034877: Pseudo dice [0.8823, 0.8937, 0.9683, 0.6751, 0.9235] 
2023-10-26 16:02:23.035127: Epoch time: 3.89 s 
2023-10-26 16:02:24.108229:  
2023-10-26 16:02:24.108534: Epoch 464 
2023-10-26 16:02:24.108776: Current learning rate: 0.0057 
2023-10-26 16:02:27.968812: train_loss -0.8711 
2023-10-26 16:02:27.969184: val_loss -0.874 
2023-10-26 16:02:27.969440: Pseudo dice [0.8862, 0.9061, 0.9704, 0.6808, 0.9341] 
2023-10-26 16:02:27.969679: Epoch time: 3.86 s 
2023-10-26 16:02:29.047954:  
2023-10-26 16:02:29.048269: Epoch 465 
2023-10-26 16:02:29.048553: Current learning rate: 0.0057 
2023-10-26 16:02:32.979181: train_loss -0.8675 
2023-10-26 16:02:32.979541: val_loss -0.8699 
2023-10-26 16:02:32.979811: Pseudo dice [0.8855, 0.9122, 0.9682, 0.7608, 0.9325] 
2023-10-26 16:02:32.980045: Epoch time: 3.93 s 
2023-10-26 16:02:34.049094:  
2023-10-26 16:02:34.049400: Epoch 466 
2023-10-26 16:02:34.049641: Current learning rate: 0.00569 
2023-10-26 16:02:37.962311: train_loss -0.8728 
2023-10-26 16:02:37.962700: val_loss -0.8732 
2023-10-26 16:02:37.963020: Pseudo dice [0.8846, 0.9151, 0.9708, 0.6897, 0.9349] 
2023-10-26 16:02:37.963266: Epoch time: 3.91 s 
2023-10-26 16:02:39.043845:  
2023-10-26 16:02:39.044182: Epoch 467 
2023-10-26 16:02:39.044459: Current learning rate: 0.00568 
2023-10-26 16:02:43.117736: train_loss -0.877 
2023-10-26 16:02:43.118210: val_loss -0.8762 
2023-10-26 16:02:43.118567: Pseudo dice [0.8862, 0.9133, 0.968, 0.7053, 0.9317] 
2023-10-26 16:02:43.118867: Epoch time: 4.07 s 
2023-10-26 16:02:44.209718:  
2023-10-26 16:02:44.210031: Epoch 468 
2023-10-26 16:02:44.210283: Current learning rate: 0.00567 
2023-10-26 16:02:48.187234: train_loss -0.8791 
2023-10-26 16:02:48.187750: val_loss -0.8717 
2023-10-26 16:02:48.188050: Pseudo dice [0.881, 0.9192, 0.9694, 0.6879, 0.9271] 
2023-10-26 16:02:48.188320: Epoch time: 3.98 s 
2023-10-26 16:02:49.242526:  
2023-10-26 16:02:49.242825: Epoch 469 
2023-10-26 16:02:49.243075: Current learning rate: 0.00566 
2023-10-26 16:02:53.246855: train_loss -0.8741 
2023-10-26 16:02:53.247290: val_loss -0.8719 
2023-10-26 16:02:53.247699: Pseudo dice [0.8859, 0.9167, 0.9684, 0.7026, 0.9318] 
2023-10-26 16:02:53.248044: Epoch time: 4.0 s 
2023-10-26 16:02:54.529852:  
2023-10-26 16:02:54.530227: Epoch 470 
2023-10-26 16:02:54.530504: Current learning rate: 0.00565 
2023-10-26 16:02:58.634959: train_loss -0.8774 
2023-10-26 16:02:58.635360: val_loss -0.8853 
2023-10-26 16:02:58.635615: Pseudo dice [0.8824, 0.9179, 0.9695, 0.6993, 0.9357] 
2023-10-26 16:02:58.635849: Epoch time: 4.11 s 
2023-10-26 16:02:59.692240:  
2023-10-26 16:02:59.692561: Epoch 471 
2023-10-26 16:02:59.692848: Current learning rate: 0.00564 
2023-10-26 16:03:03.569814: train_loss -0.8784 
2023-10-26 16:03:03.570210: val_loss -0.8747 
2023-10-26 16:03:03.570464: Pseudo dice [0.8844, 0.9093, 0.9673, 0.6571, 0.9278] 
2023-10-26 16:03:03.570698: Epoch time: 3.88 s 
2023-10-26 16:03:04.630867:  
2023-10-26 16:03:04.631177: Epoch 472 
2023-10-26 16:03:04.631418: Current learning rate: 0.00563 
2023-10-26 16:03:08.713454: train_loss -0.8747 
2023-10-26 16:03:08.713818: val_loss -0.8731 
2023-10-26 16:03:08.714076: Pseudo dice [0.8773, 0.9053, 0.9693, 0.4696, 0.9319] 
2023-10-26 16:03:08.714307: Epoch time: 4.08 s 
2023-10-26 16:03:09.820946:  
2023-10-26 16:03:09.821250: Epoch 473 
2023-10-26 16:03:09.821508: Current learning rate: 0.00562 
2023-10-26 16:03:13.796703: train_loss -0.8738 
2023-10-26 16:03:13.797261: val_loss -0.8787 
2023-10-26 16:03:13.797554: Pseudo dice [0.8755, 0.9186, 0.9686, 0.7224, 0.9387] 
2023-10-26 16:03:13.797841: Epoch time: 3.98 s 
2023-10-26 16:03:14.875309:  
2023-10-26 16:03:14.875597: Epoch 474 
2023-10-26 16:03:14.875840: Current learning rate: 0.00561 
2023-10-26 16:03:18.848466: train_loss -0.8704 
2023-10-26 16:03:18.848827: val_loss -0.8708 
2023-10-26 16:03:18.849080: Pseudo dice [0.872, 0.9054, 0.9677, 0.744, 0.9186] 
2023-10-26 16:03:18.849307: Epoch time: 3.97 s 
2023-10-26 16:03:19.943789:  
2023-10-26 16:03:19.944067: Epoch 475 
2023-10-26 16:03:19.944296: Current learning rate: 0.0056 
2023-10-26 16:03:23.905861: train_loss -0.8666 
2023-10-26 16:03:23.906241: val_loss -0.8765 
2023-10-26 16:03:23.906499: Pseudo dice [0.8767, 0.9222, 0.9694, 0.7892, 0.93] 
2023-10-26 16:03:23.906726: Epoch time: 3.96 s 
2023-10-26 16:03:24.970698:  
2023-10-26 16:03:24.971043: Epoch 476 
2023-10-26 16:03:24.971359: Current learning rate: 0.00559 
2023-10-26 16:03:28.868170: train_loss -0.8608 
2023-10-26 16:03:28.868579: val_loss -0.8438 
2023-10-26 16:03:28.868842: Pseudo dice [0.8499, 0.9083, 0.9628, 0.7516, 0.9186] 
2023-10-26 16:03:28.869075: Epoch time: 3.9 s 
2023-10-26 16:03:30.141079:  
2023-10-26 16:03:30.141417: Epoch 477 
2023-10-26 16:03:30.141663: Current learning rate: 0.00558 
2023-10-26 16:03:34.063166: train_loss -0.8624 
2023-10-26 16:03:34.063566: val_loss -0.8718 
2023-10-26 16:03:34.063822: Pseudo dice [0.8832, 0.9183, 0.9689, 0.6653, 0.9373] 
2023-10-26 16:03:34.064053: Epoch time: 3.92 s 
2023-10-26 16:03:35.130327:  
2023-10-26 16:03:35.130629: Epoch 478 
2023-10-26 16:03:35.130885: Current learning rate: 0.00557 
2023-10-26 16:03:38.865098: train_loss -0.8684 
2023-10-26 16:03:38.865548: val_loss -0.8647 
2023-10-26 16:03:38.866034: Pseudo dice [0.8759, 0.9065, 0.9662, 0.5581, 0.9287] 
2023-10-26 16:03:38.866304: Epoch time: 3.74 s 
2023-10-26 16:03:39.935172:  
2023-10-26 16:03:39.935467: Epoch 479 
2023-10-26 16:03:39.935696: Current learning rate: 0.00556 
2023-10-26 16:03:43.881767: train_loss -0.8704 
2023-10-26 16:03:43.882167: val_loss -0.8705 
2023-10-26 16:03:43.882419: Pseudo dice [0.873, 0.9106, 0.9696, 0.6192, 0.94] 
2023-10-26 16:03:43.882648: Epoch time: 3.95 s 
2023-10-26 16:03:44.972223:  
2023-10-26 16:03:44.972559: Epoch 480 
2023-10-26 16:03:44.972815: Current learning rate: 0.00555 
2023-10-26 16:03:48.885461: train_loss -0.8712 
2023-10-26 16:03:48.885853: val_loss -0.8741 
2023-10-26 16:03:48.886137: Pseudo dice [0.8806, 0.9168, 0.9688, 0.5684, 0.9249] 
2023-10-26 16:03:48.886379: Epoch time: 3.91 s 
2023-10-26 16:03:49.962051:  
2023-10-26 16:03:49.962372: Epoch 481 
2023-10-26 16:03:49.962634: Current learning rate: 0.00554 
2023-10-26 16:03:53.923741: train_loss -0.8824 
2023-10-26 16:03:53.924147: val_loss -0.8711 
2023-10-26 16:03:53.924412: Pseudo dice [0.8756, 0.9095, 0.9684, 0.6472, 0.9292] 
2023-10-26 16:03:53.924651: Epoch time: 3.96 s 
2023-10-26 16:03:55.010937:  
2023-10-26 16:03:55.011289: Epoch 482 
2023-10-26 16:03:55.011595: Current learning rate: 0.00553 
2023-10-26 16:03:59.006935: train_loss -0.8685 
2023-10-26 16:03:59.007325: val_loss -0.8892 
2023-10-26 16:03:59.007603: Pseudo dice [0.8857, 0.9233, 0.9718, 0.7932, 0.9343] 
2023-10-26 16:03:59.007840: Epoch time: 4.0 s 
2023-10-26 16:04:00.373437:  
2023-10-26 16:04:00.373802: Epoch 483 
2023-10-26 16:04:00.374133: Current learning rate: 0.00552 
2023-10-26 16:04:04.074046: train_loss -0.8729 
2023-10-26 16:04:04.074427: val_loss -0.8666 
2023-10-26 16:04:04.074678: Pseudo dice [0.8766, 0.9161, 0.9657, 0.738, 0.9119] 
2023-10-26 16:04:04.074975: Epoch time: 3.7 s 
2023-10-26 16:04:05.141848:  
2023-10-26 16:04:05.142163: Epoch 484 
2023-10-26 16:04:05.142418: Current learning rate: 0.00551 
2023-10-26 16:04:08.869420: train_loss -0.8677 
2023-10-26 16:04:08.869819: val_loss -0.8735 
2023-10-26 16:04:08.870088: Pseudo dice [0.887, 0.9122, 0.9697, 0.7557, 0.9314] 
2023-10-26 16:04:08.870330: Epoch time: 3.73 s 
2023-10-26 16:04:09.978370:  
2023-10-26 16:04:09.978689: Epoch 485 
2023-10-26 16:04:09.978949: Current learning rate: 0.0055 
2023-10-26 16:04:14.061170: train_loss -0.8769 
2023-10-26 16:04:14.061553: val_loss -0.8841 
2023-10-26 16:04:14.061815: Pseudo dice [0.8819, 0.9179, 0.9699, 0.7601, 0.9371] 
2023-10-26 16:04:14.062051: Epoch time: 4.08 s 
2023-10-26 16:04:15.159126:  
2023-10-26 16:04:15.159501: Epoch 486 
2023-10-26 16:04:15.159755: Current learning rate: 0.00549 
2023-10-26 16:04:19.186863: train_loss -0.8762 
2023-10-26 16:04:19.187317: val_loss -0.874 
2023-10-26 16:04:19.187668: Pseudo dice [0.884, 0.9141, 0.9694, 0.7186, 0.9256] 
2023-10-26 16:04:19.187908: Epoch time: 4.03 s 
2023-10-26 16:04:20.272745:  
2023-10-26 16:04:20.273032: Epoch 487 
2023-10-26 16:04:20.273276: Current learning rate: 0.00548 
2023-10-26 16:04:24.360715: train_loss -0.8733 
2023-10-26 16:04:24.361072: val_loss -0.8634 
2023-10-26 16:04:24.361323: Pseudo dice [0.8844, 0.9181, 0.9697, 0.7041, 0.9278] 
2023-10-26 16:04:24.361549: Epoch time: 4.09 s 
2023-10-26 16:04:25.413904:  
2023-10-26 16:04:25.414226: Epoch 488 
2023-10-26 16:04:25.414460: Current learning rate: 0.00547 
2023-10-26 16:04:29.508247: train_loss -0.8854 
2023-10-26 16:04:29.508712: val_loss -0.8786 
2023-10-26 16:04:29.509048: Pseudo dice [0.8873, 0.9222, 0.9672, 0.6043, 0.9356] 
2023-10-26 16:04:29.509301: Epoch time: 4.09 s 
2023-10-26 16:04:30.633143:  
2023-10-26 16:04:30.633446: Epoch 489 
2023-10-26 16:04:30.633697: Current learning rate: 0.00546 
2023-10-26 16:04:34.706381: train_loss -0.8818 
2023-10-26 16:04:34.706755: val_loss -0.8758 
2023-10-26 16:04:34.707040: Pseudo dice [0.8823, 0.9157, 0.9675, 0.7548, 0.9317] 
2023-10-26 16:04:34.707280: Epoch time: 4.07 s 
2023-10-26 16:04:35.919631:  
2023-10-26 16:04:35.919946: Epoch 490 
2023-10-26 16:04:35.920188: Current learning rate: 0.00546 
2023-10-26 16:04:39.994932: train_loss -0.877 
2023-10-26 16:04:39.995321: val_loss -0.8705 
2023-10-26 16:04:39.995594: Pseudo dice [0.8846, 0.9129, 0.9702, 0.6449, 0.9316] 
2023-10-26 16:04:39.995829: Epoch time: 4.08 s 
2023-10-26 16:04:41.089008:  
2023-10-26 16:04:41.089294: Epoch 491 
2023-10-26 16:04:41.089535: Current learning rate: 0.00545 
2023-10-26 16:04:44.981229: train_loss -0.8706 
2023-10-26 16:04:44.981738: val_loss -0.8711 
2023-10-26 16:04:44.982090: Pseudo dice [0.8832, 0.9157, 0.9703, 0.6048, 0.9186] 
2023-10-26 16:04:44.982450: Epoch time: 3.89 s 
2023-10-26 16:04:46.064658:  
2023-10-26 16:04:46.064976: Epoch 492 
2023-10-26 16:04:46.065226: Current learning rate: 0.00544 
2023-10-26 16:04:50.114407: train_loss -0.8802 
2023-10-26 16:04:50.114927: val_loss -0.8762 
2023-10-26 16:04:50.115306: Pseudo dice [0.8858, 0.9143, 0.9695, 0.6943, 0.9273] 
2023-10-26 16:04:50.115749: Epoch time: 4.05 s 
2023-10-26 16:04:51.211489:  
2023-10-26 16:04:51.211811: Epoch 493 
2023-10-26 16:04:51.212055: Current learning rate: 0.00543 
2023-10-26 16:04:55.016131: train_loss -0.8808 
2023-10-26 16:04:55.016574: val_loss -0.8767 
2023-10-26 16:04:55.016974: Pseudo dice [0.8865, 0.9191, 0.9677, 0.7201, 0.923] 
2023-10-26 16:04:55.017288: Epoch time: 3.81 s 
2023-10-26 16:04:56.099901:  
2023-10-26 16:04:56.100236: Epoch 494 
2023-10-26 16:04:56.100550: Current learning rate: 0.00542 
2023-10-26 16:05:00.158384: train_loss -0.8835 
2023-10-26 16:05:00.158809: val_loss -0.8688 
2023-10-26 16:05:00.159104: Pseudo dice [0.8786, 0.913, 0.9674, 0.6307, 0.9274] 
2023-10-26 16:05:00.159368: Epoch time: 4.06 s 
2023-10-26 16:05:01.284967:  
2023-10-26 16:05:01.285272: Epoch 495 
2023-10-26 16:05:01.285529: Current learning rate: 0.00541 
2023-10-26 16:05:05.201156: train_loss -0.885 
2023-10-26 16:05:05.201576: val_loss -0.8764 
2023-10-26 16:05:05.201848: Pseudo dice [0.8811, 0.9099, 0.9697, 0.67, 0.9322] 
2023-10-26 16:05:05.202123: Epoch time: 3.92 s 
2023-10-26 16:05:06.304769:  
2023-10-26 16:05:06.305080: Epoch 496 
2023-10-26 16:05:06.305312: Current learning rate: 0.0054 
2023-10-26 16:05:10.331382: train_loss -0.8834 
2023-10-26 16:05:10.331769: val_loss -0.8828 
2023-10-26 16:05:10.332039: Pseudo dice [0.8881, 0.9208, 0.969, 0.727, 0.9281] 
2023-10-26 16:05:10.332268: Epoch time: 4.03 s 
2023-10-26 16:05:11.409534:  
2023-10-26 16:05:11.409840: Epoch 497 
2023-10-26 16:05:11.410089: Current learning rate: 0.00539 
2023-10-26 16:05:15.356030: train_loss -0.885 
2023-10-26 16:05:15.356418: val_loss -0.8768 
2023-10-26 16:05:15.356691: Pseudo dice [0.8806, 0.9171, 0.97, 0.7247, 0.933] 
2023-10-26 16:05:15.356934: Epoch time: 3.95 s 
2023-10-26 16:05:16.427552:  
2023-10-26 16:05:16.427845: Epoch 498 
2023-10-26 16:05:16.428133: Current learning rate: 0.00538 
2023-10-26 16:05:20.405651: train_loss -0.8773 
2023-10-26 16:05:20.406032: val_loss -0.8706 
2023-10-26 16:05:20.406298: Pseudo dice [0.8822, 0.9095, 0.9698, 0.6048, 0.9267] 
2023-10-26 16:05:20.406548: Epoch time: 3.98 s 
2023-10-26 16:05:21.573270:  
2023-10-26 16:05:21.573583: Epoch 499 
2023-10-26 16:05:21.573839: Current learning rate: 0.00537 
2023-10-26 16:05:25.304927: train_loss -0.8781 
2023-10-26 16:05:25.305290: val_loss -0.877 
2023-10-26 16:05:25.305543: Pseudo dice [0.8822, 0.912, 0.9686, 0.6174, 0.9331] 
2023-10-26 16:05:25.305767: Epoch time: 3.73 s 
2023-10-26 16:05:26.508180:  
2023-10-26 16:05:26.508497: Epoch 500 
2023-10-26 16:05:26.508735: Current learning rate: 0.00536 
2023-10-26 16:05:30.402334: train_loss -0.8801 
2023-10-26 16:05:30.402785: val_loss -0.8677 
2023-10-26 16:05:30.403052: Pseudo dice [0.8812, 0.9089, 0.9708, 0.511, 0.9272] 
2023-10-26 16:05:30.403286: Epoch time: 3.89 s 
2023-10-26 16:05:31.515901:  
2023-10-26 16:05:31.516190: Epoch 501 
2023-10-26 16:05:31.516434: Current learning rate: 0.00535 
2023-10-26 16:05:35.442362: train_loss -0.8801 
2023-10-26 16:05:35.442710: val_loss -0.8683 
2023-10-26 16:05:35.442968: Pseudo dice [0.8777, 0.9145, 0.9675, 0.7017, 0.9156] 
2023-10-26 16:05:35.443196: Epoch time: 3.93 s 
2023-10-26 16:05:36.522110:  
2023-10-26 16:05:36.522405: Epoch 502 
2023-10-26 16:05:36.522641: Current learning rate: 0.00534 
2023-10-26 16:05:40.400601: train_loss -0.88 
2023-10-26 16:05:40.400972: val_loss -0.8727 
2023-10-26 16:05:40.401230: Pseudo dice [0.878, 0.9152, 0.9698, 0.472, 0.9259] 
2023-10-26 16:05:40.401459: Epoch time: 3.88 s 
2023-10-26 16:05:41.678646:  
2023-10-26 16:05:41.678966: Epoch 503 
2023-10-26 16:05:41.679224: Current learning rate: 0.00533 
2023-10-26 16:05:45.696927: train_loss -0.874 
2023-10-26 16:05:45.697318: val_loss -0.8758 
2023-10-26 16:05:45.697591: Pseudo dice [0.8837, 0.9152, 0.9694, 0.6523, 0.9344] 
2023-10-26 16:05:45.697831: Epoch time: 4.02 s 
2023-10-26 16:05:46.829731:  
2023-10-26 16:05:46.830195: Epoch 504 
2023-10-26 16:05:46.830476: Current learning rate: 0.00532 
2023-10-26 16:05:50.840937: train_loss -0.8719 
2023-10-26 16:05:50.841314: val_loss -0.8665 
2023-10-26 16:05:50.841581: Pseudo dice [0.88, 0.9039, 0.9694, 0.57, 0.9255] 
2023-10-26 16:05:50.841806: Epoch time: 4.01 s 
2023-10-26 16:05:51.895696:  
2023-10-26 16:05:51.896020: Epoch 505 
2023-10-26 16:05:51.896269: Current learning rate: 0.00531 
2023-10-26 16:05:55.654227: train_loss -0.8761 
2023-10-26 16:05:55.654593: val_loss -0.8813 
2023-10-26 16:05:55.654839: Pseudo dice [0.883, 0.9112, 0.967, 0.6711, 0.9419] 
2023-10-26 16:05:55.655064: Epoch time: 3.76 s 
2023-10-26 16:05:56.709548:  
2023-10-26 16:05:56.709854: Epoch 506 
2023-10-26 16:05:56.710111: Current learning rate: 0.0053 
2023-10-26 16:06:00.602920: train_loss -0.8708 
2023-10-26 16:06:00.603322: val_loss -0.8809 
2023-10-26 16:06:00.603587: Pseudo dice [0.8864, 0.9107, 0.9696, 0.7733, 0.9486] 
2023-10-26 16:06:00.603812: Epoch time: 3.89 s 
2023-10-26 16:06:01.691006:  
2023-10-26 16:06:01.691312: Epoch 507 
2023-10-26 16:06:01.691563: Current learning rate: 0.00529 
2023-10-26 16:06:05.729171: train_loss -0.8733 
2023-10-26 16:06:05.729533: val_loss -0.8687 
2023-10-26 16:06:05.729796: Pseudo dice [0.8751, 0.8987, 0.9672, 0.6166, 0.9273] 
2023-10-26 16:06:05.730028: Epoch time: 4.04 s 
2023-10-26 16:06:06.833610:  
2023-10-26 16:06:06.833899: Epoch 508 
2023-10-26 16:06:06.834135: Current learning rate: 0.00528 
2023-10-26 16:06:10.691647: train_loss -0.8641 
2023-10-26 16:06:10.692124: val_loss -0.8694 
2023-10-26 16:06:10.692430: Pseudo dice [0.8824, 0.9152, 0.9679, 0.6826, 0.9214] 
2023-10-26 16:06:10.692700: Epoch time: 3.86 s 
2023-10-26 16:06:11.809201:  
2023-10-26 16:06:11.809492: Epoch 509 
2023-10-26 16:06:11.809730: Current learning rate: 0.00527 
2023-10-26 16:06:15.691084: train_loss -0.8759 
2023-10-26 16:06:15.691460: val_loss -0.872 
2023-10-26 16:06:15.691716: Pseudo dice [0.8803, 0.913, 0.9683, 0.6556, 0.9168] 
2023-10-26 16:06:15.691997: Epoch time: 3.88 s 
2023-10-26 16:06:16.929822:  
2023-10-26 16:06:16.930181: Epoch 510 
2023-10-26 16:06:16.930424: Current learning rate: 0.00526 
2023-10-26 16:06:20.824482: train_loss -0.8776 
2023-10-26 16:06:20.824862: val_loss -0.8671 
2023-10-26 16:06:20.825133: Pseudo dice [0.8864, 0.9136, 0.9707, 0.6704, 0.9214] 
2023-10-26 16:06:20.825365: Epoch time: 3.9 s 
2023-10-26 16:06:21.912786:  
2023-10-26 16:06:21.913065: Epoch 511 
2023-10-26 16:06:21.913307: Current learning rate: 0.00525 
2023-10-26 16:06:25.740053: train_loss -0.8759 
2023-10-26 16:06:25.740426: val_loss -0.8821 
2023-10-26 16:06:25.740688: Pseudo dice [0.8838, 0.919, 0.9704, 0.6624, 0.9395] 
2023-10-26 16:06:25.740932: Epoch time: 3.83 s 
2023-10-26 16:06:26.817041:  
2023-10-26 16:06:26.817330: Epoch 512 
2023-10-26 16:06:26.817569: Current learning rate: 0.00524 
2023-10-26 16:06:30.666350: train_loss -0.8772 
2023-10-26 16:06:30.666725: val_loss -0.8804 
2023-10-26 16:06:30.666985: Pseudo dice [0.8838, 0.9147, 0.9703, 0.717, 0.935] 
2023-10-26 16:06:30.667205: Epoch time: 3.85 s 
2023-10-26 16:06:31.758922:  
2023-10-26 16:06:31.759216: Epoch 513 
2023-10-26 16:06:31.759452: Current learning rate: 0.00523 
2023-10-26 16:06:35.638787: train_loss -0.8816 
2023-10-26 16:06:35.639314: val_loss -0.8795 
2023-10-26 16:06:35.639896: Pseudo dice [0.8841, 0.9154, 0.9703, 0.706, 0.9254] 
2023-10-26 16:06:35.640257: Epoch time: 3.88 s 
2023-10-26 16:06:36.720176:  
2023-10-26 16:06:36.720477: Epoch 514 
2023-10-26 16:06:36.720725: Current learning rate: 0.00522 
2023-10-26 16:06:40.700891: train_loss -0.8828 
2023-10-26 16:06:40.701530: val_loss -0.8695 
2023-10-26 16:06:40.701885: Pseudo dice [0.8843, 0.9196, 0.9693, 0.6055, 0.9216] 
2023-10-26 16:06:40.702129: Epoch time: 3.98 s 
2023-10-26 16:06:41.777573:  
2023-10-26 16:06:41.777897: Epoch 515 
2023-10-26 16:06:41.778146: Current learning rate: 0.00521 
2023-10-26 16:06:45.724739: train_loss -0.8709 
2023-10-26 16:06:45.725115: val_loss -0.8827 
2023-10-26 16:06:45.725383: Pseudo dice [0.8854, 0.9177, 0.9706, 0.6996, 0.9361] 
2023-10-26 16:06:45.725617: Epoch time: 3.95 s 
2023-10-26 16:06:46.985363:  
2023-10-26 16:06:46.985663: Epoch 516 
2023-10-26 16:06:46.985910: Current learning rate: 0.0052 
2023-10-26 16:06:50.959309: train_loss -0.8594 
2023-10-26 16:06:50.959703: val_loss -0.8633 
2023-10-26 16:06:50.959971: Pseudo dice [0.8819, 0.903, 0.9685, 0.5849, 0.9361] 
2023-10-26 16:06:50.960197: Epoch time: 3.97 s 
2023-10-26 16:06:52.038616:  
2023-10-26 16:06:52.038934: Epoch 517 
2023-10-26 16:06:52.039183: Current learning rate: 0.00519 
2023-10-26 16:06:55.925777: train_loss -0.8571 
2023-10-26 16:06:55.926183: val_loss -0.8673 
2023-10-26 16:06:55.926723: Pseudo dice [0.8779, 0.9063, 0.9688, 0.7171, 0.9237] 
2023-10-26 16:06:55.927065: Epoch time: 3.89 s 
2023-10-26 16:06:57.004702:  
2023-10-26 16:06:57.005015: Epoch 518 
2023-10-26 16:06:57.005262: Current learning rate: 0.00518 
2023-10-26 16:07:00.933342: train_loss -0.8597 
2023-10-26 16:07:00.933834: val_loss -0.8629 
2023-10-26 16:07:00.934229: Pseudo dice [0.8725, 0.9162, 0.968, 0.6201, 0.9305] 
2023-10-26 16:07:00.934588: Epoch time: 3.93 s 
2023-10-26 16:07:02.006490:  
2023-10-26 16:07:02.006797: Epoch 519 
2023-10-26 16:07:02.007047: Current learning rate: 0.00518 
2023-10-26 16:07:05.956553: train_loss -0.8728 
2023-10-26 16:07:05.956949: val_loss -0.877 
2023-10-26 16:07:05.957396: Pseudo dice [0.8851, 0.9189, 0.9704, 0.7452, 0.9293] 
2023-10-26 16:07:05.957669: Epoch time: 3.95 s 
2023-10-26 16:07:07.018691:  
2023-10-26 16:07:07.019169: Epoch 520 
2023-10-26 16:07:07.019507: Current learning rate: 0.00517 
2023-10-26 16:07:10.927909: train_loss -0.8743 
2023-10-26 16:07:10.928270: val_loss -0.8833 
2023-10-26 16:07:10.928533: Pseudo dice [0.8868, 0.9123, 0.9704, 0.7514, 0.9314] 
2023-10-26 16:07:10.928764: Epoch time: 3.91 s 
2023-10-26 16:07:11.998924:  
2023-10-26 16:07:11.999211: Epoch 521 
2023-10-26 16:07:11.999442: Current learning rate: 0.00516 
2023-10-26 16:07:15.776270: train_loss -0.8783 
2023-10-26 16:07:15.776646: val_loss -0.8794 
2023-10-26 16:07:15.776916: Pseudo dice [0.8887, 0.9207, 0.9707, 0.6715, 0.9192] 
2023-10-26 16:07:15.777153: Epoch time: 3.78 s 
2023-10-26 16:07:16.890471:  
2023-10-26 16:07:16.890758: Epoch 522 
2023-10-26 16:07:16.891001: Current learning rate: 0.00515 
2023-10-26 16:07:20.837486: train_loss -0.878 
2023-10-26 16:07:20.837892: val_loss -0.8883 
2023-10-26 16:07:20.838163: Pseudo dice [0.8875, 0.9102, 0.9695, 0.7945, 0.9329] 
2023-10-26 16:07:20.838389: Epoch time: 3.95 s 
2023-10-26 16:07:22.117034:  
2023-10-26 16:07:22.117322: Epoch 523 
2023-10-26 16:07:22.117564: Current learning rate: 0.00514 
2023-10-26 16:07:25.829185: train_loss -0.8601 
2023-10-26 16:07:25.829556: val_loss -0.8686 
2023-10-26 16:07:25.829829: Pseudo dice [0.8721, 0.916, 0.9675, 0.7758, 0.9308] 
2023-10-26 16:07:25.830052: Epoch time: 3.71 s 
2023-10-26 16:07:26.903028:  
2023-10-26 16:07:26.903333: Epoch 524 
2023-10-26 16:07:26.903567: Current learning rate: 0.00513 
2023-10-26 16:07:30.744585: train_loss -0.8682 
2023-10-26 16:07:30.745082: val_loss -0.8858 
2023-10-26 16:07:30.745420: Pseudo dice [0.8809, 0.9167, 0.9688, 0.7908, 0.9373] 
2023-10-26 16:07:30.745723: Epoch time: 3.84 s 
2023-10-26 16:07:31.882860:  
2023-10-26 16:07:31.883347: Epoch 525 
2023-10-26 16:07:31.883602: Current learning rate: 0.00512 
2023-10-26 16:07:35.758024: train_loss -0.8809 
2023-10-26 16:07:35.758438: val_loss -0.88 
2023-10-26 16:07:35.758904: Pseudo dice [0.8855, 0.9174, 0.9704, 0.7698, 0.9378] 
2023-10-26 16:07:35.759156: Epoch time: 3.88 s 
2023-10-26 16:07:36.857041:  
2023-10-26 16:07:36.857340: Epoch 526 
2023-10-26 16:07:36.857582: Current learning rate: 0.00511 
2023-10-26 16:07:40.848447: train_loss -0.8765 
2023-10-26 16:07:40.848829: val_loss -0.8812 
2023-10-26 16:07:40.849109: Pseudo dice [0.8867, 0.9168, 0.9685, 0.7603, 0.9224] 
2023-10-26 16:07:40.849372: Epoch time: 3.99 s 
2023-10-26 16:07:41.937081:  
2023-10-26 16:07:41.937361: Epoch 527 
2023-10-26 16:07:41.937621: Current learning rate: 0.0051 
2023-10-26 16:07:45.754083: train_loss -0.8742 
2023-10-26 16:07:45.754453: val_loss -0.877 
2023-10-26 16:07:45.754722: Pseudo dice [0.8835, 0.9151, 0.9677, 0.757, 0.9289] 
2023-10-26 16:07:45.754947: Epoch time: 3.82 s 
2023-10-26 16:07:46.837199:  
2023-10-26 16:07:46.837479: Epoch 528 
2023-10-26 16:07:46.837715: Current learning rate: 0.00509 
2023-10-26 16:07:50.538300: train_loss -0.8769 
2023-10-26 16:07:50.538679: val_loss -0.8747 
2023-10-26 16:07:50.538950: Pseudo dice [0.887, 0.9194, 0.9693, 0.6825, 0.9283] 
2023-10-26 16:07:50.539181: Epoch time: 3.7 s 
2023-10-26 16:07:51.672610:  
2023-10-26 16:07:51.672914: Epoch 529 
2023-10-26 16:07:51.673156: Current learning rate: 0.00508 
2023-10-26 16:07:55.621445: train_loss -0.8752 
2023-10-26 16:07:55.621814: val_loss -0.8478 
2023-10-26 16:07:55.622292: Pseudo dice [0.8811, 0.9141, 0.9677, 0.2448, 0.9351] 
2023-10-26 16:07:55.622542: Epoch time: 3.95 s 
2023-10-26 16:07:56.872002:  
2023-10-26 16:07:56.872298: Epoch 530 
2023-10-26 16:07:56.872547: Current learning rate: 0.00507 
2023-10-26 16:08:00.849569: train_loss -0.8712 
2023-10-26 16:08:00.849967: val_loss -0.8596 
2023-10-26 16:08:00.850235: Pseudo dice [0.8805, 0.9171, 0.9625, 0.2751, 0.9374] 
2023-10-26 16:08:00.850542: Epoch time: 3.98 s 
2023-10-26 16:08:01.960106:  
2023-10-26 16:08:01.960401: Epoch 531 
2023-10-26 16:08:01.960648: Current learning rate: 0.00506 
2023-10-26 16:08:06.020990: train_loss -0.8722 
2023-10-26 16:08:06.021355: val_loss -0.8739 
2023-10-26 16:08:06.021623: Pseudo dice [0.8837, 0.9168, 0.9657, 0.6118, 0.9257] 
2023-10-26 16:08:06.021863: Epoch time: 4.06 s 
2023-10-26 16:08:07.083243:  
2023-10-26 16:08:07.083530: Epoch 532 
2023-10-26 16:08:07.083804: Current learning rate: 0.00505 
2023-10-26 16:08:11.036960: train_loss -0.8756 
2023-10-26 16:08:11.037349: val_loss -0.8513 
2023-10-26 16:08:11.037611: Pseudo dice [0.8772, 0.9054, 0.9666, 0.1293, 0.9208] 
2023-10-26 16:08:11.037848: Epoch time: 3.95 s 
2023-10-26 16:08:12.143328:  
2023-10-26 16:08:12.143638: Epoch 533 
2023-10-26 16:08:12.143969: Current learning rate: 0.00504 
2023-10-26 16:08:16.130580: train_loss -0.8698 
2023-10-26 16:08:16.130936: val_loss -0.8717 
2023-10-26 16:08:16.131183: Pseudo dice [0.8809, 0.9161, 0.9676, 0.6888, 0.9348] 
2023-10-26 16:08:16.131446: Epoch time: 3.99 s 
2023-10-26 16:08:17.189476:  
2023-10-26 16:08:17.189781: Epoch 534 
2023-10-26 16:08:17.190022: Current learning rate: 0.00503 
2023-10-26 16:08:21.337051: train_loss -0.8727 
2023-10-26 16:08:21.337432: val_loss -0.8753 
2023-10-26 16:08:21.337688: Pseudo dice [0.8782, 0.9127, 0.9675, 0.7674, 0.9282] 
2023-10-26 16:08:21.337924: Epoch time: 4.15 s 
2023-10-26 16:08:22.402811:  
2023-10-26 16:08:22.403102: Epoch 535 
2023-10-26 16:08:22.403355: Current learning rate: 0.00502 
2023-10-26 16:08:26.490819: train_loss -0.8643 
2023-10-26 16:08:26.491220: val_loss -0.8702 
2023-10-26 16:08:26.491485: Pseudo dice [0.8729, 0.9074, 0.9647, 0.774, 0.9335] 
2023-10-26 16:08:26.491718: Epoch time: 4.09 s 
2023-10-26 16:08:27.709624:  
2023-10-26 16:08:27.709976: Epoch 536 
2023-10-26 16:08:27.710229: Current learning rate: 0.00501 
2023-10-26 16:08:31.818068: train_loss -0.8727 
2023-10-26 16:08:31.818447: val_loss -0.8742 
2023-10-26 16:08:31.818694: Pseudo dice [0.8835, 0.9065, 0.9704, 0.6827, 0.9366] 
2023-10-26 16:08:31.818915: Epoch time: 4.11 s 
2023-10-26 16:08:32.882849:  
2023-10-26 16:08:32.883215: Epoch 537 
2023-10-26 16:08:32.883502: Current learning rate: 0.005 
2023-10-26 16:08:36.790301: train_loss -0.8829 
2023-10-26 16:08:36.790701: val_loss -0.8855 
2023-10-26 16:08:36.790957: Pseudo dice [0.8885, 0.9143, 0.9706, 0.7573, 0.9362] 
2023-10-26 16:08:36.791184: Epoch time: 3.91 s 
2023-10-26 16:08:37.865897:  
2023-10-26 16:08:37.866182: Epoch 538 
2023-10-26 16:08:37.866425: Current learning rate: 0.00499 
2023-10-26 16:08:41.862126: train_loss -0.8789 
2023-10-26 16:08:41.862530: val_loss -0.8723 
2023-10-26 16:08:41.862808: Pseudo dice [0.8862, 0.9141, 0.969, 0.7266, 0.9225] 
2023-10-26 16:08:41.863054: Epoch time: 4.0 s 
2023-10-26 16:08:42.986340:  
2023-10-26 16:08:42.986641: Epoch 539 
2023-10-26 16:08:42.986902: Current learning rate: 0.00498 
2023-10-26 16:08:46.962214: train_loss -0.8681 
2023-10-26 16:08:46.962621: val_loss -0.8643 
2023-10-26 16:08:46.962891: Pseudo dice [0.8816, 0.8987, 0.9658, 0.6804, 0.9351] 
2023-10-26 16:08:46.963152: Epoch time: 3.98 s 
2023-10-26 16:08:48.064558:  
2023-10-26 16:08:48.064891: Epoch 540 
2023-10-26 16:08:48.065152: Current learning rate: 0.00497 
2023-10-26 16:08:51.982967: train_loss -0.8696 
2023-10-26 16:08:51.983359: val_loss -0.8824 
2023-10-26 16:08:51.983608: Pseudo dice [0.8852, 0.9164, 0.969, 0.7236, 0.9333] 
2023-10-26 16:08:51.983836: Epoch time: 3.92 s 
2023-10-26 16:08:53.075190:  
2023-10-26 16:08:53.075496: Epoch 541 
2023-10-26 16:08:53.075773: Current learning rate: 0.00496 
2023-10-26 16:08:57.074476: train_loss -0.8771 
2023-10-26 16:08:57.074864: val_loss -0.8775 
2023-10-26 16:08:57.075135: Pseudo dice [0.8869, 0.9159, 0.969, 0.7964, 0.9345] 
2023-10-26 16:08:57.075392: Epoch time: 4.0 s 
2023-10-26 16:08:58.177452:  
2023-10-26 16:08:58.177759: Epoch 542 
2023-10-26 16:08:58.178020: Current learning rate: 0.00495 
2023-10-26 16:09:02.088146: train_loss -0.8781 
2023-10-26 16:09:02.088836: val_loss -0.8793 
2023-10-26 16:09:02.089126: Pseudo dice [0.8892, 0.9154, 0.9677, 0.725, 0.9429] 
2023-10-26 16:09:02.089377: Epoch time: 3.91 s 
2023-10-26 16:09:03.409358:  
2023-10-26 16:09:03.409676: Epoch 543 
2023-10-26 16:09:03.409949: Current learning rate: 0.00494 
2023-10-26 16:09:07.283141: train_loss -0.8837 
2023-10-26 16:09:07.283523: val_loss -0.8834 
2023-10-26 16:09:07.283806: Pseudo dice [0.887, 0.9152, 0.9688, 0.7158, 0.9472] 
2023-10-26 16:09:07.284036: Epoch time: 3.87 s 
2023-10-26 16:09:08.382523:  
2023-10-26 16:09:08.382826: Epoch 544 
2023-10-26 16:09:08.383071: Current learning rate: 0.00493 
2023-10-26 16:09:12.138815: train_loss -0.8755 
2023-10-26 16:09:12.139207: val_loss -0.8548 
2023-10-26 16:09:12.139460: Pseudo dice [0.8815, 0.9103, 0.9641, 0.3853, 0.9276] 
2023-10-26 16:09:12.139694: Epoch time: 3.76 s 
2023-10-26 16:09:13.228328:  
2023-10-26 16:09:13.228679: Epoch 545 
2023-10-26 16:09:13.228992: Current learning rate: 0.00492 
2023-10-26 16:09:17.172755: train_loss -0.862 
2023-10-26 16:09:17.173121: val_loss -0.8736 
2023-10-26 16:09:17.173387: Pseudo dice [0.8769, 0.9093, 0.9663, 0.7453, 0.9258] 
2023-10-26 16:09:17.173618: Epoch time: 3.95 s 
2023-10-26 16:09:18.315035:  
2023-10-26 16:09:18.315343: Epoch 546 
2023-10-26 16:09:18.315587: Current learning rate: 0.00491 
2023-10-26 16:09:22.338559: train_loss -0.8621 
2023-10-26 16:09:22.338918: val_loss -0.8638 
2023-10-26 16:09:22.339170: Pseudo dice [0.8736, 0.9095, 0.9675, 0.6697, 0.9287] 
2023-10-26 16:09:22.339397: Epoch time: 4.02 s 
2023-10-26 16:09:23.406160:  
2023-10-26 16:09:23.406468: Epoch 547 
2023-10-26 16:09:23.406712: Current learning rate: 0.0049 
2023-10-26 16:09:27.286482: train_loss -0.8697 
2023-10-26 16:09:27.286840: val_loss -0.8801 
2023-10-26 16:09:27.287102: Pseudo dice [0.8872, 0.9126, 0.9697, 0.6858, 0.9349] 
2023-10-26 16:09:27.287327: Epoch time: 3.88 s 
2023-10-26 16:09:28.351296:  
2023-10-26 16:09:28.351597: Epoch 548 
2023-10-26 16:09:28.351841: Current learning rate: 0.00489 
2023-10-26 16:09:32.274202: train_loss -0.8777 
2023-10-26 16:09:32.274614: val_loss -0.8655 
2023-10-26 16:09:32.274870: Pseudo dice [0.8733, 0.9007, 0.9666, 0.6356, 0.9236] 
2023-10-26 16:09:32.275101: Epoch time: 3.92 s 
2023-10-26 16:09:33.502115:  
2023-10-26 16:09:33.502424: Epoch 549 
2023-10-26 16:09:33.502662: Current learning rate: 0.00488 
2023-10-26 16:09:37.537296: train_loss -0.8676 
2023-10-26 16:09:37.537663: val_loss -0.8722 
2023-10-26 16:09:37.537934: Pseudo dice [0.8801, 0.9175, 0.9683, 0.7623, 0.904] 
2023-10-26 16:09:37.538171: Epoch time: 4.04 s 
2023-10-26 16:09:38.708937:  
2023-10-26 16:09:38.709243: Epoch 550 
2023-10-26 16:09:38.709502: Current learning rate: 0.00487 
2023-10-26 16:09:42.846879: train_loss -0.8734 
2023-10-26 16:09:42.847275: val_loss -0.8773 
2023-10-26 16:09:42.847553: Pseudo dice [0.8811, 0.9118, 0.966, 0.7017, 0.94] 
2023-10-26 16:09:42.847786: Epoch time: 4.14 s 
2023-10-26 16:09:43.940067:  
2023-10-26 16:09:43.940371: Epoch 551 
2023-10-26 16:09:43.940620: Current learning rate: 0.00486 
2023-10-26 16:09:47.964473: train_loss -0.88 
2023-10-26 16:09:47.964885: val_loss -0.8844 
2023-10-26 16:09:47.965147: Pseudo dice [0.8825, 0.9105, 0.9681, 0.6848, 0.9386] 
2023-10-26 16:09:47.965392: Epoch time: 4.02 s 
2023-10-26 16:09:49.051336:  
2023-10-26 16:09:49.051631: Epoch 552 
2023-10-26 16:09:49.051897: Current learning rate: 0.00485 
2023-10-26 16:09:53.057628: train_loss -0.8801 
2023-10-26 16:09:53.058011: val_loss -0.8744 
2023-10-26 16:09:53.058317: Pseudo dice [0.8864, 0.9117, 0.9696, 0.739, 0.9399] 
2023-10-26 16:09:53.058623: Epoch time: 4.01 s 
2023-10-26 16:09:54.148185:  
2023-10-26 16:09:54.148486: Epoch 553 
2023-10-26 16:09:54.148726: Current learning rate: 0.00484 
2023-10-26 16:09:58.008201: train_loss -0.8796 
2023-10-26 16:09:58.008568: val_loss -0.8767 
2023-10-26 16:09:58.008838: Pseudo dice [0.8838, 0.912, 0.9669, 0.678, 0.9276] 
2023-10-26 16:09:58.009094: Epoch time: 3.86 s 
2023-10-26 16:09:59.096745:  
2023-10-26 16:09:59.097046: Epoch 554 
2023-10-26 16:09:59.097287: Current learning rate: 0.00484 
2023-10-26 16:10:03.038048: train_loss -0.8761 
2023-10-26 16:10:03.038439: val_loss -0.8812 
2023-10-26 16:10:03.038697: Pseudo dice [0.8874, 0.9176, 0.9681, 0.7101, 0.9343] 
2023-10-26 16:10:03.038925: Epoch time: 3.94 s 
2023-10-26 16:10:04.146169:  
2023-10-26 16:10:04.146471: Epoch 555 
2023-10-26 16:10:04.146718: Current learning rate: 0.00483 
2023-10-26 16:10:08.123511: train_loss -0.8826 
2023-10-26 16:10:08.123898: val_loss -0.8824 
2023-10-26 16:10:08.124190: Pseudo dice [0.8858, 0.9117, 0.9682, 0.7556, 0.9262] 
2023-10-26 16:10:08.124437: Epoch time: 3.98 s 
2023-10-26 16:10:09.417457:  
2023-10-26 16:10:09.417779: Epoch 556 
2023-10-26 16:10:09.418047: Current learning rate: 0.00482 
2023-10-26 16:10:13.464502: train_loss -0.8721 
2023-10-26 16:10:13.465068: val_loss -0.8753 
2023-10-26 16:10:13.465511: Pseudo dice [0.881, 0.9133, 0.9682, 0.7619, 0.9246] 
2023-10-26 16:10:13.465794: Epoch time: 4.05 s 
2023-10-26 16:10:14.542770:  
2023-10-26 16:10:14.543069: Epoch 557 
2023-10-26 16:10:14.543313: Current learning rate: 0.00481 
2023-10-26 16:10:18.477521: train_loss -0.8675 
2023-10-26 16:10:18.478203: val_loss -0.8772 
2023-10-26 16:10:18.478472: Pseudo dice [0.8813, 0.9213, 0.9702, 0.6132, 0.9334] 
2023-10-26 16:10:18.478821: Epoch time: 3.94 s 
2023-10-26 16:10:19.586568:  
2023-10-26 16:10:19.586859: Epoch 558 
2023-10-26 16:10:19.587109: Current learning rate: 0.0048 
2023-10-26 16:10:23.501870: train_loss -0.8784 
2023-10-26 16:10:23.502252: val_loss -0.8725 
2023-10-26 16:10:23.502509: Pseudo dice [0.8848, 0.9149, 0.97, 0.634, 0.9381] 
2023-10-26 16:10:23.502729: Epoch time: 3.92 s 
2023-10-26 16:10:24.607249:  
2023-10-26 16:10:24.607543: Epoch 559 
2023-10-26 16:10:24.607778: Current learning rate: 0.00479 
2023-10-26 16:10:28.499222: train_loss -0.8772 
2023-10-26 16:10:28.499615: val_loss -0.8775 
2023-10-26 16:10:28.499868: Pseudo dice [0.8832, 0.9131, 0.9685, 0.7327, 0.9357] 
2023-10-26 16:10:28.500124: Epoch time: 3.89 s 
2023-10-26 16:10:29.615101:  
2023-10-26 16:10:29.615485: Epoch 560 
2023-10-26 16:10:29.615779: Current learning rate: 0.00478 
2023-10-26 16:10:33.421857: train_loss -0.8805 
2023-10-26 16:10:33.422222: val_loss -0.8779 
2023-10-26 16:10:33.422469: Pseudo dice [0.883, 0.9148, 0.9704, 0.6405, 0.9407] 
2023-10-26 16:10:33.422682: Epoch time: 3.81 s 
2023-10-26 16:10:34.506818:  
2023-10-26 16:10:34.507097: Epoch 561 
2023-10-26 16:10:34.507340: Current learning rate: 0.00477 
2023-10-26 16:10:38.517085: train_loss -0.8827 
2023-10-26 16:10:38.517463: val_loss -0.8815 
2023-10-26 16:10:38.517742: Pseudo dice [0.8874, 0.9141, 0.9699, 0.7706, 0.9336] 
2023-10-26 16:10:38.517983: Epoch time: 4.01 s 
2023-10-26 16:10:39.603706:  
2023-10-26 16:10:39.604065: Epoch 562 
2023-10-26 16:10:39.604312: Current learning rate: 0.00476 
2023-10-26 16:10:43.817362: train_loss -0.885 
2023-10-26 16:10:43.817726: val_loss -0.8809 
2023-10-26 16:10:43.817995: Pseudo dice [0.8852, 0.9132, 0.9693, 0.7702, 0.9372] 
2023-10-26 16:10:43.818211: Epoch time: 4.21 s 
2023-10-26 16:10:44.961188:  
2023-10-26 16:10:44.961509: Epoch 563 
2023-10-26 16:10:44.961748: Current learning rate: 0.00475 
2023-10-26 16:10:48.925845: train_loss -0.8791 
2023-10-26 16:10:48.926269: val_loss -0.8729 
2023-10-26 16:10:48.926533: Pseudo dice [0.8825, 0.9201, 0.9696, 0.6699, 0.9285] 
2023-10-26 16:10:48.926745: Epoch time: 3.97 s 
2023-10-26 16:10:49.999393:  
2023-10-26 16:10:49.999688: Epoch 564 
2023-10-26 16:10:49.999932: Current learning rate: 0.00474 
2023-10-26 16:10:54.128250: train_loss -0.8738 
2023-10-26 16:10:54.128618: val_loss -0.8546 
2023-10-26 16:10:54.128883: Pseudo dice [0.8823, 0.9142, 0.966, 0.4839, 0.9278] 
2023-10-26 16:10:54.129118: Epoch time: 4.13 s 
2023-10-26 16:10:55.202684:  
2023-10-26 16:10:55.202983: Epoch 565 
2023-10-26 16:10:55.203227: Current learning rate: 0.00473 
2023-10-26 16:10:59.290997: train_loss -0.877 
2023-10-26 16:10:59.291361: val_loss -0.8831 
2023-10-26 16:10:59.291604: Pseudo dice [0.8886, 0.9154, 0.9694, 0.7233, 0.9336] 
2023-10-26 16:10:59.291818: Epoch time: 4.09 s 
2023-10-26 16:11:00.356608:  
2023-10-26 16:11:00.356905: Epoch 566 
2023-10-26 16:11:00.357149: Current learning rate: 0.00472 
2023-10-26 16:11:04.299299: train_loss -0.8854 
2023-10-26 16:11:04.299732: val_loss -0.8815 
2023-10-26 16:11:04.300012: Pseudo dice [0.8852, 0.9112, 0.9679, 0.7435, 0.9337] 
2023-10-26 16:11:04.300253: Epoch time: 3.94 s 
2023-10-26 16:11:05.422394:  
2023-10-26 16:11:05.422684: Epoch 567 
2023-10-26 16:11:05.422935: Current learning rate: 0.00471 
2023-10-26 16:11:09.301073: train_loss -0.8811 
2023-10-26 16:11:09.301497: val_loss -0.8777 
2023-10-26 16:11:09.301933: Pseudo dice [0.8842, 0.913, 0.968, 0.7172, 0.9343] 
2023-10-26 16:11:09.302250: Epoch time: 3.88 s 
2023-10-26 16:11:10.380308:  
2023-10-26 16:11:10.380589: Epoch 568 
2023-10-26 16:11:10.380824: Current learning rate: 0.0047 
2023-10-26 16:11:14.272588: train_loss -0.8763 
2023-10-26 16:11:14.272997: val_loss -0.8749 
2023-10-26 16:11:14.273256: Pseudo dice [0.8827, 0.9151, 0.9673, 0.7351, 0.9323] 
2023-10-26 16:11:14.273493: Epoch time: 3.89 s 
2023-10-26 16:11:15.589399:  
2023-10-26 16:11:15.589693: Epoch 569 
2023-10-26 16:11:15.589940: Current learning rate: 0.00469 
2023-10-26 16:11:19.288197: train_loss -0.8797 
2023-10-26 16:11:19.288565: val_loss -0.8766 
2023-10-26 16:11:19.288823: Pseudo dice [0.879, 0.9156, 0.9676, 0.7329, 0.9391] 
2023-10-26 16:11:19.289053: Epoch time: 3.7 s 
2023-10-26 16:11:20.362650:  
2023-10-26 16:11:20.362961: Epoch 570 
2023-10-26 16:11:20.363205: Current learning rate: 0.00468 
2023-10-26 16:11:24.400113: train_loss -0.8738 
2023-10-26 16:11:24.400596: val_loss -0.8799 
2023-10-26 16:11:24.400913: Pseudo dice [0.8844, 0.9137, 0.9687, 0.7165, 0.9356] 
2023-10-26 16:11:24.401211: Epoch time: 4.04 s 
2023-10-26 16:11:25.549698:  
2023-10-26 16:11:25.550010: Epoch 571 
2023-10-26 16:11:25.550256: Current learning rate: 0.00467 
2023-10-26 16:11:29.427416: train_loss -0.8755 
2023-10-26 16:11:29.427783: val_loss -0.8686 
2023-10-26 16:11:29.428052: Pseudo dice [0.8827, 0.917, 0.9673, 0.7163, 0.9233] 
2023-10-26 16:11:29.428273: Epoch time: 3.88 s 
2023-10-26 16:11:30.556793:  
2023-10-26 16:11:30.557103: Epoch 572 
2023-10-26 16:11:30.557358: Current learning rate: 0.00466 
2023-10-26 16:11:34.469788: train_loss -0.8793 
2023-10-26 16:11:34.470173: val_loss -0.8767 
2023-10-26 16:11:34.470425: Pseudo dice [0.8836, 0.9171, 0.9679, 0.7455, 0.9278] 
2023-10-26 16:11:34.470651: Epoch time: 3.91 s 
2023-10-26 16:11:35.567491:  
2023-10-26 16:11:35.567791: Epoch 573 
2023-10-26 16:11:35.568040: Current learning rate: 0.00465 
2023-10-26 16:11:39.401953: train_loss -0.8882 
2023-10-26 16:11:39.402359: val_loss -0.8853 
2023-10-26 16:11:39.402626: Pseudo dice [0.8867, 0.9115, 0.9698, 0.7326, 0.9387] 
2023-10-26 16:11:39.402886: Epoch time: 3.84 s 
2023-10-26 16:11:40.507591:  
2023-10-26 16:11:40.507907: Epoch 574 
2023-10-26 16:11:40.508173: Current learning rate: 0.00464 
2023-10-26 16:11:44.365880: train_loss -0.8846 
2023-10-26 16:11:44.366228: val_loss -0.8775 
2023-10-26 16:11:44.366482: Pseudo dice [0.8786, 0.915, 0.9667, 0.7812, 0.9362] 
2023-10-26 16:11:44.366709: Epoch time: 3.86 s 
2023-10-26 16:11:45.511778:  
2023-10-26 16:11:45.512083: Epoch 575 
2023-10-26 16:11:45.512335: Current learning rate: 0.00463 
2023-10-26 16:11:49.376229: train_loss -0.8825 
2023-10-26 16:11:49.376587: val_loss -0.8766 
2023-10-26 16:11:49.376894: Pseudo dice [0.8813, 0.9132, 0.9688, 0.6511, 0.9394] 
2023-10-26 16:11:49.377115: Epoch time: 3.87 s 
2023-10-26 16:11:50.709548:  
2023-10-26 16:11:50.709855: Epoch 576 
2023-10-26 16:11:50.710111: Current learning rate: 0.00462 
2023-10-26 16:11:54.649529: train_loss -0.8786 
2023-10-26 16:11:54.649903: val_loss -0.8742 
2023-10-26 16:11:54.650154: Pseudo dice [0.8799, 0.9165, 0.9676, 0.7407, 0.9314] 
2023-10-26 16:11:54.650374: Epoch time: 3.94 s 
2023-10-26 16:11:55.746221:  
2023-10-26 16:11:55.746512: Epoch 577 
2023-10-26 16:11:55.746755: Current learning rate: 0.00461 
2023-10-26 16:11:59.664393: train_loss -0.8789 
2023-10-26 16:11:59.664761: val_loss -0.878 
2023-10-26 16:11:59.665023: Pseudo dice [0.8853, 0.914, 0.9695, 0.6822, 0.938] 
2023-10-26 16:11:59.665253: Epoch time: 3.92 s 
2023-10-26 16:12:00.748420:  
2023-10-26 16:12:00.748734: Epoch 578 
2023-10-26 16:12:00.748986: Current learning rate: 0.0046 
2023-10-26 16:12:04.670697: train_loss -0.8772 
2023-10-26 16:12:04.671087: val_loss -0.8708 
2023-10-26 16:12:04.671344: Pseudo dice [0.8818, 0.9168, 0.9686, 0.6007, 0.9307] 
2023-10-26 16:12:04.671572: Epoch time: 3.92 s 
2023-10-26 16:12:05.756175:  
2023-10-26 16:12:05.756486: Epoch 579 
2023-10-26 16:12:05.756737: Current learning rate: 0.00459 
2023-10-26 16:12:09.702922: train_loss -0.8736 
2023-10-26 16:12:09.703309: val_loss -0.8639 
2023-10-26 16:12:09.703576: Pseudo dice [0.8753, 0.9113, 0.9663, 0.6815, 0.9288] 
2023-10-26 16:12:09.703822: Epoch time: 3.95 s 
2023-10-26 16:12:10.789740:  
2023-10-26 16:12:10.790060: Epoch 580 
2023-10-26 16:12:10.790313: Current learning rate: 0.00458 
2023-10-26 16:12:14.795282: train_loss -0.872 
2023-10-26 16:12:14.795658: val_loss -0.8808 
2023-10-26 16:12:14.795915: Pseudo dice [0.8804, 0.9074, 0.9712, 0.6983, 0.9391] 
2023-10-26 16:12:14.796146: Epoch time: 4.01 s 
2023-10-26 16:12:15.882339:  
2023-10-26 16:12:15.882632: Epoch 581 
2023-10-26 16:12:15.882977: Current learning rate: 0.00457 
2023-10-26 16:12:19.564676: train_loss -0.8833 
2023-10-26 16:12:19.565065: val_loss -0.882 
2023-10-26 16:12:19.565323: Pseudo dice [0.8825, 0.916, 0.9704, 0.7674, 0.928] 
2023-10-26 16:12:19.565543: Epoch time: 3.68 s 
2023-10-26 16:12:20.817105:  
2023-10-26 16:12:20.817415: Epoch 582 
2023-10-26 16:12:20.817653: Current learning rate: 0.00456 
2023-10-26 16:12:24.567751: train_loss -0.8804 
2023-10-26 16:12:24.568316: val_loss -0.872 
2023-10-26 16:12:24.568723: Pseudo dice [0.8786, 0.9076, 0.9685, 0.7052, 0.934] 
2023-10-26 16:12:24.569203: Epoch time: 3.75 s 
2023-10-26 16:12:25.665828:  
2023-10-26 16:12:25.666134: Epoch 583 
2023-10-26 16:12:25.666374: Current learning rate: 0.00455 
2023-10-26 16:12:29.613517: train_loss -0.8837 
2023-10-26 16:12:29.613868: val_loss -0.8782 
2023-10-26 16:12:29.614126: Pseudo dice [0.8817, 0.9106, 0.9686, 0.7203, 0.9315] 
2023-10-26 16:12:29.614351: Epoch time: 3.95 s 
2023-10-26 16:12:30.717904:  
2023-10-26 16:12:30.718211: Epoch 584 
2023-10-26 16:12:30.718465: Current learning rate: 0.00454 
2023-10-26 16:12:34.647627: train_loss -0.882 
2023-10-26 16:12:34.648036: val_loss -0.8817 
2023-10-26 16:12:34.648300: Pseudo dice [0.8829, 0.9173, 0.9702, 0.693, 0.9339] 
2023-10-26 16:12:34.648563: Epoch time: 3.93 s 
2023-10-26 16:12:35.749355:  
2023-10-26 16:12:35.749648: Epoch 585 
2023-10-26 16:12:35.749898: Current learning rate: 0.00453 
2023-10-26 16:12:39.815578: train_loss -0.8843 
2023-10-26 16:12:39.815987: val_loss -0.8747 
2023-10-26 16:12:39.816262: Pseudo dice [0.8796, 0.9115, 0.9703, 0.7362, 0.9199] 
2023-10-26 16:12:39.816513: Epoch time: 4.07 s 
2023-10-26 16:12:40.932371:  
2023-10-26 16:12:40.932677: Epoch 586 
2023-10-26 16:12:40.932935: Current learning rate: 0.00452 
2023-10-26 16:12:44.896808: train_loss -0.8814 
2023-10-26 16:12:44.897178: val_loss -0.8866 
2023-10-26 16:12:44.897422: Pseudo dice [0.8832, 0.9046, 0.9688, 0.7184, 0.9299] 
2023-10-26 16:12:44.897647: Epoch time: 3.97 s 
2023-10-26 16:12:45.985884:  
2023-10-26 16:12:45.986176: Epoch 587 
2023-10-26 16:12:45.986422: Current learning rate: 0.00451 
2023-10-26 16:12:49.817231: train_loss -0.8822 
2023-10-26 16:12:49.817649: val_loss -0.8713 
2023-10-26 16:12:49.818276: Pseudo dice [0.8782, 0.9079, 0.9683, 0.661, 0.9248] 
2023-10-26 16:12:49.818554: Epoch time: 3.83 s 
2023-10-26 16:12:51.097782:  
2023-10-26 16:12:51.098095: Epoch 588 
2023-10-26 16:12:51.098346: Current learning rate: 0.0045 
2023-10-26 16:12:55.089195: train_loss -0.885 
2023-10-26 16:12:55.089552: val_loss -0.8799 
2023-10-26 16:12:55.089805: Pseudo dice [0.8847, 0.9177, 0.9689, 0.7074, 0.9282] 
2023-10-26 16:12:55.090046: Epoch time: 3.99 s 
2023-10-26 16:12:56.300928:  
2023-10-26 16:12:56.301223: Epoch 589 
2023-10-26 16:12:56.301471: Current learning rate: 0.00449 
2023-10-26 16:13:00.165654: train_loss -0.8829 
2023-10-26 16:13:00.166047: val_loss -0.8821 
2023-10-26 16:13:00.166298: Pseudo dice [0.8837, 0.913, 0.9713, 0.7593, 0.9253] 
2023-10-26 16:13:00.166521: Epoch time: 3.87 s 
2023-10-26 16:13:01.258325:  
2023-10-26 16:13:01.258622: Epoch 590 
2023-10-26 16:13:01.258882: Current learning rate: 0.00448 
2023-10-26 16:13:05.234082: train_loss -0.8862 
2023-10-26 16:13:05.234511: val_loss -0.8745 
2023-10-26 16:13:05.234806: Pseudo dice [0.8783, 0.9121, 0.9683, 0.6045, 0.9341] 
2023-10-26 16:13:05.235066: Epoch time: 3.98 s 
2023-10-26 16:13:06.343495:  
2023-10-26 16:13:06.343790: Epoch 591 
2023-10-26 16:13:06.344033: Current learning rate: 0.00447 
2023-10-26 16:13:10.283003: train_loss -0.8836 
2023-10-26 16:13:10.283371: val_loss -0.8744 
2023-10-26 16:13:10.283623: Pseudo dice [0.877, 0.9146, 0.9685, 0.6903, 0.9165] 
2023-10-26 16:13:10.283849: Epoch time: 3.94 s 
2023-10-26 16:13:11.389533:  
2023-10-26 16:13:11.389828: Epoch 592 
2023-10-26 16:13:11.390065: Current learning rate: 0.00446 
2023-10-26 16:13:15.379319: train_loss -0.8837 
2023-10-26 16:13:15.379678: val_loss -0.8806 
2023-10-26 16:13:15.379946: Pseudo dice [0.8793, 0.9133, 0.9699, 0.6528, 0.9305] 
2023-10-26 16:13:15.380183: Epoch time: 3.99 s 
2023-10-26 16:13:16.497744:  
2023-10-26 16:13:16.498073: Epoch 593 
2023-10-26 16:13:16.498314: Current learning rate: 0.00445 
2023-10-26 16:13:20.352383: train_loss -0.8833 
2023-10-26 16:13:20.352745: val_loss -0.8748 
2023-10-26 16:13:20.352999: Pseudo dice [0.8836, 0.9167, 0.9675, 0.6255, 0.9369] 
2023-10-26 16:13:20.353221: Epoch time: 3.86 s 
2023-10-26 16:13:21.461858:  
2023-10-26 16:13:21.462443: Epoch 594 
2023-10-26 16:13:21.462709: Current learning rate: 0.00444 
2023-10-26 16:13:25.383579: train_loss -0.8759 
2023-10-26 16:13:25.383987: val_loss -0.8364 
2023-10-26 16:13:25.384249: Pseudo dice [0.8732, 0.9113, 0.9657, 0.6691, 0.9025] 
2023-10-26 16:13:25.384521: Epoch time: 3.92 s 
2023-10-26 16:13:26.621367:  
2023-10-26 16:13:26.621709: Epoch 595 
2023-10-26 16:13:26.622030: Current learning rate: 0.00443 
2023-10-26 16:13:30.395204: train_loss -0.8823 
2023-10-26 16:13:30.395589: val_loss -0.8727 
2023-10-26 16:13:30.395848: Pseudo dice [0.8825, 0.9151, 0.9677, 0.7273, 0.9304] 
2023-10-26 16:13:30.396087: Epoch time: 3.77 s 
2023-10-26 16:13:31.491233:  
2023-10-26 16:13:31.491528: Epoch 596 
2023-10-26 16:13:31.491775: Current learning rate: 0.00442 
2023-10-26 16:13:35.259362: train_loss -0.8732 
2023-10-26 16:13:35.259879: val_loss -0.8712 
2023-10-26 16:13:35.260208: Pseudo dice [0.8756, 0.9089, 0.9691, 0.6774, 0.926] 
2023-10-26 16:13:35.260515: Epoch time: 3.77 s 
2023-10-26 16:13:36.397332:  
2023-10-26 16:13:36.397619: Epoch 597 
2023-10-26 16:13:36.397884: Current learning rate: 0.00441 
2023-10-26 16:13:40.347352: train_loss -0.8737 
2023-10-26 16:13:40.347766: val_loss -0.8779 
2023-10-26 16:13:40.348040: Pseudo dice [0.8846, 0.914, 0.9703, 0.7585, 0.9331] 
2023-10-26 16:13:40.348294: Epoch time: 3.95 s 
2023-10-26 16:13:41.501166:  
2023-10-26 16:13:41.501467: Epoch 598 
2023-10-26 16:13:41.501702: Current learning rate: 0.0044 
2023-10-26 16:13:45.223912: train_loss -0.8783 
2023-10-26 16:13:45.224352: val_loss -0.8705 
2023-10-26 16:13:45.224625: Pseudo dice [0.8767, 0.9049, 0.9681, 0.671, 0.9279] 
2023-10-26 16:13:45.224848: Epoch time: 3.72 s 
2023-10-26 16:13:46.336951:  
2023-10-26 16:13:46.337240: Epoch 599 
2023-10-26 16:13:46.337483: Current learning rate: 0.00439 
2023-10-26 16:13:50.194822: train_loss -0.8741 
2023-10-26 16:13:50.195213: val_loss -0.8679 
2023-10-26 16:13:50.195473: Pseudo dice [0.8827, 0.9047, 0.9692, 0.6574, 0.9255] 
2023-10-26 16:13:50.195704: Epoch time: 3.86 s 
2023-10-26 16:13:51.453691:  
2023-10-26 16:13:51.454019: Epoch 600 
2023-10-26 16:13:51.454281: Current learning rate: 0.00438 
2023-10-26 16:13:55.311998: train_loss -0.8826 
2023-10-26 16:13:55.312362: val_loss -0.8771 
2023-10-26 16:13:55.312605: Pseudo dice [0.8851, 0.9178, 0.9672, 0.6496, 0.9234] 
2023-10-26 16:13:55.312818: Epoch time: 3.86 s 
2023-10-26 16:13:56.613445:  
2023-10-26 16:13:56.613752: Epoch 601 
2023-10-26 16:13:56.614003: Current learning rate: 0.00437 
2023-10-26 16:14:00.455256: train_loss -0.8849 
2023-10-26 16:14:00.455649: val_loss -0.8772 
2023-10-26 16:14:00.455921: Pseudo dice [0.8846, 0.9025, 0.9675, 0.725, 0.9448] 
2023-10-26 16:14:00.456167: Epoch time: 3.84 s 
2023-10-26 16:14:01.608006:  
2023-10-26 16:14:01.608312: Epoch 602 
2023-10-26 16:14:01.608553: Current learning rate: 0.00436 
2023-10-26 16:14:05.502317: train_loss -0.8753 
2023-10-26 16:14:05.502782: val_loss -0.8861 
2023-10-26 16:14:05.503078: Pseudo dice [0.8851, 0.9101, 0.9695, 0.7785, 0.936] 
2023-10-26 16:14:05.503316: Epoch time: 3.89 s 
2023-10-26 16:14:06.613463:  
2023-10-26 16:14:06.613759: Epoch 603 
2023-10-26 16:14:06.614005: Current learning rate: 0.00435 
2023-10-26 16:14:10.400859: train_loss -0.8868 
2023-10-26 16:14:10.401280: val_loss -0.8691 
2023-10-26 16:14:10.401547: Pseudo dice [0.8845, 0.9105, 0.9688, 0.6711, 0.9291] 
2023-10-26 16:14:10.401791: Epoch time: 3.79 s 
2023-10-26 16:14:11.522582:  
2023-10-26 16:14:11.522957: Epoch 604 
2023-10-26 16:14:11.523354: Current learning rate: 0.00434 
2023-10-26 16:14:15.359148: train_loss -0.8855 
2023-10-26 16:14:15.359529: val_loss -0.8684 
2023-10-26 16:14:15.359792: Pseudo dice [0.8732, 0.9119, 0.9708, 0.5504, 0.9101] 
2023-10-26 16:14:15.360308: Epoch time: 3.84 s 
2023-10-26 16:14:16.477985:  
2023-10-26 16:14:16.478310: Epoch 605 
2023-10-26 16:14:16.478609: Current learning rate: 0.00433 
2023-10-26 16:14:20.452453: train_loss -0.8739 
2023-10-26 16:14:20.452821: val_loss -0.8859 
2023-10-26 16:14:20.453072: Pseudo dice [0.8786, 0.9152, 0.9691, 0.8124, 0.9332] 
2023-10-26 16:14:20.453318: Epoch time: 3.98 s 
2023-10-26 16:14:21.595738:  
2023-10-26 16:14:21.596038: Epoch 606 
2023-10-26 16:14:21.596288: Current learning rate: 0.00432 
2023-10-26 16:14:25.561400: train_loss -0.8809 
2023-10-26 16:14:25.561778: val_loss -0.8775 
2023-10-26 16:14:25.562030: Pseudo dice [0.8815, 0.9135, 0.9706, 0.72, 0.9385] 
2023-10-26 16:14:25.562280: Epoch time: 3.97 s 
2023-10-26 16:14:26.809090:  
2023-10-26 16:14:26.809377: Epoch 607 
2023-10-26 16:14:26.809612: Current learning rate: 0.00431 
2023-10-26 16:14:30.838725: train_loss -0.883 
2023-10-26 16:14:30.839143: val_loss -0.8612 
2023-10-26 16:14:30.839425: Pseudo dice [0.8793, 0.9128, 0.967, 0.4501, 0.9312] 
2023-10-26 16:14:30.839667: Epoch time: 4.03 s 
2023-10-26 16:14:31.956397:  
2023-10-26 16:14:31.956722: Epoch 608 
2023-10-26 16:14:31.956973: Current learning rate: 0.0043 
2023-10-26 16:14:36.081002: train_loss -0.8794 
2023-10-26 16:14:36.081550: val_loss -0.8748 
2023-10-26 16:14:36.081811: Pseudo dice [0.8767, 0.9178, 0.9689, 0.7191, 0.9223] 
2023-10-26 16:14:36.082034: Epoch time: 4.13 s 
2023-10-26 16:14:37.173764:  
2023-10-26 16:14:37.174072: Epoch 609 
2023-10-26 16:14:37.174320: Current learning rate: 0.00429 
2023-10-26 16:14:41.245606: train_loss -0.883 
2023-10-26 16:14:41.245968: val_loss -0.8737 
2023-10-26 16:14:41.246225: Pseudo dice [0.8841, 0.9125, 0.9691, 0.6573, 0.9212] 
2023-10-26 16:14:41.246469: Epoch time: 4.07 s 
2023-10-26 16:14:42.324022:  
2023-10-26 16:14:42.324305: Epoch 610 
2023-10-26 16:14:42.324537: Current learning rate: 0.00429 
2023-10-26 16:14:46.514529: train_loss -0.8871 
2023-10-26 16:14:46.514966: val_loss -0.8586 
2023-10-26 16:14:46.515324: Pseudo dice [0.8778, 0.9106, 0.9693, 0.3532, 0.9371] 
2023-10-26 16:14:46.515635: Epoch time: 4.19 s 
2023-10-26 16:14:47.623664:  
2023-10-26 16:14:47.624003: Epoch 611 
2023-10-26 16:14:47.624310: Current learning rate: 0.00428 
2023-10-26 16:14:51.662679: train_loss -0.8769 
2023-10-26 16:14:51.663107: val_loss -0.8799 
2023-10-26 16:14:51.663385: Pseudo dice [0.8798, 0.9162, 0.9705, 0.6519, 0.9259] 
2023-10-26 16:14:51.663643: Epoch time: 4.04 s 
2023-10-26 16:14:52.806511:  
2023-10-26 16:14:52.806836: Epoch 612 
2023-10-26 16:14:52.807094: Current learning rate: 0.00427 
2023-10-26 16:14:56.720753: train_loss -0.8814 
2023-10-26 16:14:56.721138: val_loss -0.8753 
2023-10-26 16:14:56.721444: Pseudo dice [0.8786, 0.9148, 0.9686, 0.67, 0.9253] 
2023-10-26 16:14:56.721675: Epoch time: 3.91 s 
2023-10-26 16:14:57.825329:  
2023-10-26 16:14:57.825615: Epoch 613 
2023-10-26 16:14:57.825845: Current learning rate: 0.00426 
2023-10-26 16:15:01.871674: train_loss -0.8691 
2023-10-26 16:15:01.872087: val_loss -0.874 
2023-10-26 16:15:01.872353: Pseudo dice [0.8779, 0.9146, 0.9678, 0.7654, 0.9252] 
2023-10-26 16:15:01.872624: Epoch time: 4.05 s 
2023-10-26 16:15:03.189483:  
2023-10-26 16:15:03.189785: Epoch 614 
2023-10-26 16:15:03.190040: Current learning rate: 0.00425 
2023-10-26 16:15:07.124847: train_loss -0.8806 
2023-10-26 16:15:07.125250: val_loss -0.8769 
2023-10-26 16:15:07.125503: Pseudo dice [0.8832, 0.92, 0.9706, 0.7373, 0.9322] 
2023-10-26 16:15:07.125751: Epoch time: 3.94 s 
2023-10-26 16:15:08.228632:  
2023-10-26 16:15:08.228966: Epoch 615 
2023-10-26 16:15:08.229235: Current learning rate: 0.00424 
2023-10-26 16:15:12.229082: train_loss -0.874 
2023-10-26 16:15:12.229451: val_loss -0.8796 
2023-10-26 16:15:12.229693: Pseudo dice [0.8828, 0.9133, 0.9697, 0.8053, 0.9178] 
2023-10-26 16:15:12.229916: Epoch time: 4.0 s 
2023-10-26 16:15:13.376627:  
2023-10-26 16:15:13.376929: Epoch 616 
2023-10-26 16:15:13.377167: Current learning rate: 0.00423 
2023-10-26 16:15:17.179008: train_loss -0.8807 
2023-10-26 16:15:17.179408: val_loss -0.8747 
2023-10-26 16:15:17.179676: Pseudo dice [0.8811, 0.9136, 0.9668, 0.7867, 0.9233] 
2023-10-26 16:15:17.179916: Epoch time: 3.8 s 
2023-10-26 16:15:18.361278:  
2023-10-26 16:15:18.361580: Epoch 617 
2023-10-26 16:15:18.361809: Current learning rate: 0.00422 
2023-10-26 16:15:22.264016: train_loss -0.8825 
2023-10-26 16:15:22.264418: val_loss -0.8709 
2023-10-26 16:15:22.264720: Pseudo dice [0.8793, 0.9093, 0.9658, 0.7463, 0.9244] 
2023-10-26 16:15:22.264947: Epoch time: 3.9 s 
2023-10-26 16:15:23.371618:  
2023-10-26 16:15:23.371917: Epoch 618 
2023-10-26 16:15:23.372190: Current learning rate: 0.00421 
2023-10-26 16:15:27.318173: train_loss -0.8658 
2023-10-26 16:15:27.318575: val_loss -0.8727 
2023-10-26 16:15:27.318837: Pseudo dice [0.8787, 0.9129, 0.969, 0.7615, 0.9289] 
2023-10-26 16:15:27.319081: Epoch time: 3.95 s 
2023-10-26 16:15:28.485465:  
2023-10-26 16:15:28.485766: Epoch 619 
2023-10-26 16:15:28.486013: Current learning rate: 0.0042 
2023-10-26 16:15:32.423269: train_loss -0.8749 
2023-10-26 16:15:32.423645: val_loss -0.8747 
2023-10-26 16:15:32.423911: Pseudo dice [0.8849, 0.9203, 0.97, 0.625, 0.9264] 
2023-10-26 16:15:32.424161: Epoch time: 3.94 s 
2023-10-26 16:15:33.713510:  
2023-10-26 16:15:33.713820: Epoch 620 
2023-10-26 16:15:33.714070: Current learning rate: 0.00419 
2023-10-26 16:15:37.635274: train_loss -0.8806 
2023-10-26 16:15:37.635652: val_loss -0.8683 
2023-10-26 16:15:37.635945: Pseudo dice [0.8823, 0.9199, 0.9709, 0.6822, 0.9371] 
2023-10-26 16:15:37.636287: Epoch time: 3.92 s 
2023-10-26 16:15:38.752425:  
2023-10-26 16:15:38.752713: Epoch 621 
2023-10-26 16:15:38.752948: Current learning rate: 0.00418 
2023-10-26 16:15:42.592878: train_loss -0.8867 
2023-10-26 16:15:42.595591: val_loss -0.8746 
2023-10-26 16:15:42.595894: Pseudo dice [0.8803, 0.9197, 0.969, 0.7243, 0.9221] 
2023-10-26 16:15:42.596139: Epoch time: 3.84 s 
2023-10-26 16:15:43.694291:  
2023-10-26 16:15:43.694595: Epoch 622 
2023-10-26 16:15:43.694846: Current learning rate: 0.00417 
2023-10-26 16:15:47.661649: train_loss -0.8904 
2023-10-26 16:15:47.662040: val_loss -0.8796 
2023-10-26 16:15:47.662294: Pseudo dice [0.882, 0.9097, 0.9694, 0.7519, 0.9272] 
2023-10-26 16:15:47.662522: Epoch time: 3.97 s 
2023-10-26 16:15:48.769131:  
2023-10-26 16:15:48.769436: Epoch 623 
2023-10-26 16:15:48.769683: Current learning rate: 0.00416 
2023-10-26 16:15:52.687867: train_loss -0.8807 
2023-10-26 16:15:52.688233: val_loss -0.8632 
2023-10-26 16:15:52.688484: Pseudo dice [0.8828, 0.9118, 0.9688, 0.5464, 0.9343] 
2023-10-26 16:15:52.688703: Epoch time: 3.92 s 
2023-10-26 16:15:53.832655:  
2023-10-26 16:15:53.832978: Epoch 624 
2023-10-26 16:15:53.833226: Current learning rate: 0.00415 
2023-10-26 16:15:57.769156: train_loss -0.8788 
2023-10-26 16:15:57.769529: val_loss -0.877 
2023-10-26 16:15:57.769789: Pseudo dice [0.8886, 0.9153, 0.971, 0.6957, 0.9325] 
2023-10-26 16:15:57.770025: Epoch time: 3.94 s 
2023-10-26 16:15:58.861110:  
2023-10-26 16:15:58.861411: Epoch 625 
2023-10-26 16:15:58.861651: Current learning rate: 0.00414 
2023-10-26 16:16:02.793381: train_loss -0.8846 
2023-10-26 16:16:02.793760: val_loss -0.8716 
2023-10-26 16:16:02.794027: Pseudo dice [0.8818, 0.9127, 0.9674, 0.6992, 0.9216] 
2023-10-26 16:16:02.794277: Epoch time: 3.93 s 
2023-10-26 16:16:03.912191:  
2023-10-26 16:16:03.912551: Epoch 626 
2023-10-26 16:16:03.912804: Current learning rate: 0.00413 
2023-10-26 16:16:07.727445: train_loss -0.8836 
2023-10-26 16:16:07.727829: val_loss -0.8784 
2023-10-26 16:16:07.728096: Pseudo dice [0.8821, 0.9125, 0.969, 0.684, 0.9406] 
2023-10-26 16:16:07.728312: Epoch time: 3.82 s 
2023-10-26 16:16:08.965937:  
2023-10-26 16:16:08.966231: Epoch 627 
2023-10-26 16:16:08.966477: Current learning rate: 0.00412 
2023-10-26 16:16:12.851536: train_loss -0.8881 
2023-10-26 16:16:12.851962: val_loss -0.8763 
2023-10-26 16:16:12.852236: Pseudo dice [0.8847, 0.9125, 0.968, 0.7153, 0.9333] 
2023-10-26 16:16:12.852475: Epoch time: 3.89 s 
2023-10-26 16:16:13.955904:  
2023-10-26 16:16:13.956226: Epoch 628 
2023-10-26 16:16:13.956477: Current learning rate: 0.00411 
2023-10-26 16:16:17.907247: train_loss -0.8852 
2023-10-26 16:16:17.907782: val_loss -0.8758 
2023-10-26 16:16:17.908095: Pseudo dice [0.8807, 0.9168, 0.9697, 0.6826, 0.9243] 
2023-10-26 16:16:17.908349: Epoch time: 3.95 s 
2023-10-26 16:16:19.011282:  
2023-10-26 16:16:19.011579: Epoch 629 
2023-10-26 16:16:19.011831: Current learning rate: 0.0041 
2023-10-26 16:16:22.949260: train_loss -0.8862 
2023-10-26 16:16:22.949649: val_loss -0.8843 
2023-10-26 16:16:22.949931: Pseudo dice [0.8837, 0.9136, 0.9696, 0.7056, 0.941] 
2023-10-26 16:16:22.950169: Epoch time: 3.94 s 
2023-10-26 16:16:24.089279:  
2023-10-26 16:16:24.089597: Epoch 630 
2023-10-26 16:16:24.089869: Current learning rate: 0.00409 
2023-10-26 16:16:28.003173: train_loss -0.8886 
2023-10-26 16:16:28.003565: val_loss -0.8564 
2023-10-26 16:16:28.003819: Pseudo dice [0.8847, 0.91, 0.9689, 0.6843, 0.9423] 
2023-10-26 16:16:28.004081: Epoch time: 3.91 s 
2023-10-26 16:16:29.116213:  
2023-10-26 16:16:29.116514: Epoch 631 
2023-10-26 16:16:29.116754: Current learning rate: 0.00408 
2023-10-26 16:16:32.805844: train_loss -0.8804 
2023-10-26 16:16:32.806314: val_loss -0.8719 
2023-10-26 16:16:32.806573: Pseudo dice [0.8758, 0.91, 0.9652, 0.7205, 0.9313] 
2023-10-26 16:16:32.806803: Epoch time: 3.69 s 
2023-10-26 16:16:33.934363:  
2023-10-26 16:16:33.934668: Epoch 632 
2023-10-26 16:16:33.934927: Current learning rate: 0.00407 
2023-10-26 16:16:37.820940: train_loss -0.8845 
2023-10-26 16:16:37.821303: val_loss -0.8678 
2023-10-26 16:16:37.821565: Pseudo dice [0.8814, 0.9103, 0.9705, 0.5726, 0.9328] 
2023-10-26 16:16:37.821784: Epoch time: 3.89 s 
2023-10-26 16:16:39.085818:  
2023-10-26 16:16:39.086120: Epoch 633 
2023-10-26 16:16:39.086346: Current learning rate: 0.00406 
2023-10-26 16:16:42.783204: train_loss -0.8694 
2023-10-26 16:16:42.783561: val_loss -0.869 
2023-10-26 16:16:42.783810: Pseudo dice [0.8763, 0.9129, 0.9695, 0.7219, 0.9354] 
2023-10-26 16:16:42.784056: Epoch time: 3.7 s 
2023-10-26 16:16:43.878175:  
2023-10-26 16:16:43.878468: Epoch 634 
2023-10-26 16:16:43.878711: Current learning rate: 0.00405 
2023-10-26 16:16:47.792400: train_loss -0.8693 
2023-10-26 16:16:47.792808: val_loss -0.8664 
2023-10-26 16:16:47.793129: Pseudo dice [0.8753, 0.9112, 0.9654, 0.7148, 0.9164] 
2023-10-26 16:16:47.793607: Epoch time: 3.91 s 
2023-10-26 16:16:48.893168:  
2023-10-26 16:16:48.893464: Epoch 635 
2023-10-26 16:16:48.893704: Current learning rate: 0.00404 
2023-10-26 16:16:52.807956: train_loss -0.8781 
2023-10-26 16:16:52.808393: val_loss -0.8767 
2023-10-26 16:16:52.808675: Pseudo dice [0.8785, 0.9171, 0.967, 0.709, 0.9223] 
2023-10-26 16:16:52.808942: Epoch time: 3.92 s 
2023-10-26 16:16:53.944031:  
2023-10-26 16:16:53.944355: Epoch 636 
2023-10-26 16:16:53.944607: Current learning rate: 0.00403 
2023-10-26 16:16:57.938447: train_loss -0.8788 
2023-10-26 16:16:57.938830: val_loss -0.8606 
2023-10-26 16:16:57.939108: Pseudo dice [0.8791, 0.9192, 0.9687, 0.66, 0.9265] 
2023-10-26 16:16:57.939348: Epoch time: 4.0 s 
2023-10-26 16:16:59.046574:  
2023-10-26 16:16:59.046892: Epoch 637 
2023-10-26 16:16:59.047122: Current learning rate: 0.00402 
2023-10-26 16:17:02.920386: train_loss -0.8733 
2023-10-26 16:17:02.920752: val_loss -0.8654 
2023-10-26 16:17:02.921027: Pseudo dice [0.8785, 0.9096, 0.9675, 0.4604, 0.9334] 
2023-10-26 16:17:02.921263: Epoch time: 3.87 s 
2023-10-26 16:17:04.025860:  
2023-10-26 16:17:04.026151: Epoch 638 
2023-10-26 16:17:04.026389: Current learning rate: 0.00401 
2023-10-26 16:17:07.995554: train_loss -0.8713 
2023-10-26 16:17:07.995932: val_loss -0.879 
2023-10-26 16:17:07.996195: Pseudo dice [0.8839, 0.9216, 0.9695, 0.6737, 0.9309] 
2023-10-26 16:17:07.996428: Epoch time: 3.97 s 
2023-10-26 16:17:09.271728:  
2023-10-26 16:17:09.272030: Epoch 639 
2023-10-26 16:17:09.272276: Current learning rate: 0.004 
2023-10-26 16:17:13.407685: train_loss -0.8781 
2023-10-26 16:17:13.408076: val_loss -0.8766 
2023-10-26 16:17:13.408333: Pseudo dice [0.882, 0.9072, 0.9695, 0.7776, 0.9311] 
2023-10-26 16:17:13.408572: Epoch time: 4.14 s 
2023-10-26 16:17:14.519483:  
2023-10-26 16:17:14.519781: Epoch 640 
2023-10-26 16:17:14.520028: Current learning rate: 0.00399 
2023-10-26 16:17:18.577719: train_loss -0.873 
2023-10-26 16:17:18.578093: val_loss -0.8524 
2023-10-26 16:17:18.578379: Pseudo dice [0.8776, 0.9092, 0.9693, 0.452, 0.9234] 
2023-10-26 16:17:18.578634: Epoch time: 4.06 s 
2023-10-26 16:17:19.704211:  
2023-10-26 16:17:19.704510: Epoch 641 
2023-10-26 16:17:19.704746: Current learning rate: 0.00398 
2023-10-26 16:17:23.744689: train_loss -0.8724 
2023-10-26 16:17:23.745079: val_loss -0.8667 
2023-10-26 16:17:23.745353: Pseudo dice [0.8788, 0.9107, 0.9672, 0.7445, 0.9318] 
2023-10-26 16:17:23.745585: Epoch time: 4.04 s 
2023-10-26 16:17:24.838243:  
2023-10-26 16:17:24.838541: Epoch 642 
2023-10-26 16:17:24.838779: Current learning rate: 0.00397 
2023-10-26 16:17:28.963462: train_loss -0.88 
2023-10-26 16:17:28.963848: val_loss -0.8636 
2023-10-26 16:17:28.964120: Pseudo dice [0.8831, 0.9078, 0.9653, 0.7411, 0.9166] 
2023-10-26 16:17:28.964357: Epoch time: 4.13 s 
2023-10-26 16:17:30.055534:  
2023-10-26 16:17:30.055842: Epoch 643 
2023-10-26 16:17:30.056087: Current learning rate: 0.00396 
2023-10-26 16:17:34.072912: train_loss -0.874 
2023-10-26 16:17:34.073284: val_loss -0.8835 
2023-10-26 16:17:34.073530: Pseudo dice [0.8836, 0.9117, 0.9713, 0.7907, 0.9269] 
2023-10-26 16:17:34.073755: Epoch time: 4.02 s 
2023-10-26 16:17:35.197382:  
2023-10-26 16:17:35.197668: Epoch 644 
2023-10-26 16:17:35.197913: Current learning rate: 0.00395 
2023-10-26 16:17:39.210154: train_loss -0.8839 
2023-10-26 16:17:39.210531: val_loss -0.8706 
2023-10-26 16:17:39.210790: Pseudo dice [0.8804, 0.9166, 0.9688, 0.6632, 0.9234] 
2023-10-26 16:17:39.211021: Epoch time: 4.01 s 
2023-10-26 16:17:40.320887:  
2023-10-26 16:17:40.321179: Epoch 645 
2023-10-26 16:17:40.321422: Current learning rate: 0.00394 
2023-10-26 16:17:44.159513: train_loss -0.8778 
2023-10-26 16:17:44.159947: val_loss -0.8704 
2023-10-26 16:17:44.160232: Pseudo dice [0.8778, 0.9164, 0.9645, 0.7018, 0.9314] 
2023-10-26 16:17:44.160494: Epoch time: 3.84 s 
2023-10-26 16:17:45.478312:  
2023-10-26 16:17:45.478652: Epoch 646 
2023-10-26 16:17:45.478908: Current learning rate: 0.00393 
2023-10-26 16:17:49.356724: train_loss -0.8672 
2023-10-26 16:17:49.357198: val_loss -0.8338 
2023-10-26 16:17:49.357470: Pseudo dice [0.8798, 0.914, 0.9661, 0.7925, 0.9043] 
2023-10-26 16:17:49.357699: Epoch time: 3.88 s 
2023-10-26 16:17:50.486071:  
2023-10-26 16:17:50.486467: Epoch 647 
2023-10-26 16:17:50.486816: Current learning rate: 0.00392 
2023-10-26 16:17:54.310698: train_loss -0.8633 
2023-10-26 16:17:54.311100: val_loss -0.8676 
2023-10-26 16:17:54.311368: Pseudo dice [0.8577, 0.9076, 0.9683, 0.6278, 0.9306] 
2023-10-26 16:17:54.311604: Epoch time: 3.83 s 
2023-10-26 16:17:55.474049:  
2023-10-26 16:17:55.474347: Epoch 648 
2023-10-26 16:17:55.474585: Current learning rate: 0.00391 
2023-10-26 16:17:59.360791: train_loss -0.8784 
2023-10-26 16:17:59.361152: val_loss -0.8784 
2023-10-26 16:17:59.361413: Pseudo dice [0.8871, 0.9165, 0.9676, 0.737, 0.942] 
2023-10-26 16:17:59.361635: Epoch time: 3.89 s 
2023-10-26 16:18:00.557631:  
2023-10-26 16:18:00.557933: Epoch 649 
2023-10-26 16:18:00.558183: Current learning rate: 0.0039 
2023-10-26 16:18:04.474436: train_loss -0.8758 
2023-10-26 16:18:04.474809: val_loss -0.8847 
2023-10-26 16:18:04.475066: Pseudo dice [0.8819, 0.9124, 0.9707, 0.7527, 0.9423] 
2023-10-26 16:18:04.475298: Epoch time: 3.92 s 
2023-10-26 16:18:05.693855:  
2023-10-26 16:18:05.694180: Epoch 650 
2023-10-26 16:18:05.694445: Current learning rate: 0.00389 
2023-10-26 16:18:09.555015: train_loss -0.8749 
2023-10-26 16:18:09.555412: val_loss -0.8647 
2023-10-26 16:18:09.555664: Pseudo dice [0.8791, 0.8909, 0.9679, 0.5943, 0.9281] 
2023-10-26 16:18:09.555901: Epoch time: 3.86 s 
2023-10-26 16:18:10.693395:  
2023-10-26 16:18:10.693694: Epoch 651 
2023-10-26 16:18:10.693939: Current learning rate: 0.00388 
2023-10-26 16:18:14.576488: train_loss -0.8769 
2023-10-26 16:18:14.576865: val_loss -0.8673 
2023-10-26 16:18:14.577129: Pseudo dice [0.881, 0.9048, 0.968, 0.7452, 0.9286] 
2023-10-26 16:18:14.577354: Epoch time: 3.88 s 
2023-10-26 16:18:15.864975:  
2023-10-26 16:18:15.865268: Epoch 652 
2023-10-26 16:18:15.865511: Current learning rate: 0.00387 
2023-10-26 16:18:19.814480: train_loss -0.8582 
2023-10-26 16:18:19.815190: val_loss -0.8661 
2023-10-26 16:18:19.815468: Pseudo dice [0.8624, 0.9125, 0.969, 0.7004, 0.9291] 
2023-10-26 16:18:19.815700: Epoch time: 3.95 s 
2023-10-26 16:18:20.931949:  
2023-10-26 16:18:20.932253: Epoch 653 
2023-10-26 16:18:20.932496: Current learning rate: 0.00386 
2023-10-26 16:18:24.812063: train_loss -0.8782 
2023-10-26 16:18:24.812729: val_loss -0.8796 
2023-10-26 16:18:24.812996: Pseudo dice [0.8811, 0.9104, 0.9701, 0.7591, 0.9328] 
2023-10-26 16:18:24.813239: Epoch time: 3.88 s 
2023-10-26 16:18:25.897898:  
2023-10-26 16:18:25.898194: Epoch 654 
2023-10-26 16:18:25.898440: Current learning rate: 0.00385 
2023-10-26 16:18:29.877928: train_loss -0.887 
2023-10-26 16:18:29.878598: val_loss -0.881 
2023-10-26 16:18:29.878891: Pseudo dice [0.8836, 0.9141, 0.97, 0.7553, 0.9277] 
2023-10-26 16:18:29.879124: Epoch time: 3.98 s 
2023-10-26 16:18:30.999126:  
2023-10-26 16:18:30.999413: Epoch 655 
2023-10-26 16:18:30.999645: Current learning rate: 0.00384 
2023-10-26 16:18:34.970350: train_loss -0.8846 
2023-10-26 16:18:34.971058: val_loss -0.8734 
2023-10-26 16:18:34.971345: Pseudo dice [0.8789, 0.9112, 0.9664, 0.6066, 0.9384] 
2023-10-26 16:18:34.971602: Epoch time: 3.97 s 
2023-10-26 16:18:36.077979:  
2023-10-26 16:18:36.078295: Epoch 656 
2023-10-26 16:18:36.078552: Current learning rate: 0.00383 
2023-10-26 16:18:40.122009: train_loss -0.8724 
2023-10-26 16:18:40.122676: val_loss -0.8441 
2023-10-26 16:18:40.122936: Pseudo dice [0.8823, 0.9017, 0.9675, 0.7726, 0.9346] 
2023-10-26 16:18:40.123168: Epoch time: 4.04 s 
2023-10-26 16:18:41.225702:  
2023-10-26 16:18:41.226009: Epoch 657 
2023-10-26 16:18:41.226260: Current learning rate: 0.00382 
2023-10-26 16:18:45.167126: train_loss -0.8575 
2023-10-26 16:18:45.167521: val_loss -0.8744 
2023-10-26 16:18:45.167776: Pseudo dice [0.8736, 0.9061, 0.9661, 0.6949, 0.9293] 
2023-10-26 16:18:45.168009: Epoch time: 3.94 s 
2023-10-26 16:18:46.449181:  
2023-10-26 16:18:46.449472: Epoch 658 
2023-10-26 16:18:46.449708: Current learning rate: 0.00381 
2023-10-26 16:18:50.456713: train_loss -0.8528 
2023-10-26 16:18:50.457402: val_loss -0.8554 
2023-10-26 16:18:50.457678: Pseudo dice [0.8705, 0.8911, 0.9633, 0.6431, 0.9193] 
2023-10-26 16:18:50.457922: Epoch time: 4.01 s 
2023-10-26 16:18:51.732145:  
2023-10-26 16:18:51.732445: Epoch 659 
2023-10-26 16:18:51.732684: Current learning rate: 0.0038 
2023-10-26 16:18:55.816475: train_loss -0.8515 
2023-10-26 16:18:55.817199: val_loss -0.8774 
2023-10-26 16:18:55.817470: Pseudo dice [0.8832, 0.9111, 0.9691, 0.7352, 0.9289] 
2023-10-26 16:18:55.817703: Epoch time: 4.08 s 
2023-10-26 16:18:56.940413:  
2023-10-26 16:18:56.940732: Epoch 660 
2023-10-26 16:18:56.940980: Current learning rate: 0.00379 
2023-10-26 16:19:00.914504: train_loss -0.8663 
2023-10-26 16:19:00.915263: val_loss -0.8619 
2023-10-26 16:19:00.915539: Pseudo dice [0.8721, 0.9173, 0.9708, 0.6886, 0.9341] 
2023-10-26 16:19:00.915783: Epoch time: 3.97 s 
2023-10-26 16:19:02.020758:  
2023-10-26 16:19:02.021075: Epoch 661 
2023-10-26 16:19:02.021324: Current learning rate: 0.00378 
2023-10-26 16:19:05.982013: train_loss -0.8685 
2023-10-26 16:19:05.982653: val_loss -0.8793 
2023-10-26 16:19:05.982940: Pseudo dice [0.8861, 0.9153, 0.9673, 0.78, 0.9209] 
2023-10-26 16:19:05.983171: Epoch time: 3.96 s 
2023-10-26 16:19:07.083469:  
2023-10-26 16:19:07.083762: Epoch 662 
2023-10-26 16:19:07.084010: Current learning rate: 0.00377 
2023-10-26 16:19:11.060392: train_loss -0.8591 
2023-10-26 16:19:11.061091: val_loss -0.8677 
2023-10-26 16:19:11.061386: Pseudo dice [0.8768, 0.9052, 0.9682, 0.6721, 0.9217] 
2023-10-26 16:19:11.061624: Epoch time: 3.98 s 
2023-10-26 16:19:12.163169:  
2023-10-26 16:19:12.163475: Epoch 663 
2023-10-26 16:19:12.163714: Current learning rate: 0.00376 
2023-10-26 16:19:16.091007: train_loss -0.8721 
2023-10-26 16:19:16.091490: val_loss -0.8804 
2023-10-26 16:19:16.091744: Pseudo dice [0.8879, 0.9129, 0.97, 0.7592, 0.9396] 
2023-10-26 16:19:16.091999: Epoch time: 3.93 s 
2023-10-26 16:19:17.208717:  
2023-10-26 16:19:17.209079: Epoch 664 
2023-10-26 16:19:17.209426: Current learning rate: 0.00375 
2023-10-26 16:19:21.166059: train_loss -0.8761 
2023-10-26 16:19:21.166479: val_loss -0.8701 
2023-10-26 16:19:21.166751: Pseudo dice [0.8784, 0.9095, 0.9653, 0.6327, 0.9474] 
2023-10-26 16:19:21.167004: Epoch time: 3.96 s 
2023-10-26 16:19:22.474544:  
2023-10-26 16:19:22.474931: Epoch 665 
2023-10-26 16:19:22.475271: Current learning rate: 0.00374 
2023-10-26 16:19:26.584204: train_loss -0.8724 
2023-10-26 16:19:26.584894: val_loss -0.8706 
2023-10-26 16:19:26.585167: Pseudo dice [0.8876, 0.9113, 0.9695, 0.7186, 0.9231] 
2023-10-26 16:19:26.585408: Epoch time: 4.11 s 
2023-10-26 16:19:27.722279:  
2023-10-26 16:19:27.722583: Epoch 666 
2023-10-26 16:19:27.722834: Current learning rate: 0.00373 
2023-10-26 16:19:31.725359: train_loss -0.8628 
2023-10-26 16:19:31.726051: val_loss -0.8721 
2023-10-26 16:19:31.726315: Pseudo dice [0.888, 0.9196, 0.9691, 0.7411, 0.9257] 
2023-10-26 16:19:31.726554: Epoch time: 4.0 s 
2023-10-26 16:19:32.819119:  
2023-10-26 16:19:32.819427: Epoch 667 
2023-10-26 16:19:32.819670: Current learning rate: 0.00372 
2023-10-26 16:19:36.782281: train_loss -0.8737 
2023-10-26 16:19:36.783142: val_loss -0.8669 
2023-10-26 16:19:36.783426: Pseudo dice [0.8853, 0.924, 0.9676, 0.7457, 0.9344] 
2023-10-26 16:19:36.783656: Epoch time: 3.96 s 
2023-10-26 16:19:37.916858:  
2023-10-26 16:19:37.917588: Epoch 668 
2023-10-26 16:19:37.917949: Current learning rate: 0.00371 
2023-10-26 16:19:41.907521: train_loss -0.8733 
2023-10-26 16:19:41.908295: val_loss -0.8765 
2023-10-26 16:19:41.908631: Pseudo dice [0.8809, 0.9123, 0.9665, 0.7292, 0.9342] 
2023-10-26 16:19:41.908891: Epoch time: 3.99 s 
2023-10-26 16:19:43.059853:  
2023-10-26 16:19:43.060211: Epoch 669 
2023-10-26 16:19:43.060506: Current learning rate: 0.0037 
2023-10-26 16:19:47.004729: train_loss -0.8657 
2023-10-26 16:19:47.005391: val_loss -0.88 
2023-10-26 16:19:47.005655: Pseudo dice [0.8729, 0.9177, 0.9715, 0.7392, 0.9228] 
2023-10-26 16:19:47.005891: Epoch time: 3.95 s 
2023-10-26 16:19:48.202265:  
2023-10-26 16:19:48.202562: Epoch 670 
2023-10-26 16:19:48.202796: Current learning rate: 0.00369 
2023-10-26 16:19:52.259315: train_loss -0.8749 
2023-10-26 16:19:52.259711: val_loss -0.8737 
2023-10-26 16:19:52.259966: Pseudo dice [0.8831, 0.9122, 0.9699, 0.6998, 0.9277] 
2023-10-26 16:19:52.260196: Epoch time: 4.06 s 
2023-10-26 16:19:53.524068:  
2023-10-26 16:19:53.524387: Epoch 671 
2023-10-26 16:19:53.524627: Current learning rate: 0.00368 
2023-10-26 16:19:57.464353: train_loss -0.8774 
2023-10-26 16:19:57.465070: val_loss -0.8833 
2023-10-26 16:19:57.465342: Pseudo dice [0.882, 0.9165, 0.9705, 0.7451, 0.9316] 
2023-10-26 16:19:57.465587: Epoch time: 3.94 s 
2023-10-26 16:19:58.569333:  
2023-10-26 16:19:58.569752: Epoch 672 
2023-10-26 16:19:58.570049: Current learning rate: 0.00367 
2023-10-26 16:20:02.313637: train_loss -0.8777 
2023-10-26 16:20:02.314424: val_loss -0.881 
2023-10-26 16:20:02.314935: Pseudo dice [0.887, 0.9168, 0.9698, 0.7787, 0.9326] 
2023-10-26 16:20:02.315287: Epoch time: 3.75 s 
2023-10-26 16:20:03.418726:  
2023-10-26 16:20:03.419061: Epoch 673 
2023-10-26 16:20:03.419359: Current learning rate: 0.00366 
2023-10-26 16:20:07.266442: train_loss -0.8823 
2023-10-26 16:20:07.267171: val_loss -0.8681 
2023-10-26 16:20:07.267439: Pseudo dice [0.8871, 0.9198, 0.9707, 0.4975, 0.9297] 
2023-10-26 16:20:07.267683: Epoch time: 3.85 s 
2023-10-26 16:20:08.427265:  
2023-10-26 16:20:08.430137: Epoch 674 
2023-10-26 16:20:08.430382: Current learning rate: 0.00365 
2023-10-26 16:20:12.498805: train_loss -0.8665 
2023-10-26 16:20:12.499683: val_loss -0.8776 
2023-10-26 16:20:12.500057: Pseudo dice [0.8777, 0.913, 0.9693, 0.7448, 0.9262] 
2023-10-26 16:20:12.500395: Epoch time: 4.07 s 
2023-10-26 16:20:13.655548:  
2023-10-26 16:20:13.655894: Epoch 675 
2023-10-26 16:20:13.656200: Current learning rate: 0.00364 
2023-10-26 16:20:17.641123: train_loss -0.8752 
2023-10-26 16:20:17.641913: val_loss -0.8849 
2023-10-26 16:20:17.642278: Pseudo dice [0.8851, 0.9171, 0.9711, 0.768, 0.9416] 
2023-10-26 16:20:17.642567: Epoch time: 3.99 s 
2023-10-26 16:20:18.799649:  
2023-10-26 16:20:18.799968: Epoch 676 
2023-10-26 16:20:18.800205: Current learning rate: 0.00363 
2023-10-26 16:20:22.748936: train_loss -0.8712 
2023-10-26 16:20:22.749364: val_loss -0.8796 
2023-10-26 16:20:22.749696: Pseudo dice [0.8893, 0.923, 0.9707, 0.7683, 0.933] 
2023-10-26 16:20:22.750011: Epoch time: 3.95 s 
2023-10-26 16:20:24.057544:  
2023-10-26 16:20:24.057836: Epoch 677 
2023-10-26 16:20:24.058074: Current learning rate: 0.00362 
2023-10-26 16:20:28.002816: train_loss -0.8771 
2023-10-26 16:20:28.003225: val_loss -0.8795 
2023-10-26 16:20:28.003507: Pseudo dice [0.8864, 0.9154, 0.968, 0.7426, 0.932] 
2023-10-26 16:20:28.003764: Epoch time: 3.95 s 
2023-10-26 16:20:29.135394:  
2023-10-26 16:20:29.135721: Epoch 678 
2023-10-26 16:20:29.135980: Current learning rate: 0.00361 
2023-10-26 16:20:33.057296: train_loss -0.8802 
2023-10-26 16:20:33.057698: val_loss -0.8804 
2023-10-26 16:20:33.058006: Pseudo dice [0.8856, 0.9169, 0.9701, 0.76, 0.9383] 
2023-10-26 16:20:33.058254: Epoch time: 3.92 s 
2023-10-26 16:20:34.214393:  
2023-10-26 16:20:34.214754: Epoch 679 
2023-10-26 16:20:34.215081: Current learning rate: 0.0036 
2023-10-26 16:20:38.131936: train_loss -0.8868 
2023-10-26 16:20:38.132323: val_loss -0.8819 
2023-10-26 16:20:38.132576: Pseudo dice [0.8862, 0.9151, 0.9699, 0.758, 0.9351] 
2023-10-26 16:20:38.132804: Epoch time: 3.92 s 
2023-10-26 16:20:39.260824:  
2023-10-26 16:20:39.261166: Epoch 680 
2023-10-26 16:20:39.261415: Current learning rate: 0.00359 
2023-10-26 16:20:43.183662: train_loss -0.8876 
2023-10-26 16:20:43.184071: val_loss -0.8826 
2023-10-26 16:20:43.184323: Pseudo dice [0.8843, 0.9113, 0.9694, 0.7665, 0.9371] 
2023-10-26 16:20:43.184552: Epoch time: 3.92 s 
2023-10-26 16:20:44.303326:  
2023-10-26 16:20:44.303634: Epoch 681 
2023-10-26 16:20:44.303870: Current learning rate: 0.00358 
2023-10-26 16:20:48.157579: train_loss -0.8813 
2023-10-26 16:20:48.157980: val_loss -0.8805 
2023-10-26 16:20:48.158292: Pseudo dice [0.8802, 0.9123, 0.9689, 0.767, 0.9226] 
2023-10-26 16:20:48.158531: Epoch time: 3.85 s 
2023-10-26 16:20:49.284132:  
2023-10-26 16:20:49.284421: Epoch 682 
2023-10-26 16:20:49.284668: Current learning rate: 0.00357 
2023-10-26 16:20:53.264534: train_loss -0.8833 
2023-10-26 16:20:53.264922: val_loss -0.8823 
2023-10-26 16:20:53.265196: Pseudo dice [0.8819, 0.9197, 0.9725, 0.6747, 0.9316] 
2023-10-26 16:20:53.265425: Epoch time: 3.98 s 
2023-10-26 16:20:54.589408:  
2023-10-26 16:20:54.589722: Epoch 683 
2023-10-26 16:20:54.589978: Current learning rate: 0.00356 
2023-10-26 16:20:58.472334: train_loss -0.8844 
2023-10-26 16:20:58.472722: val_loss -0.8863 
2023-10-26 16:20:58.472994: Pseudo dice [0.8872, 0.916, 0.97, 0.7602, 0.9448] 
2023-10-26 16:20:58.473271: Epoch time: 3.88 s 
2023-10-26 16:20:59.665344:  
2023-10-26 16:20:59.665691: Epoch 684 
2023-10-26 16:20:59.665994: Current learning rate: 0.00355 
2023-10-26 16:21:03.627378: train_loss -0.8881 
2023-10-26 16:21:03.627742: val_loss -0.8809 
2023-10-26 16:21:03.628006: Pseudo dice [0.8889, 0.9201, 0.9701, 0.6763, 0.9334] 
2023-10-26 16:21:03.628357: Epoch time: 3.96 s 
2023-10-26 16:21:04.745261:  
2023-10-26 16:21:04.745559: Epoch 685 
2023-10-26 16:21:04.745790: Current learning rate: 0.00354 
2023-10-26 16:21:08.612709: train_loss -0.8833 
2023-10-26 16:21:08.613086: val_loss -0.8859 
2023-10-26 16:21:08.613369: Pseudo dice [0.888, 0.9201, 0.9688, 0.7055, 0.9333] 
2023-10-26 16:21:08.613621: Epoch time: 3.87 s 
2023-10-26 16:21:09.745017:  
2023-10-26 16:21:09.745313: Epoch 686 
2023-10-26 16:21:09.745572: Current learning rate: 0.00353 
2023-10-26 16:21:13.765765: train_loss -0.8849 
2023-10-26 16:21:13.766188: val_loss -0.8778 
2023-10-26 16:21:13.766455: Pseudo dice [0.8809, 0.92, 0.9707, 0.716, 0.9322] 
2023-10-26 16:21:13.766727: Epoch time: 4.02 s 
2023-10-26 16:21:14.895197:  
2023-10-26 16:21:14.895493: Epoch 687 
2023-10-26 16:21:14.895753: Current learning rate: 0.00352 
2023-10-26 16:21:18.841688: train_loss -0.8811 
2023-10-26 16:21:18.842045: val_loss -0.8778 
2023-10-26 16:21:18.842318: Pseudo dice [0.8824, 0.918, 0.9693, 0.7297, 0.9313] 
2023-10-26 16:21:18.842553: Epoch time: 3.95 s 
2023-10-26 16:21:19.949175:  
2023-10-26 16:21:19.949469: Epoch 688 
2023-10-26 16:21:19.949716: Current learning rate: 0.00351 
2023-10-26 16:21:24.066396: train_loss -0.8868 
2023-10-26 16:21:24.066781: val_loss -0.8807 
2023-10-26 16:21:24.067070: Pseudo dice [0.8822, 0.9235, 0.9693, 0.7622, 0.9267] 
2023-10-26 16:21:24.067311: Epoch time: 4.12 s 
2023-10-26 16:21:25.250039:  
2023-10-26 16:21:25.250338: Epoch 689 
2023-10-26 16:21:25.250574: Current learning rate: 0.0035 
2023-10-26 16:21:29.264361: train_loss -0.8861 
2023-10-26 16:21:29.264712: val_loss -0.8789 
2023-10-26 16:21:29.265010: Pseudo dice [0.8837, 0.9145, 0.9711, 0.7259, 0.922] 
2023-10-26 16:21:29.265230: Epoch time: 4.01 s 
2023-10-26 16:21:30.517290:  
2023-10-26 16:21:30.517602: Epoch 690 
2023-10-26 16:21:30.517888: Current learning rate: 0.00349 
2023-10-26 16:21:34.477967: train_loss -0.8846 
2023-10-26 16:21:34.478326: val_loss -0.8602 
2023-10-26 16:21:34.478581: Pseudo dice [0.8815, 0.9149, 0.9692, 0.2012, 0.9248] 
2023-10-26 16:21:34.478816: Epoch time: 3.96 s 
2023-10-26 16:21:35.583096:  
2023-10-26 16:21:35.583383: Epoch 691 
2023-10-26 16:21:35.583619: Current learning rate: 0.00348 
2023-10-26 16:21:39.669640: train_loss -0.8803 
2023-10-26 16:21:39.670180: val_loss -0.8817 
2023-10-26 16:21:39.670693: Pseudo dice [0.8826, 0.9039, 0.9686, 0.6952, 0.9429] 
2023-10-26 16:21:39.671182: Epoch time: 4.09 s 
2023-10-26 16:21:40.793022:  
2023-10-26 16:21:40.793324: Epoch 692 
2023-10-26 16:21:40.793557: Current learning rate: 0.00346 
2023-10-26 16:21:44.798600: train_loss -0.8855 
2023-10-26 16:21:44.799056: val_loss -0.8702 
2023-10-26 16:21:44.799417: Pseudo dice [0.8793, 0.9075, 0.9671, 0.6332, 0.9189] 
2023-10-26 16:21:44.799707: Epoch time: 4.01 s 
2023-10-26 16:21:45.962767:  
2023-10-26 16:21:45.963067: Epoch 693 
2023-10-26 16:21:45.963300: Current learning rate: 0.00345 
2023-10-26 16:21:50.071029: train_loss -0.89 
2023-10-26 16:21:50.071397: val_loss -0.8716 
2023-10-26 16:21:50.071643: Pseudo dice [0.8807, 0.9165, 0.9698, 0.6373, 0.9335] 
2023-10-26 16:21:50.071860: Epoch time: 4.11 s 
2023-10-26 16:21:51.232618:  
2023-10-26 16:21:51.232911: Epoch 694 
2023-10-26 16:21:51.233153: Current learning rate: 0.00344 
2023-10-26 16:21:54.996979: train_loss -0.8859 
2023-10-26 16:21:54.997344: val_loss -0.8793 
2023-10-26 16:21:54.997598: Pseudo dice [0.8811, 0.9133, 0.9701, 0.6691, 0.93] 
2023-10-26 16:21:54.997814: Epoch time: 3.76 s 
2023-10-26 16:21:56.130192:  
2023-10-26 16:21:56.130513: Epoch 695 
2023-10-26 16:21:56.130774: Current learning rate: 0.00343 
2023-10-26 16:22:00.122905: train_loss -0.8888 
2023-10-26 16:22:00.123289: val_loss -0.8676 
2023-10-26 16:22:00.123550: Pseudo dice [0.8822, 0.9125, 0.9686, 0.6234, 0.9224] 
2023-10-26 16:22:00.123777: Epoch time: 3.99 s 
2023-10-26 16:22:01.457685:  
2023-10-26 16:22:01.457986: Epoch 696 
2023-10-26 16:22:01.458224: Current learning rate: 0.00342 
2023-10-26 16:22:05.276765: train_loss -0.8892 
2023-10-26 16:22:05.277129: val_loss -0.8752 
2023-10-26 16:22:05.277382: Pseudo dice [0.8816, 0.9232, 0.9697, 0.6815, 0.9277] 
2023-10-26 16:22:05.277634: Epoch time: 3.82 s 
2023-10-26 16:22:06.412571:  
2023-10-26 16:22:06.412901: Epoch 697 
2023-10-26 16:22:06.413143: Current learning rate: 0.00341 
2023-10-26 16:22:10.418092: train_loss -0.8871 
2023-10-26 16:22:10.418474: val_loss -0.8776 
2023-10-26 16:22:10.418738: Pseudo dice [0.8814, 0.9192, 0.9708, 0.6654, 0.9299] 
2023-10-26 16:22:10.418976: Epoch time: 4.01 s 
2023-10-26 16:22:11.542869:  
2023-10-26 16:22:11.543159: Epoch 698 
2023-10-26 16:22:11.543393: Current learning rate: 0.0034 
2023-10-26 16:22:15.535137: train_loss -0.8876 
2023-10-26 16:22:15.535528: val_loss -0.8836 
2023-10-26 16:22:15.535778: Pseudo dice [0.8788, 0.9166, 0.9698, 0.7289, 0.9368] 
2023-10-26 16:22:15.536082: Epoch time: 3.99 s 
2023-10-26 16:22:16.712861:  
2023-10-26 16:22:16.713165: Epoch 699 
2023-10-26 16:22:16.713405: Current learning rate: 0.00339 
2023-10-26 16:22:20.673941: train_loss -0.8885 
2023-10-26 16:22:20.674362: val_loss -0.8793 
2023-10-26 16:22:20.674833: Pseudo dice [0.8843, 0.9215, 0.9709, 0.6915, 0.9313] 
2023-10-26 16:22:20.675090: Epoch time: 3.96 s 
2023-10-26 16:22:21.934742:  
2023-10-26 16:22:21.935061: Epoch 700 
2023-10-26 16:22:21.935310: Current learning rate: 0.00338 
2023-10-26 16:22:25.904831: train_loss -0.8834 
2023-10-26 16:22:25.905210: val_loss -0.8815 
2023-10-26 16:22:25.905556: Pseudo dice [0.8847, 0.9133, 0.9698, 0.7675, 0.9272] 
2023-10-26 16:22:25.905851: Epoch time: 3.97 s 
2023-10-26 16:22:27.127187:  
2023-10-26 16:22:27.127511: Epoch 701 
2023-10-26 16:22:27.127802: Current learning rate: 0.00337 
2023-10-26 16:22:31.149829: train_loss -0.8899 
2023-10-26 16:22:31.150207: val_loss -0.879 
2023-10-26 16:22:31.150456: Pseudo dice [0.8826, 0.9174, 0.9714, 0.6411, 0.9409] 
2023-10-26 16:22:31.150707: Epoch time: 4.02 s 
2023-10-26 16:22:32.451986:  
2023-10-26 16:22:32.452283: Epoch 702 
2023-10-26 16:22:32.452528: Current learning rate: 0.00336 
2023-10-26 16:22:36.423448: train_loss -0.8729 
2023-10-26 16:22:36.423842: val_loss -0.8737 
2023-10-26 16:22:36.424105: Pseudo dice [0.8847, 0.9097, 0.9689, 0.5508, 0.9354] 
2023-10-26 16:22:36.424333: Epoch time: 3.97 s 
2023-10-26 16:22:37.546056:  
2023-10-26 16:22:37.546348: Epoch 703 
2023-10-26 16:22:37.546589: Current learning rate: 0.00335 
2023-10-26 16:22:41.426302: train_loss -0.8783 
2023-10-26 16:22:41.426669: val_loss -0.8792 
2023-10-26 16:22:41.426933: Pseudo dice [0.8824, 0.913, 0.967, 0.7723, 0.9323] 
2023-10-26 16:22:41.427164: Epoch time: 3.88 s 
2023-10-26 16:22:42.539546:  
2023-10-26 16:22:42.539957: Epoch 704 
2023-10-26 16:22:42.540197: Current learning rate: 0.00334 
2023-10-26 16:22:46.464089: train_loss -0.8826 
2023-10-26 16:22:46.464520: val_loss -0.8808 
2023-10-26 16:22:46.465014: Pseudo dice [0.8858, 0.9147, 0.969, 0.6833, 0.9306] 
2023-10-26 16:22:46.465275: Epoch time: 3.93 s 
2023-10-26 16:22:47.612159:  
2023-10-26 16:22:47.612454: Epoch 705 
2023-10-26 16:22:47.612700: Current learning rate: 0.00333 
2023-10-26 16:22:51.664325: train_loss -0.8857 
2023-10-26 16:22:51.664704: val_loss -0.8826 
2023-10-26 16:22:51.664963: Pseudo dice [0.8855, 0.9171, 0.9714, 0.6516, 0.9449] 
2023-10-26 16:22:51.665195: Epoch time: 4.05 s 
2023-10-26 16:22:52.816313:  
2023-10-26 16:22:52.816607: Epoch 706 
2023-10-26 16:22:52.816852: Current learning rate: 0.00332 
2023-10-26 16:22:56.840452: train_loss -0.8856 
2023-10-26 16:22:56.840816: val_loss -0.8635 
2023-10-26 16:22:56.841076: Pseudo dice [0.8807, 0.9097, 0.9662, 0.6982, 0.9245] 
2023-10-26 16:22:56.841309: Epoch time: 4.02 s 
2023-10-26 16:22:57.987056:  
2023-10-26 16:22:57.987384: Epoch 707 
2023-10-26 16:22:57.987648: Current learning rate: 0.00331 
2023-10-26 16:23:01.947777: train_loss -0.892 
2023-10-26 16:23:01.948156: val_loss -0.8768 
2023-10-26 16:23:01.948411: Pseudo dice [0.8813, 0.9175, 0.9686, 0.7215, 0.9259] 
2023-10-26 16:23:01.948640: Epoch time: 3.96 s 
2023-10-26 16:23:03.222241:  
2023-10-26 16:23:03.222546: Epoch 708 
2023-10-26 16:23:03.222775: Current learning rate: 0.0033 
2023-10-26 16:23:07.202184: train_loss -0.888 
2023-10-26 16:23:07.202544: val_loss -0.8704 
2023-10-26 16:23:07.202798: Pseudo dice [0.8827, 0.9099, 0.9656, 0.6701, 0.9317] 
2023-10-26 16:23:07.203029: Epoch time: 3.98 s 
2023-10-26 16:23:08.309021:  
2023-10-26 16:23:08.309311: Epoch 709 
2023-10-26 16:23:08.309546: Current learning rate: 0.00329 
2023-10-26 16:23:12.360843: train_loss -0.8857 
2023-10-26 16:23:12.361200: val_loss -0.8733 
2023-10-26 16:23:12.361465: Pseudo dice [0.8789, 0.9112, 0.9672, 0.7679, 0.9238] 
2023-10-26 16:23:12.361701: Epoch time: 4.05 s 
2023-10-26 16:23:13.469102:  
2023-10-26 16:23:13.469393: Epoch 710 
2023-10-26 16:23:13.469632: Current learning rate: 0.00328 
2023-10-26 16:23:17.502396: train_loss -0.8904 
2023-10-26 16:23:17.502792: val_loss -0.8721 
2023-10-26 16:23:17.503073: Pseudo dice [0.8781, 0.9156, 0.9691, 0.7098, 0.9245] 
2023-10-26 16:23:17.503301: Epoch time: 4.03 s 
2023-10-26 16:23:18.624276:  
2023-10-26 16:23:18.624573: Epoch 711 
2023-10-26 16:23:18.624804: Current learning rate: 0.00327 
2023-10-26 16:23:22.660555: train_loss -0.8656 
2023-10-26 16:23:22.660934: val_loss -0.8529 
2023-10-26 16:23:22.661192: Pseudo dice [0.8694, 0.8975, 0.9667, 0.4875, 0.9234] 
2023-10-26 16:23:22.661421: Epoch time: 4.04 s 
2023-10-26 16:23:23.793314:  
2023-10-26 16:23:23.793617: Epoch 712 
2023-10-26 16:23:23.793853: Current learning rate: 0.00326 
2023-10-26 16:23:27.762746: train_loss -0.867 
2023-10-26 16:23:27.763134: val_loss -0.8669 
2023-10-26 16:23:27.763392: Pseudo dice [0.8725, 0.9123, 0.9691, 0.7628, 0.9285] 
2023-10-26 16:23:27.763631: Epoch time: 3.97 s 
2023-10-26 16:23:28.899510:  
2023-10-26 16:23:28.899797: Epoch 713 
2023-10-26 16:23:28.900040: Current learning rate: 0.00325 
2023-10-26 16:23:32.804267: train_loss -0.864 
2023-10-26 16:23:32.804667: val_loss -0.8749 
2023-10-26 16:23:32.804966: Pseudo dice [0.8816, 0.9149, 0.969, 0.749, 0.9248] 
2023-10-26 16:23:32.805223: Epoch time: 3.91 s 
2023-10-26 16:23:34.149179:  
2023-10-26 16:23:34.149467: Epoch 714 
2023-10-26 16:23:34.149706: Current learning rate: 0.00324 
2023-10-26 16:23:38.147186: train_loss -0.8823 
2023-10-26 16:23:38.147567: val_loss -0.8806 
2023-10-26 16:23:38.147951: Pseudo dice [0.8831, 0.9188, 0.9694, 0.7301, 0.9324] 
2023-10-26 16:23:38.148210: Epoch time: 4.0 s 
2023-10-26 16:23:39.304639:  
2023-10-26 16:23:39.304963: Epoch 715 
2023-10-26 16:23:39.305207: Current learning rate: 0.00323 
2023-10-26 16:23:43.213766: train_loss -0.8852 
2023-10-26 16:23:43.214160: val_loss -0.8777 
2023-10-26 16:23:43.214428: Pseudo dice [0.8876, 0.9215, 0.9703, 0.6711, 0.9292] 
2023-10-26 16:23:43.214646: Epoch time: 3.91 s 
2023-10-26 16:23:44.364069:  
2023-10-26 16:23:44.364367: Epoch 716 
2023-10-26 16:23:44.364596: Current learning rate: 0.00322 
2023-10-26 16:23:48.253422: train_loss -0.8692 
2023-10-26 16:23:48.253811: val_loss -0.8679 
2023-10-26 16:23:48.254066: Pseudo dice [0.8834, 0.9105, 0.9653, 0.6956, 0.9288] 
2023-10-26 16:23:48.254290: Epoch time: 3.89 s 
2023-10-26 16:23:49.443497:  
2023-10-26 16:23:49.443850: Epoch 717 
2023-10-26 16:23:49.444106: Current learning rate: 0.00321 
2023-10-26 16:23:53.257577: train_loss -0.8569 
2023-10-26 16:23:53.257952: val_loss -0.8693 
2023-10-26 16:23:53.258216: Pseudo dice [0.8773, 0.8993, 0.9682, 0.7259, 0.9279] 
2023-10-26 16:23:53.258445: Epoch time: 3.81 s 
2023-10-26 16:23:54.446287:  
2023-10-26 16:23:54.446591: Epoch 718 
2023-10-26 16:23:54.446833: Current learning rate: 0.0032 
2023-10-26 16:23:58.278687: train_loss -0.8709 
2023-10-26 16:23:58.279184: val_loss -0.8722 
2023-10-26 16:23:58.279628: Pseudo dice [0.8819, 0.9155, 0.9686, 0.7113, 0.9297] 
2023-10-26 16:23:58.279944: Epoch time: 3.83 s 
2023-10-26 16:23:59.526707:  
2023-10-26 16:23:59.527039: Epoch 719 
2023-10-26 16:23:59.527286: Current learning rate: 0.00319 
2023-10-26 16:24:03.277528: train_loss -0.8768 
2023-10-26 16:24:03.277933: val_loss -0.8728 
2023-10-26 16:24:03.278426: Pseudo dice [0.8809, 0.9131, 0.9681, 0.681, 0.9359] 
2023-10-26 16:24:03.278672: Epoch time: 3.75 s 
2023-10-26 16:24:04.565261:  
2023-10-26 16:24:04.565569: Epoch 720 
2023-10-26 16:24:04.565810: Current learning rate: 0.00318 
2023-10-26 16:24:08.304428: train_loss -0.8801 
2023-10-26 16:24:08.304865: val_loss -0.881 
2023-10-26 16:24:08.305275: Pseudo dice [0.8853, 0.9177, 0.9701, 0.759, 0.9386] 
2023-10-26 16:24:08.305588: Epoch time: 3.74 s 
2023-10-26 16:24:09.538188:  
2023-10-26 16:24:09.538486: Epoch 721 
2023-10-26 16:24:09.538728: Current learning rate: 0.00317 
2023-10-26 16:24:13.418016: train_loss -0.8787 
2023-10-26 16:24:13.418375: val_loss -0.8539 
2023-10-26 16:24:13.418645: Pseudo dice [0.8798, 0.9118, 0.9687, 0.671, 0.8953] 
2023-10-26 16:24:13.418868: Epoch time: 3.88 s 
2023-10-26 16:24:14.555338:  
2023-10-26 16:24:14.555646: Epoch 722 
2023-10-26 16:24:14.555902: Current learning rate: 0.00316 
2023-10-26 16:24:18.448440: train_loss -0.8719 
2023-10-26 16:24:18.448839: val_loss -0.8744 
2023-10-26 16:24:18.449104: Pseudo dice [0.8816, 0.912, 0.9692, 0.6339, 0.9275] 
2023-10-26 16:24:18.449349: Epoch time: 3.89 s 
2023-10-26 16:24:19.580363:  
2023-10-26 16:24:19.580705: Epoch 723 
2023-10-26 16:24:19.581005: Current learning rate: 0.00315 
2023-10-26 16:24:23.587508: train_loss -0.8797 
2023-10-26 16:24:23.587855: val_loss -0.8695 
2023-10-26 16:24:23.588118: Pseudo dice [0.8825, 0.9117, 0.9683, 0.7565, 0.9197] 
2023-10-26 16:24:23.588347: Epoch time: 4.01 s 
2023-10-26 16:24:24.702212:  
2023-10-26 16:24:24.702506: Epoch 724 
2023-10-26 16:24:24.702743: Current learning rate: 0.00314 
2023-10-26 16:24:28.803504: train_loss -0.8765 
2023-10-26 16:24:28.803954: val_loss -0.8753 
2023-10-26 16:24:28.804318: Pseudo dice [0.8887, 0.9116, 0.9691, 0.6816, 0.9376] 
2023-10-26 16:24:28.804568: Epoch time: 4.1 s 
2023-10-26 16:24:29.926425:  
2023-10-26 16:24:29.926729: Epoch 725 
2023-10-26 16:24:29.926978: Current learning rate: 0.00313 
2023-10-26 16:24:34.078749: train_loss -0.877 
2023-10-26 16:24:34.079147: val_loss -0.865 
2023-10-26 16:24:34.079414: Pseudo dice [0.8759, 0.9177, 0.9684, 0.6568, 0.9307] 
2023-10-26 16:24:34.079655: Epoch time: 4.15 s 
2023-10-26 16:24:35.370513:  
2023-10-26 16:24:35.370853: Epoch 726 
2023-10-26 16:24:35.371110: Current learning rate: 0.00312 
2023-10-26 16:24:39.409077: train_loss -0.8809 
2023-10-26 16:24:39.409540: val_loss -0.876 
2023-10-26 16:24:39.409846: Pseudo dice [0.884, 0.9169, 0.9699, 0.693, 0.9277] 
2023-10-26 16:24:39.410170: Epoch time: 4.04 s 
2023-10-26 16:24:40.562356:  
2023-10-26 16:24:40.562662: Epoch 727 
2023-10-26 16:24:40.562911: Current learning rate: 0.00311 
2023-10-26 16:24:44.527462: train_loss -0.8798 
2023-10-26 16:24:44.527869: val_loss -0.8774 
2023-10-26 16:24:44.528131: Pseudo dice [0.8863, 0.9094, 0.9688, 0.6437, 0.9333] 
2023-10-26 16:24:44.528358: Epoch time: 3.97 s 
2023-10-26 16:24:45.660778:  
2023-10-26 16:24:45.661072: Epoch 728 
2023-10-26 16:24:45.661318: Current learning rate: 0.0031 
2023-10-26 16:24:49.676571: train_loss -0.882 
2023-10-26 16:24:49.677005: val_loss -0.8759 
2023-10-26 16:24:49.677268: Pseudo dice [0.8834, 0.9104, 0.969, 0.6995, 0.9276] 
2023-10-26 16:24:49.677488: Epoch time: 4.02 s 
2023-10-26 16:24:50.808061:  
2023-10-26 16:24:50.808378: Epoch 729 
2023-10-26 16:24:50.808629: Current learning rate: 0.00309 
2023-10-26 16:24:54.745645: train_loss -0.8801 
2023-10-26 16:24:54.746007: val_loss -0.8806 
2023-10-26 16:24:54.746262: Pseudo dice [0.8843, 0.9128, 0.9689, 0.746, 0.939] 
2023-10-26 16:24:54.746490: Epoch time: 3.94 s 
2023-10-26 16:24:55.883792:  
2023-10-26 16:24:55.884090: Epoch 730 
2023-10-26 16:24:55.884334: Current learning rate: 0.00308 
2023-10-26 16:24:59.729800: train_loss -0.8878 
2023-10-26 16:24:59.730218: val_loss -0.8736 
2023-10-26 16:24:59.730499: Pseudo dice [0.8831, 0.9145, 0.9698, 0.6683, 0.9245] 
2023-10-26 16:24:59.730752: Epoch time: 3.85 s 
2023-10-26 16:25:00.869004:  
2023-10-26 16:25:00.869390: Epoch 731 
2023-10-26 16:25:00.869680: Current learning rate: 0.00307 
2023-10-26 16:25:04.870740: train_loss -0.8792 
2023-10-26 16:25:04.871194: val_loss -0.8787 
2023-10-26 16:25:04.871647: Pseudo dice [0.8782, 0.9149, 0.9693, 0.7398, 0.9262] 
2023-10-26 16:25:04.871962: Epoch time: 4.0 s 
2023-10-26 16:25:06.042862:  
2023-10-26 16:25:06.043166: Epoch 732 
2023-10-26 16:25:06.043412: Current learning rate: 0.00306 
2023-10-26 16:25:09.947107: train_loss -0.8892 
2023-10-26 16:25:09.947498: val_loss -0.8755 
2023-10-26 16:25:09.947757: Pseudo dice [0.8887, 0.9188, 0.9699, 0.7044, 0.9384] 
2023-10-26 16:25:09.948009: Epoch time: 3.9 s 
2023-10-26 16:25:11.268337:  
2023-10-26 16:25:11.268641: Epoch 733 
2023-10-26 16:25:11.268884: Current learning rate: 0.00305 
2023-10-26 16:25:15.204597: train_loss -0.8872 
2023-10-26 16:25:15.204957: val_loss -0.8682 
2023-10-26 16:25:15.205204: Pseudo dice [0.8818, 0.9183, 0.9687, 0.6854, 0.9278] 
2023-10-26 16:25:15.205422: Epoch time: 3.94 s 
2023-10-26 16:25:16.336502:  
2023-10-26 16:25:16.336807: Epoch 734 
2023-10-26 16:25:16.337059: Current learning rate: 0.00304 
2023-10-26 16:25:20.180501: train_loss -0.8873 
2023-10-26 16:25:20.180946: val_loss -0.8734 
2023-10-26 16:25:20.181363: Pseudo dice [0.8816, 0.9133, 0.9695, 0.6719, 0.9274] 
2023-10-26 16:25:20.181787: Epoch time: 3.84 s 
2023-10-26 16:25:21.328493:  
2023-10-26 16:25:21.328806: Epoch 735 
2023-10-26 16:25:21.329058: Current learning rate: 0.00303 
2023-10-26 16:25:25.230093: train_loss -0.8885 
2023-10-26 16:25:25.230766: val_loss -0.8781 
2023-10-26 16:25:25.231287: Pseudo dice [0.8814, 0.9149, 0.9671, 0.7641, 0.9296] 
2023-10-26 16:25:25.231563: Epoch time: 3.9 s 
2023-10-26 16:25:26.407305:  
2023-10-26 16:25:26.407655: Epoch 736 
2023-10-26 16:25:26.407931: Current learning rate: 0.00302 
2023-10-26 16:25:30.268699: train_loss -0.8868 
2023-10-26 16:25:30.269104: val_loss -0.8717 
2023-10-26 16:25:30.269371: Pseudo dice [0.8861, 0.9133, 0.9718, 0.7208, 0.9328] 
2023-10-26 16:25:30.269613: Epoch time: 3.86 s 
2023-10-26 16:25:31.400880:  
2023-10-26 16:25:31.401205: Epoch 737 
2023-10-26 16:25:31.401470: Current learning rate: 0.00301 
2023-10-26 16:25:35.347736: train_loss -0.8773 
2023-10-26 16:25:35.348122: val_loss -0.8717 
2023-10-26 16:25:35.348375: Pseudo dice [0.8803, 0.913, 0.9683, 0.7425, 0.9336] 
2023-10-26 16:25:35.348610: Epoch time: 3.95 s 
2023-10-26 16:25:36.494063:  
2023-10-26 16:25:36.494380: Epoch 738 
2023-10-26 16:25:36.494632: Current learning rate: 0.003 
2023-10-26 16:25:40.475639: train_loss -0.8877 
2023-10-26 16:25:40.476019: val_loss -0.8732 
2023-10-26 16:25:40.476290: Pseudo dice [0.8812, 0.9172, 0.9702, 0.7358, 0.9323] 
2023-10-26 16:25:40.476523: Epoch time: 3.98 s 
2023-10-26 16:25:41.790606:  
2023-10-26 16:25:41.791008: Epoch 739 
2023-10-26 16:25:41.791252: Current learning rate: 0.00299 
2023-10-26 16:25:45.879920: train_loss -0.8823 
2023-10-26 16:25:45.880310: val_loss -0.869 
2023-10-26 16:25:45.880563: Pseudo dice [0.8738, 0.9037, 0.9682, 0.7164, 0.9454] 
2023-10-26 16:25:45.880796: Epoch time: 4.09 s 
2023-10-26 16:25:47.029715:  
2023-10-26 16:25:47.030056: Epoch 740 
2023-10-26 16:25:47.030314: Current learning rate: 0.00297 
2023-10-26 16:25:51.125450: train_loss -0.8778 
2023-10-26 16:25:51.125830: val_loss -0.8694 
2023-10-26 16:25:51.126084: Pseudo dice [0.8761, 0.9094, 0.9663, 0.7068, 0.9336] 
2023-10-26 16:25:51.126310: Epoch time: 4.1 s 
2023-10-26 16:25:52.229332:  
2023-10-26 16:25:52.229634: Epoch 741 
2023-10-26 16:25:52.229882: Current learning rate: 0.00296 
2023-10-26 16:25:56.300766: train_loss -0.8837 
2023-10-26 16:25:56.301214: val_loss -0.8781 
2023-10-26 16:25:56.301546: Pseudo dice [0.8867, 0.9133, 0.9693, 0.7887, 0.928] 
2023-10-26 16:25:56.301792: Epoch time: 4.07 s 
2023-10-26 16:25:57.475061:  
2023-10-26 16:25:57.475394: Epoch 742 
2023-10-26 16:25:57.475679: Current learning rate: 0.00295 
2023-10-26 16:26:01.574193: train_loss -0.8907 
2023-10-26 16:26:01.574574: val_loss -0.8773 
2023-10-26 16:26:01.574830: Pseudo dice [0.879, 0.9133, 0.9699, 0.7704, 0.9248] 
2023-10-26 16:26:01.575057: Epoch time: 4.1 s 
2023-10-26 16:26:02.690458:  
2023-10-26 16:26:02.690771: Epoch 743 
2023-10-26 16:26:02.691043: Current learning rate: 0.00294 
2023-10-26 16:26:06.586224: train_loss -0.8856 
2023-10-26 16:26:06.586595: val_loss -0.8793 
2023-10-26 16:26:06.586850: Pseudo dice [0.8838, 0.9151, 0.9705, 0.759, 0.9341] 
2023-10-26 16:26:06.587092: Epoch time: 3.9 s 
2023-10-26 16:26:07.705386:  
2023-10-26 16:26:07.705688: Epoch 744 
2023-10-26 16:26:07.705940: Current learning rate: 0.00293 
2023-10-26 16:26:11.605932: train_loss -0.889 
2023-10-26 16:26:11.606293: val_loss -0.8786 
2023-10-26 16:26:11.606578: Pseudo dice [0.8832, 0.9167, 0.9701, 0.7007, 0.9288] 
2023-10-26 16:26:11.606817: Epoch time: 3.9 s 
2023-10-26 16:26:12.906769:  
2023-10-26 16:26:12.907063: Epoch 745 
2023-10-26 16:26:12.907311: Current learning rate: 0.00292 
2023-10-26 16:26:16.750295: train_loss -0.8872 
2023-10-26 16:26:16.750698: val_loss -0.8793 
2023-10-26 16:26:16.750992: Pseudo dice [0.885, 0.9165, 0.9693, 0.7614, 0.9407] 
2023-10-26 16:26:16.751232: Epoch time: 3.84 s 
2023-10-26 16:26:17.926155:  
2023-10-26 16:26:17.926458: Epoch 746 
2023-10-26 16:26:17.926698: Current learning rate: 0.00291 
2023-10-26 16:26:21.965828: train_loss -0.8852 
2023-10-26 16:26:21.966229: val_loss -0.8791 
2023-10-26 16:26:21.966486: Pseudo dice [0.8874, 0.9178, 0.9669, 0.7629, 0.9412] 
2023-10-26 16:26:21.966726: Epoch time: 4.04 s 
2023-10-26 16:26:23.119131:  
2023-10-26 16:26:23.119747: Epoch 747 
2023-10-26 16:26:23.120016: Current learning rate: 0.0029 
2023-10-26 16:26:27.155233: train_loss -0.8846 
2023-10-26 16:26:27.155706: val_loss -0.8786 
2023-10-26 16:26:27.156129: Pseudo dice [0.887, 0.9145, 0.9686, 0.7191, 0.9389] 
2023-10-26 16:26:27.156484: Epoch time: 4.04 s 
2023-10-26 16:26:28.324651:  
2023-10-26 16:26:28.324950: Epoch 748 
2023-10-26 16:26:28.325186: Current learning rate: 0.00289 
2023-10-26 16:26:32.311462: train_loss -0.8969 
2023-10-26 16:26:32.311839: val_loss -0.8698 
2023-10-26 16:26:32.312126: Pseudo dice [0.8834, 0.9124, 0.9701, 0.7482, 0.9142] 
2023-10-26 16:26:32.312368: Epoch time: 3.99 s 
2023-10-26 16:26:33.457598:  
2023-10-26 16:26:33.457912: Epoch 749 
2023-10-26 16:26:33.458179: Current learning rate: 0.00288 
2023-10-26 16:26:37.404338: train_loss -0.8896 
2023-10-26 16:26:37.404682: val_loss -0.8796 
2023-10-26 16:26:37.404962: Pseudo dice [0.8836, 0.9144, 0.9691, 0.7189, 0.9322] 
2023-10-26 16:26:37.405216: Epoch time: 3.95 s 
2023-10-26 16:26:38.724396:  
2023-10-26 16:26:38.724695: Epoch 750 
2023-10-26 16:26:38.724963: Current learning rate: 0.00287 
2023-10-26 16:26:42.708541: train_loss -0.8833 
2023-10-26 16:26:42.709050: val_loss -0.8673 
2023-10-26 16:26:42.709313: Pseudo dice [0.8742, 0.9012, 0.9655, 0.7329, 0.9232] 
2023-10-26 16:26:42.709646: Epoch time: 3.98 s 
2023-10-26 16:26:44.004305:  
2023-10-26 16:26:44.004607: Epoch 751 
2023-10-26 16:26:44.004844: Current learning rate: 0.00286 
2023-10-26 16:26:47.801023: train_loss -0.8772 
2023-10-26 16:26:47.801400: val_loss -0.8694 
2023-10-26 16:26:47.801673: Pseudo dice [0.8823, 0.913, 0.9664, 0.6378, 0.925] 
2023-10-26 16:26:47.801920: Epoch time: 3.8 s 
2023-10-26 16:26:48.955323:  
2023-10-26 16:26:48.955645: Epoch 752 
2023-10-26 16:26:48.955950: Current learning rate: 0.00285 
2023-10-26 16:26:52.911592: train_loss -0.8807 
2023-10-26 16:26:52.911985: val_loss -0.8762 
2023-10-26 16:26:52.912242: Pseudo dice [0.8784, 0.9165, 0.97, 0.733, 0.9364] 
2023-10-26 16:26:52.912495: Epoch time: 3.96 s 
2023-10-26 16:26:54.089735:  
2023-10-26 16:26:54.090078: Epoch 753 
2023-10-26 16:26:54.090328: Current learning rate: 0.00284 
2023-10-26 16:26:57.894354: train_loss -0.8871 
2023-10-26 16:26:57.894723: val_loss -0.8807 
2023-10-26 16:26:57.895010: Pseudo dice [0.8868, 0.9202, 0.9726, 0.6653, 0.9329] 
2023-10-26 16:26:57.895268: Epoch time: 3.81 s 
2023-10-26 16:26:59.029772:  
2023-10-26 16:26:59.030106: Epoch 754 
2023-10-26 16:26:59.030403: Current learning rate: 0.00283 
2023-10-26 16:27:02.891712: train_loss -0.8888 
2023-10-26 16:27:02.892067: val_loss -0.8709 
2023-10-26 16:27:02.892314: Pseudo dice [0.8777, 0.9128, 0.9692, 0.7333, 0.9088] 
2023-10-26 16:27:02.892581: Epoch time: 3.86 s 
2023-10-26 16:27:03.999565:  
2023-10-26 16:27:03.999863: Epoch 755 
2023-10-26 16:27:04.000120: Current learning rate: 0.00282 
2023-10-26 16:27:07.948306: train_loss -0.8882 
2023-10-26 16:27:07.948650: val_loss -0.8791 
2023-10-26 16:27:07.948895: Pseudo dice [0.8843, 0.914, 0.9704, 0.7103, 0.9324] 
2023-10-26 16:27:07.949108: Epoch time: 3.95 s 
2023-10-26 16:27:09.064000:  
2023-10-26 16:27:09.064293: Epoch 756 
2023-10-26 16:27:09.064574: Current learning rate: 0.00281 
2023-10-26 16:27:12.988485: train_loss -0.893 
2023-10-26 16:27:12.988857: val_loss -0.8762 
2023-10-26 16:27:12.989110: Pseudo dice [0.8828, 0.9159, 0.9686, 0.6747, 0.9283] 
2023-10-26 16:27:12.989335: Epoch time: 3.93 s 
2023-10-26 16:27:14.251697:  
2023-10-26 16:27:14.251997: Epoch 757 
2023-10-26 16:27:14.252239: Current learning rate: 0.0028 
2023-10-26 16:27:18.264997: train_loss -0.8936 
2023-10-26 16:27:18.265378: val_loss -0.8734 
2023-10-26 16:27:18.265635: Pseudo dice [0.8802, 0.9137, 0.9691, 0.7006, 0.9374] 
2023-10-26 16:27:18.265862: Epoch time: 4.01 s 
2023-10-26 16:27:19.389811:  
2023-10-26 16:27:19.390099: Epoch 758 
2023-10-26 16:27:19.390334: Current learning rate: 0.00279 
2023-10-26 16:27:23.408153: train_loss -0.8799 
2023-10-26 16:27:23.408533: val_loss -0.8725 
2023-10-26 16:27:23.408797: Pseudo dice [0.8826, 0.9117, 0.9683, 0.6993, 0.9381] 
2023-10-26 16:27:23.409028: Epoch time: 4.02 s 
2023-10-26 16:27:24.529808:  
2023-10-26 16:27:24.530092: Epoch 759 
2023-10-26 16:27:24.530326: Current learning rate: 0.00278 
2023-10-26 16:27:28.475445: train_loss -0.8892 
2023-10-26 16:27:28.475893: val_loss -0.8753 
2023-10-26 16:27:28.476179: Pseudo dice [0.8819, 0.9156, 0.9697, 0.6831, 0.9344] 
2023-10-26 16:27:28.476413: Epoch time: 3.95 s 
2023-10-26 16:27:29.622243:  
2023-10-26 16:27:29.622561: Epoch 760 
2023-10-26 16:27:29.622828: Current learning rate: 0.00277 
2023-10-26 16:27:33.535730: train_loss -0.8839 
2023-10-26 16:27:33.536116: val_loss -0.8775 
2023-10-26 16:27:33.536374: Pseudo dice [0.8837, 0.9212, 0.9689, 0.679, 0.9305] 
2023-10-26 16:27:33.536600: Epoch time: 3.91 s 
2023-10-26 16:27:34.676638:  
2023-10-26 16:27:34.676934: Epoch 761 
2023-10-26 16:27:34.677178: Current learning rate: 0.00276 
2023-10-26 16:27:38.549628: train_loss -0.8849 
2023-10-26 16:27:38.550143: val_loss -0.8754 
2023-10-26 16:27:38.550395: Pseudo dice [0.8872, 0.9127, 0.9707, 0.6917, 0.9262] 
2023-10-26 16:27:38.550625: Epoch time: 3.87 s 
2023-10-26 16:27:39.684104:  
2023-10-26 16:27:39.684391: Epoch 762 
2023-10-26 16:27:39.684624: Current learning rate: 0.00275 
2023-10-26 16:27:43.831896: train_loss -0.8749 
2023-10-26 16:27:43.832279: val_loss -0.8791 
2023-10-26 16:27:43.832539: Pseudo dice [0.8833, 0.911, 0.9695, 0.7143, 0.9175] 
2023-10-26 16:27:43.832763: Epoch time: 4.15 s 
2023-10-26 16:27:45.150792:  
2023-10-26 16:27:45.151180: Epoch 763 
2023-10-26 16:27:45.151516: Current learning rate: 0.00274 
2023-10-26 16:27:49.179600: train_loss -0.8923 
2023-10-26 16:27:49.179976: val_loss -0.875 
2023-10-26 16:27:49.180234: Pseudo dice [0.8827, 0.9131, 0.9706, 0.6501, 0.9266] 
2023-10-26 16:27:49.180461: Epoch time: 4.03 s 
2023-10-26 16:27:50.296966:  
2023-10-26 16:27:50.297253: Epoch 764 
2023-10-26 16:27:50.297486: Current learning rate: 0.00273 
2023-10-26 16:27:54.336268: train_loss -0.8907 
2023-10-26 16:27:54.337283: val_loss -0.871 
2023-10-26 16:27:54.337544: Pseudo dice [0.8847, 0.9234, 0.9698, 0.5506, 0.932] 
2023-10-26 16:27:54.337782: Epoch time: 4.04 s 
2023-10-26 16:27:55.469464:  
2023-10-26 16:27:55.469753: Epoch 765 
2023-10-26 16:27:55.470003: Current learning rate: 0.00272 
2023-10-26 16:27:59.368615: train_loss -0.8822 
2023-10-26 16:27:59.369159: val_loss -0.8782 
2023-10-26 16:27:59.369448: Pseudo dice [0.8803, 0.9139, 0.9702, 0.7622, 0.9357] 
2023-10-26 16:27:59.369729: Epoch time: 3.9 s 
2023-10-26 16:28:00.501823:  
2023-10-26 16:28:00.502128: Epoch 766 
2023-10-26 16:28:00.502370: Current learning rate: 0.00271 
2023-10-26 16:28:04.390591: train_loss -0.8899 
2023-10-26 16:28:04.391019: val_loss -0.8795 
2023-10-26 16:28:04.391294: Pseudo dice [0.8825, 0.9122, 0.9702, 0.7549, 0.9389] 
2023-10-26 16:28:04.391532: Epoch time: 3.89 s 
2023-10-26 16:28:05.555495:  
2023-10-26 16:28:05.555808: Epoch 767 
2023-10-26 16:28:05.556078: Current learning rate: 0.0027 
2023-10-26 16:28:09.433820: train_loss -0.8906 
2023-10-26 16:28:09.434501: val_loss -0.876 
2023-10-26 16:28:09.434966: Pseudo dice [0.8823, 0.9134, 0.9686, 0.6793, 0.9212] 
2023-10-26 16:28:09.435344: Epoch time: 3.88 s 
2023-10-26 16:28:10.567029:  
2023-10-26 16:28:10.567325: Epoch 768 
2023-10-26 16:28:10.567568: Current learning rate: 0.00268 
2023-10-26 16:28:16.111934: train_loss -0.8944 
2023-10-26 16:28:16.112485: val_loss -0.8835 
2023-10-26 16:28:16.112896: Pseudo dice [0.8853, 0.9119, 0.9702, 0.7473, 0.9408] 
2023-10-26 16:28:16.113193: Epoch time: 5.55 s 
2023-10-26 16:28:18.103171:  
2023-10-26 16:28:18.103502: Epoch 769 
2023-10-26 16:28:18.103741: Current learning rate: 0.00267 
2023-10-26 16:28:22.080816: train_loss -0.8908 
2023-10-26 16:28:22.081198: val_loss -0.8794 
2023-10-26 16:28:22.081448: Pseudo dice [0.8832, 0.9147, 0.9704, 0.7154, 0.9341] 
2023-10-26 16:28:22.081669: Epoch time: 3.98 s 
2023-10-26 16:28:23.237427:  
2023-10-26 16:28:23.237722: Epoch 770 
2023-10-26 16:28:23.237960: Current learning rate: 0.00266 
2023-10-26 16:28:27.350885: train_loss -0.8942 
2023-10-26 16:28:27.351567: val_loss -0.8752 
2023-10-26 16:28:27.351828: Pseudo dice [0.8824, 0.915, 0.9699, 0.7309, 0.932] 
2023-10-26 16:28:27.352424: Epoch time: 4.11 s 
2023-10-26 16:28:28.501895:  
2023-10-26 16:28:28.502199: Epoch 771 
2023-10-26 16:28:28.502447: Current learning rate: 0.00265 
2023-10-26 16:28:32.584043: train_loss -0.9003 
2023-10-26 16:28:32.584429: val_loss -0.8778 
2023-10-26 16:28:32.584689: Pseudo dice [0.8824, 0.9156, 0.9693, 0.7194, 0.9309] 
2023-10-26 16:28:32.584933: Epoch time: 4.08 s 
2023-10-26 16:28:33.733219:  
2023-10-26 16:28:33.733499: Epoch 772 
2023-10-26 16:28:33.733729: Current learning rate: 0.00264 
2023-10-26 16:28:37.659436: train_loss -0.8965 
2023-10-26 16:28:37.659972: val_loss -0.8803 
2023-10-26 16:28:37.660431: Pseudo dice [0.886, 0.9177, 0.9691, 0.7256, 0.9286] 
2023-10-26 16:28:37.660793: Epoch time: 3.93 s 
2023-10-26 16:28:38.825138:  
2023-10-26 16:28:38.825459: Epoch 773 
2023-10-26 16:28:38.825751: Current learning rate: 0.00263 
2023-10-26 16:28:42.694696: train_loss -0.8914 
2023-10-26 16:28:42.695069: val_loss -0.882 
2023-10-26 16:28:42.695325: Pseudo dice [0.8845, 0.9116, 0.9696, 0.7233, 0.9347] 
2023-10-26 16:28:42.695568: Epoch time: 3.87 s 
2023-10-26 16:28:43.865556:  
2023-10-26 16:28:43.865865: Epoch 774 
2023-10-26 16:28:43.866159: Current learning rate: 0.00262 
2023-10-26 16:28:47.717582: train_loss -0.8977 
2023-10-26 16:28:47.718005: val_loss -0.6848 
2023-10-26 16:28:47.718392: Pseudo dice [0.8855, 0.92, 0.9707, 0.7229, 0.6605] 
2023-10-26 16:28:47.718652: Epoch time: 3.85 s 
2023-10-26 16:28:49.063994:  
2023-10-26 16:28:49.064313: Epoch 775 
2023-10-26 16:28:49.064599: Current learning rate: 0.00261 
2023-10-26 16:28:53.053421: train_loss -0.8802 
2023-10-26 16:28:53.053929: val_loss -0.8722 
2023-10-26 16:28:53.054286: Pseudo dice [0.8832, 0.9119, 0.9712, 0.713, 0.933] 
2023-10-26 16:28:53.054609: Epoch time: 3.99 s 
2023-10-26 16:28:54.275984:  
2023-10-26 16:28:54.276281: Epoch 776 
2023-10-26 16:28:54.276521: Current learning rate: 0.0026 
2023-10-26 16:28:58.200973: train_loss -0.8862 
2023-10-26 16:28:58.201407: val_loss -0.878 
2023-10-26 16:28:58.201677: Pseudo dice [0.8839, 0.9176, 0.9692, 0.7459, 0.936] 
2023-10-26 16:28:58.201923: Epoch time: 3.93 s 
2023-10-26 16:28:59.414914:  
2023-10-26 16:28:59.415227: Epoch 777 
2023-10-26 16:28:59.415465: Current learning rate: 0.00259 
2023-10-26 16:29:03.432830: train_loss -0.8889 
2023-10-26 16:29:03.433208: val_loss -0.882 
2023-10-26 16:29:03.433517: Pseudo dice [0.8851, 0.9175, 0.9713, 0.6863, 0.9368] 
2023-10-26 16:29:03.433769: Epoch time: 4.02 s 
2023-10-26 16:29:04.616943:  
2023-10-26 16:29:04.617254: Epoch 778 
2023-10-26 16:29:04.617509: Current learning rate: 0.00258 
2023-10-26 16:29:08.577435: train_loss -0.8887 
2023-10-26 16:29:08.577840: val_loss -0.8811 
2023-10-26 16:29:08.578108: Pseudo dice [0.8825, 0.9185, 0.9696, 0.6761, 0.9297] 
2023-10-26 16:29:08.578334: Epoch time: 3.96 s 
2023-10-26 16:29:09.696469:  
2023-10-26 16:29:09.696765: Epoch 779 
2023-10-26 16:29:09.697020: Current learning rate: 0.00257 
2023-10-26 16:29:13.624362: train_loss -0.8922 
2023-10-26 16:29:13.624740: val_loss -0.8787 
2023-10-26 16:29:13.625012: Pseudo dice [0.8816, 0.9158, 0.9676, 0.689, 0.9352] 
2023-10-26 16:29:13.625250: Epoch time: 3.93 s 
2023-10-26 16:29:14.737312:  
2023-10-26 16:29:14.737626: Epoch 780 
2023-10-26 16:29:14.737877: Current learning rate: 0.00256 
2023-10-26 16:29:18.803543: train_loss -0.8994 
2023-10-26 16:29:18.803958: val_loss -0.8762 
2023-10-26 16:29:18.804412: Pseudo dice [0.8877, 0.9168, 0.9704, 0.7513, 0.9377] 
2023-10-26 16:29:18.804684: Epoch time: 4.07 s 
2023-10-26 16:29:20.074143:  
2023-10-26 16:29:20.074431: Epoch 781 
2023-10-26 16:29:20.074667: Current learning rate: 0.00255 
2023-10-26 16:29:24.100165: train_loss -0.874 
2023-10-26 16:29:24.100520: val_loss -0.8689 
2023-10-26 16:29:24.100768: Pseudo dice [0.887, 0.9156, 0.9681, 0.704, 0.9203] 
2023-10-26 16:29:24.100991: Epoch time: 4.03 s 
2023-10-26 16:29:25.259957:  
2023-10-26 16:29:25.260247: Epoch 782 
2023-10-26 16:29:25.260485: Current learning rate: 0.00254 
2023-10-26 16:29:29.179831: train_loss -0.8761 
2023-10-26 16:29:29.180239: val_loss -0.8787 
2023-10-26 16:29:29.180514: Pseudo dice [0.8848, 0.911, 0.9692, 0.706, 0.9361] 
2023-10-26 16:29:29.180733: Epoch time: 3.92 s 
2023-10-26 16:29:30.312423:  
2023-10-26 16:29:30.312718: Epoch 783 
2023-10-26 16:29:30.312965: Current learning rate: 0.00253 
2023-10-26 16:29:34.329355: train_loss -0.8869 
2023-10-26 16:29:34.329813: val_loss -0.883 
2023-10-26 16:29:34.330165: Pseudo dice [0.8816, 0.9168, 0.971, 0.6976, 0.9257] 
2023-10-26 16:29:34.330519: Epoch time: 4.02 s 
2023-10-26 16:29:35.536211:  
2023-10-26 16:29:35.536548: Epoch 784 
2023-10-26 16:29:35.536803: Current learning rate: 0.00252 
2023-10-26 16:29:39.437300: train_loss -0.8878 
2023-10-26 16:29:39.437707: val_loss -0.8735 
2023-10-26 16:29:39.437963: Pseudo dice [0.8805, 0.9144, 0.9681, 0.744, 0.9308] 
2023-10-26 16:29:39.438196: Epoch time: 3.9 s 
2023-10-26 16:29:40.591966:  
2023-10-26 16:29:40.592277: Epoch 785 
2023-10-26 16:29:40.592524: Current learning rate: 0.00251 
2023-10-26 16:29:44.387007: train_loss -0.8917 
2023-10-26 16:29:44.387368: val_loss -0.8807 
2023-10-26 16:29:44.387613: Pseudo dice [0.8832, 0.9185, 0.971, 0.7433, 0.9311] 
2023-10-26 16:29:44.387828: Epoch time: 3.8 s 
2023-10-26 16:29:45.517969:  
2023-10-26 16:29:45.518263: Epoch 786 
2023-10-26 16:29:45.518502: Current learning rate: 0.0025 
2023-10-26 16:29:49.568398: train_loss -0.8851 
2023-10-26 16:29:49.568792: val_loss -0.8806 
2023-10-26 16:29:49.569078: Pseudo dice [0.8812, 0.9164, 0.9692, 0.7282, 0.9352] 
2023-10-26 16:29:49.569308: Epoch time: 4.05 s 
2023-10-26 16:29:50.901845:  
2023-10-26 16:29:50.902167: Epoch 787 
2023-10-26 16:29:50.902402: Current learning rate: 0.00249 
2023-10-26 16:29:54.905971: train_loss -0.8885 
2023-10-26 16:29:54.906358: val_loss -0.8825 
2023-10-26 16:29:54.906639: Pseudo dice [0.885, 0.9135, 0.9696, 0.7209, 0.9381] 
2023-10-26 16:29:54.906859: Epoch time: 4.0 s 
2023-10-26 16:29:56.038768:  
2023-10-26 16:29:56.039064: Epoch 788 
2023-10-26 16:29:56.039313: Current learning rate: 0.00248 
2023-10-26 16:29:59.934506: train_loss -0.891 
2023-10-26 16:29:59.934951: val_loss -0.8797 
2023-10-26 16:29:59.935219: Pseudo dice [0.883, 0.9088, 0.9684, 0.7413, 0.9224] 
2023-10-26 16:29:59.935545: Epoch time: 3.9 s 
2023-10-26 16:30:01.080394:  
2023-10-26 16:30:01.080683: Epoch 789 
2023-10-26 16:30:01.080936: Current learning rate: 0.00247 
2023-10-26 16:30:05.121020: train_loss -0.892 
2023-10-26 16:30:05.121408: val_loss -0.8793 
2023-10-26 16:30:05.121665: Pseudo dice [0.8876, 0.9185, 0.969, 0.7587, 0.9346] 
2023-10-26 16:30:05.121918: Epoch time: 4.04 s 
2023-10-26 16:30:06.304228:  
2023-10-26 16:30:06.304515: Epoch 790 
2023-10-26 16:30:06.304782: Current learning rate: 0.00245 
2023-10-26 16:30:10.261739: train_loss -0.8886 
2023-10-26 16:30:10.262140: val_loss -0.8772 
2023-10-26 16:30:10.262401: Pseudo dice [0.8803, 0.9162, 0.9687, 0.7179, 0.9235] 
2023-10-26 16:30:10.262637: Epoch time: 3.96 s 
2023-10-26 16:30:11.462250:  
2023-10-26 16:30:11.462545: Epoch 791 
2023-10-26 16:30:11.462783: Current learning rate: 0.00244 
2023-10-26 16:30:15.408448: train_loss -0.8869 
2023-10-26 16:30:15.408808: val_loss -0.876 
2023-10-26 16:30:15.409076: Pseudo dice [0.8863, 0.9154, 0.969, 0.7341, 0.9306] 
2023-10-26 16:30:15.409303: Epoch time: 3.95 s 
2023-10-26 16:30:16.551890:  
2023-10-26 16:30:16.552195: Epoch 792 
2023-10-26 16:30:16.552442: Current learning rate: 0.00243 
2023-10-26 16:30:20.555197: train_loss -0.8909 
2023-10-26 16:30:20.555604: val_loss -0.8718 
2023-10-26 16:30:20.555888: Pseudo dice [0.8785, 0.9182, 0.9694, 0.634, 0.9202] 
2023-10-26 16:30:20.556181: Epoch time: 4.0 s 
2023-10-26 16:30:21.735412:  
2023-10-26 16:30:21.735732: Epoch 793 
2023-10-26 16:30:21.735975: Current learning rate: 0.00242 
2023-10-26 16:30:25.763807: train_loss -0.8883 
2023-10-26 16:30:25.764166: val_loss -0.8805 
2023-10-26 16:30:25.764415: Pseudo dice [0.8823, 0.9147, 0.9682, 0.7134, 0.9328] 
2023-10-26 16:30:25.764643: Epoch time: 4.03 s 
2023-10-26 16:30:27.079450:  
2023-10-26 16:30:27.079780: Epoch 794 
2023-10-26 16:30:27.080048: Current learning rate: 0.00241 
2023-10-26 16:30:31.109547: train_loss -0.8972 
2023-10-26 16:30:31.110183: val_loss -0.8787 
2023-10-26 16:30:31.110444: Pseudo dice [0.8843, 0.9147, 0.9698, 0.7282, 0.9311] 
2023-10-26 16:30:31.110684: Epoch time: 4.03 s 
2023-10-26 16:30:32.254297:  
2023-10-26 16:30:32.254595: Epoch 795 
2023-10-26 16:30:32.254869: Current learning rate: 0.0024 
2023-10-26 16:30:36.304856: train_loss -0.894 
2023-10-26 16:30:36.305256: val_loss -0.883 
2023-10-26 16:30:36.305520: Pseudo dice [0.8882, 0.9216, 0.9699, 0.7679, 0.9336] 
2023-10-26 16:30:36.305791: Epoch time: 4.05 s 
2023-10-26 16:30:37.530759:  
2023-10-26 16:30:37.531110: Epoch 796 
2023-10-26 16:30:37.531452: Current learning rate: 0.00239 
2023-10-26 16:30:41.571748: train_loss -0.8774 
2023-10-26 16:30:41.572121: val_loss -0.8795 
2023-10-26 16:30:41.572374: Pseudo dice [0.8807, 0.9169, 0.9684, 0.7824, 0.9163] 
2023-10-26 16:30:41.572601: Epoch time: 4.04 s 
2023-10-26 16:30:42.698329:  
2023-10-26 16:30:42.698637: Epoch 797 
2023-10-26 16:30:42.698896: Current learning rate: 0.00238 
2023-10-26 16:30:46.743665: train_loss -0.8864 
2023-10-26 16:30:46.744038: val_loss -0.8726 
2023-10-26 16:30:46.744327: Pseudo dice [0.8788, 0.9192, 0.9671, 0.7377, 0.9262] 
2023-10-26 16:30:46.744565: Epoch time: 4.05 s 
2023-10-26 16:30:47.867805:  
2023-10-26 16:30:47.868097: Epoch 798 
2023-10-26 16:30:47.868361: Current learning rate: 0.00237 
2023-10-26 16:30:51.925529: train_loss -0.8841 
2023-10-26 16:30:51.925934: val_loss -0.8632 
2023-10-26 16:30:51.926197: Pseudo dice [0.8779, 0.9155, 0.9685, 0.5941, 0.9311] 
2023-10-26 16:30:51.926449: Epoch time: 4.06 s 
2023-10-26 16:30:53.126244:  
2023-10-26 16:30:53.126595: Epoch 799 
2023-10-26 16:30:53.126914: Current learning rate: 0.00236 
2023-10-26 16:30:57.174718: train_loss -0.8851 
2023-10-26 16:30:57.175135: val_loss -0.8637 
2023-10-26 16:30:57.175441: Pseudo dice [0.8794, 0.9062, 0.9672, 0.7581, 0.9234] 
2023-10-26 16:30:57.175700: Epoch time: 4.05 s 
2023-10-26 16:30:58.593444:  
2023-10-26 16:30:58.593774: Epoch 800 
2023-10-26 16:30:58.594065: Current learning rate: 0.00235 
2023-10-26 16:31:02.680480: train_loss -0.8814 
2023-10-26 16:31:02.680897: val_loss -0.876 
2023-10-26 16:31:02.681175: Pseudo dice [0.8844, 0.9164, 0.9687, 0.7704, 0.9301] 
2023-10-26 16:31:02.681411: Epoch time: 4.09 s 
2023-10-26 16:31:03.820001:  
2023-10-26 16:31:03.820321: Epoch 801 
2023-10-26 16:31:03.820565: Current learning rate: 0.00234 
2023-10-26 16:31:07.797381: train_loss -0.8929 
2023-10-26 16:31:07.798124: val_loss -0.8843 
2023-10-26 16:31:07.798427: Pseudo dice [0.8835, 0.9137, 0.9693, 0.7642, 0.9313] 
2023-10-26 16:31:07.798670: Epoch time: 3.98 s 
2023-10-26 16:31:08.948144:  
2023-10-26 16:31:08.948450: Epoch 802 
2023-10-26 16:31:08.948682: Current learning rate: 0.00233 
2023-10-26 16:31:12.911815: train_loss -0.8939 
2023-10-26 16:31:12.912218: val_loss -0.8812 
2023-10-26 16:31:12.912483: Pseudo dice [0.8811, 0.9178, 0.9709, 0.752, 0.927] 
2023-10-26 16:31:12.912711: Epoch time: 3.96 s 
2023-10-26 16:31:14.077948:  
2023-10-26 16:31:14.078235: Epoch 803 
2023-10-26 16:31:14.078480: Current learning rate: 0.00232 
2023-10-26 16:31:17.790980: train_loss -0.8903 
2023-10-26 16:31:17.791438: val_loss -0.8843 
2023-10-26 16:31:17.791702: Pseudo dice [0.8844, 0.9128, 0.9696, 0.7601, 0.9243] 
2023-10-26 16:31:17.791941: Epoch time: 3.71 s 
2023-10-26 16:31:18.918950:  
2023-10-26 16:31:18.919240: Epoch 804 
2023-10-26 16:31:18.919482: Current learning rate: 0.00231 
2023-10-26 16:31:22.850650: train_loss -0.8933 
2023-10-26 16:31:22.851043: val_loss -0.8806 
2023-10-26 16:31:22.851378: Pseudo dice [0.8812, 0.9113, 0.9706, 0.6813, 0.9338] 
2023-10-26 16:31:22.851686: Epoch time: 3.93 s 
2023-10-26 16:31:24.175136:  
2023-10-26 16:31:24.175437: Epoch 805 
2023-10-26 16:31:24.175677: Current learning rate: 0.0023 
2023-10-26 16:31:28.216515: train_loss -0.8899 
2023-10-26 16:31:28.216898: val_loss -0.8736 
2023-10-26 16:31:28.217138: Pseudo dice [0.8796, 0.916, 0.9693, 0.7889, 0.9276] 
2023-10-26 16:31:28.217349: Epoch time: 4.04 s 
2023-10-26 16:31:29.350151:  
2023-10-26 16:31:29.350449: Epoch 806 
2023-10-26 16:31:29.350701: Current learning rate: 0.00229 
2023-10-26 16:31:33.351759: train_loss -0.8942 
2023-10-26 16:31:33.352186: val_loss -0.8758 
2023-10-26 16:31:33.352454: Pseudo dice [0.8807, 0.9116, 0.9685, 0.7485, 0.9234] 
2023-10-26 16:31:33.352704: Epoch time: 4.0 s 
2023-10-26 16:31:34.495270:  
2023-10-26 16:31:34.495567: Epoch 807 
2023-10-26 16:31:34.495811: Current learning rate: 0.00228 
2023-10-26 16:31:38.376727: train_loss -0.8887 
2023-10-26 16:31:38.377127: val_loss -0.8718 
2023-10-26 16:31:38.377408: Pseudo dice [0.881, 0.908, 0.9667, 0.6904, 0.9437] 
2023-10-26 16:31:38.377652: Epoch time: 3.88 s 
2023-10-26 16:31:39.546624:  
2023-10-26 16:31:39.546917: Epoch 808 
2023-10-26 16:31:39.547166: Current learning rate: 0.00226 
2023-10-26 16:31:43.336260: train_loss -0.8981 
2023-10-26 16:31:43.336626: val_loss -0.8819 
2023-10-26 16:31:43.336885: Pseudo dice [0.8887, 0.9174, 0.9697, 0.7881, 0.9319] 
2023-10-26 16:31:43.337159: Epoch time: 3.79 s 
2023-10-26 16:31:44.533777:  
2023-10-26 16:31:44.534118: Epoch 809 
2023-10-26 16:31:44.534365: Current learning rate: 0.00225 
2023-10-26 16:31:48.428955: train_loss -0.8856 
2023-10-26 16:31:48.429362: val_loss -0.8803 
2023-10-26 16:31:48.429636: Pseudo dice [0.8869, 0.9143, 0.9704, 0.7778, 0.9308] 
2023-10-26 16:31:48.429910: Epoch time: 3.9 s 
2023-10-26 16:31:49.581496:  
2023-10-26 16:31:49.581789: Epoch 810 
2023-10-26 16:31:49.582026: Current learning rate: 0.00224 
2023-10-26 16:31:53.539845: train_loss -0.8919 
2023-10-26 16:31:53.540252: val_loss -0.8788 
2023-10-26 16:31:53.540543: Pseudo dice [0.8825, 0.9071, 0.968, 0.7571, 0.9335] 
2023-10-26 16:31:53.540815: Epoch time: 3.96 s 
2023-10-26 16:31:54.909316:  
2023-10-26 16:31:54.909611: Epoch 811 
2023-10-26 16:31:54.909843: Current learning rate: 0.00223 
2023-10-26 16:31:58.827351: train_loss -0.8912 
2023-10-26 16:31:58.827705: val_loss -0.8838 
2023-10-26 16:31:58.827953: Pseudo dice [0.8879, 0.9165, 0.9688, 0.7233, 0.9258] 
2023-10-26 16:31:58.828169: Epoch time: 3.92 s 
2023-10-26 16:31:59.983134:  
2023-10-26 16:31:59.983445: Epoch 812 
2023-10-26 16:31:59.983707: Current learning rate: 0.00222 
2023-10-26 16:32:04.008133: train_loss -0.8889 
2023-10-26 16:32:04.008680: val_loss -0.8663 
2023-10-26 16:32:04.009010: Pseudo dice [0.8799, 0.9175, 0.9666, 0.7129, 0.9248] 
2023-10-26 16:32:04.009284: Epoch time: 4.03 s 
2023-10-26 16:32:05.172076:  
2023-10-26 16:32:05.172375: Epoch 813 
2023-10-26 16:32:05.172618: Current learning rate: 0.00221 
2023-10-26 16:32:09.158075: train_loss -0.8894 
2023-10-26 16:32:09.158517: val_loss -0.8749 
2023-10-26 16:32:09.158931: Pseudo dice [0.8823, 0.914, 0.9696, 0.6867, 0.9332] 
2023-10-26 16:32:09.159320: Epoch time: 3.99 s 
2023-10-26 16:32:10.288527:  
2023-10-26 16:32:10.288810: Epoch 814 
2023-10-26 16:32:10.289138: Current learning rate: 0.0022 
2023-10-26 16:32:14.311909: train_loss -0.8926 
2023-10-26 16:32:14.312262: val_loss -0.8772 
2023-10-26 16:32:14.312504: Pseudo dice [0.8867, 0.9126, 0.9699, 0.7544, 0.926] 
2023-10-26 16:32:14.312750: Epoch time: 4.02 s 
2023-10-26 16:32:15.435567:  
2023-10-26 16:32:15.435860: Epoch 815 
2023-10-26 16:32:15.436124: Current learning rate: 0.00219 
2023-10-26 16:32:19.443708: train_loss -0.8947 
2023-10-26 16:32:19.444098: val_loss -0.8788 
2023-10-26 16:32:19.444359: Pseudo dice [0.881, 0.9172, 0.97, 0.7048, 0.9268] 
2023-10-26 16:32:19.444624: Epoch time: 4.01 s 
2023-10-26 16:32:20.598903:  
2023-10-26 16:32:20.599195: Epoch 816 
2023-10-26 16:32:20.599437: Current learning rate: 0.00218 
2023-10-26 16:32:24.655663: train_loss -0.8991 
2023-10-26 16:32:24.656062: val_loss -0.881 
2023-10-26 16:32:24.656320: Pseudo dice [0.8903, 0.917, 0.9699, 0.7381, 0.9401] 
2023-10-26 16:32:24.656559: Epoch time: 4.06 s 
2023-10-26 16:32:25.958366:  
2023-10-26 16:32:25.958681: Epoch 817 
2023-10-26 16:32:25.958925: Current learning rate: 0.00217 
2023-10-26 16:32:29.973587: train_loss -0.9011 
2023-10-26 16:32:29.974014: val_loss -0.8776 
2023-10-26 16:32:29.974301: Pseudo dice [0.8871, 0.9168, 0.9679, 0.7827, 0.9351] 
2023-10-26 16:32:29.974546: Epoch time: 4.02 s 
2023-10-26 16:32:31.127475:  
2023-10-26 16:32:31.127806: Epoch 818 
2023-10-26 16:32:31.128121: Current learning rate: 0.00216 
2023-10-26 16:32:35.099368: train_loss -0.8946 
2023-10-26 16:32:35.099891: val_loss -0.8714 
2023-10-26 16:32:35.100159: Pseudo dice [0.8838, 0.9127, 0.968, 0.6485, 0.926] 
2023-10-26 16:32:35.100395: Epoch time: 3.97 s 
2023-10-26 16:32:36.249328:  
2023-10-26 16:32:36.249651: Epoch 819 
2023-10-26 16:32:36.249949: Current learning rate: 0.00215 
2023-10-26 16:32:40.230756: train_loss -0.8917 
2023-10-26 16:32:40.231153: val_loss -0.8673 
2023-10-26 16:32:40.231418: Pseudo dice [0.8822, 0.919, 0.9676, 0.7242, 0.9095] 
2023-10-26 16:32:40.231652: Epoch time: 3.98 s 
2023-10-26 16:32:41.338312:  
2023-10-26 16:32:41.338616: Epoch 820 
2023-10-26 16:32:41.338855: Current learning rate: 0.00214 
2023-10-26 16:32:45.229006: train_loss -0.8882 
2023-10-26 16:32:45.229374: val_loss -0.8743 
2023-10-26 16:32:45.229626: Pseudo dice [0.8812, 0.9125, 0.9671, 0.7443, 0.9358] 
2023-10-26 16:32:45.229881: Epoch time: 3.89 s 
2023-10-26 16:32:46.315109:  
2023-10-26 16:32:46.315413: Epoch 821 
2023-10-26 16:32:46.315673: Current learning rate: 0.00213 
2023-10-26 16:32:50.285990: train_loss -0.886 
2023-10-26 16:32:50.286382: val_loss -0.8697 
2023-10-26 16:32:50.286651: Pseudo dice [0.882, 0.9203, 0.9702, 0.7085, 0.9303] 
2023-10-26 16:32:50.286910: Epoch time: 3.97 s 
2023-10-26 16:32:51.415754:  
2023-10-26 16:32:51.416066: Epoch 822 
2023-10-26 16:32:51.416310: Current learning rate: 0.00212 
2023-10-26 16:32:55.332499: train_loss -0.8932 
2023-10-26 16:32:55.332902: val_loss -0.8766 
2023-10-26 16:32:55.333170: Pseudo dice [0.8803, 0.9167, 0.9698, 0.6943, 0.9349] 
2023-10-26 16:32:55.333413: Epoch time: 3.92 s 
2023-10-26 16:32:56.633462:  
2023-10-26 16:32:56.633769: Epoch 823 
2023-10-26 16:32:56.634020: Current learning rate: 0.0021 
2023-10-26 16:33:00.669589: train_loss -0.895 
2023-10-26 16:33:00.670027: val_loss -0.8831 
2023-10-26 16:33:00.670303: Pseudo dice [0.8841, 0.9183, 0.9711, 0.7557, 0.9386] 
2023-10-26 16:33:00.670545: Epoch time: 4.04 s 
2023-10-26 16:33:01.756109:  
2023-10-26 16:33:01.756403: Epoch 824 
2023-10-26 16:33:01.756642: Current learning rate: 0.00209 
2023-10-26 16:33:05.555586: train_loss -0.8883 
2023-10-26 16:33:05.555995: val_loss -0.8784 
2023-10-26 16:33:05.556261: Pseudo dice [0.8834, 0.9119, 0.9685, 0.7757, 0.9257] 
2023-10-26 16:33:05.556489: Epoch time: 3.8 s 
2023-10-26 16:33:06.650069:  
2023-10-26 16:33:06.650364: Epoch 825 
2023-10-26 16:33:06.650615: Current learning rate: 0.00208 
2023-10-26 16:33:10.560001: train_loss -0.886 
2023-10-26 16:33:10.560367: val_loss -0.8811 
2023-10-26 16:33:10.560623: Pseudo dice [0.8841, 0.9144, 0.968, 0.673, 0.9358] 
2023-10-26 16:33:10.560840: Epoch time: 3.91 s 
2023-10-26 16:33:11.651646:  
2023-10-26 16:33:11.651953: Epoch 826 
2023-10-26 16:33:11.652195: Current learning rate: 0.00207 
2023-10-26 16:33:15.606310: train_loss -0.8976 
2023-10-26 16:33:15.606714: val_loss -0.8774 
2023-10-26 16:33:15.607151: Pseudo dice [0.8803, 0.9176, 0.9696, 0.7392, 0.9363] 
2023-10-26 16:33:15.607454: Epoch time: 3.96 s 
2023-10-26 16:33:16.732208:  
2023-10-26 16:33:16.732516: Epoch 827 
2023-10-26 16:33:16.732766: Current learning rate: 0.00206 
2023-10-26 16:33:20.710676: train_loss -0.8953 
2023-10-26 16:33:20.711144: val_loss -0.8812 
2023-10-26 16:33:20.711429: Pseudo dice [0.8817, 0.9183, 0.9718, 0.7432, 0.9363] 
2023-10-26 16:33:20.711745: Epoch time: 3.98 s 
2023-10-26 16:33:21.793130:  
2023-10-26 16:33:21.793435: Epoch 828 
2023-10-26 16:33:21.793682: Current learning rate: 0.00205 
2023-10-26 16:33:25.855006: train_loss -0.8963 
2023-10-26 16:33:25.855360: val_loss -0.8761 
2023-10-26 16:33:25.855621: Pseudo dice [0.8794, 0.9162, 0.9681, 0.7601, 0.9302] 
2023-10-26 16:33:25.855858: Epoch time: 4.06 s 
2023-10-26 16:33:27.007137:  
2023-10-26 16:33:27.007427: Epoch 829 
2023-10-26 16:33:27.007681: Current learning rate: 0.00204 
2023-10-26 16:33:31.009571: train_loss -0.8933 
2023-10-26 16:33:31.009947: val_loss -0.8775 
2023-10-26 16:33:31.010272: Pseudo dice [0.8815, 0.9113, 0.9703, 0.7797, 0.9289] 
2023-10-26 16:33:31.010507: Epoch time: 4.0 s 
2023-10-26 16:33:32.231269:  
2023-10-26 16:33:32.231580: Epoch 830 
2023-10-26 16:33:32.231839: Current learning rate: 0.00203 
2023-10-26 16:33:36.258364: train_loss -0.9012 
2023-10-26 16:33:36.258763: val_loss -0.8842 
2023-10-26 16:33:36.259034: Pseudo dice [0.8832, 0.9104, 0.9701, 0.7489, 0.9349] 
2023-10-26 16:33:36.259268: Epoch time: 4.03 s 
2023-10-26 16:33:37.349344:  
2023-10-26 16:33:37.349628: Epoch 831 
2023-10-26 16:33:37.349858: Current learning rate: 0.00202 
2023-10-26 16:33:41.350825: train_loss -0.9015 
2023-10-26 16:33:41.351201: val_loss -0.881 
2023-10-26 16:33:41.351458: Pseudo dice [0.8858, 0.9167, 0.9709, 0.7539, 0.9367] 
2023-10-26 16:33:41.351680: Epoch time: 4.0 s 
2023-10-26 16:33:42.448153:  
2023-10-26 16:33:42.448434: Epoch 832 
2023-10-26 16:33:42.448672: Current learning rate: 0.00201 
2023-10-26 16:33:46.455393: train_loss -0.8987 
2023-10-26 16:33:46.455754: val_loss -0.8694 
2023-10-26 16:33:46.456032: Pseudo dice [0.8823, 0.9069, 0.97, 0.6627, 0.9296] 
2023-10-26 16:33:46.456290: Epoch time: 4.01 s 
2023-10-26 16:33:47.582008:  
2023-10-26 16:33:47.582291: Epoch 833 
2023-10-26 16:33:47.582540: Current learning rate: 0.002 
2023-10-26 16:33:51.551289: train_loss -0.8921 
2023-10-26 16:33:51.551660: val_loss -0.8737 
2023-10-26 16:33:51.551922: Pseudo dice [0.8771, 0.9115, 0.9675, 0.6716, 0.9311] 
2023-10-26 16:33:51.552146: Epoch time: 3.97 s 
2023-10-26 16:33:52.644413:  
2023-10-26 16:33:52.644711: Epoch 834 
2023-10-26 16:33:52.644953: Current learning rate: 0.00199 
2023-10-26 16:33:56.680267: train_loss -0.8979 
2023-10-26 16:33:56.680763: val_loss -0.8848 
2023-10-26 16:33:56.681141: Pseudo dice [0.8803, 0.9116, 0.9681, 0.7224, 0.9397] 
2023-10-26 16:33:56.681425: Epoch time: 4.04 s 
2023-10-26 16:33:57.799420:  
2023-10-26 16:33:57.799715: Epoch 835 
2023-10-26 16:33:57.799971: Current learning rate: 0.00198 
2023-10-26 16:34:01.790999: train_loss -0.8943 
2023-10-26 16:34:01.791371: val_loss -0.8757 
2023-10-26 16:34:01.791629: Pseudo dice [0.8818, 0.9159, 0.9689, 0.7499, 0.9255] 
2023-10-26 16:34:01.791858: Epoch time: 3.99 s 
2023-10-26 16:34:02.895135:  
2023-10-26 16:34:02.895436: Epoch 836 
2023-10-26 16:34:02.895700: Current learning rate: 0.00196 
2023-10-26 16:34:06.743481: train_loss -0.8948 
2023-10-26 16:34:06.743860: val_loss -0.886 
2023-10-26 16:34:06.744123: Pseudo dice [0.8814, 0.9139, 0.9706, 0.7709, 0.9388] 
2023-10-26 16:34:06.744346: Epoch time: 3.85 s 
2023-10-26 16:34:08.035756:  
2023-10-26 16:34:08.036067: Epoch 837 
2023-10-26 16:34:08.036332: Current learning rate: 0.00195 
2023-10-26 16:34:11.946009: train_loss -0.8976 
2023-10-26 16:34:11.946384: val_loss -0.8773 
2023-10-26 16:34:11.946641: Pseudo dice [0.8815, 0.9106, 0.9692, 0.7691, 0.9223] 
2023-10-26 16:34:11.946865: Epoch time: 3.91 s 
2023-10-26 16:34:13.061963:  
2023-10-26 16:34:13.062254: Epoch 838 
2023-10-26 16:34:13.062497: Current learning rate: 0.00194 
2023-10-26 16:34:16.961740: train_loss -0.8972 
2023-10-26 16:34:16.962122: val_loss -0.8649 
2023-10-26 16:34:16.962394: Pseudo dice [0.8778, 0.9138, 0.9687, 0.7212, 0.9217] 
2023-10-26 16:34:16.962629: Epoch time: 3.9 s 
2023-10-26 16:34:18.066138:  
2023-10-26 16:34:18.066475: Epoch 839 
2023-10-26 16:34:18.066726: Current learning rate: 0.00193 
2023-10-26 16:34:22.076866: train_loss -0.8977 
2023-10-26 16:34:22.077262: val_loss -0.8817 
2023-10-26 16:34:22.077517: Pseudo dice [0.8854, 0.9213, 0.9715, 0.7559, 0.9292] 
2023-10-26 16:34:22.077753: Epoch time: 4.01 s 
2023-10-26 16:34:23.194173:  
2023-10-26 16:34:23.194480: Epoch 840 
2023-10-26 16:34:23.194724: Current learning rate: 0.00192 
2023-10-26 16:34:27.200638: train_loss -0.9002 
2023-10-26 16:34:27.201008: val_loss -0.8787 
2023-10-26 16:34:27.201277: Pseudo dice [0.8785, 0.9139, 0.9711, 0.7606, 0.9374] 
2023-10-26 16:34:27.201524: Epoch time: 4.01 s 
2023-10-26 16:34:28.311542:  
2023-10-26 16:34:28.311844: Epoch 841 
2023-10-26 16:34:28.312365: Current learning rate: 0.00191 
2023-10-26 16:34:32.409916: train_loss -0.8969 
2023-10-26 16:34:32.410334: val_loss -0.8801 
2023-10-26 16:34:32.410646: Pseudo dice [0.8823, 0.9128, 0.9701, 0.7547, 0.9356] 
2023-10-26 16:34:32.410893: Epoch time: 4.1 s 
2023-10-26 16:34:33.530230:  
2023-10-26 16:34:33.530537: Epoch 842 
2023-10-26 16:34:33.530823: Current learning rate: 0.0019 
2023-10-26 16:34:37.475276: train_loss -0.8945 
2023-10-26 16:34:37.475648: val_loss -0.8731 
2023-10-26 16:34:37.475898: Pseudo dice [0.8827, 0.9168, 0.9689, 0.7452, 0.9305] 
2023-10-26 16:34:37.476131: Epoch time: 3.95 s 
2023-10-26 16:34:38.831674:  
2023-10-26 16:34:38.831983: Epoch 843 
2023-10-26 16:34:38.832223: Current learning rate: 0.00189 
2023-10-26 16:34:42.706429: train_loss -0.8898 
2023-10-26 16:34:42.706791: val_loss -0.8779 
2023-10-26 16:34:42.707043: Pseudo dice [0.8762, 0.916, 0.9677, 0.7056, 0.9361] 
2023-10-26 16:34:42.707259: Epoch time: 3.88 s 
2023-10-26 16:34:43.803257:  
2023-10-26 16:34:43.803557: Epoch 844 
2023-10-26 16:34:43.803795: Current learning rate: 0.00188 
2023-10-26 16:34:47.627234: train_loss -0.8918 
2023-10-26 16:34:47.627580: val_loss -0.8774 
2023-10-26 16:34:47.627839: Pseudo dice [0.883, 0.9187, 0.9707, 0.6944, 0.9315] 
2023-10-26 16:34:47.628064: Epoch time: 3.82 s 
2023-10-26 16:34:48.704420:  
2023-10-26 16:34:48.704723: Epoch 845 
2023-10-26 16:34:48.704978: Current learning rate: 0.00187 
2023-10-26 16:34:52.763867: train_loss -0.8989 
2023-10-26 16:34:52.764260: val_loss -0.874 
2023-10-26 16:34:52.764515: Pseudo dice [0.8791, 0.914, 0.968, 0.728, 0.9309] 
2023-10-26 16:34:52.764760: Epoch time: 4.06 s 
2023-10-26 16:34:53.843768:  
2023-10-26 16:34:53.844088: Epoch 846 
2023-10-26 16:34:53.844327: Current learning rate: 0.00186 
2023-10-26 16:34:57.784649: train_loss -0.8949 
2023-10-26 16:34:57.785267: val_loss -0.8748 
2023-10-26 16:34:57.785548: Pseudo dice [0.8812, 0.913, 0.9687, 0.7292, 0.9271] 
2023-10-26 16:34:57.785772: Epoch time: 3.94 s 
2023-10-26 16:34:58.871834:  
2023-10-26 16:34:58.872153: Epoch 847 
2023-10-26 16:34:58.872389: Current learning rate: 0.00185 
2023-10-26 16:35:02.702282: train_loss -0.9003 
2023-10-26 16:35:02.702649: val_loss -0.8832 
2023-10-26 16:35:02.702926: Pseudo dice [0.8821, 0.9129, 0.971, 0.736, 0.9357] 
2023-10-26 16:35:02.703159: Epoch time: 3.83 s 
2023-10-26 16:35:03.804799:  
2023-10-26 16:35:03.805083: Epoch 848 
2023-10-26 16:35:03.805330: Current learning rate: 0.00184 
2023-10-26 16:35:07.776284: train_loss -0.8996 
2023-10-26 16:35:07.776702: val_loss -0.8792 
2023-10-26 16:35:07.776964: Pseudo dice [0.8839, 0.9155, 0.9698, 0.6983, 0.9321] 
2023-10-26 16:35:07.777198: Epoch time: 3.97 s 
2023-10-26 16:35:08.898770:  
2023-10-26 16:35:08.899077: Epoch 849 
2023-10-26 16:35:08.899335: Current learning rate: 0.00182 
2023-10-26 16:35:12.814201: train_loss -0.8984 
2023-10-26 16:35:12.814573: val_loss -0.8804 
2023-10-26 16:35:12.814823: Pseudo dice [0.887, 0.9121, 0.9699, 0.7738, 0.9304] 
2023-10-26 16:35:12.815051: Epoch time: 3.92 s 
2023-10-26 16:35:14.209440:  
2023-10-26 16:35:14.209784: Epoch 850 
2023-10-26 16:35:14.210106: Current learning rate: 0.00181 
2023-10-26 16:35:18.247857: train_loss -0.8957 
2023-10-26 16:35:18.248541: val_loss -0.8769 
2023-10-26 16:35:18.248825: Pseudo dice [0.8792, 0.914, 0.9709, 0.7634, 0.936] 
2023-10-26 16:35:18.249082: Epoch time: 4.04 s 
2023-10-26 16:35:19.353719:  
2023-10-26 16:35:19.354061: Epoch 851 
2023-10-26 16:35:19.354312: Current learning rate: 0.0018 
2023-10-26 16:35:23.307591: train_loss -0.8972 
2023-10-26 16:35:23.308456: val_loss -0.8826 
2023-10-26 16:35:23.308846: Pseudo dice [0.8826, 0.9132, 0.9693, 0.7696, 0.9327] 
2023-10-26 16:35:23.309141: Epoch time: 3.95 s 
2023-10-26 16:35:24.393182:  
2023-10-26 16:35:24.393490: Epoch 852 
2023-10-26 16:35:24.393733: Current learning rate: 0.00179 
2023-10-26 16:35:28.363588: train_loss -0.8943 
2023-10-26 16:35:28.364271: val_loss -0.8753 
2023-10-26 16:35:28.364534: Pseudo dice [0.8831, 0.9158, 0.9695, 0.6693, 0.9348] 
2023-10-26 16:35:28.364762: Epoch time: 3.97 s 
2023-10-26 16:35:29.459583:  
2023-10-26 16:35:29.459907: Epoch 853 
2023-10-26 16:35:29.460176: Current learning rate: 0.00178 
2023-10-26 16:35:33.396166: train_loss -0.8886 
2023-10-26 16:35:33.396930: val_loss -0.8766 
2023-10-26 16:35:33.397212: Pseudo dice [0.8774, 0.9135, 0.9677, 0.6991, 0.9336] 
2023-10-26 16:35:33.397458: Epoch time: 3.94 s 
2023-10-26 16:35:34.487221:  
2023-10-26 16:35:34.487530: Epoch 854 
2023-10-26 16:35:34.487788: Current learning rate: 0.00177 
2023-10-26 16:35:38.453010: train_loss -0.8959 
2023-10-26 16:35:38.453693: val_loss -0.8797 
2023-10-26 16:35:38.453956: Pseudo dice [0.8842, 0.9144, 0.9708, 0.7106, 0.9335] 
2023-10-26 16:35:38.454193: Epoch time: 3.97 s 
2023-10-26 16:35:39.575611:  
2023-10-26 16:35:39.575926: Epoch 855 
2023-10-26 16:35:39.576202: Current learning rate: 0.00176 
2023-10-26 16:35:43.555577: train_loss -0.9009 
2023-10-26 16:35:43.556259: val_loss -0.885 
2023-10-26 16:35:43.556523: Pseudo dice [0.8861, 0.9167, 0.9706, 0.6946, 0.9328] 
2023-10-26 16:35:43.556748: Epoch time: 3.98 s 
2023-10-26 16:35:44.775352:  
2023-10-26 16:35:44.775667: Epoch 856 
2023-10-26 16:35:44.775913: Current learning rate: 0.00175 
2023-10-26 16:35:48.765817: train_loss -0.8959 
2023-10-26 16:35:48.766335: val_loss -0.8728 
2023-10-26 16:35:48.766653: Pseudo dice [0.8813, 0.9074, 0.9677, 0.6597, 0.9408] 
2023-10-26 16:35:48.766988: Epoch time: 3.99 s 
2023-10-26 16:35:49.886720:  
2023-10-26 16:35:49.887058: Epoch 857 
2023-10-26 16:35:49.887315: Current learning rate: 0.00174 
2023-10-26 16:35:53.986858: train_loss -0.8915 
2023-10-26 16:35:53.987223: val_loss -0.8735 
2023-10-26 16:35:53.987485: Pseudo dice [0.8789, 0.9125, 0.9688, 0.6904, 0.9352] 
2023-10-26 16:35:53.987725: Epoch time: 4.1 s 
2023-10-26 16:35:55.077478:  
2023-10-26 16:35:55.077766: Epoch 858 
2023-10-26 16:35:55.078012: Current learning rate: 0.00173 
2023-10-26 16:35:59.126737: train_loss -0.9005 
2023-10-26 16:35:59.127136: val_loss -0.8708 
2023-10-26 16:35:59.127385: Pseudo dice [0.8771, 0.9165, 0.9688, 0.7478, 0.9159] 
2023-10-26 16:35:59.127619: Epoch time: 4.05 s 
2023-10-26 16:36:00.186322:  
2023-10-26 16:36:00.186631: Epoch 859 
2023-10-26 16:36:00.186883: Current learning rate: 0.00172 
2023-10-26 16:36:04.322007: train_loss -0.8939 
2023-10-26 16:36:04.322394: val_loss -0.8746 
2023-10-26 16:36:04.322680: Pseudo dice [0.8771, 0.9127, 0.9695, 0.7684, 0.9347] 
2023-10-26 16:36:04.322916: Epoch time: 4.14 s 
2023-10-26 16:36:05.390884:  
2023-10-26 16:36:05.391160: Epoch 860 
2023-10-26 16:36:05.391405: Current learning rate: 0.0017 
2023-10-26 16:36:09.369314: train_loss -0.8991 
2023-10-26 16:36:09.369706: val_loss -0.8765 
2023-10-26 16:36:09.369967: Pseudo dice [0.8788, 0.9194, 0.97, 0.7561, 0.9333] 
2023-10-26 16:36:09.370196: Epoch time: 3.98 s 
2023-10-26 16:36:10.454085:  
2023-10-26 16:36:10.454379: Epoch 861 
2023-10-26 16:36:10.454617: Current learning rate: 0.00169 
2023-10-26 16:36:14.460104: train_loss -0.8988 
2023-10-26 16:36:14.460507: val_loss -0.8838 
2023-10-26 16:36:14.460781: Pseudo dice [0.881, 0.9198, 0.9708, 0.7767, 0.9323] 
2023-10-26 16:36:14.461040: Epoch time: 4.01 s 
2023-10-26 16:36:15.547696:  
2023-10-26 16:36:15.547990: Epoch 862 
2023-10-26 16:36:15.548227: Current learning rate: 0.00168 
2023-10-26 16:36:19.448477: train_loss -0.8963 
2023-10-26 16:36:19.448954: val_loss -0.8533 
2023-10-26 16:36:19.449389: Pseudo dice [0.8779, 0.9161, 0.9649, 0.6348, 0.9278] 
2023-10-26 16:36:19.449720: Epoch time: 3.9 s 
2023-10-26 16:36:20.701070:  
2023-10-26 16:36:20.701462: Epoch 863 
2023-10-26 16:36:20.701714: Current learning rate: 0.00167 
2023-10-26 16:36:24.670100: train_loss -0.8985 
2023-10-26 16:36:24.670670: val_loss -0.8783 
2023-10-26 16:36:24.671141: Pseudo dice [0.884, 0.9129, 0.969, 0.7149, 0.9306] 
2023-10-26 16:36:24.671525: Epoch time: 3.97 s 
2023-10-26 16:36:25.769173:  
2023-10-26 16:36:25.769497: Epoch 864 
2023-10-26 16:36:25.769749: Current learning rate: 0.00166 
2023-10-26 16:36:29.834397: train_loss -0.8994 
2023-10-26 16:36:29.834791: val_loss -0.8774 
2023-10-26 16:36:29.835067: Pseudo dice [0.8784, 0.9203, 0.9707, 0.6812, 0.9305] 
2023-10-26 16:36:29.835305: Epoch time: 4.07 s 
2023-10-26 16:36:30.916931:  
2023-10-26 16:36:30.917279: Epoch 865 
2023-10-26 16:36:30.917522: Current learning rate: 0.00165 
2023-10-26 16:36:34.860375: train_loss -0.8984 
2023-10-26 16:36:34.860783: val_loss -0.8854 
2023-10-26 16:36:34.861359: Pseudo dice [0.8858, 0.9188, 0.9685, 0.7608, 0.947] 
2023-10-26 16:36:34.861640: Epoch time: 3.94 s 
2023-10-26 16:36:36.022444:  
2023-10-26 16:36:36.022738: Epoch 866 
2023-10-26 16:36:36.022995: Current learning rate: 0.00164 
2023-10-26 16:36:39.986994: train_loss -0.8957 
2023-10-26 16:36:39.987406: val_loss -0.8784 
2023-10-26 16:36:39.987677: Pseudo dice [0.8844, 0.9197, 0.9685, 0.7204, 0.9307] 
2023-10-26 16:36:39.987932: Epoch time: 3.97 s 
2023-10-26 16:36:41.076888:  
2023-10-26 16:36:41.077187: Epoch 867 
2023-10-26 16:36:41.077421: Current learning rate: 0.00163 
2023-10-26 16:36:44.985587: train_loss -0.8976 
2023-10-26 16:36:44.985946: val_loss -0.8765 
2023-10-26 16:36:44.986213: Pseudo dice [0.88, 0.9183, 0.9713, 0.7438, 0.9344] 
2023-10-26 16:36:44.986508: Epoch time: 3.91 s 
2023-10-26 16:36:46.065907:  
2023-10-26 16:36:46.066203: Epoch 868 
2023-10-26 16:36:46.066456: Current learning rate: 0.00162 
2023-10-26 16:36:49.971632: train_loss -0.8926 
2023-10-26 16:36:49.972010: val_loss -0.8666 
2023-10-26 16:36:49.972263: Pseudo dice [0.8809, 0.9153, 0.9681, 0.6934, 0.9325] 
2023-10-26 16:36:49.972485: Epoch time: 3.91 s 
2023-10-26 16:36:51.205981:  
2023-10-26 16:36:51.206273: Epoch 869 
2023-10-26 16:36:51.206507: Current learning rate: 0.00161 
2023-10-26 16:36:55.145498: train_loss -0.8913 
2023-10-26 16:36:55.145894: val_loss -0.8707 
2023-10-26 16:36:55.146152: Pseudo dice [0.8805, 0.9162, 0.9688, 0.6934, 0.9258] 
2023-10-26 16:36:55.146374: Epoch time: 3.94 s 
2023-10-26 16:36:56.427313:  
2023-10-26 16:36:56.427613: Epoch 870 
2023-10-26 16:36:56.427849: Current learning rate: 0.00159 
2023-10-26 16:37:00.429003: train_loss -0.8932 
2023-10-26 16:37:00.429446: val_loss -0.8751 
2023-10-26 16:37:00.429856: Pseudo dice [0.8814, 0.9131, 0.9693, 0.7619, 0.9363] 
2023-10-26 16:37:00.430166: Epoch time: 4.0 s 
2023-10-26 16:37:01.560533:  
2023-10-26 16:37:01.560834: Epoch 871 
2023-10-26 16:37:01.561077: Current learning rate: 0.00158 
2023-10-26 16:37:05.470829: train_loss -0.8961 
2023-10-26 16:37:05.471203: val_loss -0.8756 
2023-10-26 16:37:05.471467: Pseudo dice [0.8876, 0.9163, 0.97, 0.7373, 0.9276] 
2023-10-26 16:37:05.471694: Epoch time: 3.91 s 
2023-10-26 16:37:06.539983:  
2023-10-26 16:37:06.540268: Epoch 872 
2023-10-26 16:37:06.540496: Current learning rate: 0.00157 
2023-10-26 16:37:10.556261: train_loss -0.899 
2023-10-26 16:37:10.556651: val_loss -0.8818 
2023-10-26 16:37:10.556909: Pseudo dice [0.8853, 0.9218, 0.971, 0.6629, 0.9354] 
2023-10-26 16:37:10.557135: Epoch time: 4.02 s 
2023-10-26 16:37:11.637203:  
2023-10-26 16:37:11.637494: Epoch 873 
2023-10-26 16:37:11.637727: Current learning rate: 0.00156 
2023-10-26 16:37:15.646142: train_loss -0.9002 
2023-10-26 16:37:15.646609: val_loss -0.8767 
2023-10-26 16:37:15.647063: Pseudo dice [0.8832, 0.9171, 0.9694, 0.7244, 0.936] 
2023-10-26 16:37:15.647365: Epoch time: 4.01 s 
2023-10-26 16:37:16.722156:  
2023-10-26 16:37:16.722444: Epoch 874 
2023-10-26 16:37:16.722682: Current learning rate: 0.00155 
2023-10-26 16:37:20.755737: train_loss -0.8969 
2023-10-26 16:37:20.756129: val_loss -0.8814 
2023-10-26 16:37:20.756384: Pseudo dice [0.8829, 0.9177, 0.9694, 0.7454, 0.9392] 
2023-10-26 16:37:20.756605: Epoch time: 4.03 s 
2023-10-26 16:37:21.821032:  
2023-10-26 16:37:21.821326: Epoch 875 
2023-10-26 16:37:21.821570: Current learning rate: 0.00154 
2023-10-26 16:37:25.826907: train_loss -0.8935 
2023-10-26 16:37:25.827245: val_loss -0.8745 
2023-10-26 16:37:25.827492: Pseudo dice [0.8765, 0.9152, 0.9701, 0.7185, 0.9356] 
2023-10-26 16:37:25.827724: Epoch time: 4.01 s 
2023-10-26 16:37:26.918227:  
2023-10-26 16:37:26.918525: Epoch 876 
2023-10-26 16:37:26.918764: Current learning rate: 0.00153 
2023-10-26 16:37:30.893165: train_loss -0.8904 
2023-10-26 16:37:30.893559: val_loss -0.8715 
2023-10-26 16:37:30.893826: Pseudo dice [0.8781, 0.9059, 0.9667, 0.701, 0.9389] 
2023-10-26 16:37:30.894070: Epoch time: 3.98 s 
2023-10-26 16:37:32.198405:  
2023-10-26 16:37:32.198684: Epoch 877 
2023-10-26 16:37:32.198919: Current learning rate: 0.00152 
2023-10-26 16:37:36.269481: train_loss -0.8944 
2023-10-26 16:37:36.269869: val_loss -0.8664 
2023-10-26 16:37:36.270155: Pseudo dice [0.8786, 0.9196, 0.9718, 0.6558, 0.9331] 
2023-10-26 16:37:36.270398: Epoch time: 4.07 s 
2023-10-26 16:37:37.362901:  
2023-10-26 16:37:37.363189: Epoch 878 
2023-10-26 16:37:37.363425: Current learning rate: 0.00151 
2023-10-26 16:37:41.320683: train_loss -0.8986 
2023-10-26 16:37:41.321082: val_loss -0.8769 
2023-10-26 16:37:41.321340: Pseudo dice [0.8823, 0.9116, 0.9684, 0.7543, 0.9272] 
2023-10-26 16:37:41.321569: Epoch time: 3.96 s 
2023-10-26 16:37:42.402290:  
2023-10-26 16:37:42.402615: Epoch 879 
2023-10-26 16:37:42.402867: Current learning rate: 0.00149 
2023-10-26 16:37:46.279906: train_loss -0.9074 
2023-10-26 16:37:46.280265: val_loss -0.8766 
2023-10-26 16:37:46.280520: Pseudo dice [0.8806, 0.9198, 0.9706, 0.6851, 0.9289] 
2023-10-26 16:37:46.280739: Epoch time: 3.88 s 
2023-10-26 16:37:47.358522:  
2023-10-26 16:37:47.358806: Epoch 880 
2023-10-26 16:37:47.359054: Current learning rate: 0.00148 
2023-10-26 16:37:51.267361: train_loss -0.9053 
2023-10-26 16:37:51.267744: val_loss -0.8716 
2023-10-26 16:37:51.268001: Pseudo dice [0.8758, 0.9175, 0.9694, 0.6709, 0.9303] 
2023-10-26 16:37:51.268230: Epoch time: 3.91 s 
2023-10-26 16:37:52.338522:  
2023-10-26 16:37:52.338804: Epoch 881 
2023-10-26 16:37:52.339041: Current learning rate: 0.00147 
2023-10-26 16:37:56.185555: train_loss -0.899 
2023-10-26 16:37:56.185919: val_loss -0.8682 
2023-10-26 16:37:56.186177: Pseudo dice [0.8805, 0.9164, 0.9706, 0.6142, 0.9338] 
2023-10-26 16:37:56.186389: Epoch time: 3.85 s 
2023-10-26 16:37:57.277729:  
2023-10-26 16:37:57.278018: Epoch 882 
2023-10-26 16:37:57.278254: Current learning rate: 0.00146 
2023-10-26 16:38:01.208036: train_loss -0.8989 
2023-10-26 16:38:01.208407: val_loss -0.8731 
2023-10-26 16:38:01.208658: Pseudo dice [0.8802, 0.9173, 0.9688, 0.7285, 0.9295] 
2023-10-26 16:38:01.208870: Epoch time: 3.93 s 
2023-10-26 16:38:02.320003:  
2023-10-26 16:38:02.320300: Epoch 883 
2023-10-26 16:38:02.320548: Current learning rate: 0.00145 
2023-10-26 16:38:06.157378: train_loss -0.9021 
2023-10-26 16:38:06.157724: val_loss -0.8837 
2023-10-26 16:38:06.157964: Pseudo dice [0.8833, 0.9172, 0.9704, 0.7569, 0.9359] 
2023-10-26 16:38:06.158204: Epoch time: 3.84 s 
2023-10-26 16:38:07.432986:  
2023-10-26 16:38:07.433287: Epoch 884 
2023-10-26 16:38:07.433535: Current learning rate: 0.00144 
2023-10-26 16:38:11.369708: train_loss -0.8947 
2023-10-26 16:38:11.370097: val_loss -0.8758 
2023-10-26 16:38:11.370348: Pseudo dice [0.879, 0.9154, 0.9697, 0.7317, 0.9301] 
2023-10-26 16:38:11.370561: Epoch time: 3.94 s 
2023-10-26 16:38:12.455559:  
2023-10-26 16:38:12.455839: Epoch 885 
2023-10-26 16:38:12.456075: Current learning rate: 0.00143 
2023-10-26 16:38:16.384893: train_loss -0.8948 
2023-10-26 16:38:16.385241: val_loss -0.877 
2023-10-26 16:38:16.385488: Pseudo dice [0.8806, 0.9152, 0.9705, 0.7283, 0.9322] 
2023-10-26 16:38:16.385704: Epoch time: 3.93 s 
2023-10-26 16:38:17.475681:  
2023-10-26 16:38:17.475988: Epoch 886 
2023-10-26 16:38:17.476235: Current learning rate: 0.00142 
2023-10-26 16:38:21.523017: train_loss -0.8967 
2023-10-26 16:38:21.523405: val_loss -0.8834 
2023-10-26 16:38:21.523662: Pseudo dice [0.8832, 0.918, 0.9709, 0.7413, 0.9362] 
2023-10-26 16:38:21.523917: Epoch time: 4.05 s 
2023-10-26 16:38:22.661533:  
2023-10-26 16:38:22.661829: Epoch 887 
2023-10-26 16:38:22.662093: Current learning rate: 0.00141 
2023-10-26 16:38:26.718330: train_loss -0.8935 
2023-10-26 16:38:26.718686: val_loss -0.8655 
2023-10-26 16:38:26.718931: Pseudo dice [0.8775, 0.9097, 0.97, 0.4605, 0.9305] 
2023-10-26 16:38:26.719161: Epoch time: 4.06 s 
2023-10-26 16:38:27.817689:  
2023-10-26 16:38:27.817984: Epoch 888 
2023-10-26 16:38:27.818215: Current learning rate: 0.00139 
2023-10-26 16:38:31.916170: train_loss -0.894 
2023-10-26 16:38:31.916529: val_loss -0.8748 
2023-10-26 16:38:31.916786: Pseudo dice [0.8785, 0.9198, 0.9698, 0.762, 0.9267] 
2023-10-26 16:38:31.917015: Epoch time: 4.1 s 
2023-10-26 16:38:32.983284:  
2023-10-26 16:38:32.983572: Epoch 889 
2023-10-26 16:38:32.983833: Current learning rate: 0.00138 
2023-10-26 16:38:37.059790: train_loss -0.9013 
2023-10-26 16:38:37.060294: val_loss -0.8806 
2023-10-26 16:38:37.060749: Pseudo dice [0.8807, 0.9169, 0.9693, 0.7452, 0.9199] 
2023-10-26 16:38:37.061314: Epoch time: 4.08 s 
2023-10-26 16:38:38.129515:  
2023-10-26 16:38:38.129812: Epoch 890 
2023-10-26 16:38:38.130049: Current learning rate: 0.00137 
2023-10-26 16:38:42.183172: train_loss -0.8945 
2023-10-26 16:38:42.183556: val_loss -0.871 
2023-10-26 16:38:42.183816: Pseudo dice [0.8831, 0.9175, 0.9691, 0.7091, 0.9235] 
2023-10-26 16:38:42.184046: Epoch time: 4.05 s 
2023-10-26 16:38:43.447219:  
2023-10-26 16:38:43.447540: Epoch 891 
2023-10-26 16:38:43.447811: Current learning rate: 0.00136 
2023-10-26 16:38:47.359143: train_loss -0.9006 
2023-10-26 16:38:47.359539: val_loss -0.8736 
2023-10-26 16:38:47.359812: Pseudo dice [0.8801, 0.9197, 0.9699, 0.7053, 0.9263] 
2023-10-26 16:38:47.360083: Epoch time: 3.91 s 
2023-10-26 16:38:48.445026:  
2023-10-26 16:38:48.445328: Epoch 892 
2023-10-26 16:38:48.445580: Current learning rate: 0.00135 
2023-10-26 16:38:52.456977: train_loss -0.8946 
2023-10-26 16:38:52.457347: val_loss -0.8713 
2023-10-26 16:38:52.457597: Pseudo dice [0.8811, 0.913, 0.9708, 0.7358, 0.9184] 
2023-10-26 16:38:52.457827: Epoch time: 4.01 s 
2023-10-26 16:38:53.580389:  
2023-10-26 16:38:53.580686: Epoch 893 
2023-10-26 16:38:53.580935: Current learning rate: 0.00134 
2023-10-26 16:38:57.464869: train_loss -0.8926 
2023-10-26 16:38:57.465254: val_loss -0.8729 
2023-10-26 16:38:57.465514: Pseudo dice [0.8834, 0.9191, 0.9699, 0.7455, 0.9272] 
2023-10-26 16:38:57.465746: Epoch time: 3.89 s 
2023-10-26 16:38:58.563669:  
2023-10-26 16:38:58.563972: Epoch 894 
2023-10-26 16:38:58.564210: Current learning rate: 0.00133 
2023-10-26 16:39:02.545537: train_loss -0.8962 
2023-10-26 16:39:02.545950: val_loss -0.8762 
2023-10-26 16:39:02.546239: Pseudo dice [0.8798, 0.9118, 0.9704, 0.7079, 0.9326] 
2023-10-26 16:39:02.546493: Epoch time: 3.98 s 
2023-10-26 16:39:03.668993:  
2023-10-26 16:39:03.669427: Epoch 895 
2023-10-26 16:39:03.669795: Current learning rate: 0.00132 
2023-10-26 16:39:07.618974: train_loss -0.8955 
2023-10-26 16:39:07.619360: val_loss -0.8788 
2023-10-26 16:39:07.619618: Pseudo dice [0.8829, 0.9119, 0.9693, 0.7599, 0.9243] 
2023-10-26 16:39:07.619841: Epoch time: 3.95 s 
2023-10-26 16:39:08.700640:  
2023-10-26 16:39:08.700953: Epoch 896 
2023-10-26 16:39:08.701204: Current learning rate: 0.0013 
2023-10-26 16:39:12.662599: train_loss -0.8914 
2023-10-26 16:39:12.663100: val_loss -0.8623 
2023-10-26 16:39:12.663634: Pseudo dice [0.8802, 0.915, 0.9686, 0.7393, 0.9206] 
2023-10-26 16:39:12.663995: Epoch time: 3.96 s 
2023-10-26 16:39:13.923798:  
2023-10-26 16:39:13.924087: Epoch 897 
2023-10-26 16:39:13.924326: Current learning rate: 0.00129 
2023-10-26 16:39:17.848701: train_loss -0.8959 
2023-10-26 16:39:17.849107: val_loss -0.8827 
2023-10-26 16:39:17.849363: Pseudo dice [0.8857, 0.9191, 0.9701, 0.7345, 0.9413] 
2023-10-26 16:39:17.849600: Epoch time: 3.93 s 
2023-10-26 16:39:18.932295:  
2023-10-26 16:39:18.932615: Epoch 898 
2023-10-26 16:39:18.932865: Current learning rate: 0.00128 
2023-10-26 16:39:22.729829: train_loss -0.8991 
2023-10-26 16:39:22.730267: val_loss -0.8785 
2023-10-26 16:39:22.730628: Pseudo dice [0.8845, 0.913, 0.9693, 0.7447, 0.9333] 
2023-10-26 16:39:22.730884: Epoch time: 3.8 s 
2023-10-26 16:39:23.820575:  
2023-10-26 16:39:23.820892: Epoch 899 
2023-10-26 16:39:23.821148: Current learning rate: 0.00127 
2023-10-26 16:39:27.603485: train_loss -0.904 
2023-10-26 16:39:27.603846: val_loss -0.8804 
2023-10-26 16:39:27.604103: Pseudo dice [0.8839, 0.9169, 0.9713, 0.6959, 0.9313] 
2023-10-26 16:39:27.604324: Epoch time: 3.78 s 
2023-10-26 16:39:28.790068:  
2023-10-26 16:39:28.790350: Epoch 900 
2023-10-26 16:39:28.790583: Current learning rate: 0.00126 
2023-10-26 16:39:32.768112: train_loss -0.8982 
2023-10-26 16:39:32.768498: val_loss -0.8706 
2023-10-26 16:39:32.768756: Pseudo dice [0.8797, 0.9103, 0.9676, 0.7207, 0.9269] 
2023-10-26 16:39:32.769016: Epoch time: 3.98 s 
2023-10-26 16:39:33.882552:  
2023-10-26 16:39:33.882863: Epoch 901 
2023-10-26 16:39:33.883137: Current learning rate: 0.00125 
2023-10-26 16:39:37.907600: train_loss -0.9034 
2023-10-26 16:39:37.907979: val_loss -0.8785 
2023-10-26 16:39:37.908281: Pseudo dice [0.8857, 0.9203, 0.9709, 0.709, 0.9298] 
2023-10-26 16:39:37.908509: Epoch time: 4.03 s 
2023-10-26 16:39:38.969435:  
2023-10-26 16:39:38.969712: Epoch 902 
2023-10-26 16:39:38.969952: Current learning rate: 0.00124 
2023-10-26 16:39:43.027220: train_loss -0.8991 
2023-10-26 16:39:43.027632: val_loss -0.8766 
2023-10-26 16:39:43.027907: Pseudo dice [0.8837, 0.9118, 0.9699, 0.7267, 0.9391] 
2023-10-26 16:39:43.028155: Epoch time: 4.06 s 
2023-10-26 16:39:44.105927:  
2023-10-26 16:39:44.106227: Epoch 903 
2023-10-26 16:39:44.106473: Current learning rate: 0.00122 
2023-10-26 16:39:48.104082: train_loss -0.9041 
2023-10-26 16:39:48.104482: val_loss -0.8717 
2023-10-26 16:39:48.104740: Pseudo dice [0.8781, 0.9136, 0.9689, 0.7067, 0.9211] 
2023-10-26 16:39:48.104983: Epoch time: 4.0 s 
2023-10-26 16:39:49.349181:  
2023-10-26 16:39:49.349482: Epoch 904 
2023-10-26 16:39:49.349719: Current learning rate: 0.00121 
2023-10-26 16:39:53.453182: train_loss -0.8959 
2023-10-26 16:39:53.453605: val_loss -0.873 
2023-10-26 16:39:53.453870: Pseudo dice [0.8837, 0.9171, 0.9701, 0.6832, 0.929] 
2023-10-26 16:39:53.454148: Epoch time: 4.1 s 
2023-10-26 16:39:54.537300:  
2023-10-26 16:39:54.537622: Epoch 905 
2023-10-26 16:39:54.537865: Current learning rate: 0.0012 
2023-10-26 16:39:58.491616: train_loss -0.8986 
2023-10-26 16:39:58.492010: val_loss -0.8825 
2023-10-26 16:39:58.492289: Pseudo dice [0.8843, 0.9119, 0.9712, 0.7113, 0.9368] 
2023-10-26 16:39:58.492535: Epoch time: 3.95 s 
2023-10-26 16:39:59.575850:  
2023-10-26 16:39:59.576169: Epoch 906 
2023-10-26 16:39:59.576414: Current learning rate: 0.00119 
2023-10-26 16:40:03.427964: train_loss -0.901 
2023-10-26 16:40:03.428372: val_loss -0.8724 
2023-10-26 16:40:03.428638: Pseudo dice [0.8811, 0.9154, 0.9683, 0.674, 0.9353] 
2023-10-26 16:40:03.428881: Epoch time: 3.85 s 
2023-10-26 16:40:04.508825:  
2023-10-26 16:40:04.509153: Epoch 907 
2023-10-26 16:40:04.509395: Current learning rate: 0.00118 
2023-10-26 16:40:08.327345: train_loss -0.9017 
2023-10-26 16:40:08.327732: val_loss -0.8757 
2023-10-26 16:40:08.327993: Pseudo dice [0.8813, 0.9127, 0.9681, 0.7431, 0.9266] 
2023-10-26 16:40:08.328214: Epoch time: 3.82 s 
2023-10-26 16:40:09.448762:  
2023-10-26 16:40:09.449080: Epoch 908 
2023-10-26 16:40:09.449326: Current learning rate: 0.00117 
2023-10-26 16:40:13.301921: train_loss -0.8969 
2023-10-26 16:40:13.302332: val_loss -0.8754 
2023-10-26 16:40:13.302616: Pseudo dice [0.8808, 0.9195, 0.9704, 0.7059, 0.9199] 
2023-10-26 16:40:13.302848: Epoch time: 3.85 s 
2023-10-26 16:40:14.386237:  
2023-10-26 16:40:14.386532: Epoch 909 
2023-10-26 16:40:14.386777: Current learning rate: 0.00116 
2023-10-26 16:40:18.344746: train_loss -0.8945 
2023-10-26 16:40:18.345127: val_loss -0.8713 
2023-10-26 16:40:18.345370: Pseudo dice [0.8761, 0.9168, 0.97, 0.6793, 0.9367] 
2023-10-26 16:40:18.345588: Epoch time: 3.96 s 
2023-10-26 16:40:19.545453:  
2023-10-26 16:40:19.545793: Epoch 910 
2023-10-26 16:40:19.546100: Current learning rate: 0.00115 
2023-10-26 16:40:23.429575: train_loss -0.8935 
2023-10-26 16:40:23.429941: val_loss -0.8743 
2023-10-26 16:40:23.430205: Pseudo dice [0.8803, 0.9152, 0.9705, 0.7062, 0.9218] 
2023-10-26 16:40:23.430427: Epoch time: 3.89 s 
2023-10-26 16:40:24.701320:  
2023-10-26 16:40:24.701639: Epoch 911 
2023-10-26 16:40:24.701908: Current learning rate: 0.00113 
2023-10-26 16:40:28.592222: train_loss -0.8992 
2023-10-26 16:40:28.592581: val_loss -0.8715 
2023-10-26 16:40:28.592963: Pseudo dice [0.8807, 0.9108, 0.9692, 0.725, 0.9389] 
2023-10-26 16:40:28.593271: Epoch time: 3.89 s 
2023-10-26 16:40:29.667501:  
2023-10-26 16:40:29.667805: Epoch 912 
2023-10-26 16:40:29.668053: Current learning rate: 0.00112 
2023-10-26 16:40:33.691093: train_loss -0.9012 
2023-10-26 16:40:33.691492: val_loss -0.8768 
2023-10-26 16:40:33.691757: Pseudo dice [0.879, 0.9097, 0.9695, 0.7791, 0.9255] 
2023-10-26 16:40:33.692110: Epoch time: 4.02 s 
2023-10-26 16:40:34.765387:  
2023-10-26 16:40:34.765857: Epoch 913 
2023-10-26 16:40:34.766117: Current learning rate: 0.00111 
2023-10-26 16:40:38.753900: train_loss -0.8986 
2023-10-26 16:40:38.754414: val_loss -0.8789 
2023-10-26 16:40:38.754656: Pseudo dice [0.8844, 0.918, 0.9711, 0.6877, 0.9305] 
2023-10-26 16:40:38.754870: Epoch time: 3.99 s 
2023-10-26 16:40:39.858466:  
2023-10-26 16:40:39.858772: Epoch 914 
2023-10-26 16:40:39.859016: Current learning rate: 0.0011 
2023-10-26 16:40:43.884153: train_loss -0.8984 
2023-10-26 16:40:43.884554: val_loss -0.8731 
2023-10-26 16:40:43.884811: Pseudo dice [0.878, 0.9136, 0.97, 0.7099, 0.93] 
2023-10-26 16:40:43.885065: Epoch time: 4.03 s 
2023-10-26 16:40:44.949216:  
2023-10-26 16:40:44.949517: Epoch 915 
2023-10-26 16:40:44.949747: Current learning rate: 0.00109 
2023-10-26 16:40:48.901003: train_loss -0.9052 
2023-10-26 16:40:48.901365: val_loss -0.8779 
2023-10-26 16:40:48.901625: Pseudo dice [0.8803, 0.9142, 0.9706, 0.7482, 0.9279] 
2023-10-26 16:40:48.901856: Epoch time: 3.95 s 
2023-10-26 16:40:49.984625:  
2023-10-26 16:40:49.984918: Epoch 916 
2023-10-26 16:40:49.985163: Current learning rate: 0.00108 
2023-10-26 16:40:54.006182: train_loss -0.905 
2023-10-26 16:40:54.006579: val_loss -0.8778 
2023-10-26 16:40:54.006852: Pseudo dice [0.8807, 0.9181, 0.9708, 0.7321, 0.9356] 
2023-10-26 16:40:54.007093: Epoch time: 4.02 s 
2023-10-26 16:40:55.114712:  
2023-10-26 16:40:55.115075: Epoch 917 
2023-10-26 16:40:55.115313: Current learning rate: 0.00106 
2023-10-26 16:40:58.988349: train_loss -0.9048 
2023-10-26 16:40:58.988725: val_loss -0.8778 
2023-10-26 16:40:58.988987: Pseudo dice [0.8838, 0.922, 0.97, 0.7506, 0.9273] 
2023-10-26 16:40:58.989231: Epoch time: 3.87 s 
2023-10-26 16:41:00.288015:  
2023-10-26 16:41:00.288314: Epoch 918 
2023-10-26 16:41:00.288567: Current learning rate: 0.00105 
2023-10-26 16:41:04.338597: train_loss -0.9052 
2023-10-26 16:41:04.339365: val_loss -0.8811 
2023-10-26 16:41:04.339866: Pseudo dice [0.8814, 0.9164, 0.971, 0.7737, 0.9263] 
2023-10-26 16:41:04.340343: Epoch time: 4.05 s 
2023-10-26 16:41:05.410337:  
2023-10-26 16:41:05.410625: Epoch 919 
2023-10-26 16:41:05.410859: Current learning rate: 0.00104 
2023-10-26 16:41:09.294147: train_loss -0.8983 
2023-10-26 16:41:09.294528: val_loss -0.8792 
2023-10-26 16:41:09.294792: Pseudo dice [0.8813, 0.9148, 0.9686, 0.7656, 0.926] 
2023-10-26 16:41:09.295034: Epoch time: 3.88 s 
2023-10-26 16:41:10.368028:  
2023-10-26 16:41:10.368315: Epoch 920 
2023-10-26 16:41:10.368550: Current learning rate: 0.00103 
2023-10-26 16:41:14.416858: train_loss -0.9012 
2023-10-26 16:41:14.417270: val_loss -0.8801 
2023-10-26 16:41:14.417525: Pseudo dice [0.8823, 0.9188, 0.9698, 0.7171, 0.9311] 
2023-10-26 16:41:14.417755: Epoch time: 4.05 s 
2023-10-26 16:41:15.497954:  
2023-10-26 16:41:15.498246: Epoch 921 
2023-10-26 16:41:15.498487: Current learning rate: 0.00102 
2023-10-26 16:41:19.220778: train_loss -0.8997 
2023-10-26 16:41:19.221295: val_loss -0.8758 
2023-10-26 16:41:19.221815: Pseudo dice [0.8804, 0.9173, 0.971, 0.7431, 0.9354] 
2023-10-26 16:41:19.222107: Epoch time: 3.72 s 
2023-10-26 16:41:20.291248:  
2023-10-26 16:41:20.291536: Epoch 922 
2023-10-26 16:41:20.291765: Current learning rate: 0.00101 
2023-10-26 16:41:24.326426: train_loss -0.8997 
2023-10-26 16:41:24.326790: val_loss -0.8752 
2023-10-26 16:41:24.327066: Pseudo dice [0.8822, 0.9147, 0.9678, 0.7749, 0.9236] 
2023-10-26 16:41:24.327290: Epoch time: 4.04 s 
2023-10-26 16:41:25.400703:  
2023-10-26 16:41:25.401000: Epoch 923 
2023-10-26 16:41:25.401235: Current learning rate: 0.001 
2023-10-26 16:41:29.360378: train_loss -0.9017 
2023-10-26 16:41:29.360780: val_loss -0.8795 
2023-10-26 16:41:29.361057: Pseudo dice [0.883, 0.9219, 0.9726, 0.7327, 0.9315] 
2023-10-26 16:41:29.361317: Epoch time: 3.96 s 
2023-10-26 16:41:30.633514:  
2023-10-26 16:41:30.633820: Epoch 924 
2023-10-26 16:41:30.634080: Current learning rate: 0.00098 
2023-10-26 16:41:34.410497: train_loss -0.9018 
2023-10-26 16:41:34.410903: val_loss -0.8822 
2023-10-26 16:41:34.411164: Pseudo dice [0.8844, 0.9177, 0.9704, 0.7456, 0.9379] 
2023-10-26 16:41:34.411400: Epoch time: 3.78 s 
2023-10-26 16:41:35.521264:  
2023-10-26 16:41:35.521563: Epoch 925 
2023-10-26 16:41:35.521820: Current learning rate: 0.00097 
2023-10-26 16:41:39.338741: train_loss -0.8986 
2023-10-26 16:41:39.339327: val_loss -0.8772 
2023-10-26 16:41:39.339619: Pseudo dice [0.8805, 0.9125, 0.9677, 0.7284, 0.9277] 
2023-10-26 16:41:39.339901: Epoch time: 3.82 s 
2023-10-26 16:41:40.431697:  
2023-10-26 16:41:40.431998: Epoch 926 
2023-10-26 16:41:40.432239: Current learning rate: 0.00096 
2023-10-26 16:41:44.431065: train_loss -0.8984 
2023-10-26 16:41:44.431443: val_loss -0.8695 
2023-10-26 16:41:44.431713: Pseudo dice [0.8801, 0.9107, 0.9684, 0.7547, 0.9363] 
2023-10-26 16:41:44.431952: Epoch time: 4.0 s 
2023-10-26 16:41:45.528454:  
2023-10-26 16:41:45.528789: Epoch 927 
2023-10-26 16:41:45.529052: Current learning rate: 0.00095 
2023-10-26 16:41:49.523558: train_loss -0.8985 
2023-10-26 16:41:49.523950: val_loss -0.8743 
2023-10-26 16:41:49.524220: Pseudo dice [0.8785, 0.914, 0.9685, 0.75, 0.9267] 
2023-10-26 16:41:49.524461: Epoch time: 4.0 s 
2023-10-26 16:41:50.600307:  
2023-10-26 16:41:50.600624: Epoch 928 
2023-10-26 16:41:50.600870: Current learning rate: 0.00094 
2023-10-26 16:41:54.398870: train_loss -0.9044 
2023-10-26 16:41:54.399217: val_loss -0.8814 
2023-10-26 16:41:54.399481: Pseudo dice [0.8841, 0.9157, 0.9725, 0.7746, 0.9353] 
2023-10-26 16:41:54.399701: Epoch time: 3.8 s 
2023-10-26 16:41:55.464468:  
2023-10-26 16:41:55.464767: Epoch 929 
2023-10-26 16:41:55.465012: Current learning rate: 0.00092 
2023-10-26 16:41:59.493097: train_loss -0.9005 
2023-10-26 16:41:59.493452: val_loss -0.8786 
2023-10-26 16:41:59.493707: Pseudo dice [0.8834, 0.9183, 0.971, 0.7705, 0.9413] 
2023-10-26 16:41:59.493934: Epoch time: 4.03 s 
2023-10-26 16:42:00.560391:  
2023-10-26 16:42:00.560683: Epoch 930 
2023-10-26 16:42:00.560935: Current learning rate: 0.00091 
2023-10-26 16:42:04.601495: train_loss -0.8995 
2023-10-26 16:42:04.601939: val_loss -0.8704 
2023-10-26 16:42:04.602249: Pseudo dice [0.8797, 0.9164, 0.9673, 0.7628, 0.9162] 
2023-10-26 16:42:04.602490: Epoch time: 4.04 s 
2023-10-26 16:42:05.876661:  
2023-10-26 16:42:05.876970: Epoch 931 
2023-10-26 16:42:05.877225: Current learning rate: 0.0009 
2023-10-26 16:42:09.866323: train_loss -0.9045 
2023-10-26 16:42:09.866721: val_loss -0.8772 
2023-10-26 16:42:09.866989: Pseudo dice [0.8802, 0.9143, 0.969, 0.7455, 0.9398] 
2023-10-26 16:42:09.867216: Epoch time: 3.99 s 
2023-10-26 16:42:11.077783:  
2023-10-26 16:42:11.078097: Epoch 932 
2023-10-26 16:42:11.078387: Current learning rate: 0.00089 
2023-10-26 16:42:15.138621: train_loss -0.8994 
2023-10-26 16:42:15.139035: val_loss -0.8721 
2023-10-26 16:42:15.139289: Pseudo dice [0.8756, 0.9118, 0.9684, 0.7533, 0.9284] 
2023-10-26 16:42:15.139522: Epoch time: 4.06 s 
2023-10-26 16:42:16.225380:  
2023-10-26 16:42:16.225680: Epoch 933 
2023-10-26 16:42:16.225932: Current learning rate: 0.00088 
2023-10-26 16:42:20.185851: train_loss -0.899 
2023-10-26 16:42:20.186234: val_loss -0.8824 
2023-10-26 16:42:20.186489: Pseudo dice [0.8818, 0.9174, 0.9698, 0.7251, 0.933] 
2023-10-26 16:42:20.186709: Epoch time: 3.96 s 
2023-10-26 16:42:21.267727:  
2023-10-26 16:42:21.268021: Epoch 934 
2023-10-26 16:42:21.268259: Current learning rate: 0.00087 
2023-10-26 16:42:25.215240: train_loss -0.9047 
2023-10-26 16:42:25.215646: val_loss -0.8729 
2023-10-26 16:42:25.215911: Pseudo dice [0.8807, 0.9151, 0.9699, 0.6928, 0.9269] 
2023-10-26 16:42:25.216157: Epoch time: 3.95 s 
2023-10-26 16:42:26.305177:  
2023-10-26 16:42:26.305471: Epoch 935 
2023-10-26 16:42:26.305701: Current learning rate: 0.00085 
2023-10-26 16:42:30.248966: train_loss -0.9023 
2023-10-26 16:42:30.249378: val_loss -0.8829 
2023-10-26 16:42:30.249643: Pseudo dice [0.8816, 0.9163, 0.9703, 0.7366, 0.937] 
2023-10-26 16:42:30.249946: Epoch time: 3.94 s 
2023-10-26 16:42:31.369451:  
2023-10-26 16:42:31.369795: Epoch 936 
2023-10-26 16:42:31.370091: Current learning rate: 0.00084 
2023-10-26 16:42:35.336523: train_loss -0.9012 
2023-10-26 16:42:35.336941: val_loss -0.8782 
2023-10-26 16:42:35.337203: Pseudo dice [0.88, 0.9144, 0.9698, 0.6884, 0.9318] 
2023-10-26 16:42:35.337529: Epoch time: 3.97 s 
2023-10-26 16:42:36.413132:  
2023-10-26 16:42:36.413439: Epoch 937 
2023-10-26 16:42:36.413688: Current learning rate: 0.00083 
2023-10-26 16:42:40.276778: train_loss -0.9087 
2023-10-26 16:42:40.277432: val_loss -0.8853 
2023-10-26 16:42:40.277753: Pseudo dice [0.8848, 0.9182, 0.9723, 0.758, 0.9411] 
2023-10-26 16:42:40.278111: Epoch time: 3.86 s 
2023-10-26 16:42:41.567740:  
2023-10-26 16:42:41.568058: Epoch 938 
2023-10-26 16:42:41.568379: Current learning rate: 0.00082 
2023-10-26 16:42:45.531118: train_loss -0.9035 
2023-10-26 16:42:45.531811: val_loss -0.8788 
2023-10-26 16:42:45.532118: Pseudo dice [0.8807, 0.9117, 0.9679, 0.7245, 0.9383] 
2023-10-26 16:42:45.532377: Epoch time: 3.96 s 
2023-10-26 16:42:46.613720:  
2023-10-26 16:42:46.614040: Epoch 939 
2023-10-26 16:42:46.614278: Current learning rate: 0.00081 
2023-10-26 16:42:50.557989: train_loss -0.902 
2023-10-26 16:42:50.558386: val_loss -0.8826 
2023-10-26 16:42:50.558650: Pseudo dice [0.8816, 0.9142, 0.9713, 0.7827, 0.9369] 
2023-10-26 16:42:50.558908: Epoch time: 3.94 s 
2023-10-26 16:42:51.644526:  
2023-10-26 16:42:51.644851: Epoch 940 
2023-10-26 16:42:51.645101: Current learning rate: 0.00079 
2023-10-26 16:42:55.591446: train_loss -0.909 
2023-10-26 16:42:55.591867: val_loss -0.8831 
2023-10-26 16:42:55.592263: Pseudo dice [0.8814, 0.9134, 0.9703, 0.764, 0.9381] 
2023-10-26 16:42:55.592497: Epoch time: 3.95 s 
2023-10-26 16:42:56.676852:  
2023-10-26 16:42:56.677166: Epoch 941 
2023-10-26 16:42:56.677411: Current learning rate: 0.00078 
2023-10-26 16:43:00.691833: train_loss -0.9054 
2023-10-26 16:43:00.692255: val_loss -0.88 
2023-10-26 16:43:00.692519: Pseudo dice [0.8801, 0.9101, 0.9701, 0.7722, 0.9306] 
2023-10-26 16:43:00.692750: Epoch time: 4.02 s 
2023-10-26 16:43:01.779365:  
2023-10-26 16:43:01.779691: Epoch 942 
2023-10-26 16:43:01.779972: Current learning rate: 0.00077 
2023-10-26 16:43:05.793679: train_loss -0.9037 
2023-10-26 16:43:05.794104: val_loss -0.868 
2023-10-26 16:43:05.794436: Pseudo dice [0.8801, 0.9147, 0.9702, 0.7141, 0.926] 
2023-10-26 16:43:05.794692: Epoch time: 4.02 s 
2023-10-26 16:43:06.906104:  
2023-10-26 16:43:06.906426: Epoch 943 
2023-10-26 16:43:06.906678: Current learning rate: 0.00076 
2023-10-26 16:43:10.885269: train_loss -0.9004 
2023-10-26 16:43:10.885633: val_loss -0.8788 
2023-10-26 16:43:10.885897: Pseudo dice [0.8782, 0.9098, 0.9685, 0.7597, 0.9255] 
2023-10-26 16:43:10.886122: Epoch time: 3.98 s 
2023-10-26 16:43:11.947013:  
2023-10-26 16:43:11.947310: Epoch 944 
2023-10-26 16:43:11.947555: Current learning rate: 0.00075 
2023-10-26 16:43:16.094438: train_loss -0.907 
2023-10-26 16:43:16.094842: val_loss -0.8728 
2023-10-26 16:43:16.095114: Pseudo dice [0.8792, 0.916, 0.9705, 0.7221, 0.9325] 
2023-10-26 16:43:16.095343: Epoch time: 4.15 s 
2023-10-26 16:43:17.313714:  
2023-10-26 16:43:17.314021: Epoch 945 
2023-10-26 16:43:17.314260: Current learning rate: 0.00074 
2023-10-26 16:43:21.312580: train_loss -0.9029 
2023-10-26 16:43:21.312964: val_loss -0.8781 
2023-10-26 16:43:21.313261: Pseudo dice [0.8795, 0.9123, 0.971, 0.7123, 0.9426] 
2023-10-26 16:43:21.313478: Epoch time: 4.0 s 
2023-10-26 16:43:22.372098:  
2023-10-26 16:43:22.372375: Epoch 946 
2023-10-26 16:43:22.372615: Current learning rate: 0.00072 
2023-10-26 16:43:26.399898: train_loss -0.906 
2023-10-26 16:43:26.400278: val_loss -0.8776 
2023-10-26 16:43:26.400533: Pseudo dice [0.8807, 0.9096, 0.9695, 0.7493, 0.9378] 
2023-10-26 16:43:26.400766: Epoch time: 4.03 s 
2023-10-26 16:43:27.474307:  
2023-10-26 16:43:27.474594: Epoch 947 
2023-10-26 16:43:27.474827: Current learning rate: 0.00071 
2023-10-26 16:43:31.512426: train_loss -0.9062 
2023-10-26 16:43:31.512816: val_loss -0.8773 
2023-10-26 16:43:31.513081: Pseudo dice [0.8803, 0.9141, 0.9698, 0.7443, 0.9346] 
2023-10-26 16:43:31.513313: Epoch time: 4.04 s 
2023-10-26 16:43:32.620019:  
2023-10-26 16:43:32.620342: Epoch 948 
2023-10-26 16:43:32.620632: Current learning rate: 0.0007 
2023-10-26 16:43:36.603194: train_loss -0.9065 
2023-10-26 16:43:36.603689: val_loss -0.8786 
2023-10-26 16:43:36.604049: Pseudo dice [0.8786, 0.9146, 0.9705, 0.7464, 0.9251] 
2023-10-26 16:43:36.604358: Epoch time: 3.98 s 
2023-10-26 16:43:37.760684:  
2023-10-26 16:43:37.760976: Epoch 949 
2023-10-26 16:43:37.761210: Current learning rate: 0.00069 
2023-10-26 16:43:41.762769: train_loss -0.9066 
2023-10-26 16:43:41.763176: val_loss -0.8653 
2023-10-26 16:43:41.763449: Pseudo dice [0.8804, 0.9187, 0.9692, 0.7105, 0.9362] 
2023-10-26 16:43:41.763695: Epoch time: 4.0 s 
2023-10-26 16:43:42.951427:  
2023-10-26 16:43:42.951708: Epoch 950 
2023-10-26 16:43:42.951954: Current learning rate: 0.00067 
2023-10-26 16:43:46.811491: train_loss -0.9062 
2023-10-26 16:43:46.811916: val_loss -0.8773 
2023-10-26 16:43:46.812184: Pseudo dice [0.8797, 0.9132, 0.97, 0.7538, 0.9245] 
2023-10-26 16:43:46.812399: Epoch time: 3.86 s 
2023-10-26 16:43:48.067488:  
2023-10-26 16:43:48.067781: Epoch 951 
2023-10-26 16:43:48.068034: Current learning rate: 0.00066 
2023-10-26 16:43:52.028110: train_loss -0.9103 
2023-10-26 16:43:52.028512: val_loss -0.8785 
2023-10-26 16:43:52.028762: Pseudo dice [0.8765, 0.9113, 0.9686, 0.7267, 0.9331] 
2023-10-26 16:43:52.029007: Epoch time: 3.96 s 
2023-10-26 16:43:53.120547:  
2023-10-26 16:43:53.120864: Epoch 952 
2023-10-26 16:43:53.121109: Current learning rate: 0.00065 
2023-10-26 16:43:57.046763: train_loss -0.909 
2023-10-26 16:43:57.047210: val_loss -0.8787 
2023-10-26 16:43:57.047586: Pseudo dice [0.8789, 0.9126, 0.9709, 0.762, 0.934] 
2023-10-26 16:43:57.047910: Epoch time: 3.93 s 
2023-10-26 16:43:58.128764:  
2023-10-26 16:43:58.129063: Epoch 953 
2023-10-26 16:43:58.129305: Current learning rate: 0.00064 
2023-10-26 16:44:02.037760: train_loss -0.9079 
2023-10-26 16:44:02.038194: val_loss -0.8751 
2023-10-26 16:44:02.038641: Pseudo dice [0.8802, 0.9178, 0.9704, 0.7589, 0.9364] 
2023-10-26 16:44:02.038952: Epoch time: 3.91 s 
2023-10-26 16:44:03.137995:  
2023-10-26 16:44:03.138304: Epoch 954 
2023-10-26 16:44:03.138548: Current learning rate: 0.00063 
2023-10-26 16:44:06.997000: train_loss -0.9092 
2023-10-26 16:44:06.997402: val_loss -0.8776 
2023-10-26 16:44:06.997678: Pseudo dice [0.8801, 0.9178, 0.9705, 0.7275, 0.9402] 
2023-10-26 16:44:06.997930: Epoch time: 3.86 s 
2023-10-26 16:44:08.152651:  
2023-10-26 16:44:08.152999: Epoch 955 
2023-10-26 16:44:08.153262: Current learning rate: 0.00061 
2023-10-26 16:44:12.086162: train_loss -0.9099 
2023-10-26 16:44:12.086549: val_loss -0.88 
2023-10-26 16:44:12.086807: Pseudo dice [0.8815, 0.9164, 0.971, 0.7625, 0.9339] 
2023-10-26 16:44:12.087070: Epoch time: 3.93 s 
2023-10-26 16:44:13.263153:  
2023-10-26 16:44:13.263467: Epoch 956 
2023-10-26 16:44:13.263717: Current learning rate: 0.0006 
2023-10-26 16:44:17.295590: train_loss -0.9067 
2023-10-26 16:44:17.296011: val_loss -0.8777 
2023-10-26 16:44:17.296300: Pseudo dice [0.8775, 0.9144, 0.97, 0.7573, 0.9363] 
2023-10-26 16:44:17.296542: Epoch time: 4.03 s 
2023-10-26 16:44:18.386036:  
2023-10-26 16:44:18.386341: Epoch 957 
2023-10-26 16:44:18.386609: Current learning rate: 0.00059 
2023-10-26 16:44:22.261749: train_loss -0.9003 
2023-10-26 16:44:22.262187: val_loss -0.8721 
2023-10-26 16:44:22.262451: Pseudo dice [0.8825, 0.914, 0.9698, 0.6857, 0.9341] 
2023-10-26 16:44:22.262686: Epoch time: 3.88 s 
2023-10-26 16:44:23.599135:  
2023-10-26 16:44:23.599451: Epoch 958 
2023-10-26 16:44:23.599694: Current learning rate: 0.00058 
2023-10-26 16:44:27.510631: train_loss -0.907 
2023-10-26 16:44:27.511027: val_loss -0.8666 
2023-10-26 16:44:27.511279: Pseudo dice [0.8809, 0.9125, 0.9694, 0.7464, 0.9325] 
2023-10-26 16:44:27.511513: Epoch time: 3.91 s 
2023-10-26 16:44:28.601997:  
2023-10-26 16:44:28.602291: Epoch 959 
2023-10-26 16:44:28.602543: Current learning rate: 0.00056 
2023-10-26 16:44:32.663344: train_loss -0.9049 
2023-10-26 16:44:32.663736: val_loss -0.8829 
2023-10-26 16:44:32.664028: Pseudo dice [0.8787, 0.9147, 0.9702, 0.7784, 0.9324] 
2023-10-26 16:44:32.664264: Epoch time: 4.06 s 
2023-10-26 16:44:33.752150:  
2023-10-26 16:44:33.752485: Epoch 960 
2023-10-26 16:44:33.752741: Current learning rate: 0.00055 
2023-10-26 16:44:37.714098: train_loss -0.9023 
2023-10-26 16:44:37.714456: val_loss -0.8762 
2023-10-26 16:44:37.714841: Pseudo dice [0.8793, 0.9155, 0.9702, 0.7775, 0.9326] 
2023-10-26 16:44:37.715075: Epoch time: 3.96 s 
2023-10-26 16:44:38.785643:  
2023-10-26 16:44:38.785928: Epoch 961 
2023-10-26 16:44:38.786167: Current learning rate: 0.00054 
2023-10-26 16:44:42.717471: train_loss -0.9089 
2023-10-26 16:44:42.717865: val_loss -0.8717 
2023-10-26 16:44:42.718143: Pseudo dice [0.8828, 0.9147, 0.9704, 0.7532, 0.9251] 
2023-10-26 16:44:42.718382: Epoch time: 3.93 s 
2023-10-26 16:44:43.804314:  
2023-10-26 16:44:43.804613: Epoch 962 
2023-10-26 16:44:43.804855: Current learning rate: 0.00053 
2023-10-26 16:44:47.955197: train_loss -0.9078 
2023-10-26 16:44:47.955586: val_loss -0.8747 
2023-10-26 16:44:47.955835: Pseudo dice [0.8837, 0.9168, 0.9694, 0.7519, 0.9228] 
2023-10-26 16:44:47.956066: Epoch time: 4.15 s 
2023-10-26 16:44:49.050686:  
2023-10-26 16:44:49.050995: Epoch 963 
2023-10-26 16:44:49.051234: Current learning rate: 0.00051 
2023-10-26 16:44:53.050644: train_loss -0.9035 
2023-10-26 16:44:53.051258: val_loss -0.8683 
2023-10-26 16:44:53.051541: Pseudo dice [0.8792, 0.9186, 0.9695, 0.7449, 0.9296] 
2023-10-26 16:44:53.051925: Epoch time: 4.0 s 
2023-10-26 16:44:54.143965:  
2023-10-26 16:44:54.144248: Epoch 964 
2023-10-26 16:44:54.144503: Current learning rate: 0.0005 
2023-10-26 16:44:58.216903: train_loss -0.9068 
2023-10-26 16:44:58.217301: val_loss -0.8748 
2023-10-26 16:44:58.217601: Pseudo dice [0.8814, 0.9144, 0.9682, 0.7773, 0.9286] 
2023-10-26 16:44:58.217839: Epoch time: 4.07 s 
2023-10-26 16:44:58.218085: Yayy! New best EMA pseudo Dice: 0.8894 
2023-10-26 16:44:59.419794:  
2023-10-26 16:44:59.420091: Epoch 965 
2023-10-26 16:44:59.420338: Current learning rate: 0.00049 
2023-10-26 16:45:03.367098: train_loss -0.907 
2023-10-26 16:45:03.367490: val_loss -0.8754 
2023-10-26 16:45:03.367761: Pseudo dice [0.8815, 0.9179, 0.9701, 0.742, 0.9261] 
2023-10-26 16:45:03.368012: Epoch time: 3.95 s 
2023-10-26 16:45:04.477786:  
2023-10-26 16:45:04.478087: Epoch 966 
2023-10-26 16:45:04.478337: Current learning rate: 0.00048 
2023-10-26 16:45:08.375643: train_loss -0.9081 
2023-10-26 16:45:08.376030: val_loss -0.8806 
2023-10-26 16:45:08.376288: Pseudo dice [0.8817, 0.911, 0.9698, 0.7448, 0.9418] 
2023-10-26 16:45:08.376506: Epoch time: 3.9 s 
2023-10-26 16:45:09.481969:  
2023-10-26 16:45:09.482259: Epoch 967 
2023-10-26 16:45:09.482491: Current learning rate: 0.00046 
2023-10-26 16:45:13.372037: train_loss -0.9035 
2023-10-26 16:45:13.372448: val_loss -0.8781 
2023-10-26 16:45:13.372724: Pseudo dice [0.8808, 0.9093, 0.9702, 0.7792, 0.938] 
2023-10-26 16:45:13.372976: Epoch time: 3.89 s 
2023-10-26 16:45:13.373265: Yayy! New best EMA pseudo Dice: 0.8899 
2023-10-26 16:45:14.626439:  
2023-10-26 16:45:14.626804: Epoch 968 
2023-10-26 16:45:14.627053: Current learning rate: 0.00045 
2023-10-26 16:45:18.438553: train_loss -0.908 
2023-10-26 16:45:18.438951: val_loss -0.8768 
2023-10-26 16:45:18.439215: Pseudo dice [0.8804, 0.9148, 0.9699, 0.7362, 0.9332] 
2023-10-26 16:45:18.439435: Epoch time: 3.81 s 
2023-10-26 16:45:19.541928:  
2023-10-26 16:45:19.542300: Epoch 969 
2023-10-26 16:45:19.542604: Current learning rate: 0.00044 
2023-10-26 16:45:23.347498: train_loss -0.9056 
2023-10-26 16:45:23.347886: val_loss -0.8769 
2023-10-26 16:45:23.348340: Pseudo dice [0.8768, 0.9166, 0.9716, 0.7538, 0.9353] 
2023-10-26 16:45:23.348589: Epoch time: 3.81 s 
2023-10-26 16:45:24.445658:  
2023-10-26 16:45:24.445958: Epoch 970 
2023-10-26 16:45:24.446209: Current learning rate: 0.00043 
2023-10-26 16:45:28.359687: train_loss -0.9071 
2023-10-26 16:45:28.360374: val_loss -0.8801 
2023-10-26 16:45:28.360651: Pseudo dice [0.8772, 0.9099, 0.9696, 0.7878, 0.9201] 
2023-10-26 16:45:28.360893: Epoch time: 3.91 s 
2023-10-26 16:45:28.361105: Yayy! New best EMA pseudo Dice: 0.8901 
2023-10-26 16:45:29.786186:  
2023-10-26 16:45:29.786538: Epoch 971 
2023-10-26 16:45:29.786785: Current learning rate: 0.00041 
2023-10-26 16:45:33.880293: train_loss -0.9102 
2023-10-26 16:45:33.880689: val_loss -0.8796 
2023-10-26 16:45:33.880964: Pseudo dice [0.8821, 0.9179, 0.9711, 0.7357, 0.9401] 
2023-10-26 16:45:33.881209: Epoch time: 4.09 s 
2023-10-26 16:45:34.986354:  
2023-10-26 16:45:34.986646: Epoch 972 
2023-10-26 16:45:34.986901: Current learning rate: 0.0004 
2023-10-26 16:45:38.890864: train_loss -0.9038 
2023-10-26 16:45:38.891239: val_loss -0.8797 
2023-10-26 16:45:38.891492: Pseudo dice [0.8811, 0.9135, 0.9703, 0.7469, 0.9376] 
2023-10-26 16:45:38.891778: Epoch time: 3.91 s 
2023-10-26 16:45:39.991960:  
2023-10-26 16:45:39.992250: Epoch 973 
2023-10-26 16:45:39.992489: Current learning rate: 0.00039 
2023-10-26 16:45:43.899270: train_loss -0.9096 
2023-10-26 16:45:43.899646: val_loss -0.877 
2023-10-26 16:45:43.899918: Pseudo dice [0.8797, 0.9145, 0.9703, 0.7023, 0.9289] 
2023-10-26 16:45:43.900158: Epoch time: 3.91 s 
2023-10-26 16:45:45.002087:  
2023-10-26 16:45:45.002478: Epoch 974 
2023-10-26 16:45:45.002716: Current learning rate: 0.00037 
2023-10-26 16:45:49.194594: train_loss -0.9076 
2023-10-26 16:45:49.195010: val_loss -0.8754 
2023-10-26 16:45:49.195279: Pseudo dice [0.8827, 0.9157, 0.9706, 0.6994, 0.9285] 
2023-10-26 16:45:49.195514: Epoch time: 4.19 s 
2023-10-26 16:45:50.298883:  
2023-10-26 16:45:50.299188: Epoch 975 
2023-10-26 16:45:50.299433: Current learning rate: 0.00036 
2023-10-26 16:45:54.407383: train_loss -0.9034 
2023-10-26 16:45:54.407773: val_loss -0.8782 
2023-10-26 16:45:54.408038: Pseudo dice [0.8789, 0.9183, 0.9713, 0.7279, 0.9359] 
2023-10-26 16:45:54.408268: Epoch time: 4.11 s 
2023-10-26 16:45:55.490713:  
2023-10-26 16:45:55.491018: Epoch 976 
2023-10-26 16:45:55.491250: Current learning rate: 0.00035 
2023-10-26 16:45:59.367115: train_loss -0.9079 
2023-10-26 16:45:59.367486: val_loss -0.8803 
2023-10-26 16:45:59.367745: Pseudo dice [0.8793, 0.9106, 0.9697, 0.7489, 0.941] 
2023-10-26 16:45:59.367984: Epoch time: 3.88 s 
2023-10-26 16:46:00.444853:  
2023-10-26 16:46:00.445153: Epoch 977 
2023-10-26 16:46:00.445396: Current learning rate: 0.00034 
2023-10-26 16:46:04.406520: train_loss -0.9102 
2023-10-26 16:46:04.406888: val_loss -0.8791 
2023-10-26 16:46:04.407233: Pseudo dice [0.8812, 0.9126, 0.9699, 0.7648, 0.9378] 
2023-10-26 16:46:04.407655: Epoch time: 3.96 s 
2023-10-26 16:46:05.482499:  
2023-10-26 16:46:05.482810: Epoch 978 
2023-10-26 16:46:05.483062: Current learning rate: 0.00032 
2023-10-26 16:46:09.565075: train_loss -0.9068 
2023-10-26 16:46:09.565476: val_loss -0.8798 
2023-10-26 16:46:09.565727: Pseudo dice [0.8804, 0.9155, 0.9709, 0.7412, 0.9398] 
2023-10-26 16:46:09.566022: Epoch time: 4.08 s 
2023-10-26 16:46:10.680799:  
2023-10-26 16:46:10.681119: Epoch 979 
2023-10-26 16:46:10.681366: Current learning rate: 0.00031 
2023-10-26 16:46:14.588133: train_loss -0.9072 
2023-10-26 16:46:14.588507: val_loss -0.8733 
2023-10-26 16:46:14.588789: Pseudo dice [0.8807, 0.915, 0.9691, 0.7343, 0.9368] 
2023-10-26 16:46:14.589042: Epoch time: 3.91 s 
2023-10-26 16:46:15.742398:  
2023-10-26 16:46:15.742702: Epoch 980 
2023-10-26 16:46:15.742944: Current learning rate: 0.0003 
2023-10-26 16:46:19.688767: train_loss -0.9039 
2023-10-26 16:46:19.689247: val_loss -0.8804 
2023-10-26 16:46:19.689583: Pseudo dice [0.8842, 0.9131, 0.9706, 0.7416, 0.9369] 
2023-10-26 16:46:19.689861: Epoch time: 3.95 s 
2023-10-26 16:46:20.816846:  
2023-10-26 16:46:20.817147: Epoch 981 
2023-10-26 16:46:20.817384: Current learning rate: 0.00028 
2023-10-26 16:46:24.768914: train_loss -0.9097 
2023-10-26 16:46:24.769619: val_loss -0.8773 
2023-10-26 16:46:24.769908: Pseudo dice [0.883, 0.9112, 0.9702, 0.7399, 0.9373] 
2023-10-26 16:46:24.770152: Epoch time: 3.95 s 
2023-10-26 16:46:25.881658:  
2023-10-26 16:46:25.881974: Epoch 982 
2023-10-26 16:46:25.882221: Current learning rate: 0.00027 
2023-10-26 16:46:29.788120: train_loss -0.9086 
2023-10-26 16:46:29.788857: val_loss -0.8835 
2023-10-26 16:46:29.789150: Pseudo dice [0.8816, 0.9169, 0.9703, 0.7387, 0.935] 
2023-10-26 16:46:29.789558: Epoch time: 3.91 s 
2023-10-26 16:46:30.942371:  
2023-10-26 16:46:30.942697: Epoch 983 
2023-10-26 16:46:30.942946: Current learning rate: 0.00026 
2023-10-26 16:46:34.887420: train_loss -0.899 
2023-10-26 16:46:34.887828: val_loss -0.8814 
2023-10-26 16:46:34.888101: Pseudo dice [0.8823, 0.9157, 0.9704, 0.752, 0.9401] 
2023-10-26 16:46:34.888546: Epoch time: 3.95 s 
2023-10-26 16:46:36.190430:  
2023-10-26 16:46:36.190750: Epoch 984 
2023-10-26 16:46:36.191006: Current learning rate: 0.00024 
2023-10-26 16:46:40.003789: train_loss -0.9095 
2023-10-26 16:46:40.004190: val_loss -0.8749 
2023-10-26 16:46:40.004441: Pseudo dice [0.8822, 0.9165, 0.9705, 0.7147, 0.933] 
2023-10-26 16:46:40.004672: Epoch time: 3.81 s 
2023-10-26 16:46:41.115220:  
2023-10-26 16:46:41.115519: Epoch 985 
2023-10-26 16:46:41.115769: Current learning rate: 0.00023 
2023-10-26 16:46:45.042841: train_loss -0.9035 
2023-10-26 16:46:45.043235: val_loss -0.873 
2023-10-26 16:46:45.043526: Pseudo dice [0.8815, 0.9175, 0.9693, 0.7085, 0.9285] 
2023-10-26 16:46:45.043785: Epoch time: 3.93 s 
2023-10-26 16:46:46.153910:  
2023-10-26 16:46:46.154234: Epoch 986 
2023-10-26 16:46:46.154506: Current learning rate: 0.00021 
2023-10-26 16:46:50.119526: train_loss -0.9073 
2023-10-26 16:46:50.119928: val_loss -0.8783 
2023-10-26 16:46:50.120190: Pseudo dice [0.8815, 0.9183, 0.9709, 0.7262, 0.9385] 
2023-10-26 16:46:50.120414: Epoch time: 3.97 s 
2023-10-26 16:46:51.260469:  
2023-10-26 16:46:51.260764: Epoch 987 
2023-10-26 16:46:51.261013: Current learning rate: 0.0002 
2023-10-26 16:46:55.211256: train_loss -0.9093 
2023-10-26 16:46:55.211630: val_loss -0.8847 
2023-10-26 16:46:55.211905: Pseudo dice [0.884, 0.9177, 0.9709, 0.7611, 0.932] 
2023-10-26 16:46:55.212172: Epoch time: 3.95 s 
2023-10-26 16:46:56.342314:  
2023-10-26 16:46:56.342648: Epoch 988 
2023-10-26 16:46:56.342934: Current learning rate: 0.00019 
2023-10-26 16:47:00.183138: train_loss -0.908 
2023-10-26 16:47:00.183486: val_loss -0.8857 
2023-10-26 16:47:00.183737: Pseudo dice [0.8839, 0.912, 0.9701, 0.7706, 0.9435] 
2023-10-26 16:47:00.183965: Epoch time: 3.84 s 
2023-10-26 16:47:01.273381:  
2023-10-26 16:47:01.273695: Epoch 989 
2023-10-26 16:47:01.273951: Current learning rate: 0.00017 
2023-10-26 16:47:05.382097: train_loss -0.9047 
2023-10-26 16:47:05.382525: val_loss -0.8781 
2023-10-26 16:47:05.382821: Pseudo dice [0.8793, 0.9178, 0.9698, 0.7356, 0.9311] 
2023-10-26 16:47:05.383073: Epoch time: 4.11 s 
2023-10-26 16:47:06.498847:  
2023-10-26 16:47:06.499181: Epoch 990 
2023-10-26 16:47:06.499440: Current learning rate: 0.00016 
2023-10-26 16:47:10.506599: train_loss -0.9044 
2023-10-26 16:47:10.506976: val_loss -0.8755 
2023-10-26 16:47:10.507246: Pseudo dice [0.8783, 0.91, 0.969, 0.7766, 0.9258] 
2023-10-26 16:47:10.507477: Epoch time: 4.01 s 
2023-10-26 16:47:11.793446:  
2023-10-26 16:47:11.793776: Epoch 991 
2023-10-26 16:47:11.794054: Current learning rate: 0.00014 
2023-10-26 16:47:15.858270: train_loss -0.9096 
2023-10-26 16:47:15.858630: val_loss -0.8811 
2023-10-26 16:47:15.858907: Pseudo dice [0.8805, 0.9157, 0.9698, 0.7416, 0.9295] 
2023-10-26 16:47:15.859142: Epoch time: 4.07 s 
2023-10-26 16:47:16.952571:  
2023-10-26 16:47:16.952859: Epoch 992 
2023-10-26 16:47:16.953117: Current learning rate: 0.00013 
2023-10-26 16:47:20.891655: train_loss -0.9064 
2023-10-26 16:47:20.892046: val_loss -0.8785 
2023-10-26 16:47:20.892299: Pseudo dice [0.8819, 0.915, 0.9698, 0.738, 0.9302] 
2023-10-26 16:47:20.892520: Epoch time: 3.94 s 
2023-10-26 16:47:21.986395:  
2023-10-26 16:47:21.986693: Epoch 993 
2023-10-26 16:47:21.986936: Current learning rate: 0.00011 
2023-10-26 16:47:25.690364: train_loss -0.9104 
2023-10-26 16:47:25.690739: val_loss -0.8759 
2023-10-26 16:47:25.691017: Pseudo dice [0.8823, 0.9138, 0.9697, 0.7774, 0.9263] 
2023-10-26 16:47:25.691242: Epoch time: 3.7 s 
2023-10-26 16:47:26.786506:  
2023-10-26 16:47:26.786808: Epoch 994 
2023-10-26 16:47:26.787055: Current learning rate: 0.0001 
2023-10-26 16:47:30.543889: train_loss -0.906 
2023-10-26 16:47:30.544285: val_loss -0.8742 
2023-10-26 16:47:30.544716: Pseudo dice [0.8806, 0.9146, 0.9694, 0.7367, 0.9409] 
2023-10-26 16:47:30.544999: Epoch time: 3.76 s 
2023-10-26 16:47:31.663123:  
2023-10-26 16:47:31.663472: Epoch 995 
2023-10-26 16:47:31.671562: Current learning rate: 8e-05 
2023-10-26 16:47:35.480787: train_loss -0.908 
2023-10-26 16:47:35.481230: val_loss -0.8702 
2023-10-26 16:47:35.481493: Pseudo dice [0.8837, 0.9199, 0.9705, 0.7508, 0.9334] 
2023-10-26 16:47:35.481723: Epoch time: 3.82 s 
2023-10-26 16:47:36.582244:  
2023-10-26 16:47:36.582530: Epoch 996 
2023-10-26 16:47:36.582859: Current learning rate: 7e-05 
2023-10-26 16:47:40.409258: train_loss -0.9076 
2023-10-26 16:47:40.409671: val_loss -0.8724 
2023-10-26 16:47:40.409934: Pseudo dice [0.8829, 0.9193, 0.9702, 0.742, 0.934] 
2023-10-26 16:47:40.410167: Epoch time: 3.83 s 
2023-10-26 16:47:41.687184:  
2023-10-26 16:47:41.687508: Epoch 997 
2023-10-26 16:47:41.687767: Current learning rate: 5e-05 
2023-10-26 16:47:45.436371: train_loss -0.9093 
2023-10-26 16:47:45.436750: val_loss -0.8799 
2023-10-26 16:47:45.437015: Pseudo dice [0.8809, 0.9175, 0.9697, 0.7415, 0.9379] 
2023-10-26 16:47:45.437248: Epoch time: 3.75 s 
2023-10-26 16:47:46.527972:  
2023-10-26 16:47:46.528269: Epoch 998 
2023-10-26 16:47:46.528511: Current learning rate: 4e-05 
2023-10-26 16:47:50.357195: train_loss -0.9084 
2023-10-26 16:47:50.357599: val_loss -0.8736 
2023-10-26 16:47:50.357860: Pseudo dice [0.8781, 0.9165, 0.9696, 0.7485, 0.9365] 
2023-10-26 16:47:50.358104: Epoch time: 3.83 s 
2023-10-26 16:47:51.461912:  
2023-10-26 16:47:51.462240: Epoch 999 
2023-10-26 16:47:51.462537: Current learning rate: 2e-05 
2023-10-26 16:47:55.312158: train_loss -0.9057 
2023-10-26 16:47:55.312519: val_loss -0.876 
2023-10-26 16:47:55.312777: Pseudo dice [0.8795, 0.9123, 0.9698, 0.7507, 0.937] 
2023-10-26 16:47:55.313016: Epoch time: 3.85 s 
2023-10-26 16:47:56.743342: Training done. 
2023-10-26 16:47:56.846300: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-26 16:47:56.849865: The split file contains 5 splits. 
2023-10-26 16:47:56.850127: Desired fold for training: 0 
2023-10-26 16:47:56.850361: This split has 34 training and 10 validation cases. 
2023-10-26 16:47:56.850696: predicting t2_haste_tra_2_2mm_006 
2023-10-26 16:47:58.941171: predicting t2_haste_tra_2_2mm_010 
2023-10-26 16:47:58.965643: predicting t2_haste_tra_2_2mm_014 
2023-10-26 16:47:58.988864: predicting t2_haste_tra_2_2mm_017 
2023-10-26 16:47:59.011991: predicting t2_haste_tra_2_2mm_106 
2023-10-26 16:47:59.034979: predicting t2_haste_tra_2_2mm_110 
2023-10-26 16:47:59.057751: predicting t2_haste_tra_2_2mm_114 
2023-10-26 16:47:59.080891: predicting t2_haste_tra_2_2mm_117 
2023-10-26 16:47:59.104061: predicting t2_haste_tra_2_2mm_206 
2023-10-26 16:47:59.126969: predicting t2_haste_tra_2_2mm_210 
2023-10-26 16:49:33.911765: Validation complete 
2023-10-26 16:49:33.912043: Mean Validation Dice:  0.8996637807391543 
