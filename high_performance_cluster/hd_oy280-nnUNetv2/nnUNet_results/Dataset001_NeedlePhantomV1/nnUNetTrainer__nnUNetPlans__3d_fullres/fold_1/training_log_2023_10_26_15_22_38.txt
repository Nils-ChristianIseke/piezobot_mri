
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [24, 56, 40], 'median_image_size_in_voxels': [22.0, 56.0, 36.0], 'spacing': [2.419999837875366, 1.46875, 1.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [2, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_NeedlePhantomV1', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.419999837875366, 1.46875, 1.46875], 'original_median_shape_after_transp': [22, 56, 36], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1610.0, 'mean': 465.1097412109375, 'median': 478.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 1246.0, 'std': 241.4937744140625}}} 
 
2023-10-26 15:22:39.648476: unpacking dataset... 
2023-10-26 15:22:51.471925: unpacking done... 
2023-10-26 15:22:51.472779: do_dummy_2d_data_aug: False 
2023-10-26 15:22:51.473631: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-26 15:22:51.474124: The split file contains 5 splits. 
2023-10-26 15:22:51.474322: Desired fold for training: 1 
2023-10-26 15:22:51.474507: This split has 35 training and 9 validation cases. 
2023-10-26 15:23:01.841238: Unable to plot network architecture: 
2023-10-26 15:23:01.841733: 'torch._C.Node' object is not subscriptable 
2023-10-26 15:23:01.867041:  
2023-10-26 15:23:01.867340: Epoch 0 
2023-10-26 15:23:01.867665: Current learning rate: 0.01 
2023-10-26 15:23:11.146130: train_loss -0.1127 
2023-10-26 15:23:11.146553: val_loss -0.3632 
2023-10-26 15:23:11.146832: Pseudo dice [0.2521, 0.7099, 0.9381, 0.0, 0.679] 
2023-10-26 15:23:11.147078: Epoch time: 9.28 s 
2023-10-26 15:23:11.147384: Yayy! New best EMA pseudo Dice: 0.5158 
2023-10-26 15:23:12.190277:  
2023-10-26 15:23:12.190598: Epoch 1 
2023-10-26 15:23:12.190900: Current learning rate: 0.00999 
2023-10-26 15:23:16.241477: train_loss -0.4582 
2023-10-26 15:23:16.241868: val_loss -0.5533 
2023-10-26 15:23:16.242140: Pseudo dice [0.7888, 0.8462, 0.9565, 0.0, 0.8467] 
2023-10-26 15:23:16.242377: Epoch time: 4.05 s 
2023-10-26 15:23:16.242594: Yayy! New best EMA pseudo Dice: 0.533 
2023-10-26 15:23:17.317603:  
2023-10-26 15:23:17.317903: Epoch 2 
2023-10-26 15:23:17.318158: Current learning rate: 0.00998 
2023-10-26 15:23:21.692208: train_loss -0.5581 
2023-10-26 15:23:21.692620: val_loss -0.5993 
2023-10-26 15:23:21.692895: Pseudo dice [0.8218, 0.8415, 0.9546, 0.0, 0.8138] 
2023-10-26 15:23:21.693133: Epoch time: 4.38 s 
2023-10-26 15:23:21.693378: Yayy! New best EMA pseudo Dice: 0.5483 
2023-10-26 15:23:22.999691:  
2023-10-26 15:23:23.000015: Epoch 3 
2023-10-26 15:23:23.000282: Current learning rate: 0.00997 
2023-10-26 15:23:27.198434: train_loss -0.6196 
2023-10-26 15:23:27.198810: val_loss -0.6303 
2023-10-26 15:23:27.199087: Pseudo dice [0.838, 0.8796, 0.9564, 0.0, 0.8473] 
2023-10-26 15:23:27.199323: Epoch time: 4.2 s 
2023-10-26 15:23:27.199558: Yayy! New best EMA pseudo Dice: 0.5639 
2023-10-26 15:23:28.305978:  
2023-10-26 15:23:28.306293: Epoch 4 
2023-10-26 15:23:28.306544: Current learning rate: 0.00996 
2023-10-26 15:23:32.599694: train_loss -0.6466 
2023-10-26 15:23:32.600163: val_loss -0.6554 
2023-10-26 15:23:32.600562: Pseudo dice [0.8528, 0.8861, 0.9585, 0.0, 0.8786] 
2023-10-26 15:23:32.601061: Epoch time: 4.29 s 
2023-10-26 15:23:32.601627: Yayy! New best EMA pseudo Dice: 0.5791 
2023-10-26 15:23:33.776201:  
2023-10-26 15:23:33.776511: Epoch 5 
2023-10-26 15:23:33.776767: Current learning rate: 0.00995 
2023-10-26 15:23:38.010461: train_loss -0.6384 
2023-10-26 15:23:38.010862: val_loss -0.637 
2023-10-26 15:23:38.011155: Pseudo dice [0.8516, 0.8624, 0.9619, 0.0, 0.8484] 
2023-10-26 15:23:38.011422: Epoch time: 4.23 s 
2023-10-26 15:23:38.011646: Yayy! New best EMA pseudo Dice: 0.5916 
2023-10-26 15:23:39.116311:  
2023-10-26 15:23:39.116616: Epoch 6 
2023-10-26 15:23:39.116880: Current learning rate: 0.00995 
2023-10-26 15:23:43.333028: train_loss -0.6443 
2023-10-26 15:23:43.333421: val_loss -0.6507 
2023-10-26 15:23:43.333689: Pseudo dice [0.8573, 0.8713, 0.9637, 0.0, 0.8657] 
2023-10-26 15:23:43.333927: Epoch time: 4.22 s 
2023-10-26 15:23:43.334138: Yayy! New best EMA pseudo Dice: 0.6036 
2023-10-26 15:23:44.470720:  
2023-10-26 15:23:44.471030: Epoch 7 
2023-10-26 15:23:44.471326: Current learning rate: 0.00994 
2023-10-26 15:23:48.628814: train_loss -0.6565 
2023-10-26 15:23:48.629229: val_loss -0.6532 
2023-10-26 15:23:48.629513: Pseudo dice [0.8494, 0.8915, 0.9645, 0.1647, 0.8159] 
2023-10-26 15:23:48.629856: Epoch time: 4.16 s 
2023-10-26 15:23:48.630103: Yayy! New best EMA pseudo Dice: 0.617 
2023-10-26 15:23:49.748846:  
2023-10-26 15:23:49.749168: Epoch 8 
2023-10-26 15:23:49.749418: Current learning rate: 0.00993 
2023-10-26 15:23:53.955121: train_loss -0.6718 
2023-10-26 15:23:53.955633: val_loss -0.6632 
2023-10-26 15:23:53.955962: Pseudo dice [0.8527, 0.8808, 0.9646, 0.5717, 0.8466] 
2023-10-26 15:23:53.956230: Epoch time: 4.21 s 
2023-10-26 15:23:53.956470: Yayy! New best EMA pseudo Dice: 0.6376 
2023-10-26 15:23:55.259822:  
2023-10-26 15:23:55.260150: Epoch 9 
2023-10-26 15:23:55.260400: Current learning rate: 0.00992 
2023-10-26 15:23:59.315986: train_loss -0.6841 
2023-10-26 15:23:59.316412: val_loss -0.6776 
2023-10-26 15:23:59.316688: Pseudo dice [0.86, 0.8922, 0.9627, 0.7543, 0.8381] 
2023-10-26 15:23:59.316931: Epoch time: 4.06 s 
2023-10-26 15:23:59.317158: Yayy! New best EMA pseudo Dice: 0.66 
2023-10-26 15:24:00.515248:  
2023-10-26 15:24:00.515558: Epoch 10 
2023-10-26 15:24:00.515813: Current learning rate: 0.00991 
2023-10-26 15:24:04.493597: train_loss -0.6935 
2023-10-26 15:24:04.494089: val_loss -0.6705 
2023-10-26 15:24:04.494610: Pseudo dice [0.8631, 0.8829, 0.9655, 0.7334, 0.8316] 
2023-10-26 15:24:04.495005: Epoch time: 3.98 s 
2023-10-26 15:24:04.495444: Yayy! New best EMA pseudo Dice: 0.6795 
2023-10-26 15:24:05.607151:  
2023-10-26 15:24:05.607458: Epoch 11 
2023-10-26 15:24:05.607694: Current learning rate: 0.0099 
2023-10-26 15:24:09.639195: train_loss -0.6848 
2023-10-26 15:24:09.639611: val_loss -0.6923 
2023-10-26 15:24:09.639880: Pseudo dice [0.8572, 0.8875, 0.9652, 0.612, 0.9137] 
2023-10-26 15:24:09.640121: Epoch time: 4.03 s 
2023-10-26 15:24:09.640339: Yayy! New best EMA pseudo Dice: 0.6963 
2023-10-26 15:24:10.758343:  
2023-10-26 15:24:10.758683: Epoch 12 
2023-10-26 15:24:10.759032: Current learning rate: 0.00989 
2023-10-26 15:24:14.859959: train_loss -0.6888 
2023-10-26 15:24:14.860395: val_loss -0.6909 
2023-10-26 15:24:14.860764: Pseudo dice [0.8628, 0.8874, 0.9649, 0.6957, 0.8749] 
2023-10-26 15:24:14.861044: Epoch time: 4.1 s 
2023-10-26 15:24:14.861279: Yayy! New best EMA pseudo Dice: 0.7124 
2023-10-26 15:24:15.978659:  
2023-10-26 15:24:15.978987: Epoch 13 
2023-10-26 15:24:15.979254: Current learning rate: 0.00988 
2023-10-26 15:24:19.992842: train_loss -0.6943 
2023-10-26 15:24:19.993202: val_loss -0.6853 
2023-10-26 15:24:19.993468: Pseudo dice [0.8625, 0.8876, 0.966, 0.7877, 0.8663] 
2023-10-26 15:24:19.993700: Epoch time: 4.01 s 
2023-10-26 15:24:19.993923: Yayy! New best EMA pseudo Dice: 0.7285 
2023-10-26 15:24:21.115170:  
2023-10-26 15:24:21.115517: Epoch 14 
2023-10-26 15:24:21.115766: Current learning rate: 0.00987 
2023-10-26 15:24:25.129760: train_loss -0.6935 
2023-10-26 15:24:25.130290: val_loss -0.6707 
2023-10-26 15:24:25.130756: Pseudo dice [0.8639, 0.9022, 0.9626, 0.7304, 0.8375] 
2023-10-26 15:24:25.131130: Epoch time: 4.02 s 
2023-10-26 15:24:25.131418: Yayy! New best EMA pseudo Dice: 0.7416 
2023-10-26 15:24:26.264534:  
2023-10-26 15:24:26.264857: Epoch 15 
2023-10-26 15:24:26.265129: Current learning rate: 0.00986 
2023-10-26 15:24:30.263034: train_loss -0.6968 
2023-10-26 15:24:30.263396: val_loss -0.6947 
2023-10-26 15:24:30.263672: Pseudo dice [0.8675, 0.8942, 0.967, 0.6855, 0.8913] 
2023-10-26 15:24:30.263918: Epoch time: 4.0 s 
2023-10-26 15:24:30.264139: Yayy! New best EMA pseudo Dice: 0.7536 
2023-10-26 15:24:31.575863:  
2023-10-26 15:24:31.576176: Epoch 16 
2023-10-26 15:24:31.576426: Current learning rate: 0.00986 
2023-10-26 15:24:35.652786: train_loss -0.703 
2023-10-26 15:24:35.653214: val_loss -0.6849 
2023-10-26 15:24:35.653486: Pseudo dice [0.8567, 0.8869, 0.9646, 0.7578, 0.8835] 
2023-10-26 15:24:35.653725: Epoch time: 4.08 s 
2023-10-26 15:24:35.653968: Yayy! New best EMA pseudo Dice: 0.7652 
2023-10-26 15:24:36.857358:  
2023-10-26 15:24:36.857690: Epoch 17 
2023-10-26 15:24:36.857947: Current learning rate: 0.00985 
2023-10-26 15:24:40.976327: train_loss -0.693 
2023-10-26 15:24:40.976779: val_loss -0.6961 
2023-10-26 15:24:40.977177: Pseudo dice [0.8614, 0.9038, 0.9623, 0.7154, 0.8856] 
2023-10-26 15:24:40.977435: Epoch time: 4.12 s 
2023-10-26 15:24:40.977700: Yayy! New best EMA pseudo Dice: 0.7752 
2023-10-26 15:24:42.125433:  
2023-10-26 15:24:42.125754: Epoch 18 
2023-10-26 15:24:42.126028: Current learning rate: 0.00984 
2023-10-26 15:24:46.222886: train_loss -0.6971 
2023-10-26 15:24:46.223292: val_loss -0.6913 
2023-10-26 15:24:46.223562: Pseudo dice [0.8643, 0.8939, 0.9646, 0.7205, 0.8858] 
2023-10-26 15:24:46.223797: Epoch time: 4.1 s 
2023-10-26 15:24:46.224044: Yayy! New best EMA pseudo Dice: 0.7843 
2023-10-26 15:24:47.387163:  
2023-10-26 15:24:47.387506: Epoch 19 
2023-10-26 15:24:47.387770: Current learning rate: 0.00983 
2023-10-26 15:24:51.532396: train_loss -0.6983 
2023-10-26 15:24:51.532785: val_loss -0.6872 
2023-10-26 15:24:51.533070: Pseudo dice [0.8635, 0.8927, 0.9652, 0.7055, 0.868] 
2023-10-26 15:24:51.533310: Epoch time: 4.15 s 
2023-10-26 15:24:51.533521: Yayy! New best EMA pseudo Dice: 0.7918 
2023-10-26 15:24:52.729015:  
2023-10-26 15:24:52.729328: Epoch 20 
2023-10-26 15:24:52.729619: Current learning rate: 0.00982 
2023-10-26 15:24:56.859647: train_loss -0.6965 
2023-10-26 15:24:56.860093: val_loss -0.6814 
2023-10-26 15:24:56.860367: Pseudo dice [0.8549, 0.8901, 0.9644, 0.7857, 0.854] 
2023-10-26 15:24:56.860619: Epoch time: 4.13 s 
2023-10-26 15:24:56.860838: Yayy! New best EMA pseudo Dice: 0.7996 
2023-10-26 15:24:58.175018:  
2023-10-26 15:24:58.175326: Epoch 21 
2023-10-26 15:24:58.175573: Current learning rate: 0.00981 
2023-10-26 15:25:02.292099: train_loss -0.695 
2023-10-26 15:25:02.292499: val_loss -0.6913 
2023-10-26 15:25:02.292765: Pseudo dice [0.8583, 0.8914, 0.9636, 0.741, 0.8885] 
2023-10-26 15:25:02.293024: Epoch time: 4.12 s 
2023-10-26 15:25:02.293246: Yayy! New best EMA pseudo Dice: 0.8065 
2023-10-26 15:25:03.464608:  
2023-10-26 15:25:03.464916: Epoch 22 
2023-10-26 15:25:03.465233: Current learning rate: 0.0098 
2023-10-26 15:25:07.495656: train_loss -0.6983 
2023-10-26 15:25:07.496077: val_loss -0.6843 
2023-10-26 15:25:07.496366: Pseudo dice [0.8575, 0.8894, 0.9657, 0.6317, 0.8702] 
2023-10-26 15:25:07.496624: Epoch time: 4.03 s 
2023-10-26 15:25:07.496867: Yayy! New best EMA pseudo Dice: 0.8101 
2023-10-26 15:25:08.615921:  
2023-10-26 15:25:08.616222: Epoch 23 
2023-10-26 15:25:08.616631: Current learning rate: 0.00979 
2023-10-26 15:25:12.654531: train_loss -0.7046 
2023-10-26 15:25:12.654926: val_loss -0.6904 
2023-10-26 15:25:12.655197: Pseudo dice [0.8665, 0.9013, 0.9656, 0.6233, 0.8809] 
2023-10-26 15:25:12.655447: Epoch time: 4.04 s 
2023-10-26 15:25:12.655696: Yayy! New best EMA pseudo Dice: 0.8139 
2023-10-26 15:25:13.761035:  
2023-10-26 15:25:13.761330: Epoch 24 
2023-10-26 15:25:13.761575: Current learning rate: 0.00978 
2023-10-26 15:25:17.824610: train_loss -0.7076 
2023-10-26 15:25:17.825049: val_loss -0.7363 
2023-10-26 15:25:17.825342: Pseudo dice [0.8404, 0.8621, 0.9517, 0.243, 0.8674] 
2023-10-26 15:25:17.825608: Epoch time: 4.06 s 
2023-10-26 15:25:18.873903:  
2023-10-26 15:25:18.874202: Epoch 25 
2023-10-26 15:25:18.874458: Current learning rate: 0.00977 
2023-10-26 15:25:22.873580: train_loss -0.7887 
2023-10-26 15:25:22.873980: val_loss -0.7342 
2023-10-26 15:25:22.874243: Pseudo dice [0.8413, 0.8738, 0.9611, 0.0, 0.8554] 
2023-10-26 15:25:22.874477: Epoch time: 4.0 s 
2023-10-26 15:25:23.970678:  
2023-10-26 15:25:23.970984: Epoch 26 
2023-10-26 15:25:23.971229: Current learning rate: 0.00977 
2023-10-26 15:25:27.941952: train_loss -0.7971 
2023-10-26 15:25:27.942350: val_loss -0.7513 
2023-10-26 15:25:27.942616: Pseudo dice [0.8638, 0.8902, 0.9642, 0.0, 0.7972] 
2023-10-26 15:25:27.942846: Epoch time: 3.97 s 
2023-10-26 15:25:29.168482:  
2023-10-26 15:25:29.168781: Epoch 27 
2023-10-26 15:25:29.169048: Current learning rate: 0.00976 
2023-10-26 15:25:33.267646: train_loss -0.7977 
2023-10-26 15:25:33.268069: val_loss -0.7702 
2023-10-26 15:25:33.268338: Pseudo dice [0.866, 0.8858, 0.9624, 0.0, 0.8555] 
2023-10-26 15:25:33.268602: Epoch time: 4.1 s 
2023-10-26 15:25:34.312430:  
2023-10-26 15:25:34.312727: Epoch 28 
2023-10-26 15:25:34.312976: Current learning rate: 0.00975 
2023-10-26 15:25:38.477202: train_loss -0.8108 
2023-10-26 15:25:38.477597: val_loss -0.7882 
2023-10-26 15:25:38.477863: Pseudo dice [0.8596, 0.8913, 0.9639, 0.0, 0.8621] 
2023-10-26 15:25:38.478101: Epoch time: 4.17 s 
2023-10-26 15:25:39.529162:  
2023-10-26 15:25:39.529547: Epoch 29 
2023-10-26 15:25:39.529832: Current learning rate: 0.00974 
2023-10-26 15:25:43.761031: train_loss -0.8085 
2023-10-26 15:25:43.761410: val_loss -0.7696 
2023-10-26 15:25:43.761675: Pseudo dice [0.8499, 0.887, 0.9632, 0.299, 0.8317] 
2023-10-26 15:25:43.761925: Epoch time: 4.23 s 
2023-10-26 15:25:44.845861:  
2023-10-26 15:25:44.846164: Epoch 30 
2023-10-26 15:25:44.846417: Current learning rate: 0.00973 
2023-10-26 15:25:48.855154: train_loss -0.8046 
2023-10-26 15:25:48.855552: val_loss -0.7672 
2023-10-26 15:25:48.855845: Pseudo dice [0.8604, 0.8951, 0.965, 0.8035, 0.8497] 
2023-10-26 15:25:48.856098: Epoch time: 4.01 s 
2023-10-26 15:25:49.922360:  
2023-10-26 15:25:49.922673: Epoch 31 
2023-10-26 15:25:49.922937: Current learning rate: 0.00972 
2023-10-26 15:25:54.026800: train_loss -0.7997 
2023-10-26 15:25:54.027330: val_loss -0.7879 
2023-10-26 15:25:54.027634: Pseudo dice [0.8581, 0.8842, 0.9626, 0.0, 0.8783] 
2023-10-26 15:25:54.027895: Epoch time: 4.11 s 
2023-10-26 15:25:55.113646:  
2023-10-26 15:25:55.113934: Epoch 32 
2023-10-26 15:25:55.114197: Current learning rate: 0.00971 
2023-10-26 15:25:59.212304: train_loss -0.8051 
2023-10-26 15:25:59.212796: val_loss -0.7691 
2023-10-26 15:25:59.213095: Pseudo dice [0.8626, 0.886, 0.9625, 0.0, 0.8688] 
2023-10-26 15:25:59.213362: Epoch time: 4.1 s 
2023-10-26 15:26:00.510691:  
2023-10-26 15:26:00.511069: Epoch 33 
2023-10-26 15:26:00.511327: Current learning rate: 0.0097 
2023-10-26 15:26:04.630330: train_loss -0.8105 
2023-10-26 15:26:04.630726: val_loss -0.7856 
2023-10-26 15:26:04.630995: Pseudo dice [0.8658, 0.881, 0.9638, 0.0, 0.9102] 
2023-10-26 15:26:04.631239: Epoch time: 4.12 s 
2023-10-26 15:26:05.718920:  
2023-10-26 15:26:05.719233: Epoch 34 
2023-10-26 15:26:05.719485: Current learning rate: 0.00969 
2023-10-26 15:26:09.828511: train_loss -0.8103 
2023-10-26 15:26:09.829077: val_loss -0.7627 
2023-10-26 15:26:09.829518: Pseudo dice [0.8537, 0.8836, 0.9615, 0.0, 0.826] 
2023-10-26 15:26:09.829931: Epoch time: 4.11 s 
2023-10-26 15:26:10.911979:  
2023-10-26 15:26:10.912290: Epoch 35 
2023-10-26 15:26:10.912596: Current learning rate: 0.00968 
2023-10-26 15:26:15.042711: train_loss -0.8003 
2023-10-26 15:26:15.043265: val_loss -0.7785 
2023-10-26 15:26:15.043717: Pseudo dice [0.8707, 0.8998, 0.9651, 0.0, 0.8261] 
2023-10-26 15:26:15.044235: Epoch time: 4.13 s 
2023-10-26 15:26:16.156175:  
2023-10-26 15:26:16.156506: Epoch 36 
2023-10-26 15:26:16.156750: Current learning rate: 0.00968 
2023-10-26 15:26:20.176221: train_loss -0.8071 
2023-10-26 15:26:20.176630: val_loss -0.7986 
2023-10-26 15:26:20.176917: Pseudo dice [0.8631, 0.8922, 0.9662, 0.0, 0.8866] 
2023-10-26 15:26:20.177156: Epoch time: 4.02 s 
2023-10-26 15:26:21.301485:  
2023-10-26 15:26:21.301784: Epoch 37 
2023-10-26 15:26:21.302038: Current learning rate: 0.00967 
2023-10-26 15:26:25.409137: train_loss -0.8196 
2023-10-26 15:26:25.409566: val_loss -0.8013 
2023-10-26 15:26:25.409840: Pseudo dice [0.8706, 0.8964, 0.9659, 0.0, 0.8964] 
2023-10-26 15:26:25.410077: Epoch time: 4.11 s 
2023-10-26 15:26:26.509713:  
2023-10-26 15:26:26.510027: Epoch 38 
2023-10-26 15:26:26.510275: Current learning rate: 0.00966 
2023-10-26 15:26:30.609085: train_loss -0.8212 
2023-10-26 15:26:30.609524: val_loss -0.7977 
2023-10-26 15:26:30.609986: Pseudo dice [0.8617, 0.8935, 0.9625, 0.3723, 0.851] 
2023-10-26 15:26:30.610524: Epoch time: 4.1 s 
2023-10-26 15:26:31.868163:  
2023-10-26 15:26:31.868474: Epoch 39 
2023-10-26 15:26:31.868722: Current learning rate: 0.00965 
2023-10-26 15:26:35.954731: train_loss -0.8163 
2023-10-26 15:26:35.955110: val_loss -0.7892 
2023-10-26 15:26:35.955374: Pseudo dice [0.856, 0.8761, 0.9611, 0.7692, 0.8465] 
2023-10-26 15:26:35.955609: Epoch time: 4.09 s 
2023-10-26 15:26:37.052533:  
2023-10-26 15:26:37.052968: Epoch 40 
2023-10-26 15:26:37.053220: Current learning rate: 0.00964 
2023-10-26 15:26:41.043169: train_loss -0.8158 
2023-10-26 15:26:41.043587: val_loss -0.7964 
2023-10-26 15:26:41.043860: Pseudo dice [0.8633, 0.8924, 0.9651, 0.7815, 0.8417] 
2023-10-26 15:26:41.044117: Epoch time: 3.99 s 
2023-10-26 15:26:42.191581:  
2023-10-26 15:26:42.191906: Epoch 41 
2023-10-26 15:26:42.192505: Current learning rate: 0.00963 
2023-10-26 15:26:46.189583: train_loss -0.8204 
2023-10-26 15:26:46.189970: val_loss -0.779 
2023-10-26 15:26:46.190240: Pseudo dice [0.8544, 0.8973, 0.9645, 0.7316, 0.8457] 
2023-10-26 15:26:46.190474: Epoch time: 4.0 s 
2023-10-26 15:26:47.228727:  
2023-10-26 15:26:47.229032: Epoch 42 
2023-10-26 15:26:47.229331: Current learning rate: 0.00962 
2023-10-26 15:26:51.242500: train_loss -0.8004 
2023-10-26 15:26:51.242891: val_loss -0.7528 
2023-10-26 15:26:51.243160: Pseudo dice [0.8594, 0.8842, 0.9622, 0.0, 0.8419] 
2023-10-26 15:26:51.243396: Epoch time: 4.01 s 
2023-10-26 15:26:52.316346:  
2023-10-26 15:26:52.316654: Epoch 43 
2023-10-26 15:26:52.316910: Current learning rate: 0.00961 
2023-10-26 15:26:56.411111: train_loss -0.8106 
2023-10-26 15:26:56.411581: val_loss -0.78 
2023-10-26 15:26:56.411845: Pseudo dice [0.8619, 0.8887, 0.9645, 0.0, 0.867] 
2023-10-26 15:26:56.412095: Epoch time: 4.1 s 
2023-10-26 15:26:57.465649:  
2023-10-26 15:26:57.465979: Epoch 44 
2023-10-26 15:26:57.466235: Current learning rate: 0.0096 
2023-10-26 15:27:01.498214: train_loss -0.8246 
2023-10-26 15:27:01.498636: val_loss -0.7936 
2023-10-26 15:27:01.498909: Pseudo dice [0.8653, 0.8876, 0.9646, 0.6697, 0.8571] 
2023-10-26 15:27:01.499143: Epoch time: 4.03 s 
2023-10-26 15:27:02.551609:  
2023-10-26 15:27:02.551910: Epoch 45 
2023-10-26 15:27:02.552161: Current learning rate: 0.00959 
2023-10-26 15:27:06.478788: train_loss -0.8198 
2023-10-26 15:27:06.479254: val_loss -0.7905 
2023-10-26 15:27:06.479555: Pseudo dice [0.8676, 0.8852, 0.9598, 0.0, 0.8806] 
2023-10-26 15:27:06.479819: Epoch time: 3.93 s 
2023-10-26 15:27:07.708797:  
2023-10-26 15:27:07.709130: Epoch 46 
2023-10-26 15:27:07.709426: Current learning rate: 0.00959 
2023-10-26 15:27:11.761618: train_loss -0.7974 
2023-10-26 15:27:11.762342: val_loss -0.7656 
2023-10-26 15:27:11.762661: Pseudo dice [0.8629, 0.8887, 0.9636, 0.0, 0.8597] 
2023-10-26 15:27:11.763024: Epoch time: 4.05 s 
2023-10-26 15:27:12.809175:  
2023-10-26 15:27:12.809492: Epoch 47 
2023-10-26 15:27:12.809736: Current learning rate: 0.00958 
2023-10-26 15:27:16.906526: train_loss -0.8111 
2023-10-26 15:27:16.906966: val_loss -0.7908 
2023-10-26 15:27:16.907282: Pseudo dice [0.8656, 0.8917, 0.9669, 0.0, 0.8709] 
2023-10-26 15:27:16.907525: Epoch time: 4.1 s 
2023-10-26 15:27:17.968734:  
2023-10-26 15:27:17.969040: Epoch 48 
2023-10-26 15:27:17.969297: Current learning rate: 0.00957 
2023-10-26 15:27:22.034437: train_loss -0.8223 
2023-10-26 15:27:22.034858: val_loss -0.803 
2023-10-26 15:27:22.035160: Pseudo dice [0.8624, 0.8897, 0.9607, 0.6255, 0.8938] 
2023-10-26 15:27:22.035411: Epoch time: 4.07 s 
2023-10-26 15:27:23.086168:  
2023-10-26 15:27:23.086476: Epoch 49 
2023-10-26 15:27:23.086719: Current learning rate: 0.00956 
2023-10-26 15:27:27.229239: train_loss -0.8252 
2023-10-26 15:27:27.229633: val_loss -0.8001 
2023-10-26 15:27:27.229908: Pseudo dice [0.8582, 0.8883, 0.9661, 0.6964, 0.8752] 
2023-10-26 15:27:27.230355: Epoch time: 4.14 s 
2023-10-26 15:27:28.345637:  
2023-10-26 15:27:28.345933: Epoch 50 
2023-10-26 15:27:28.346177: Current learning rate: 0.00955 
2023-10-26 15:27:32.510034: train_loss -0.8299 
2023-10-26 15:27:32.510410: val_loss -0.7597 
2023-10-26 15:27:32.510693: Pseudo dice [0.8616, 0.8758, 0.9653, 0.118, 0.8294] 
2023-10-26 15:27:32.510933: Epoch time: 4.16 s 
2023-10-26 15:27:33.714372:  
2023-10-26 15:27:33.714676: Epoch 51 
2023-10-26 15:27:33.714987: Current learning rate: 0.00954 
2023-10-26 15:27:37.973890: train_loss -0.8277 
2023-10-26 15:27:37.974347: val_loss -0.8029 
2023-10-26 15:27:37.974616: Pseudo dice [0.8626, 0.886, 0.9658, 0.3943, 0.8861] 
2023-10-26 15:27:37.974954: Epoch time: 4.26 s 
2023-10-26 15:27:39.054831:  
2023-10-26 15:27:39.055194: Epoch 52 
2023-10-26 15:27:39.055439: Current learning rate: 0.00953 
2023-10-26 15:27:43.251491: train_loss -0.8347 
2023-10-26 15:27:43.251978: val_loss -0.8178 
2023-10-26 15:27:43.252370: Pseudo dice [0.8656, 0.8942, 0.9642, 0.708, 0.9121] 
2023-10-26 15:27:43.252717: Epoch time: 4.2 s 
2023-10-26 15:27:44.321448:  
2023-10-26 15:27:44.321765: Epoch 53 
2023-10-26 15:27:44.322020: Current learning rate: 0.00952 
2023-10-26 15:27:48.246224: train_loss -0.8387 
2023-10-26 15:27:48.246613: val_loss -0.8079 
2023-10-26 15:27:48.246887: Pseudo dice [0.869, 0.8928, 0.9659, 0.614, 0.8648] 
2023-10-26 15:27:48.247124: Epoch time: 3.93 s 
2023-10-26 15:27:49.322808:  
2023-10-26 15:27:49.323103: Epoch 54 
2023-10-26 15:27:49.323359: Current learning rate: 0.00951 
2023-10-26 15:27:53.383803: train_loss -0.8404 
2023-10-26 15:27:53.384716: val_loss -0.7893 
2023-10-26 15:27:53.385174: Pseudo dice [0.8616, 0.8955, 0.9654, 0.5877, 0.8418] 
2023-10-26 15:27:53.385512: Epoch time: 4.06 s 
2023-10-26 15:27:54.444104:  
2023-10-26 15:27:54.444384: Epoch 55 
2023-10-26 15:27:54.444630: Current learning rate: 0.0095 
2023-10-26 15:27:58.472292: train_loss -0.8331 
2023-10-26 15:27:58.472677: val_loss -0.7599 
2023-10-26 15:27:58.472946: Pseudo dice [0.8569, 0.8877, 0.965, 0.0, 0.8529] 
2023-10-26 15:27:58.473189: Epoch time: 4.03 s 
2023-10-26 15:27:59.552993:  
2023-10-26 15:27:59.553295: Epoch 56 
2023-10-26 15:27:59.553536: Current learning rate: 0.00949 
2023-10-26 15:28:03.540605: train_loss -0.8328 
2023-10-26 15:28:03.541006: val_loss -0.7959 
2023-10-26 15:28:03.541280: Pseudo dice [0.8637, 0.8991, 0.9659, 0.3294, 0.886] 
2023-10-26 15:28:03.541583: Epoch time: 3.99 s 
2023-10-26 15:28:04.599027:  
2023-10-26 15:28:04.599406: Epoch 57 
2023-10-26 15:28:04.599680: Current learning rate: 0.00949 
2023-10-26 15:28:08.623104: train_loss -0.8361 
2023-10-26 15:28:08.623493: val_loss -0.7979 
2023-10-26 15:28:08.623827: Pseudo dice [0.8642, 0.9061, 0.9652, 0.764, 0.8343] 
2023-10-26 15:28:08.624142: Epoch time: 4.02 s 
2023-10-26 15:28:09.978369:  
2023-10-26 15:28:09.978690: Epoch 58 
2023-10-26 15:28:09.978949: Current learning rate: 0.00948 
2023-10-26 15:28:13.889142: train_loss -0.8441 
2023-10-26 15:28:13.889532: val_loss -0.8131 
2023-10-26 15:28:13.889796: Pseudo dice [0.8674, 0.9015, 0.964, 0.8063, 0.8756] 
2023-10-26 15:28:13.890033: Epoch time: 3.91 s 
2023-10-26 15:28:14.967032:  
2023-10-26 15:28:14.967445: Epoch 59 
2023-10-26 15:28:14.967735: Current learning rate: 0.00947 
2023-10-26 15:28:18.992087: train_loss -0.843 
2023-10-26 15:28:18.992467: val_loss -0.7933 
2023-10-26 15:28:18.992731: Pseudo dice [0.8692, 0.8926, 0.9643, 0.4749, 0.847] 
2023-10-26 15:28:18.992966: Epoch time: 4.03 s 
2023-10-26 15:28:20.074003:  
2023-10-26 15:28:20.074311: Epoch 60 
2023-10-26 15:28:20.074558: Current learning rate: 0.00946 
2023-10-26 15:28:24.103450: train_loss -0.8342 
2023-10-26 15:28:24.103884: val_loss -0.7859 
2023-10-26 15:28:24.104158: Pseudo dice [0.856, 0.8845, 0.9656, 0.4715, 0.8536] 
2023-10-26 15:28:24.104409: Epoch time: 4.03 s 
2023-10-26 15:28:25.206171:  
2023-10-26 15:28:25.206497: Epoch 61 
2023-10-26 15:28:25.206766: Current learning rate: 0.00945 
2023-10-26 15:28:29.270224: train_loss -0.8374 
2023-10-26 15:28:29.270657: val_loss -0.7825 
2023-10-26 15:28:29.270950: Pseudo dice [0.8677, 0.9033, 0.9662, 0.5078, 0.8315] 
2023-10-26 15:28:29.271308: Epoch time: 4.06 s 
2023-10-26 15:28:30.339432:  
2023-10-26 15:28:30.339727: Epoch 62 
2023-10-26 15:28:30.339988: Current learning rate: 0.00944 
2023-10-26 15:28:34.310963: train_loss -0.8407 
2023-10-26 15:28:34.311472: val_loss -0.7878 
2023-10-26 15:28:34.311861: Pseudo dice [0.8593, 0.8838, 0.9658, 0.3994, 0.8724] 
2023-10-26 15:28:34.312276: Epoch time: 3.97 s 
2023-10-26 15:28:35.397621:  
2023-10-26 15:28:35.397979: Epoch 63 
2023-10-26 15:28:35.398355: Current learning rate: 0.00943 
2023-10-26 15:28:39.320024: train_loss -0.8346 
2023-10-26 15:28:39.320510: val_loss -0.8084 
2023-10-26 15:28:39.321024: Pseudo dice [0.8617, 0.895, 0.9661, 0.6823, 0.8686] 
2023-10-26 15:28:39.321334: Epoch time: 3.92 s 
2023-10-26 15:28:40.563289:  
2023-10-26 15:28:40.563613: Epoch 64 
2023-10-26 15:28:40.563933: Current learning rate: 0.00942 
2023-10-26 15:28:44.533652: train_loss -0.8338 
2023-10-26 15:28:44.534280: val_loss -0.8157 
2023-10-26 15:28:44.534769: Pseudo dice [0.8585, 0.9069, 0.9659, 0.8092, 0.8566] 
2023-10-26 15:28:44.535101: Epoch time: 3.97 s 
2023-10-26 15:28:44.535492: Yayy! New best EMA pseudo Dice: 0.8166 
2023-10-26 15:28:45.682324:  
2023-10-26 15:28:45.682648: Epoch 65 
2023-10-26 15:28:45.682906: Current learning rate: 0.00941 
2023-10-26 15:28:49.724036: train_loss -0.8438 
2023-10-26 15:28:49.724399: val_loss -0.8062 
2023-10-26 15:28:49.724661: Pseudo dice [0.8652, 0.8929, 0.9662, 0.7255, 0.8841] 
2023-10-26 15:28:49.724905: Epoch time: 4.04 s 
2023-10-26 15:28:49.725114: Yayy! New best EMA pseudo Dice: 0.8216 
2023-10-26 15:28:50.870998:  
2023-10-26 15:28:50.871299: Epoch 66 
2023-10-26 15:28:50.871552: Current learning rate: 0.0094 
2023-10-26 15:28:54.977341: train_loss -0.8465 
2023-10-26 15:28:54.977822: val_loss -0.8088 
2023-10-26 15:28:54.978212: Pseudo dice [0.8627, 0.9039, 0.9648, 0.6571, 0.8925] 
2023-10-26 15:28:54.978536: Epoch time: 4.11 s 
2023-10-26 15:28:54.978947: Yayy! New best EMA pseudo Dice: 0.8251 
2023-10-26 15:28:56.120414:  
2023-10-26 15:28:56.120706: Epoch 67 
2023-10-26 15:28:56.120948: Current learning rate: 0.00939 
2023-10-26 15:29:00.086751: train_loss -0.8511 
2023-10-26 15:29:00.087143: val_loss -0.7981 
2023-10-26 15:29:00.087410: Pseudo dice [0.8667, 0.901, 0.9633, 0.5557, 0.8783] 
2023-10-26 15:29:00.087648: Epoch time: 3.97 s 
2023-10-26 15:29:00.087865: Yayy! New best EMA pseudo Dice: 0.8259 
2023-10-26 15:29:01.241916:  
2023-10-26 15:29:01.242204: Epoch 68 
2023-10-26 15:29:01.242471: Current learning rate: 0.00939 
2023-10-26 15:29:05.355672: train_loss -0.8434 
2023-10-26 15:29:05.356120: val_loss -0.7978 
2023-10-26 15:29:05.356416: Pseudo dice [0.8709, 0.9021, 0.9659, 0.6447, 0.8196] 
2023-10-26 15:29:05.356662: Epoch time: 4.11 s 
2023-10-26 15:29:05.356937: Yayy! New best EMA pseudo Dice: 0.8274 
2023-10-26 15:29:06.514164:  
2023-10-26 15:29:06.514486: Epoch 69 
2023-10-26 15:29:06.514736: Current learning rate: 0.00938 
2023-10-26 15:29:10.425392: train_loss -0.8458 
2023-10-26 15:29:10.425758: val_loss -0.799 
2023-10-26 15:29:10.426052: Pseudo dice [0.8702, 0.8993, 0.9649, 0.6937, 0.8334] 
2023-10-26 15:29:10.426301: Epoch time: 3.91 s 
2023-10-26 15:29:10.426512: Yayy! New best EMA pseudo Dice: 0.8299 
2023-10-26 15:29:11.849458:  
2023-10-26 15:29:11.849781: Epoch 70 
2023-10-26 15:29:11.850034: Current learning rate: 0.00937 
2023-10-26 15:29:15.990979: train_loss -0.8425 
2023-10-26 15:29:15.991379: val_loss -0.8167 
2023-10-26 15:29:15.991645: Pseudo dice [0.8678, 0.8979, 0.9663, 0.7717, 0.8591] 
2023-10-26 15:29:15.991926: Epoch time: 4.14 s 
2023-10-26 15:29:15.992142: Yayy! New best EMA pseudo Dice: 0.8341 
2023-10-26 15:29:17.150731:  
2023-10-26 15:29:17.151041: Epoch 71 
2023-10-26 15:29:17.151284: Current learning rate: 0.00936 
2023-10-26 15:29:21.254205: train_loss -0.8346 
2023-10-26 15:29:21.254663: val_loss -0.8224 
2023-10-26 15:29:21.254948: Pseudo dice [0.8713, 0.9, 0.9667, 0.7592, 0.8846] 
2023-10-26 15:29:21.255212: Epoch time: 4.1 s 
2023-10-26 15:29:21.255445: Yayy! New best EMA pseudo Dice: 0.8384 
2023-10-26 15:29:22.413075:  
2023-10-26 15:29:22.413406: Epoch 72 
2023-10-26 15:29:22.413655: Current learning rate: 0.00935 
2023-10-26 15:29:26.580622: train_loss -0.8418 
2023-10-26 15:29:26.581034: val_loss -0.8107 
2023-10-26 15:29:26.581303: Pseudo dice [0.8652, 0.8873, 0.9662, 0.6112, 0.8854] 
2023-10-26 15:29:26.581530: Epoch time: 4.17 s 
2023-10-26 15:29:26.581741: Yayy! New best EMA pseudo Dice: 0.8388 
2023-10-26 15:29:27.744248:  
2023-10-26 15:29:27.744551: Epoch 73 
2023-10-26 15:29:27.744790: Current learning rate: 0.00934 
2023-10-26 15:29:31.818102: train_loss -0.8491 
2023-10-26 15:29:31.818465: val_loss -0.8103 
2023-10-26 15:29:31.818717: Pseudo dice [0.8619, 0.8968, 0.9657, 0.5761, 0.8892] 
2023-10-26 15:29:31.818943: Epoch time: 4.07 s 
2023-10-26 15:29:32.986331:  
2023-10-26 15:29:32.986629: Epoch 74 
2023-10-26 15:29:32.986890: Current learning rate: 0.00933 
2023-10-26 15:29:36.945118: train_loss -0.8508 
2023-10-26 15:29:36.945501: val_loss -0.8038 
2023-10-26 15:29:36.945757: Pseudo dice [0.8637, 0.9026, 0.9659, 0.8353, 0.8399] 
2023-10-26 15:29:36.945989: Epoch time: 3.96 s 
2023-10-26 15:29:36.946203: Yayy! New best EMA pseudo Dice: 0.843 
2023-10-26 15:29:38.105379:  
2023-10-26 15:29:38.105664: Epoch 75 
2023-10-26 15:29:38.105912: Current learning rate: 0.00932 
2023-10-26 15:29:42.110997: train_loss -0.8515 
2023-10-26 15:29:42.111436: val_loss -0.8054 
2023-10-26 15:29:42.111707: Pseudo dice [0.8652, 0.9028, 0.9656, 0.5888, 0.8882] 
2023-10-26 15:29:42.111954: Epoch time: 4.01 s 
2023-10-26 15:29:43.378645:  
2023-10-26 15:29:43.378935: Epoch 76 
2023-10-26 15:29:43.379179: Current learning rate: 0.00931 
2023-10-26 15:29:47.483036: train_loss -0.847 
2023-10-26 15:29:47.483454: val_loss -0.7926 
2023-10-26 15:29:47.483731: Pseudo dice [0.8651, 0.9001, 0.9644, 0.8619, 0.8282] 
2023-10-26 15:29:47.483978: Epoch time: 4.1 s 
2023-10-26 15:29:47.484205: Yayy! New best EMA pseudo Dice: 0.847 
2023-10-26 15:29:48.644685:  
2023-10-26 15:29:48.644994: Epoch 77 
2023-10-26 15:29:48.645241: Current learning rate: 0.0093 
2023-10-26 15:29:52.794280: train_loss -0.8549 
2023-10-26 15:29:52.794657: val_loss -0.7948 
2023-10-26 15:29:52.794937: Pseudo dice [0.8672, 0.9023, 0.9656, 0.7791, 0.848] 
2023-10-26 15:29:52.795181: Epoch time: 4.15 s 
2023-10-26 15:29:52.795397: Yayy! New best EMA pseudo Dice: 0.8496 
2023-10-26 15:29:54.015896:  
2023-10-26 15:29:54.016192: Epoch 78 
2023-10-26 15:29:54.016440: Current learning rate: 0.0093 
2023-10-26 15:29:58.118546: train_loss -0.8476 
2023-10-26 15:29:58.118933: val_loss -0.8044 
2023-10-26 15:29:58.119234: Pseudo dice [0.8751, 0.8995, 0.9649, 0.6071, 0.8704] 
2023-10-26 15:29:58.119468: Epoch time: 4.1 s 
2023-10-26 15:29:59.241740:  
2023-10-26 15:29:59.242037: Epoch 79 
2023-10-26 15:29:59.242280: Current learning rate: 0.00929 
2023-10-26 15:30:03.351253: train_loss -0.8342 
2023-10-26 15:30:03.351656: val_loss -0.7781 
2023-10-26 15:30:03.351949: Pseudo dice [0.8636, 0.8882, 0.9673, 0.4228, 0.8288] 
2023-10-26 15:30:03.352269: Epoch time: 4.11 s 
2023-10-26 15:30:04.461480:  
2023-10-26 15:30:04.461780: Epoch 80 
2023-10-26 15:30:04.462062: Current learning rate: 0.00928 
2023-10-26 15:30:08.557272: train_loss -0.8409 
2023-10-26 15:30:08.557703: val_loss -0.8011 
2023-10-26 15:30:08.557981: Pseudo dice [0.8587, 0.9028, 0.9617, 0.4884, 0.8659] 
2023-10-26 15:30:08.558228: Epoch time: 4.1 s 
2023-10-26 15:30:09.709442:  
2023-10-26 15:30:09.709738: Epoch 81 
2023-10-26 15:30:09.709996: Current learning rate: 0.00927 
2023-10-26 15:30:13.800604: train_loss -0.8443 
2023-10-26 15:30:13.800997: val_loss -0.8048 
2023-10-26 15:30:13.801272: Pseudo dice [0.8676, 0.9003, 0.9664, 0.7144, 0.8711] 
2023-10-26 15:30:13.801508: Epoch time: 4.09 s 
2023-10-26 15:30:15.092188:  
2023-10-26 15:30:15.092499: Epoch 82 
2023-10-26 15:30:15.092748: Current learning rate: 0.00926 
2023-10-26 15:30:19.305971: train_loss -0.8524 
2023-10-26 15:30:19.306375: val_loss -0.8145 
2023-10-26 15:30:19.306663: Pseudo dice [0.8724, 0.9031, 0.9682, 0.6482, 0.8621] 
2023-10-26 15:30:19.306916: Epoch time: 4.21 s 
2023-10-26 15:30:20.375160:  
2023-10-26 15:30:20.375473: Epoch 83 
2023-10-26 15:30:20.375735: Current learning rate: 0.00925 
2023-10-26 15:30:24.434443: train_loss -0.8508 
2023-10-26 15:30:24.434909: val_loss -0.7948 
2023-10-26 15:30:24.435349: Pseudo dice [0.8721, 0.8997, 0.9646, 0.6401, 0.8525] 
2023-10-26 15:30:24.435655: Epoch time: 4.06 s 
2023-10-26 15:30:25.522187:  
2023-10-26 15:30:25.522510: Epoch 84 
2023-10-26 15:30:25.522801: Current learning rate: 0.00924 
2023-10-26 15:30:29.535618: train_loss -0.8459 
2023-10-26 15:30:29.536006: val_loss -0.8255 
2023-10-26 15:30:29.536271: Pseudo dice [0.8674, 0.9023, 0.9678, 0.8073, 0.8973] 
2023-10-26 15:30:29.536508: Epoch time: 4.01 s 
2023-10-26 15:30:30.645054:  
2023-10-26 15:30:30.645394: Epoch 85 
2023-10-26 15:30:30.645671: Current learning rate: 0.00923 
2023-10-26 15:30:34.718306: train_loss -0.8454 
2023-10-26 15:30:34.718781: val_loss -0.8151 
2023-10-26 15:30:34.719104: Pseudo dice [0.8712, 0.9027, 0.9667, 0.7724, 0.8592] 
2023-10-26 15:30:34.719369: Epoch time: 4.07 s 
2023-10-26 15:30:34.719611: Yayy! New best EMA pseudo Dice: 0.851 
2023-10-26 15:30:35.898710:  
2023-10-26 15:30:35.899022: Epoch 86 
2023-10-26 15:30:35.899272: Current learning rate: 0.00922 
2023-10-26 15:30:40.105345: train_loss -0.8398 
2023-10-26 15:30:40.105753: val_loss -0.7994 
2023-10-26 15:30:40.106041: Pseudo dice [0.8591, 0.8876, 0.9645, 0.2606, 0.8902] 
2023-10-26 15:30:40.106315: Epoch time: 4.21 s 
2023-10-26 15:30:41.166076:  
2023-10-26 15:30:41.166374: Epoch 87 
2023-10-26 15:30:41.166618: Current learning rate: 0.00921 
2023-10-26 15:30:45.222715: train_loss -0.8409 
2023-10-26 15:30:45.223222: val_loss -0.8132 
2023-10-26 15:30:45.223489: Pseudo dice [0.8689, 0.894, 0.9654, 0.5489, 0.9097] 
2023-10-26 15:30:45.223718: Epoch time: 4.06 s 
2023-10-26 15:30:46.467385:  
2023-10-26 15:30:46.467690: Epoch 88 
2023-10-26 15:30:46.467937: Current learning rate: 0.0092 
2023-10-26 15:30:50.600683: train_loss -0.8457 
2023-10-26 15:30:50.601110: val_loss -0.8136 
2023-10-26 15:30:50.601382: Pseudo dice [0.8662, 0.9028, 0.9648, 0.6445, 0.8862] 
2023-10-26 15:30:50.601605: Epoch time: 4.13 s 
2023-10-26 15:30:51.648850:  
2023-10-26 15:30:51.649192: Epoch 89 
2023-10-26 15:30:51.649440: Current learning rate: 0.0092 
2023-10-26 15:30:55.724273: train_loss -0.8402 
2023-10-26 15:30:55.724713: val_loss -0.8174 
2023-10-26 15:30:55.725017: Pseudo dice [0.8662, 0.8909, 0.9642, 0.6909, 0.8964] 
2023-10-26 15:30:55.725277: Epoch time: 4.08 s 
2023-10-26 15:30:56.781191:  
2023-10-26 15:30:56.781499: Epoch 90 
2023-10-26 15:30:56.781753: Current learning rate: 0.00919 
2023-10-26 15:31:00.897322: train_loss -0.8272 
2023-10-26 15:31:00.897814: val_loss -0.7549 
2023-10-26 15:31:00.898091: Pseudo dice [0.8465, 0.8849, 0.9658, 0.2529, 0.8576] 
2023-10-26 15:31:00.898335: Epoch time: 4.12 s 
2023-10-26 15:31:01.944729:  
2023-10-26 15:31:01.945040: Epoch 91 
2023-10-26 15:31:01.945293: Current learning rate: 0.00918 
2023-10-26 15:31:06.084668: train_loss -0.8312 
2023-10-26 15:31:06.085093: val_loss -0.8063 
2023-10-26 15:31:06.085387: Pseudo dice [0.8626, 0.894, 0.9658, 0.7504, 0.8669] 
2023-10-26 15:31:06.085621: Epoch time: 4.14 s 
2023-10-26 15:31:07.145421:  
2023-10-26 15:31:07.145727: Epoch 92 
2023-10-26 15:31:07.145976: Current learning rate: 0.00917 
2023-10-26 15:31:11.341996: train_loss -0.843 
2023-10-26 15:31:11.342375: val_loss -0.8241 
2023-10-26 15:31:11.342643: Pseudo dice [0.8677, 0.906, 0.9646, 0.7778, 0.8801] 
2023-10-26 15:31:11.342879: Epoch time: 4.2 s 
2023-10-26 15:31:12.402036:  
2023-10-26 15:31:12.402356: Epoch 93 
2023-10-26 15:31:12.402602: Current learning rate: 0.00916 
2023-10-26 15:31:16.558774: train_loss -0.8395 
2023-10-26 15:31:16.559262: val_loss -0.8052 
2023-10-26 15:31:16.559593: Pseudo dice [0.8709, 0.8982, 0.9635, 0.7455, 0.8339] 
2023-10-26 15:31:16.559839: Epoch time: 4.16 s 
2023-10-26 15:31:17.757011:  
2023-10-26 15:31:17.757319: Epoch 94 
2023-10-26 15:31:17.757567: Current learning rate: 0.00915 
2023-10-26 15:31:21.969085: train_loss -0.8528 
2023-10-26 15:31:21.969480: val_loss -0.8087 
2023-10-26 15:31:21.969738: Pseudo dice [0.8686, 0.9034, 0.965, 0.772, 0.8473] 
2023-10-26 15:31:21.969975: Epoch time: 4.21 s 
2023-10-26 15:31:23.013046:  
2023-10-26 15:31:23.013358: Epoch 95 
2023-10-26 15:31:23.013615: Current learning rate: 0.00914 
2023-10-26 15:31:27.210183: train_loss -0.8423 
2023-10-26 15:31:27.210558: val_loss -0.81 
2023-10-26 15:31:27.210814: Pseudo dice [0.8663, 0.9026, 0.967, 0.671, 0.8833] 
2023-10-26 15:31:27.211047: Epoch time: 4.2 s 
2023-10-26 15:31:28.243611:  
2023-10-26 15:31:28.243944: Epoch 96 
2023-10-26 15:31:28.244205: Current learning rate: 0.00913 
2023-10-26 15:31:32.429565: train_loss -0.8456 
2023-10-26 15:31:32.429997: val_loss -0.8096 
2023-10-26 15:31:32.430275: Pseudo dice [0.8635, 0.8989, 0.9651, 0.724, 0.863] 
2023-10-26 15:31:32.430525: Epoch time: 4.19 s 
2023-10-26 15:31:33.481979:  
2023-10-26 15:31:33.482326: Epoch 97 
2023-10-26 15:31:33.482660: Current learning rate: 0.00912 
2023-10-26 15:31:37.707175: train_loss -0.8469 
2023-10-26 15:31:37.707559: val_loss -0.8191 
2023-10-26 15:31:37.707828: Pseudo dice [0.8665, 0.9003, 0.9654, 0.7356, 0.8831] 
2023-10-26 15:31:37.708062: Epoch time: 4.23 s 
2023-10-26 15:31:37.708284: Yayy! New best EMA pseudo Dice: 0.8527 
2023-10-26 15:31:38.840282:  
2023-10-26 15:31:38.840633: Epoch 98 
2023-10-26 15:31:38.840904: Current learning rate: 0.00911 
2023-10-26 15:31:42.893225: train_loss -0.844 
2023-10-26 15:31:42.893724: val_loss -0.8161 
2023-10-26 15:31:42.893995: Pseudo dice [0.8679, 0.8994, 0.9663, 0.6527, 0.8863] 
2023-10-26 15:31:42.894245: Epoch time: 4.05 s 
2023-10-26 15:31:42.894457: Yayy! New best EMA pseudo Dice: 0.8529 
2023-10-26 15:31:44.011666:  
2023-10-26 15:31:44.011965: Epoch 99 
2023-10-26 15:31:44.012213: Current learning rate: 0.0091 
2023-10-26 15:31:48.064157: train_loss -0.8452 
2023-10-26 15:31:48.064615: val_loss -0.7949 
2023-10-26 15:31:48.064994: Pseudo dice [0.8705, 0.9081, 0.9657, 0.7607, 0.8253] 
2023-10-26 15:31:48.065326: Epoch time: 4.05 s 
2023-10-26 15:31:48.139393: Yayy! New best EMA pseudo Dice: 0.8542 
2023-10-26 15:31:49.251791:  
2023-10-26 15:31:49.252118: Epoch 100 
2023-10-26 15:31:49.252371: Current learning rate: 0.0091 
2023-10-26 15:31:53.444221: train_loss -0.8525 
2023-10-26 15:31:53.444626: val_loss -0.812 
2023-10-26 15:31:53.444890: Pseudo dice [0.8741, 0.9086, 0.9668, 0.3612, 0.868] 
2023-10-26 15:31:53.445155: Epoch time: 4.19 s 
2023-10-26 15:31:54.680690:  
2023-10-26 15:31:54.681009: Epoch 101 
2023-10-26 15:31:54.681257: Current learning rate: 0.00909 
2023-10-26 15:31:58.968415: train_loss -0.8554 
2023-10-26 15:31:58.968776: val_loss -0.796 
2023-10-26 15:31:58.969057: Pseudo dice [0.8704, 0.9036, 0.9669, 0.6961, 0.8584] 
2023-10-26 15:31:58.969300: Epoch time: 4.29 s 
2023-10-26 15:32:00.012444:  
2023-10-26 15:32:00.012758: Epoch 102 
2023-10-26 15:32:00.013011: Current learning rate: 0.00908 
2023-10-26 15:32:04.130330: train_loss -0.8462 
2023-10-26 15:32:04.130730: val_loss -0.8232 
2023-10-26 15:32:04.131004: Pseudo dice [0.8707, 0.8976, 0.9667, 0.699, 0.8955] 
2023-10-26 15:32:04.131240: Epoch time: 4.12 s 
2023-10-26 15:32:05.197314:  
2023-10-26 15:32:05.197657: Epoch 103 
2023-10-26 15:32:05.197919: Current learning rate: 0.00907 
2023-10-26 15:32:09.470386: train_loss -0.857 
2023-10-26 15:32:09.470782: val_loss -0.8017 
2023-10-26 15:32:09.471045: Pseudo dice [0.8666, 0.9006, 0.9669, 0.5908, 0.8666] 
2023-10-26 15:32:09.471283: Epoch time: 4.27 s 
2023-10-26 15:32:10.522142:  
2023-10-26 15:32:10.522456: Epoch 104 
2023-10-26 15:32:10.522717: Current learning rate: 0.00906 
2023-10-26 15:32:14.667855: train_loss -0.8524 
2023-10-26 15:32:14.668282: val_loss -0.8063 
2023-10-26 15:32:14.668543: Pseudo dice [0.8672, 0.9044, 0.9662, 0.8407, 0.8564] 
2023-10-26 15:32:14.668771: Epoch time: 4.15 s 
2023-10-26 15:32:15.719420:  
2023-10-26 15:32:15.719725: Epoch 105 
2023-10-26 15:32:15.719979: Current learning rate: 0.00905 
2023-10-26 15:32:19.919410: train_loss -0.8545 
2023-10-26 15:32:19.919804: val_loss -0.8118 
2023-10-26 15:32:19.920071: Pseudo dice [0.8717, 0.9021, 0.9665, 0.7075, 0.8499] 
2023-10-26 15:32:19.920334: Epoch time: 4.2 s 
2023-10-26 15:32:20.999504:  
2023-10-26 15:32:20.999821: Epoch 106 
2023-10-26 15:32:21.000101: Current learning rate: 0.00904 
2023-10-26 15:32:25.170406: train_loss -0.8487 
2023-10-26 15:32:25.170823: val_loss -0.8003 
2023-10-26 15:32:25.171095: Pseudo dice [0.8753, 0.9162, 0.9661, 0.6634, 0.8296] 
2023-10-26 15:32:25.171331: Epoch time: 4.17 s 
2023-10-26 15:32:26.376883:  
2023-10-26 15:32:26.377227: Epoch 107 
2023-10-26 15:32:26.377551: Current learning rate: 0.00903 
2023-10-26 15:32:30.501405: train_loss -0.8526 
2023-10-26 15:32:30.501864: val_loss -0.8346 
2023-10-26 15:32:30.502270: Pseudo dice [0.864, 0.8992, 0.9667, 0.7073, 0.921] 
2023-10-26 15:32:30.502528: Epoch time: 4.13 s 
2023-10-26 15:32:30.502792: Yayy! New best EMA pseudo Dice: 0.8555 
2023-10-26 15:32:31.616013:  
2023-10-26 15:32:31.616323: Epoch 108 
2023-10-26 15:32:31.616572: Current learning rate: 0.00902 
2023-10-26 15:32:35.760682: train_loss -0.8582 
2023-10-26 15:32:35.761047: val_loss -0.8152 
2023-10-26 15:32:35.761309: Pseudo dice [0.8678, 0.9069, 0.9663, 0.752, 0.8451] 
2023-10-26 15:32:35.761541: Epoch time: 4.15 s 
2023-10-26 15:32:35.761760: Yayy! New best EMA pseudo Dice: 0.8567 
2023-10-26 15:32:36.917264:  
2023-10-26 15:32:36.917569: Epoch 109 
2023-10-26 15:32:36.917817: Current learning rate: 0.00901 
2023-10-26 15:32:41.034813: train_loss -0.857 
2023-10-26 15:32:41.035295: val_loss -0.8043 
2023-10-26 15:32:41.035651: Pseudo dice [0.8647, 0.9072, 0.9644, 0.6222, 0.8469] 
2023-10-26 15:32:41.035958: Epoch time: 4.12 s 
2023-10-26 15:32:42.086695:  
2023-10-26 15:32:42.087013: Epoch 110 
2023-10-26 15:32:42.087263: Current learning rate: 0.009 
2023-10-26 15:32:46.192178: train_loss -0.8505 
2023-10-26 15:32:46.192549: val_loss -0.8219 
2023-10-26 15:32:46.192810: Pseudo dice [0.8704, 0.9127, 0.9655, 0.6908, 0.8786] 
2023-10-26 15:32:46.193037: Epoch time: 4.11 s 
2023-10-26 15:32:47.235064:  
2023-10-26 15:32:47.235442: Epoch 111 
2023-10-26 15:32:47.235819: Current learning rate: 0.009 
2023-10-26 15:32:51.455676: train_loss -0.8522 
2023-10-26 15:32:51.456068: val_loss -0.8069 
2023-10-26 15:32:51.456498: Pseudo dice [0.8567, 0.8931, 0.9671, 0.4297, 0.8863] 
2023-10-26 15:32:51.456772: Epoch time: 4.22 s 
2023-10-26 15:32:52.500692:  
2023-10-26 15:32:52.500989: Epoch 112 
2023-10-26 15:32:52.501246: Current learning rate: 0.00899 
2023-10-26 15:32:56.702029: train_loss -0.8481 
2023-10-26 15:32:56.702384: val_loss -0.7899 
2023-10-26 15:32:56.702657: Pseudo dice [0.8654, 0.9027, 0.9655, 0.6643, 0.8362] 
2023-10-26 15:32:56.702904: Epoch time: 4.2 s 
2023-10-26 15:32:57.901647:  
2023-10-26 15:32:57.902018: Epoch 113 
2023-10-26 15:32:57.902347: Current learning rate: 0.00898 
2023-10-26 15:33:02.120679: train_loss -0.8558 
2023-10-26 15:33:02.121085: val_loss -0.806 
2023-10-26 15:33:02.121350: Pseudo dice [0.8612, 0.9058, 0.9651, 0.733, 0.8509] 
2023-10-26 15:33:02.121604: Epoch time: 4.22 s 
2023-10-26 15:33:03.169860:  
2023-10-26 15:33:03.170197: Epoch 114 
2023-10-26 15:33:03.170471: Current learning rate: 0.00897 
2023-10-26 15:33:07.412896: train_loss -0.8576 
2023-10-26 15:33:07.413249: val_loss -0.8024 
2023-10-26 15:33:07.413510: Pseudo dice [0.8681, 0.9003, 0.9663, 0.5348, 0.874] 
2023-10-26 15:33:07.413732: Epoch time: 4.24 s 
2023-10-26 15:33:08.530496:  
2023-10-26 15:33:08.530840: Epoch 115 
2023-10-26 15:33:08.531101: Current learning rate: 0.00896 
2023-10-26 15:33:12.714170: train_loss -0.855 
2023-10-26 15:33:12.714607: val_loss -0.802 
2023-10-26 15:33:12.715064: Pseudo dice [0.8649, 0.8992, 0.9658, 0.6846, 0.8758] 
2023-10-26 15:33:12.715375: Epoch time: 4.18 s 
2023-10-26 15:33:13.785509:  
2023-10-26 15:33:13.785848: Epoch 116 
2023-10-26 15:33:13.786142: Current learning rate: 0.00895 
2023-10-26 15:33:18.001657: train_loss -0.8526 
2023-10-26 15:33:18.002239: val_loss -0.8141 
2023-10-26 15:33:18.002509: Pseudo dice [0.8642, 0.9057, 0.9661, 0.4909, 0.8972] 
2023-10-26 15:33:18.002757: Epoch time: 4.22 s 
2023-10-26 15:33:19.062443:  
2023-10-26 15:33:19.062768: Epoch 117 
2023-10-26 15:33:19.063034: Current learning rate: 0.00894 
2023-10-26 15:33:23.199064: train_loss -0.8545 
2023-10-26 15:33:23.199438: val_loss -0.8035 
2023-10-26 15:33:23.199698: Pseudo dice [0.861, 0.8934, 0.9682, 0.7605, 0.8793] 
2023-10-26 15:33:23.199928: Epoch time: 4.14 s 
2023-10-26 15:33:24.266350:  
2023-10-26 15:33:24.266658: Epoch 118 
2023-10-26 15:33:24.266917: Current learning rate: 0.00893 
2023-10-26 15:33:28.401023: train_loss -0.8558 
2023-10-26 15:33:28.401429: val_loss -0.7862 
2023-10-26 15:33:28.401693: Pseudo dice [0.8681, 0.8983, 0.966, 0.512, 0.8418] 
2023-10-26 15:33:28.401939: Epoch time: 4.14 s 
2023-10-26 15:33:29.710935:  
2023-10-26 15:33:29.711265: Epoch 119 
2023-10-26 15:33:29.711518: Current learning rate: 0.00892 
2023-10-26 15:33:33.798642: train_loss -0.8543 
2023-10-26 15:33:33.799028: val_loss -0.8337 
2023-10-26 15:33:33.799299: Pseudo dice [0.87, 0.9069, 0.9677, 0.6879, 0.9042] 
2023-10-26 15:33:33.799525: Epoch time: 4.09 s 
2023-10-26 15:33:34.858463:  
2023-10-26 15:33:34.858764: Epoch 120 
2023-10-26 15:33:34.859024: Current learning rate: 0.00891 
2023-10-26 15:33:39.020059: train_loss -0.8595 
2023-10-26 15:33:39.020468: val_loss -0.8367 
2023-10-26 15:33:39.020753: Pseudo dice [0.8605, 0.9016, 0.9659, 0.8328, 0.9132] 
2023-10-26 15:33:39.021087: Epoch time: 4.16 s 
2023-10-26 15:33:40.094515:  
2023-10-26 15:33:40.094819: Epoch 121 
2023-10-26 15:33:40.095071: Current learning rate: 0.0089 
2023-10-26 15:33:44.290398: train_loss -0.8537 
2023-10-26 15:33:44.290779: val_loss -0.8018 
2023-10-26 15:33:44.291061: Pseudo dice [0.868, 0.908, 0.9654, 0.5915, 0.8434] 
2023-10-26 15:33:44.291290: Epoch time: 4.2 s 
2023-10-26 15:33:45.357561:  
2023-10-26 15:33:45.357879: Epoch 122 
2023-10-26 15:33:45.358127: Current learning rate: 0.00889 
2023-10-26 15:33:49.510182: train_loss -0.8554 
2023-10-26 15:33:49.510640: val_loss -0.7721 
2023-10-26 15:33:49.510945: Pseudo dice [0.8585, 0.8703, 0.964, 0.0, 0.8876] 
2023-10-26 15:33:49.511190: Epoch time: 4.15 s 
2023-10-26 15:33:50.591409:  
2023-10-26 15:33:50.591842: Epoch 123 
2023-10-26 15:33:50.592197: Current learning rate: 0.00889 
2023-10-26 15:33:54.758012: train_loss -0.806 
2023-10-26 15:33:54.758428: val_loss -0.7783 
2023-10-26 15:33:54.758704: Pseudo dice [0.866, 0.8882, 0.9643, 0.0, 0.8845] 
2023-10-26 15:33:54.758968: Epoch time: 4.17 s 
2023-10-26 15:33:55.825198:  
2023-10-26 15:33:55.825500: Epoch 124 
2023-10-26 15:33:55.825757: Current learning rate: 0.00888 
2023-10-26 15:34:00.094586: train_loss -0.8162 
2023-10-26 15:34:00.095061: val_loss -0.8074 
2023-10-26 15:34:00.095369: Pseudo dice [0.8672, 0.8935, 0.9669, 0.0, 0.9017] 
2023-10-26 15:34:00.095850: Epoch time: 4.27 s 
2023-10-26 15:34:01.192330:  
2023-10-26 15:34:01.192633: Epoch 125 
2023-10-26 15:34:01.192908: Current learning rate: 0.00887 
2023-10-26 15:34:05.303134: train_loss -0.8179 
2023-10-26 15:34:05.303505: val_loss -0.7918 
2023-10-26 15:34:05.303776: Pseudo dice [0.8704, 0.8998, 0.9664, 0.0, 0.9] 
2023-10-26 15:34:05.304023: Epoch time: 4.11 s 
2023-10-26 15:34:06.559424:  
2023-10-26 15:34:06.559744: Epoch 126 
2023-10-26 15:34:06.560003: Current learning rate: 0.00886 
2023-10-26 15:34:10.414623: train_loss -0.8279 
2023-10-26 15:34:10.415031: val_loss -0.7956 
2023-10-26 15:34:10.415307: Pseudo dice [0.8704, 0.8992, 0.967, 0.0204, 0.8893] 
2023-10-26 15:34:10.415547: Epoch time: 3.86 s 
2023-10-26 15:34:11.479807:  
2023-10-26 15:34:11.480227: Epoch 127 
2023-10-26 15:34:11.480483: Current learning rate: 0.00885 
2023-10-26 15:34:15.606240: train_loss -0.8349 
2023-10-26 15:34:15.606632: val_loss -0.7921 
2023-10-26 15:34:15.606914: Pseudo dice [0.8583, 0.892, 0.9612, 0.4702, 0.8726] 
2023-10-26 15:34:15.607175: Epoch time: 4.13 s 
2023-10-26 15:34:16.675961:  
2023-10-26 15:34:16.676282: Epoch 128 
2023-10-26 15:34:16.676553: Current learning rate: 0.00884 
2023-10-26 15:34:20.758690: train_loss -0.8446 
2023-10-26 15:34:20.759102: val_loss -0.7962 
2023-10-26 15:34:20.759369: Pseudo dice [0.8576, 0.8952, 0.9624, 0.5867, 0.8896] 
2023-10-26 15:34:20.759606: Epoch time: 4.08 s 
2023-10-26 15:34:21.876184:  
2023-10-26 15:34:21.876501: Epoch 129 
2023-10-26 15:34:21.876748: Current learning rate: 0.00883 
2023-10-26 15:34:26.105574: train_loss -0.8596 
2023-10-26 15:34:26.105934: val_loss -0.8012 
2023-10-26 15:34:26.106189: Pseudo dice [0.8463, 0.8899, 0.9645, 0.6641, 0.8972] 
2023-10-26 15:34:26.106419: Epoch time: 4.23 s 
2023-10-26 15:34:27.171704:  
2023-10-26 15:34:27.172023: Epoch 130 
2023-10-26 15:34:27.172272: Current learning rate: 0.00882 
2023-10-26 15:34:31.286952: train_loss -0.8552 
2023-10-26 15:34:31.287775: val_loss -0.8223 
2023-10-26 15:34:31.288288: Pseudo dice [0.8622, 0.9058, 0.9646, 0.8183, 0.8799] 
2023-10-26 15:34:31.288586: Epoch time: 4.12 s 
2023-10-26 15:34:32.379088:  
2023-10-26 15:34:32.379398: Epoch 131 
2023-10-26 15:34:32.379652: Current learning rate: 0.00881 
2023-10-26 15:34:36.536549: train_loss -0.8552 
2023-10-26 15:34:36.536945: val_loss -0.8288 
2023-10-26 15:34:36.537240: Pseudo dice [0.8523, 0.901, 0.9661, 0.694, 0.8943] 
2023-10-26 15:34:36.537471: Epoch time: 4.16 s 
2023-10-26 15:34:37.853917:  
2023-10-26 15:34:37.854240: Epoch 132 
2023-10-26 15:34:37.854491: Current learning rate: 0.0088 
2023-10-26 15:34:42.067136: train_loss -0.855 
2023-10-26 15:34:42.067504: val_loss -0.8029 
2023-10-26 15:34:42.067770: Pseudo dice [0.87, 0.895, 0.9638, 0.4707, 0.8794] 
2023-10-26 15:34:42.068034: Epoch time: 4.21 s 
2023-10-26 15:34:43.175443:  
2023-10-26 15:34:43.175749: Epoch 133 
2023-10-26 15:34:43.176003: Current learning rate: 0.00879 
2023-10-26 15:34:47.274343: train_loss -0.8542 
2023-10-26 15:34:47.274813: val_loss -0.8182 
2023-10-26 15:34:47.275163: Pseudo dice [0.8631, 0.8881, 0.9654, 0.6987, 0.8993] 
2023-10-26 15:34:47.275657: Epoch time: 4.1 s 
2023-10-26 15:34:48.344131:  
2023-10-26 15:34:48.344449: Epoch 134 
2023-10-26 15:34:48.344696: Current learning rate: 0.00879 
2023-10-26 15:34:52.596925: train_loss -0.8532 
2023-10-26 15:34:52.597576: val_loss -0.8116 
2023-10-26 15:34:52.597960: Pseudo dice [0.8602, 0.8972, 0.9664, 0.69, 0.8575] 
2023-10-26 15:34:52.598445: Epoch time: 4.25 s 
2023-10-26 15:34:53.723298:  
2023-10-26 15:34:53.723668: Epoch 135 
2023-10-26 15:34:53.723952: Current learning rate: 0.00878 
2023-10-26 15:34:57.777072: train_loss -0.8593 
2023-10-26 15:34:57.777479: val_loss -0.8036 
2023-10-26 15:34:57.777743: Pseudo dice [0.8409, 0.8961, 0.9661, 0.4229, 0.8824] 
2023-10-26 15:34:57.777989: Epoch time: 4.05 s 
2023-10-26 15:34:58.856076:  
2023-10-26 15:34:58.856415: Epoch 136 
2023-10-26 15:34:58.856670: Current learning rate: 0.00877 
2023-10-26 15:35:02.931033: train_loss -0.8526 
2023-10-26 15:35:02.931451: val_loss -0.8177 
2023-10-26 15:35:02.931719: Pseudo dice [0.8667, 0.8956, 0.9662, 0.5977, 0.8804] 
2023-10-26 15:35:02.931969: Epoch time: 4.08 s 
2023-10-26 15:35:04.016585:  
2023-10-26 15:35:04.016892: Epoch 137 
2023-10-26 15:35:04.017149: Current learning rate: 0.00876 
2023-10-26 15:35:08.183594: train_loss -0.861 
2023-10-26 15:35:08.184008: val_loss -0.8215 
2023-10-26 15:35:08.184296: Pseudo dice [0.8571, 0.894, 0.9636, 0.6248, 0.8619] 
2023-10-26 15:35:08.184538: Epoch time: 4.17 s 
2023-10-26 15:35:09.514310:  
2023-10-26 15:35:09.514613: Epoch 138 
2023-10-26 15:35:09.514861: Current learning rate: 0.00875 
2023-10-26 15:35:13.586671: train_loss -0.8643 
2023-10-26 15:35:13.587039: val_loss -0.8095 
2023-10-26 15:35:13.587295: Pseudo dice [0.8671, 0.9051, 0.9651, 0.7669, 0.8404] 
2023-10-26 15:35:13.587521: Epoch time: 4.07 s 
2023-10-26 15:35:14.670522:  
2023-10-26 15:35:14.670844: Epoch 139 
2023-10-26 15:35:14.671110: Current learning rate: 0.00874 
2023-10-26 15:35:18.631759: train_loss -0.8646 
2023-10-26 15:35:18.632187: val_loss -0.8211 
2023-10-26 15:35:18.632450: Pseudo dice [0.8593, 0.8979, 0.9665, 0.5466, 0.8992] 
2023-10-26 15:35:18.632689: Epoch time: 3.96 s 
2023-10-26 15:35:19.717503:  
2023-10-26 15:35:19.717876: Epoch 140 
2023-10-26 15:35:19.718129: Current learning rate: 0.00873 
2023-10-26 15:35:23.810319: train_loss -0.865 
2023-10-26 15:35:23.810701: val_loss -0.8127 
2023-10-26 15:35:23.810995: Pseudo dice [0.8633, 0.9036, 0.9656, 0.6393, 0.8528] 
2023-10-26 15:35:23.811225: Epoch time: 4.09 s 
2023-10-26 15:35:24.896415:  
2023-10-26 15:35:24.896737: Epoch 141 
2023-10-26 15:35:24.896994: Current learning rate: 0.00872 
2023-10-26 15:35:29.023633: train_loss -0.863 
2023-10-26 15:35:29.024043: val_loss -0.8143 
2023-10-26 15:35:29.024507: Pseudo dice [0.867, 0.8983, 0.9649, 0.7363, 0.8831] 
2023-10-26 15:35:29.024916: Epoch time: 4.13 s 
2023-10-26 15:35:30.108102:  
2023-10-26 15:35:30.108389: Epoch 142 
2023-10-26 15:35:30.108632: Current learning rate: 0.00871 
2023-10-26 15:35:34.313821: train_loss -0.8697 
2023-10-26 15:35:34.314217: val_loss -0.8184 
2023-10-26 15:35:34.314473: Pseudo dice [0.8532, 0.8976, 0.9659, 0.5875, 0.8961] 
2023-10-26 15:35:34.314707: Epoch time: 4.21 s 
2023-10-26 15:35:35.418679:  
2023-10-26 15:35:35.418996: Epoch 143 
2023-10-26 15:35:35.419249: Current learning rate: 0.0087 
2023-10-26 15:35:39.682430: train_loss -0.8488 
2023-10-26 15:35:39.682817: val_loss -0.806 
2023-10-26 15:35:39.683080: Pseudo dice [0.8711, 0.9081, 0.9662, 0.6763, 0.8491] 
2023-10-26 15:35:39.683307: Epoch time: 4.26 s 
2023-10-26 15:35:41.055641:  
2023-10-26 15:35:41.056029: Epoch 144 
2023-10-26 15:35:41.056353: Current learning rate: 0.00869 
2023-10-26 15:35:45.197089: train_loss -0.8637 
2023-10-26 15:35:45.197621: val_loss -0.8152 
2023-10-26 15:35:45.197896: Pseudo dice [0.872, 0.9061, 0.9649, 0.7607, 0.8783] 
2023-10-26 15:35:45.198133: Epoch time: 4.14 s 
2023-10-26 15:35:46.290438:  
2023-10-26 15:35:46.290768: Epoch 145 
2023-10-26 15:35:46.291036: Current learning rate: 0.00868 
2023-10-26 15:35:50.499977: train_loss -0.8588 
2023-10-26 15:35:50.500381: val_loss -0.8103 
2023-10-26 15:35:50.500646: Pseudo dice [0.8689, 0.8994, 0.9642, 0.5691, 0.8709] 
2023-10-26 15:35:50.500887: Epoch time: 4.21 s 
2023-10-26 15:35:51.589162:  
2023-10-26 15:35:51.589481: Epoch 146 
2023-10-26 15:35:51.589741: Current learning rate: 0.00868 
2023-10-26 15:35:55.609416: train_loss -0.8508 
2023-10-26 15:35:55.609830: val_loss -0.8279 
2023-10-26 15:35:55.610113: Pseudo dice [0.8637, 0.8981, 0.9671, 0.7457, 0.9145] 
2023-10-26 15:35:55.610354: Epoch time: 4.02 s 
2023-10-26 15:35:56.688635:  
2023-10-26 15:35:56.688975: Epoch 147 
2023-10-26 15:35:56.689245: Current learning rate: 0.00867 
2023-10-26 15:36:00.845595: train_loss -0.8637 
2023-10-26 15:36:00.846017: val_loss -0.8252 
2023-10-26 15:36:00.846288: Pseudo dice [0.8724, 0.9024, 0.9665, 0.6598, 0.889] 
2023-10-26 15:36:00.846524: Epoch time: 4.16 s 
2023-10-26 15:36:01.923076:  
2023-10-26 15:36:01.923380: Epoch 148 
2023-10-26 15:36:01.923621: Current learning rate: 0.00866 
2023-10-26 15:36:06.085938: train_loss -0.8668 
2023-10-26 15:36:06.086339: val_loss -0.8044 
2023-10-26 15:36:06.086629: Pseudo dice [0.8688, 0.9045, 0.9686, 0.6244, 0.8569] 
2023-10-26 15:36:06.086883: Epoch time: 4.16 s 
2023-10-26 15:36:07.236021:  
2023-10-26 15:36:07.236329: Epoch 149 
2023-10-26 15:36:07.236579: Current learning rate: 0.00865 
2023-10-26 15:36:11.376399: train_loss -0.8662 
2023-10-26 15:36:11.376751: val_loss -0.8346 
2023-10-26 15:36:11.377020: Pseudo dice [0.8671, 0.913, 0.9675, 0.8096, 0.8803] 
2023-10-26 15:36:11.377245: Epoch time: 4.14 s 
2023-10-26 15:36:12.699460:  
2023-10-26 15:36:12.699797: Epoch 150 
2023-10-26 15:36:12.700076: Current learning rate: 0.00864 
2023-10-26 15:36:16.823774: train_loss -0.8627 
2023-10-26 15:36:16.824186: val_loss -0.8215 
2023-10-26 15:36:16.824444: Pseudo dice [0.869, 0.8982, 0.9672, 0.6984, 0.8901] 
2023-10-26 15:36:16.824690: Epoch time: 4.12 s 
2023-10-26 15:36:17.924464:  
2023-10-26 15:36:17.924770: Epoch 151 
2023-10-26 15:36:17.925021: Current learning rate: 0.00863 
2023-10-26 15:36:22.010738: train_loss -0.8668 
2023-10-26 15:36:22.011360: val_loss -0.8214 
2023-10-26 15:36:22.011880: Pseudo dice [0.8671, 0.8999, 0.9654, 0.4756, 0.8936] 
2023-10-26 15:36:22.012197: Epoch time: 4.09 s 
2023-10-26 15:36:23.113856:  
2023-10-26 15:36:23.114184: Epoch 152 
2023-10-26 15:36:23.114434: Current learning rate: 0.00862 
2023-10-26 15:36:27.097537: train_loss -0.8701 
2023-10-26 15:36:27.097981: val_loss -0.823 
2023-10-26 15:36:27.098331: Pseudo dice [0.8709, 0.9076, 0.9678, 0.7213, 0.859] 
2023-10-26 15:36:27.098703: Epoch time: 3.98 s 
2023-10-26 15:36:28.223217:  
2023-10-26 15:36:28.223595: Epoch 153 
2023-10-26 15:36:28.223934: Current learning rate: 0.00861 
2023-10-26 15:36:32.305409: train_loss -0.8678 
2023-10-26 15:36:32.305898: val_loss -0.8176 
2023-10-26 15:36:32.306290: Pseudo dice [0.8621, 0.8981, 0.967, 0.5324, 0.8646] 
2023-10-26 15:36:32.306618: Epoch time: 4.08 s 
2023-10-26 15:36:33.431183:  
2023-10-26 15:36:33.431522: Epoch 154 
2023-10-26 15:36:33.431784: Current learning rate: 0.0086 
2023-10-26 15:36:37.617585: train_loss -0.853 
2023-10-26 15:36:37.617958: val_loss -0.8017 
2023-10-26 15:36:37.618223: Pseudo dice [0.8538, 0.8931, 0.9634, 0.6301, 0.8762] 
2023-10-26 15:36:37.618455: Epoch time: 4.19 s 
2023-10-26 15:36:38.947845:  
2023-10-26 15:36:38.948208: Epoch 155 
2023-10-26 15:36:38.948472: Current learning rate: 0.00859 
2023-10-26 15:36:43.243972: train_loss -0.8546 
2023-10-26 15:36:43.244409: val_loss -0.8208 
2023-10-26 15:36:43.244833: Pseudo dice [0.8565, 0.8976, 0.9644, 0.7712, 0.8728] 
2023-10-26 15:36:43.245181: Epoch time: 4.3 s 
2023-10-26 15:36:44.388848:  
2023-10-26 15:36:44.389235: Epoch 156 
2023-10-26 15:36:44.389538: Current learning rate: 0.00858 
2023-10-26 15:36:48.896643: train_loss -0.8561 
2023-10-26 15:36:48.897086: val_loss -0.8111 
2023-10-26 15:36:48.897414: Pseudo dice [0.8669, 0.9003, 0.9659, 0.583, 0.8687] 
2023-10-26 15:36:48.897676: Epoch time: 4.51 s 
2023-10-26 15:36:50.012986:  
2023-10-26 15:36:50.013292: Epoch 157 
2023-10-26 15:36:50.013546: Current learning rate: 0.00858 
2023-10-26 15:36:54.140509: train_loss -0.8571 
2023-10-26 15:36:54.140960: val_loss -0.8263 
2023-10-26 15:36:54.141226: Pseudo dice [0.8696, 0.91, 0.9677, 0.7284, 0.8604] 
2023-10-26 15:36:54.141455: Epoch time: 4.13 s 
2023-10-26 15:36:55.311970:  
2023-10-26 15:36:55.312294: Epoch 158 
2023-10-26 15:36:55.312543: Current learning rate: 0.00857 
2023-10-26 15:36:59.332957: train_loss -0.8625 
2023-10-26 15:36:59.333657: val_loss -0.8141 
2023-10-26 15:36:59.334015: Pseudo dice [0.8696, 0.9069, 0.9632, 0.6504, 0.8743] 
2023-10-26 15:36:59.334517: Epoch time: 4.02 s 
2023-10-26 15:37:00.504038:  
2023-10-26 15:37:00.504432: Epoch 159 
2023-10-26 15:37:00.504795: Current learning rate: 0.00856 
2023-10-26 15:37:04.543247: train_loss -0.8711 
2023-10-26 15:37:04.543619: val_loss -0.8258 
2023-10-26 15:37:04.543891: Pseudo dice [0.8702, 0.9072, 0.9658, 0.7206, 0.8907] 
2023-10-26 15:37:04.544121: Epoch time: 4.04 s 
2023-10-26 15:37:05.657234:  
2023-10-26 15:37:05.657539: Epoch 160 
2023-10-26 15:37:05.657786: Current learning rate: 0.00855 
2023-10-26 15:37:09.826402: train_loss -0.8653 
2023-10-26 15:37:09.826828: val_loss -0.7949 
2023-10-26 15:37:09.827117: Pseudo dice [0.8661, 0.8837, 0.9661, 0.3207, 0.8836] 
2023-10-26 15:37:09.827554: Epoch time: 4.17 s 
2023-10-26 15:37:11.145549:  
2023-10-26 15:37:11.145881: Epoch 161 
2023-10-26 15:37:11.146124: Current learning rate: 0.00854 
2023-10-26 15:37:15.219137: train_loss -0.8645 
2023-10-26 15:37:15.219569: val_loss -0.8257 
2023-10-26 15:37:15.219851: Pseudo dice [0.8753, 0.8979, 0.9674, 0.7505, 0.8823] 
2023-10-26 15:37:15.220114: Epoch time: 4.07 s 
2023-10-26 15:37:16.342462:  
2023-10-26 15:37:16.342782: Epoch 162 
2023-10-26 15:37:16.343035: Current learning rate: 0.00853 
2023-10-26 15:37:20.414578: train_loss -0.867 
2023-10-26 15:37:20.414958: val_loss -0.7971 
2023-10-26 15:37:20.415242: Pseudo dice [0.8713, 0.9002, 0.9674, 0.6192, 0.8254] 
2023-10-26 15:37:20.415475: Epoch time: 4.07 s 
2023-10-26 15:37:21.536179:  
2023-10-26 15:37:21.536482: Epoch 163 
2023-10-26 15:37:21.536729: Current learning rate: 0.00852 
2023-10-26 15:37:25.611681: train_loss -0.8725 
2023-10-26 15:37:25.612076: val_loss -0.8104 
2023-10-26 15:37:25.612337: Pseudo dice [0.8681, 0.8992, 0.9629, 0.6346, 0.8554] 
2023-10-26 15:37:25.612580: Epoch time: 4.08 s 
2023-10-26 15:37:26.739471:  
2023-10-26 15:37:26.739778: Epoch 164 
2023-10-26 15:37:26.740100: Current learning rate: 0.00851 
2023-10-26 15:37:30.738468: train_loss -0.8707 
2023-10-26 15:37:30.738880: val_loss -0.8221 
2023-10-26 15:37:30.739146: Pseudo dice [0.8657, 0.8905, 0.9666, 0.6302, 0.8946] 
2023-10-26 15:37:30.739374: Epoch time: 4.0 s 
2023-10-26 15:37:31.842435:  
2023-10-26 15:37:31.842755: Epoch 165 
2023-10-26 15:37:31.843105: Current learning rate: 0.0085 
2023-10-26 15:37:36.343098: train_loss -0.8746 
2023-10-26 15:37:36.343543: val_loss -0.8175 
2023-10-26 15:37:36.343815: Pseudo dice [0.8724, 0.9102, 0.9652, 0.6147, 0.8799] 
2023-10-26 15:37:36.344139: Epoch time: 4.5 s 
2023-10-26 15:37:37.441001:  
2023-10-26 15:37:37.441310: Epoch 166 
2023-10-26 15:37:37.441566: Current learning rate: 0.00849 
2023-10-26 15:37:41.529235: train_loss -0.8701 
2023-10-26 15:37:41.529916: val_loss -0.8296 
2023-10-26 15:37:41.530189: Pseudo dice [0.8635, 0.9054, 0.9669, 0.6351, 0.8836] 
2023-10-26 15:37:41.530413: Epoch time: 4.09 s 
2023-10-26 15:37:42.885172:  
2023-10-26 15:37:42.885488: Epoch 167 
2023-10-26 15:37:42.885729: Current learning rate: 0.00848 
2023-10-26 15:37:46.952740: train_loss -0.87 
2023-10-26 15:37:46.953176: val_loss -0.8034 
2023-10-26 15:37:46.953451: Pseudo dice [0.8723, 0.905, 0.965, 0.479, 0.8688] 
2023-10-26 15:37:46.953796: Epoch time: 4.07 s 
2023-10-26 15:37:48.063057:  
2023-10-26 15:37:48.063372: Epoch 168 
2023-10-26 15:37:48.063627: Current learning rate: 0.00847 
2023-10-26 15:37:52.206502: train_loss -0.8715 
2023-10-26 15:37:52.206867: val_loss -0.8312 
2023-10-26 15:37:52.207143: Pseudo dice [0.8669, 0.8998, 0.9652, 0.6989, 0.8944] 
2023-10-26 15:37:52.207369: Epoch time: 4.14 s 
2023-10-26 15:37:53.318301:  
2023-10-26 15:37:53.318619: Epoch 169 
2023-10-26 15:37:53.318895: Current learning rate: 0.00847 
2023-10-26 15:37:57.419115: train_loss -0.8704 
2023-10-26 15:37:57.419510: val_loss -0.8327 
2023-10-26 15:37:57.419785: Pseudo dice [0.8655, 0.9103, 0.9661, 0.6747, 0.875] 
2023-10-26 15:37:57.420066: Epoch time: 4.1 s 
2023-10-26 15:37:58.571964:  
2023-10-26 15:37:58.572277: Epoch 170 
2023-10-26 15:37:58.572515: Current learning rate: 0.00846 
2023-10-26 15:38:02.586210: train_loss -0.8635 
2023-10-26 15:38:02.586612: val_loss -0.8222 
2023-10-26 15:38:02.586883: Pseudo dice [0.87, 0.9037, 0.9676, 0.7258, 0.8517] 
2023-10-26 15:38:02.587166: Epoch time: 4.01 s 
2023-10-26 15:38:03.690483:  
2023-10-26 15:38:03.690783: Epoch 171 
2023-10-26 15:38:03.691034: Current learning rate: 0.00845 
2023-10-26 15:38:07.791151: train_loss -0.8598 
2023-10-26 15:38:07.791545: val_loss -0.814 
2023-10-26 15:38:07.791815: Pseudo dice [0.8709, 0.901, 0.9664, 0.6956, 0.8483] 
2023-10-26 15:38:07.792050: Epoch time: 4.1 s 
2023-10-26 15:38:08.904618:  
2023-10-26 15:38:08.904980: Epoch 172 
2023-10-26 15:38:08.905291: Current learning rate: 0.00844 
2023-10-26 15:38:12.725728: train_loss -0.8623 
2023-10-26 15:38:12.726105: val_loss -0.8216 
2023-10-26 15:38:12.726361: Pseudo dice [0.8677, 0.8966, 0.9673, 0.6543, 0.8641] 
2023-10-26 15:38:12.726588: Epoch time: 3.82 s 
2023-10-26 15:38:14.060730:  
2023-10-26 15:38:14.061046: Epoch 173 
2023-10-26 15:38:14.061295: Current learning rate: 0.00843 
2023-10-26 15:38:17.993184: train_loss -0.8623 
2023-10-26 15:38:17.993664: val_loss -0.805 
2023-10-26 15:38:17.993952: Pseudo dice [0.8626, 0.8909, 0.9673, 0.4928, 0.8784] 
2023-10-26 15:38:17.994293: Epoch time: 3.93 s 
2023-10-26 15:38:19.099420:  
2023-10-26 15:38:19.099737: Epoch 174 
2023-10-26 15:38:19.099984: Current learning rate: 0.00842 
2023-10-26 15:38:23.166282: train_loss -0.8686 
2023-10-26 15:38:23.166652: val_loss -0.8243 
2023-10-26 15:38:23.166924: Pseudo dice [0.8686, 0.9021, 0.968, 0.7009, 0.9033] 
2023-10-26 15:38:23.167159: Epoch time: 4.07 s 
2023-10-26 15:38:24.333809:  
2023-10-26 15:38:24.334131: Epoch 175 
2023-10-26 15:38:24.334381: Current learning rate: 0.00841 
2023-10-26 15:38:28.327620: train_loss -0.8729 
2023-10-26 15:38:28.328061: val_loss -0.8015 
2023-10-26 15:38:28.328320: Pseudo dice [0.8591, 0.8952, 0.9648, 0.5569, 0.8686] 
2023-10-26 15:38:28.328542: Epoch time: 3.99 s 
2023-10-26 15:38:29.444948:  
2023-10-26 15:38:29.445249: Epoch 176 
2023-10-26 15:38:29.445519: Current learning rate: 0.0084 
2023-10-26 15:38:33.523987: train_loss -0.8564 
2023-10-26 15:38:33.524467: val_loss -0.8053 
2023-10-26 15:38:33.524793: Pseudo dice [0.8698, 0.8928, 0.9682, 0.572, 0.8501] 
2023-10-26 15:38:33.525172: Epoch time: 4.08 s 
2023-10-26 15:38:34.637250:  
2023-10-26 15:38:34.637566: Epoch 177 
2023-10-26 15:38:34.637810: Current learning rate: 0.00839 
2023-10-26 15:38:38.788878: train_loss -0.8664 
2023-10-26 15:38:38.789258: val_loss -0.8262 
2023-10-26 15:38:38.789521: Pseudo dice [0.8694, 0.9002, 0.9667, 0.6559, 0.8968] 
2023-10-26 15:38:38.789754: Epoch time: 4.15 s 
2023-10-26 15:38:39.902577:  
2023-10-26 15:38:39.902890: Epoch 178 
2023-10-26 15:38:39.903156: Current learning rate: 0.00838 
2023-10-26 15:38:43.846711: train_loss -0.8668 
2023-10-26 15:38:43.847143: val_loss -0.8229 
2023-10-26 15:38:43.847414: Pseudo dice [0.8666, 0.9027, 0.964, 0.8104, 0.8705] 
2023-10-26 15:38:43.847648: Epoch time: 3.94 s 
2023-10-26 15:38:45.132821:  
2023-10-26 15:38:45.133138: Epoch 179 
2023-10-26 15:38:45.133389: Current learning rate: 0.00837 
2023-10-26 15:38:49.149172: train_loss -0.8641 
2023-10-26 15:38:49.149625: val_loss -0.7609 
2023-10-26 15:38:49.149957: Pseudo dice [0.8619, 0.9034, 0.9633, 0.0212, 0.8368] 
2023-10-26 15:38:49.150278: Epoch time: 4.02 s 
2023-10-26 15:38:50.258625:  
2023-10-26 15:38:50.258916: Epoch 180 
2023-10-26 15:38:50.259164: Current learning rate: 0.00836 
2023-10-26 15:38:54.396270: train_loss -0.8532 
2023-10-26 15:38:54.396667: val_loss -0.8133 
2023-10-26 15:38:54.396935: Pseudo dice [0.8646, 0.8957, 0.964, 0.6467, 0.8622] 
2023-10-26 15:38:54.397158: Epoch time: 4.14 s 
2023-10-26 15:38:55.513606:  
2023-10-26 15:38:55.513941: Epoch 181 
2023-10-26 15:38:55.514211: Current learning rate: 0.00836 
2023-10-26 15:38:59.663572: train_loss -0.8601 
2023-10-26 15:38:59.663964: val_loss -0.819 
2023-10-26 15:38:59.664238: Pseudo dice [0.8728, 0.9035, 0.9639, 0.7431, 0.8465] 
2023-10-26 15:38:59.664481: Epoch time: 4.15 s 
2023-10-26 15:39:00.811731:  
2023-10-26 15:39:00.812041: Epoch 182 
2023-10-26 15:39:00.812309: Current learning rate: 0.00835 
2023-10-26 15:39:04.961449: train_loss -0.8656 
2023-10-26 15:39:04.962043: val_loss -0.8164 
2023-10-26 15:39:04.962317: Pseudo dice [0.8671, 0.8956, 0.9661, 0.5761, 0.8779] 
2023-10-26 15:39:04.962562: Epoch time: 4.15 s 
2023-10-26 15:39:06.078723:  
2023-10-26 15:39:06.079036: Epoch 183 
2023-10-26 15:39:06.079280: Current learning rate: 0.00834 
2023-10-26 15:39:10.175141: train_loss -0.8656 
2023-10-26 15:39:10.175618: val_loss -0.8226 
2023-10-26 15:39:10.175996: Pseudo dice [0.8793, 0.9058, 0.9676, 0.5825, 0.8886] 
2023-10-26 15:39:10.176247: Epoch time: 4.1 s 
2023-10-26 15:39:11.282126:  
2023-10-26 15:39:11.282429: Epoch 184 
2023-10-26 15:39:11.282687: Current learning rate: 0.00833 
2023-10-26 15:39:15.500906: train_loss -0.8624 
2023-10-26 15:39:15.501427: val_loss -0.8198 
2023-10-26 15:39:15.501866: Pseudo dice [0.8677, 0.9013, 0.9648, 0.6437, 0.8555] 
2023-10-26 15:39:15.502233: Epoch time: 4.22 s 
2023-10-26 15:39:16.782695:  
2023-10-26 15:39:16.783013: Epoch 185 
2023-10-26 15:39:16.783267: Current learning rate: 0.00832 
2023-10-26 15:39:20.884059: train_loss -0.8725 
2023-10-26 15:39:20.884632: val_loss -0.8136 
2023-10-26 15:39:20.884976: Pseudo dice [0.8607, 0.8942, 0.968, 0.4815, 0.881] 
2023-10-26 15:39:20.885237: Epoch time: 4.1 s 
2023-10-26 15:39:22.033761:  
2023-10-26 15:39:22.034088: Epoch 186 
2023-10-26 15:39:22.034343: Current learning rate: 0.00831 
2023-10-26 15:39:26.188804: train_loss -0.8682 
2023-10-26 15:39:26.189224: val_loss -0.8312 
2023-10-26 15:39:26.189490: Pseudo dice [0.8698, 0.9047, 0.9672, 0.7966, 0.8658] 
2023-10-26 15:39:26.189734: Epoch time: 4.16 s 
2023-10-26 15:39:27.305961:  
2023-10-26 15:39:27.306287: Epoch 187 
2023-10-26 15:39:27.306546: Current learning rate: 0.0083 
2023-10-26 15:39:31.508922: train_loss -0.8669 
2023-10-26 15:39:31.509416: val_loss -0.8147 
2023-10-26 15:39:31.509696: Pseudo dice [0.8737, 0.9039, 0.9689, 0.6256, 0.8479] 
2023-10-26 15:39:31.509959: Epoch time: 4.2 s 
2023-10-26 15:39:32.626790:  
2023-10-26 15:39:32.627097: Epoch 188 
2023-10-26 15:39:32.627345: Current learning rate: 0.00829 
2023-10-26 15:39:36.660751: train_loss -0.8743 
2023-10-26 15:39:36.661151: val_loss -0.8335 
2023-10-26 15:39:36.661478: Pseudo dice [0.8685, 0.9056, 0.9671, 0.6827, 0.8849] 
2023-10-26 15:39:36.661888: Epoch time: 4.03 s 
2023-10-26 15:39:37.784688:  
2023-10-26 15:39:37.785027: Epoch 189 
2023-10-26 15:39:37.785292: Current learning rate: 0.00828 
2023-10-26 15:39:41.879847: train_loss -0.8751 
2023-10-26 15:39:41.880803: val_loss -0.8168 
2023-10-26 15:39:41.881193: Pseudo dice [0.8672, 0.9033, 0.966, 0.6128, 0.8565] 
2023-10-26 15:39:41.881463: Epoch time: 4.1 s 
2023-10-26 15:39:43.028800:  
2023-10-26 15:39:43.029099: Epoch 190 
2023-10-26 15:39:43.029347: Current learning rate: 0.00827 
2023-10-26 15:39:47.062553: train_loss -0.8751 
2023-10-26 15:39:47.062946: val_loss -0.8132 
2023-10-26 15:39:47.063213: Pseudo dice [0.8738, 0.9045, 0.9685, 0.5164, 0.858] 
2023-10-26 15:39:47.063439: Epoch time: 4.03 s 
2023-10-26 15:39:48.359833:  
2023-10-26 15:39:48.360140: Epoch 191 
2023-10-26 15:39:48.360388: Current learning rate: 0.00826 
2023-10-26 15:39:52.438369: train_loss -0.8778 
2023-10-26 15:39:52.438761: val_loss -0.8233 
2023-10-26 15:39:52.439047: Pseudo dice [0.8719, 0.9072, 0.9683, 0.6424, 0.8759] 
2023-10-26 15:39:52.439288: Epoch time: 4.08 s 
2023-10-26 15:39:53.577662:  
2023-10-26 15:39:53.577982: Epoch 192 
2023-10-26 15:39:53.578229: Current learning rate: 0.00825 
2023-10-26 15:39:57.768730: train_loss -0.8744 
2023-10-26 15:39:57.769169: val_loss -0.8363 
2023-10-26 15:39:57.769456: Pseudo dice [0.8662, 0.9042, 0.9691, 0.6729, 0.894] 
2023-10-26 15:39:57.769703: Epoch time: 4.19 s 
2023-10-26 15:39:58.889461:  
2023-10-26 15:39:58.889765: Epoch 193 
2023-10-26 15:39:58.890023: Current learning rate: 0.00824 
2023-10-26 15:40:03.036212: train_loss -0.868 
2023-10-26 15:40:03.036608: val_loss -0.8067 
2023-10-26 15:40:03.036898: Pseudo dice [0.8704, 0.9024, 0.9655, 0.5919, 0.8327] 
2023-10-26 15:40:03.037134: Epoch time: 4.15 s 
2023-10-26 15:40:04.183980:  
2023-10-26 15:40:04.184306: Epoch 194 
2023-10-26 15:40:04.184560: Current learning rate: 0.00824 
2023-10-26 15:40:08.226222: train_loss -0.8695 
2023-10-26 15:40:08.226900: val_loss -0.8274 
2023-10-26 15:40:08.227170: Pseudo dice [0.869, 0.9091, 0.9662, 0.7278, 0.8715] 
2023-10-26 15:40:08.227397: Epoch time: 4.04 s 
2023-10-26 15:40:09.372685:  
2023-10-26 15:40:09.372994: Epoch 195 
2023-10-26 15:40:09.373299: Current learning rate: 0.00823 
2023-10-26 15:40:13.348975: train_loss -0.8634 
2023-10-26 15:40:13.349411: val_loss -0.8184 
2023-10-26 15:40:13.349677: Pseudo dice [0.8654, 0.895, 0.9671, 0.4947, 0.9031] 
2023-10-26 15:40:13.349921: Epoch time: 3.98 s 
2023-10-26 15:40:14.480976:  
2023-10-26 15:40:14.481284: Epoch 196 
2023-10-26 15:40:14.481532: Current learning rate: 0.00822 
2023-10-26 15:40:18.459956: train_loss -0.8694 
2023-10-26 15:40:18.460477: val_loss -0.8395 
2023-10-26 15:40:18.460732: Pseudo dice [0.8646, 0.9076, 0.9676, 0.7324, 0.8899] 
2023-10-26 15:40:18.460959: Epoch time: 3.98 s 
2023-10-26 15:40:19.781564:  
2023-10-26 15:40:19.781883: Epoch 197 
2023-10-26 15:40:19.782142: Current learning rate: 0.00821 
2023-10-26 15:40:23.865625: train_loss -0.8765 
2023-10-26 15:40:23.866037: val_loss -0.8172 
2023-10-26 15:40:23.866332: Pseudo dice [0.8725, 0.9078, 0.9666, 0.709, 0.8235] 
2023-10-26 15:40:23.866571: Epoch time: 4.08 s 
2023-10-26 15:40:24.989816:  
2023-10-26 15:40:24.990133: Epoch 198 
2023-10-26 15:40:24.990383: Current learning rate: 0.0082 
2023-10-26 15:40:28.838520: train_loss -0.8755 
2023-10-26 15:40:28.838911: val_loss -0.8256 
2023-10-26 15:40:28.839187: Pseudo dice [0.8687, 0.9049, 0.9669, 0.571, 0.9037] 
2023-10-26 15:40:28.839439: Epoch time: 3.85 s 
2023-10-26 15:40:30.047328:  
2023-10-26 15:40:30.047637: Epoch 199 
2023-10-26 15:40:30.047890: Current learning rate: 0.00819 
2023-10-26 15:40:34.086002: train_loss -0.8712 
2023-10-26 15:40:34.086419: val_loss -0.8067 
2023-10-26 15:40:34.086696: Pseudo dice [0.8681, 0.9034, 0.966, 0.6536, 0.8725] 
2023-10-26 15:40:34.086933: Epoch time: 4.04 s 
2023-10-26 15:40:35.307379:  
2023-10-26 15:40:35.307721: Epoch 200 
2023-10-26 15:40:35.308012: Current learning rate: 0.00818 
2023-10-26 15:40:39.493670: train_loss -0.8687 
2023-10-26 15:40:39.494089: val_loss -0.8179 
2023-10-26 15:40:39.494365: Pseudo dice [0.8703, 0.9017, 0.9675, 0.6783, 0.853] 
2023-10-26 15:40:39.494600: Epoch time: 4.19 s 
2023-10-26 15:40:40.715045:  
2023-10-26 15:40:40.715352: Epoch 201 
2023-10-26 15:40:40.715611: Current learning rate: 0.00817 
2023-10-26 15:40:44.870326: train_loss -0.867 
2023-10-26 15:40:44.870853: val_loss -0.8402 
2023-10-26 15:40:44.871306: Pseudo dice [0.8683, 0.8983, 0.9664, 0.6605, 0.8892] 
2023-10-26 15:40:44.871746: Epoch time: 4.16 s 
2023-10-26 15:40:46.185902:  
2023-10-26 15:40:46.186257: Epoch 202 
2023-10-26 15:40:46.186543: Current learning rate: 0.00816 
2023-10-26 15:40:50.394619: train_loss -0.8757 
2023-10-26 15:40:50.395059: val_loss -0.8309 
2023-10-26 15:40:50.395325: Pseudo dice [0.8636, 0.898, 0.9668, 0.6799, 0.8936] 
2023-10-26 15:40:50.395565: Epoch time: 4.21 s 
2023-10-26 15:40:51.562621:  
2023-10-26 15:40:51.562917: Epoch 203 
2023-10-26 15:40:51.563172: Current learning rate: 0.00815 
2023-10-26 15:40:55.677594: train_loss -0.8671 
2023-10-26 15:40:55.677997: val_loss -0.8136 
2023-10-26 15:40:55.678287: Pseudo dice [0.8693, 0.8993, 0.9657, 0.4614, 0.8858] 
2023-10-26 15:40:55.678533: Epoch time: 4.12 s 
2023-10-26 15:40:56.806679:  
2023-10-26 15:40:56.807006: Epoch 204 
2023-10-26 15:40:56.807263: Current learning rate: 0.00814 
2023-10-26 15:41:00.905627: train_loss -0.8689 
2023-10-26 15:41:00.906028: val_loss -0.8267 
2023-10-26 15:41:00.906292: Pseudo dice [0.8641, 0.9053, 0.9668, 0.6787, 0.8881] 
2023-10-26 15:41:00.906528: Epoch time: 4.1 s 
2023-10-26 15:41:02.047470:  
2023-10-26 15:41:02.047775: Epoch 205 
2023-10-26 15:41:02.048078: Current learning rate: 0.00813 
2023-10-26 15:41:06.154364: train_loss -0.8759 
2023-10-26 15:41:06.154758: val_loss -0.8171 
2023-10-26 15:41:06.155044: Pseudo dice [0.8697, 0.9019, 0.9656, 0.6723, 0.8679] 
2023-10-26 15:41:06.155294: Epoch time: 4.11 s 
2023-10-26 15:41:07.235275:  
2023-10-26 15:41:07.235573: Epoch 206 
2023-10-26 15:41:07.235923: Current learning rate: 0.00813 
2023-10-26 15:41:11.362753: train_loss -0.8746 
2023-10-26 15:41:11.363458: val_loss -0.807 
2023-10-26 15:41:11.363749: Pseudo dice [0.8635, 0.8976, 0.9656, 0.5042, 0.8901] 
2023-10-26 15:41:11.364012: Epoch time: 4.13 s 
2023-10-26 15:41:12.461157:  
2023-10-26 15:41:12.461461: Epoch 207 
2023-10-26 15:41:12.461708: Current learning rate: 0.00812 
2023-10-26 15:41:16.670651: train_loss -0.8775 
2023-10-26 15:41:16.671101: val_loss -0.821 
2023-10-26 15:41:16.671375: Pseudo dice [0.8637, 0.9081, 0.9647, 0.6955, 0.8574] 
2023-10-26 15:41:16.671621: Epoch time: 4.21 s 
2023-10-26 15:41:17.970283:  
2023-10-26 15:41:17.970608: Epoch 208 
2023-10-26 15:41:17.970868: Current learning rate: 0.00811 
2023-10-26 15:41:22.199845: train_loss -0.871 
2023-10-26 15:41:22.200244: val_loss -0.8385 
2023-10-26 15:41:22.200512: Pseudo dice [0.8612, 0.9036, 0.9658, 0.6776, 0.9187] 
2023-10-26 15:41:22.200745: Epoch time: 4.23 s 
2023-10-26 15:41:23.265898:  
2023-10-26 15:41:23.266242: Epoch 209 
2023-10-26 15:41:23.266522: Current learning rate: 0.0081 
2023-10-26 15:41:27.292105: train_loss -0.875 
2023-10-26 15:41:27.292516: val_loss -0.8188 
2023-10-26 15:41:27.292810: Pseudo dice [0.8673, 0.8991, 0.964, 0.6558, 0.8816] 
2023-10-26 15:41:27.293057: Epoch time: 4.03 s 
2023-10-26 15:41:28.356302:  
2023-10-26 15:41:28.356601: Epoch 210 
2023-10-26 15:41:28.356848: Current learning rate: 0.00809 
2023-10-26 15:41:32.584798: train_loss -0.8699 
2023-10-26 15:41:32.585281: val_loss -0.8441 
2023-10-26 15:41:32.585620: Pseudo dice [0.8746, 0.9001, 0.9668, 0.7707, 0.9047] 
2023-10-26 15:41:32.585862: Epoch time: 4.23 s 
2023-10-26 15:41:33.744986:  
2023-10-26 15:41:33.745337: Epoch 211 
2023-10-26 15:41:33.745648: Current learning rate: 0.00808 
2023-10-26 15:41:38.174908: train_loss -0.8714 
2023-10-26 15:41:38.175328: val_loss -0.8204 
2023-10-26 15:41:38.175599: Pseudo dice [0.8621, 0.8933, 0.9668, 0.6269, 0.8997] 
2023-10-26 15:41:38.175836: Epoch time: 4.43 s 
2023-10-26 15:41:39.237766:  
2023-10-26 15:41:39.238076: Epoch 212 
2023-10-26 15:41:39.238320: Current learning rate: 0.00807 
2023-10-26 15:41:43.536818: train_loss -0.8695 
2023-10-26 15:41:43.537277: val_loss -0.8078 
2023-10-26 15:41:43.537653: Pseudo dice [0.8628, 0.9065, 0.969, 0.6655, 0.8506] 
2023-10-26 15:41:43.537965: Epoch time: 4.3 s 
2023-10-26 15:41:44.608627:  
2023-10-26 15:41:44.616739: Epoch 213 
2023-10-26 15:41:44.617123: Current learning rate: 0.00806 
2023-10-26 15:41:48.875567: train_loss -0.8708 
2023-10-26 15:41:48.875974: val_loss -0.8135 
2023-10-26 15:41:48.876248: Pseudo dice [0.8678, 0.9033, 0.9665, 0.6338, 0.8851] 
2023-10-26 15:41:48.876489: Epoch time: 4.27 s 
2023-10-26 15:41:49.952486:  
2023-10-26 15:41:49.952780: Epoch 214 
2023-10-26 15:41:49.953034: Current learning rate: 0.00805 
2023-10-26 15:41:54.218601: train_loss -0.8702 
2023-10-26 15:41:54.219018: val_loss -0.7965 
2023-10-26 15:41:54.219309: Pseudo dice [0.8753, 0.9024, 0.9677, 0.2945, 0.8862] 
2023-10-26 15:41:54.219548: Epoch time: 4.27 s 
2023-10-26 15:41:55.441314:  
2023-10-26 15:41:55.441671: Epoch 215 
2023-10-26 15:41:55.441930: Current learning rate: 0.00804 
2023-10-26 15:41:59.575130: train_loss -0.8783 
2023-10-26 15:41:59.575528: val_loss -0.8335 
2023-10-26 15:41:59.575795: Pseudo dice [0.8636, 0.9117, 0.9671, 0.6655, 0.8974] 
2023-10-26 15:41:59.576046: Epoch time: 4.13 s 
2023-10-26 15:42:00.654794:  
2023-10-26 15:42:00.655096: Epoch 216 
2023-10-26 15:42:00.655361: Current learning rate: 0.00803 
2023-10-26 15:42:04.969154: train_loss -0.8745 
2023-10-26 15:42:04.969551: val_loss -0.8198 
2023-10-26 15:42:04.969814: Pseudo dice [0.8609, 0.9039, 0.9655, 0.6644, 0.9034] 
2023-10-26 15:42:04.970074: Epoch time: 4.31 s 
2023-10-26 15:42:06.026804:  
2023-10-26 15:42:06.027131: Epoch 217 
2023-10-26 15:42:06.027378: Current learning rate: 0.00802 
2023-10-26 15:42:10.128918: train_loss -0.8646 
2023-10-26 15:42:10.129281: val_loss -0.8061 
2023-10-26 15:42:10.129548: Pseudo dice [0.8704, 0.9036, 0.9669, 0.4189, 0.8403] 
2023-10-26 15:42:10.129786: Epoch time: 4.1 s 
2023-10-26 15:42:11.189653:  
2023-10-26 15:42:11.189992: Epoch 218 
2023-10-26 15:42:11.190265: Current learning rate: 0.00801 
2023-10-26 15:42:15.384712: train_loss -0.8733 
2023-10-26 15:42:15.385139: val_loss -0.7993 
2023-10-26 15:42:15.385406: Pseudo dice [0.8672, 0.8992, 0.9686, 0.5664, 0.8769] 
2023-10-26 15:42:15.385641: Epoch time: 4.2 s 
2023-10-26 15:42:16.488194:  
2023-10-26 15:42:16.488572: Epoch 219 
2023-10-26 15:42:16.488892: Current learning rate: 0.00801 
2023-10-26 15:42:20.655693: train_loss -0.8756 
2023-10-26 15:42:20.657000: val_loss -0.8136 
2023-10-26 15:42:20.657298: Pseudo dice [0.8736, 0.9098, 0.968, 0.6411, 0.8417] 
2023-10-26 15:42:20.657547: Epoch time: 4.17 s 
2023-10-26 15:42:21.735764:  
2023-10-26 15:42:21.736092: Epoch 220 
2023-10-26 15:42:21.736359: Current learning rate: 0.008 
2023-10-26 15:42:25.787942: train_loss -0.8718 
2023-10-26 15:42:25.788335: val_loss -0.8256 
2023-10-26 15:42:25.788606: Pseudo dice [0.8645, 0.8996, 0.9646, 0.6299, 0.8628] 
2023-10-26 15:42:25.788856: Epoch time: 4.05 s 
2023-10-26 15:42:27.010688:  
2023-10-26 15:42:27.011041: Epoch 221 
2023-10-26 15:42:27.011310: Current learning rate: 0.00799 
2023-10-26 15:42:30.984885: train_loss -0.8713 
2023-10-26 15:42:30.985264: val_loss -0.8183 
2023-10-26 15:42:30.985557: Pseudo dice [0.8671, 0.9062, 0.9667, 0.6495, 0.8906] 
2023-10-26 15:42:30.985815: Epoch time: 3.97 s 
2023-10-26 15:42:32.057342:  
2023-10-26 15:42:32.057704: Epoch 222 
2023-10-26 15:42:32.058029: Current learning rate: 0.00798 
2023-10-26 15:42:36.237617: train_loss -0.8729 
2023-10-26 15:42:36.238009: val_loss -0.8236 
2023-10-26 15:42:36.238272: Pseudo dice [0.8685, 0.9011, 0.9652, 0.7415, 0.862] 
2023-10-26 15:42:36.238511: Epoch time: 4.18 s 
2023-10-26 15:42:37.353117:  
2023-10-26 15:42:37.353433: Epoch 223 
2023-10-26 15:42:37.353692: Current learning rate: 0.00797 
2023-10-26 15:42:41.611136: train_loss -0.8704 
2023-10-26 15:42:41.611537: val_loss -0.81 
2023-10-26 15:42:41.611793: Pseudo dice [0.8611, 0.8897, 0.9669, 0.5109, 0.9013] 
2023-10-26 15:42:41.612030: Epoch time: 4.26 s 
2023-10-26 15:42:42.682636:  
2023-10-26 15:42:42.682971: Epoch 224 
2023-10-26 15:42:42.683228: Current learning rate: 0.00796 
2023-10-26 15:42:46.874187: train_loss -0.8793 
2023-10-26 15:42:46.874568: val_loss -0.8049 
2023-10-26 15:42:46.874853: Pseudo dice [0.8678, 0.9035, 0.967, 0.5221, 0.8607] 
2023-10-26 15:42:46.875094: Epoch time: 4.19 s 
2023-10-26 15:42:47.946390:  
2023-10-26 15:42:47.946792: Epoch 225 
2023-10-26 15:42:47.947108: Current learning rate: 0.00795 
2023-10-26 15:42:52.083025: train_loss -0.8804 
2023-10-26 15:42:52.083451: val_loss -0.8243 
2023-10-26 15:42:52.083726: Pseudo dice [0.8698, 0.9098, 0.9686, 0.6905, 0.8456] 
2023-10-26 15:42:52.083976: Epoch time: 4.14 s 
2023-10-26 15:42:53.140745:  
2023-10-26 15:42:53.141082: Epoch 226 
2023-10-26 15:42:53.141352: Current learning rate: 0.00794 
2023-10-26 15:42:57.301509: train_loss -0.8778 
2023-10-26 15:42:57.301912: val_loss -0.7919 
2023-10-26 15:42:57.302181: Pseudo dice [0.8752, 0.9037, 0.9681, 0.6064, 0.8355] 
2023-10-26 15:42:57.302419: Epoch time: 4.16 s 
2023-10-26 15:42:58.546755:  
2023-10-26 15:42:58.547136: Epoch 227 
2023-10-26 15:42:58.547402: Current learning rate: 0.00793 
2023-10-26 15:43:02.747272: train_loss -0.8778 
2023-10-26 15:43:02.747670: val_loss -0.8275 
2023-10-26 15:43:02.747945: Pseudo dice [0.8608, 0.9011, 0.9678, 0.7701, 0.8703] 
2023-10-26 15:43:02.748196: Epoch time: 4.2 s 
2023-10-26 15:43:03.807059:  
2023-10-26 15:43:03.807449: Epoch 228 
2023-10-26 15:43:03.807702: Current learning rate: 0.00792 
2023-10-26 15:43:08.054861: train_loss -0.8485 
2023-10-26 15:43:08.055316: val_loss -0.813 
2023-10-26 15:43:08.055634: Pseudo dice [0.8587, 0.9002, 0.9645, 0.5562, 0.8902] 
2023-10-26 15:43:08.055897: Epoch time: 4.25 s 
2023-10-26 15:43:09.137940:  
2023-10-26 15:43:09.138262: Epoch 229 
2023-10-26 15:43:09.138512: Current learning rate: 0.00791 
2023-10-26 15:43:13.281923: train_loss -0.8597 
2023-10-26 15:43:13.282340: val_loss -0.8166 
2023-10-26 15:43:13.282653: Pseudo dice [0.8646, 0.8918, 0.9657, 0.6008, 0.8861] 
2023-10-26 15:43:13.283148: Epoch time: 4.14 s 
2023-10-26 15:43:14.335629:  
2023-10-26 15:43:14.335952: Epoch 230 
2023-10-26 15:43:14.336200: Current learning rate: 0.0079 
2023-10-26 15:43:18.426164: train_loss -0.8723 
2023-10-26 15:43:18.426565: val_loss -0.8341 
2023-10-26 15:43:18.426867: Pseudo dice [0.8669, 0.8991, 0.9662, 0.6745, 0.9251] 
2023-10-26 15:43:18.427243: Epoch time: 4.09 s 
2023-10-26 15:43:19.523173:  
2023-10-26 15:43:19.523474: Epoch 231 
2023-10-26 15:43:19.523729: Current learning rate: 0.00789 
2023-10-26 15:43:23.554091: train_loss -0.8647 
2023-10-26 15:43:23.554486: val_loss -0.8017 
2023-10-26 15:43:23.554749: Pseudo dice [0.8611, 0.8923, 0.9669, 0.6186, 0.866] 
2023-10-26 15:43:23.554994: Epoch time: 4.03 s 
2023-10-26 15:43:24.617774:  
2023-10-26 15:43:24.618091: Epoch 232 
2023-10-26 15:43:24.618379: Current learning rate: 0.00789 
2023-10-26 15:43:28.706054: train_loss -0.8615 
2023-10-26 15:43:28.706406: val_loss -0.8146 
2023-10-26 15:43:28.706673: Pseudo dice [0.8699, 0.9019, 0.9675, 0.6907, 0.8708] 
2023-10-26 15:43:28.706922: Epoch time: 4.09 s 
2023-10-26 15:43:29.780822:  
2023-10-26 15:43:29.781125: Epoch 233 
2023-10-26 15:43:29.781382: Current learning rate: 0.00788 
2023-10-26 15:43:33.890327: train_loss -0.8713 
2023-10-26 15:43:33.890726: val_loss -0.8067 
2023-10-26 15:43:33.891027: Pseudo dice [0.8681, 0.8967, 0.967, 0.6427, 0.8355] 
2023-10-26 15:43:33.891272: Epoch time: 4.11 s 
2023-10-26 15:43:35.124627:  
2023-10-26 15:43:35.124944: Epoch 234 
2023-10-26 15:43:35.125192: Current learning rate: 0.00787 
2023-10-26 15:43:39.237682: train_loss -0.8676 
2023-10-26 15:43:39.238052: val_loss -0.8236 
2023-10-26 15:43:39.238315: Pseudo dice [0.8718, 0.9112, 0.9666, 0.7696, 0.8509] 
2023-10-26 15:43:39.238562: Epoch time: 4.11 s 
2023-10-26 15:43:40.284662:  
2023-10-26 15:43:40.284992: Epoch 235 
2023-10-26 15:43:40.285234: Current learning rate: 0.00786 
2023-10-26 15:43:44.469998: train_loss -0.8634 
2023-10-26 15:43:44.470399: val_loss -0.8122 
2023-10-26 15:43:44.470669: Pseudo dice [0.8687, 0.9033, 0.9652, 0.6632, 0.858] 
2023-10-26 15:43:44.470921: Epoch time: 4.19 s 
2023-10-26 15:43:45.515279:  
2023-10-26 15:43:45.515606: Epoch 236 
2023-10-26 15:43:45.515906: Current learning rate: 0.00785 
2023-10-26 15:43:49.533897: train_loss -0.8754 
2023-10-26 15:43:49.534272: val_loss -0.8136 
2023-10-26 15:43:49.534535: Pseudo dice [0.8733, 0.9029, 0.968, 0.6364, 0.8581] 
2023-10-26 15:43:49.534762: Epoch time: 4.02 s 
2023-10-26 15:43:50.617765:  
2023-10-26 15:43:50.618079: Epoch 237 
2023-10-26 15:43:50.618334: Current learning rate: 0.00784 
2023-10-26 15:43:54.710496: train_loss -0.8698 
2023-10-26 15:43:54.710894: val_loss -0.8219 
2023-10-26 15:43:54.711156: Pseudo dice [0.8638, 0.8979, 0.9682, 0.7379, 0.8468] 
2023-10-26 15:43:54.711391: Epoch time: 4.09 s 
2023-10-26 15:43:55.762379:  
2023-10-26 15:43:55.762683: Epoch 238 
2023-10-26 15:43:55.762937: Current learning rate: 0.00783 
2023-10-26 15:43:59.978893: train_loss -0.8743 
2023-10-26 15:43:59.979250: val_loss -0.8246 
2023-10-26 15:43:59.979511: Pseudo dice [0.8685, 0.9052, 0.9657, 0.6454, 0.868] 
2023-10-26 15:43:59.979736: Epoch time: 4.22 s 
2023-10-26 15:44:01.090489:  
2023-10-26 15:44:01.090807: Epoch 239 
2023-10-26 15:44:01.091065: Current learning rate: 0.00782 
2023-10-26 15:44:05.238838: train_loss -0.8786 
2023-10-26 15:44:05.239202: val_loss -0.815 
2023-10-26 15:44:05.239459: Pseudo dice [0.8618, 0.895, 0.9641, 0.7172, 0.8944] 
2023-10-26 15:44:05.239687: Epoch time: 4.15 s 
2023-10-26 15:44:06.527102:  
2023-10-26 15:44:06.527432: Epoch 240 
2023-10-26 15:44:06.527697: Current learning rate: 0.00781 
2023-10-26 15:44:10.740383: train_loss -0.8694 
2023-10-26 15:44:10.740955: val_loss -0.829 
2023-10-26 15:44:10.741287: Pseudo dice [0.8649, 0.898, 0.9642, 0.6699, 0.9078] 
2023-10-26 15:44:10.741588: Epoch time: 4.21 s 
2023-10-26 15:44:11.812886:  
2023-10-26 15:44:11.813187: Epoch 241 
2023-10-26 15:44:11.813460: Current learning rate: 0.0078 
2023-10-26 15:44:15.979223: train_loss -0.8802 
2023-10-26 15:44:15.979869: val_loss -0.8236 
2023-10-26 15:44:15.980388: Pseudo dice [0.8671, 0.899, 0.9688, 0.639, 0.8779] 
2023-10-26 15:44:15.980786: Epoch time: 4.17 s 
2023-10-26 15:44:17.093102:  
2023-10-26 15:44:17.093417: Epoch 242 
2023-10-26 15:44:17.093664: Current learning rate: 0.00779 
2023-10-26 15:44:21.236370: train_loss -0.8762 
2023-10-26 15:44:21.236771: val_loss -0.7998 
2023-10-26 15:44:21.237049: Pseudo dice [0.8698, 0.9023, 0.9666, 0.6325, 0.8385] 
2023-10-26 15:44:21.237293: Epoch time: 4.14 s 
2023-10-26 15:44:22.311496:  
2023-10-26 15:44:22.311816: Epoch 243 
2023-10-26 15:44:22.312065: Current learning rate: 0.00778 
2023-10-26 15:44:26.472139: train_loss -0.8688 
2023-10-26 15:44:26.472545: val_loss -0.7859 
2023-10-26 15:44:26.472814: Pseudo dice [0.8578, 0.8983, 0.9645, 0.525, 0.8313] 
2023-10-26 15:44:26.473063: Epoch time: 4.16 s 
2023-10-26 15:44:27.581886:  
2023-10-26 15:44:27.582190: Epoch 244 
2023-10-26 15:44:27.582439: Current learning rate: 0.00777 
2023-10-26 15:44:31.620770: train_loss -0.8707 
2023-10-26 15:44:31.621178: val_loss -0.8074 
2023-10-26 15:44:31.621444: Pseudo dice [0.8645, 0.8958, 0.9669, 0.6705, 0.8774] 
2023-10-26 15:44:31.621680: Epoch time: 4.04 s 
2023-10-26 15:44:32.703317:  
2023-10-26 15:44:32.703622: Epoch 245 
2023-10-26 15:44:32.703881: Current learning rate: 0.00777 
2023-10-26 15:44:36.760077: train_loss -0.8566 
2023-10-26 15:44:36.760544: val_loss -0.7964 
2023-10-26 15:44:36.760854: Pseudo dice [0.864, 0.8934, 0.9673, 0.2544, 0.865] 
2023-10-26 15:44:36.761135: Epoch time: 4.06 s 
2023-10-26 15:44:37.888393:  
2023-10-26 15:44:37.888711: Epoch 246 
2023-10-26 15:44:37.888963: Current learning rate: 0.00776 
2023-10-26 15:44:41.973053: train_loss -0.8683 
2023-10-26 15:44:41.973429: val_loss -0.8094 
2023-10-26 15:44:41.973799: Pseudo dice [0.8576, 0.8927, 0.9655, 0.4627, 0.8918] 
2023-10-26 15:44:41.974046: Epoch time: 4.09 s 
2023-10-26 15:44:43.294246:  
2023-10-26 15:44:43.294553: Epoch 247 
2023-10-26 15:44:43.294804: Current learning rate: 0.00775 
2023-10-26 15:44:47.322176: train_loss -0.8685 
2023-10-26 15:44:47.322595: val_loss -0.7848 
2023-10-26 15:44:47.322886: Pseudo dice [0.8597, 0.9023, 0.9676, 0.2962, 0.8842] 
2023-10-26 15:44:47.323131: Epoch time: 4.03 s 
2023-10-26 15:44:48.400310:  
2023-10-26 15:44:48.400635: Epoch 248 
2023-10-26 15:44:48.400935: Current learning rate: 0.00774 
2023-10-26 15:44:52.405074: train_loss -0.87 
2023-10-26 15:44:52.405488: val_loss -0.7987 
2023-10-26 15:44:52.405752: Pseudo dice [0.864, 0.8968, 0.9654, 0.4914, 0.885] 
2023-10-26 15:44:52.405988: Epoch time: 4.01 s 
2023-10-26 15:44:53.477627:  
2023-10-26 15:44:53.477929: Epoch 249 
2023-10-26 15:44:53.478173: Current learning rate: 0.00773 
2023-10-26 15:44:57.475244: train_loss -0.8757 
2023-10-26 15:44:57.475782: val_loss -0.826 
2023-10-26 15:44:57.476143: Pseudo dice [0.8692, 0.909, 0.9677, 0.6272, 0.8902] 
2023-10-26 15:44:57.476485: Epoch time: 4.0 s 
2023-10-26 15:44:58.636562:  
2023-10-26 15:44:58.636886: Epoch 250 
2023-10-26 15:44:58.637138: Current learning rate: 0.00772 
2023-10-26 15:45:02.846476: train_loss -0.8781 
2023-10-26 15:45:02.846870: val_loss -0.8069 
2023-10-26 15:45:02.847140: Pseudo dice [0.864, 0.8938, 0.9674, 0.4444, 0.8894] 
2023-10-26 15:45:02.847387: Epoch time: 4.21 s 
2023-10-26 15:45:03.928771:  
2023-10-26 15:45:03.929074: Epoch 251 
2023-10-26 15:45:03.929320: Current learning rate: 0.00771 
2023-10-26 15:45:07.941222: train_loss -0.8766 
2023-10-26 15:45:07.941646: val_loss -0.817 
2023-10-26 15:45:07.941927: Pseudo dice [0.8671, 0.8941, 0.9687, 0.6307, 0.8742] 
2023-10-26 15:45:07.942194: Epoch time: 4.01 s 
2023-10-26 15:45:09.029703:  
2023-10-26 15:45:09.030026: Epoch 252 
2023-10-26 15:45:09.030275: Current learning rate: 0.0077 
2023-10-26 15:45:13.088223: train_loss -0.876 
2023-10-26 15:45:13.088665: val_loss -0.7864 
2023-10-26 15:45:13.088958: Pseudo dice [0.866, 0.8968, 0.9681, 0.5938, 0.8222] 
2023-10-26 15:45:13.089208: Epoch time: 4.06 s 
2023-10-26 15:45:14.368819:  
2023-10-26 15:45:14.369153: Epoch 253 
2023-10-26 15:45:14.369413: Current learning rate: 0.00769 
2023-10-26 15:45:18.593953: train_loss -0.8799 
2023-10-26 15:45:18.594398: val_loss -0.8262 
2023-10-26 15:45:18.594898: Pseudo dice [0.8665, 0.905, 0.9677, 0.6853, 0.8501] 
2023-10-26 15:45:18.595156: Epoch time: 4.23 s 
2023-10-26 15:45:19.698813:  
2023-10-26 15:45:19.699128: Epoch 254 
2023-10-26 15:45:19.699373: Current learning rate: 0.00768 
2023-10-26 15:45:23.769610: train_loss -0.8831 
2023-10-26 15:45:23.770012: val_loss -0.8084 
2023-10-26 15:45:23.770290: Pseudo dice [0.8604, 0.8935, 0.9677, 0.405, 0.8926] 
2023-10-26 15:45:23.770539: Epoch time: 4.07 s 
2023-10-26 15:45:24.854743:  
2023-10-26 15:45:24.855073: Epoch 255 
2023-10-26 15:45:24.855324: Current learning rate: 0.00767 
2023-10-26 15:45:28.824659: train_loss -0.8836 
2023-10-26 15:45:28.825108: val_loss -0.8169 
2023-10-26 15:45:28.825415: Pseudo dice [0.8608, 0.9022, 0.9672, 0.4382, 0.8735] 
2023-10-26 15:45:28.825675: Epoch time: 3.97 s 
2023-10-26 15:45:29.915644:  
2023-10-26 15:45:29.915972: Epoch 256 
2023-10-26 15:45:29.916224: Current learning rate: 0.00766 
2023-10-26 15:45:33.996946: train_loss -0.8704 
2023-10-26 15:45:33.997420: val_loss -0.8195 
2023-10-26 15:45:33.997698: Pseudo dice [0.8662, 0.8901, 0.9681, 0.5916, 0.8969] 
2023-10-26 15:45:33.998074: Epoch time: 4.08 s 
2023-10-26 15:45:35.144633:  
2023-10-26 15:45:35.144954: Epoch 257 
2023-10-26 15:45:35.145206: Current learning rate: 0.00765 
2023-10-26 15:45:39.118972: train_loss -0.8784 
2023-10-26 15:45:39.119354: val_loss -0.8194 
2023-10-26 15:45:39.119613: Pseudo dice [0.8685, 0.9001, 0.9671, 0.5527, 0.8938] 
2023-10-26 15:45:39.119846: Epoch time: 3.97 s 
2023-10-26 15:45:40.225416:  
2023-10-26 15:45:40.225725: Epoch 258 
2023-10-26 15:45:40.225981: Current learning rate: 0.00764 
2023-10-26 15:45:44.060338: train_loss -0.872 
2023-10-26 15:45:44.060697: val_loss -0.7982 
2023-10-26 15:45:44.060964: Pseudo dice [0.8666, 0.8933, 0.967, 0.4867, 0.8702] 
2023-10-26 15:45:44.061195: Epoch time: 3.84 s 
2023-10-26 15:45:45.313982:  
2023-10-26 15:45:45.314283: Epoch 259 
2023-10-26 15:45:45.314541: Current learning rate: 0.00764 
2023-10-26 15:45:49.251813: train_loss -0.8708 
2023-10-26 15:45:49.252360: val_loss -0.8046 
2023-10-26 15:45:49.252661: Pseudo dice [0.8668, 0.8963, 0.9678, 0.6389, 0.8547] 
2023-10-26 15:45:49.252929: Epoch time: 3.94 s 
2023-10-26 15:45:50.345178:  
2023-10-26 15:45:50.345481: Epoch 260 
2023-10-26 15:45:50.345726: Current learning rate: 0.00763 
2023-10-26 15:45:54.274706: train_loss -0.8778 
2023-10-26 15:45:54.275126: val_loss -0.8139 
2023-10-26 15:45:54.275412: Pseudo dice [0.8601, 0.8992, 0.9679, 0.4956, 0.8785] 
2023-10-26 15:45:54.275656: Epoch time: 3.93 s 
2023-10-26 15:45:55.354367:  
2023-10-26 15:45:55.354664: Epoch 261 
2023-10-26 15:45:55.354918: Current learning rate: 0.00762 
2023-10-26 15:45:59.392139: train_loss -0.8787 
2023-10-26 15:45:59.392652: val_loss -0.8253 
2023-10-26 15:45:59.392931: Pseudo dice [0.8649, 0.9085, 0.965, 0.6477, 0.8544] 
2023-10-26 15:45:59.393176: Epoch time: 4.04 s 
2023-10-26 15:46:00.482383:  
2023-10-26 15:46:00.482704: Epoch 262 
2023-10-26 15:46:00.482961: Current learning rate: 0.00761 
2023-10-26 15:46:04.646011: train_loss -0.8698 
2023-10-26 15:46:04.646450: val_loss -0.8101 
2023-10-26 15:46:04.646909: Pseudo dice [0.8678, 0.8976, 0.9661, 0.5518, 0.883] 
2023-10-26 15:46:04.647318: Epoch time: 4.16 s 
2023-10-26 15:46:05.810700:  
2023-10-26 15:46:05.811026: Epoch 263 
2023-10-26 15:46:05.811296: Current learning rate: 0.0076 
2023-10-26 15:46:09.948336: train_loss -0.8706 
2023-10-26 15:46:09.948787: val_loss -0.8305 
2023-10-26 15:46:09.949080: Pseudo dice [0.8678, 0.9036, 0.9648, 0.7542, 0.8986] 
2023-10-26 15:46:09.949324: Epoch time: 4.14 s 
2023-10-26 15:46:11.022608:  
2023-10-26 15:46:11.022909: Epoch 264 
2023-10-26 15:46:11.023154: Current learning rate: 0.00759 
2023-10-26 15:46:15.119362: train_loss -0.8749 
2023-10-26 15:46:15.119766: val_loss -0.8203 
2023-10-26 15:46:15.120040: Pseudo dice [0.8727, 0.9018, 0.968, 0.5839, 0.8847] 
2023-10-26 15:46:15.120291: Epoch time: 4.1 s 
2023-10-26 15:46:16.207011:  
2023-10-26 15:46:16.207387: Epoch 265 
2023-10-26 15:46:16.207663: Current learning rate: 0.00758 
2023-10-26 15:46:20.203797: train_loss -0.8793 
2023-10-26 15:46:20.204609: val_loss -0.8283 
2023-10-26 15:46:20.205008: Pseudo dice [0.8691, 0.9003, 0.9665, 0.5576, 0.913] 
2023-10-26 15:46:20.205274: Epoch time: 4.0 s 
2023-10-26 15:46:21.537882:  
2023-10-26 15:46:21.538192: Epoch 266 
2023-10-26 15:46:21.538447: Current learning rate: 0.00757 
2023-10-26 15:46:25.601110: train_loss -0.8788 
2023-10-26 15:46:25.601564: val_loss -0.816 
2023-10-26 15:46:25.601839: Pseudo dice [0.8675, 0.9031, 0.967, 0.6492, 0.8845] 
2023-10-26 15:46:25.602086: Epoch time: 4.06 s 
2023-10-26 15:46:26.687629:  
2023-10-26 15:46:26.687951: Epoch 267 
2023-10-26 15:46:26.688239: Current learning rate: 0.00756 
2023-10-26 15:46:30.732729: train_loss -0.8789 
2023-10-26 15:46:30.733127: val_loss -0.7993 
2023-10-26 15:46:30.733385: Pseudo dice [0.855, 0.877, 0.9644, 0.4939, 0.8745] 
2023-10-26 15:46:30.733609: Epoch time: 4.05 s 
2023-10-26 15:46:31.842124:  
2023-10-26 15:46:31.842426: Epoch 268 
2023-10-26 15:46:31.842677: Current learning rate: 0.00755 
2023-10-26 15:46:35.929457: train_loss -0.87 
2023-10-26 15:46:35.930032: val_loss -0.8169 
2023-10-26 15:46:35.930319: Pseudo dice [0.8725, 0.9011, 0.9657, 0.685, 0.8667] 
2023-10-26 15:46:35.930564: Epoch time: 4.09 s 
2023-10-26 15:46:37.034948:  
2023-10-26 15:46:37.035251: Epoch 269 
2023-10-26 15:46:37.035506: Current learning rate: 0.00754 
2023-10-26 15:46:41.083278: train_loss -0.8683 
2023-10-26 15:46:41.083825: val_loss -0.8256 
2023-10-26 15:46:41.084370: Pseudo dice [0.8723, 0.9044, 0.9663, 0.7032, 0.8732] 
2023-10-26 15:46:41.084931: Epoch time: 4.05 s 
2023-10-26 15:46:42.170495:  
2023-10-26 15:46:42.170812: Epoch 270 
2023-10-26 15:46:42.171060: Current learning rate: 0.00753 
2023-10-26 15:46:46.215595: train_loss -0.8714 
2023-10-26 15:46:46.215998: val_loss -0.8174 
2023-10-26 15:46:46.216258: Pseudo dice [0.8623, 0.9033, 0.9659, 0.5067, 0.8981] 
2023-10-26 15:46:46.216483: Epoch time: 4.05 s 
2023-10-26 15:46:47.295413:  
2023-10-26 15:46:47.295724: Epoch 271 
2023-10-26 15:46:47.295977: Current learning rate: 0.00752 
2023-10-26 15:46:51.371719: train_loss -0.8686 
2023-10-26 15:46:51.372111: val_loss -0.8204 
2023-10-26 15:46:51.372384: Pseudo dice [0.8678, 0.8992, 0.9662, 0.7484, 0.8528] 
2023-10-26 15:46:51.372609: Epoch time: 4.08 s 
2023-10-26 15:46:52.628892:  
2023-10-26 15:46:52.629248: Epoch 272 
2023-10-26 15:46:52.629570: Current learning rate: 0.00751 
2023-10-26 15:46:56.782767: train_loss -0.877 
2023-10-26 15:46:56.783169: val_loss -0.822 
2023-10-26 15:46:56.783446: Pseudo dice [0.8735, 0.8927, 0.9684, 0.5511, 0.8898] 
2023-10-26 15:46:56.783680: Epoch time: 4.15 s 
2023-10-26 15:46:57.880501:  
2023-10-26 15:46:57.880817: Epoch 273 
2023-10-26 15:46:57.881060: Current learning rate: 0.00751 
2023-10-26 15:47:01.966418: train_loss -0.8719 
2023-10-26 15:47:01.966832: val_loss -0.8398 
2023-10-26 15:47:01.967091: Pseudo dice [0.8725, 0.9099, 0.9651, 0.7547, 0.892] 
2023-10-26 15:47:01.967321: Epoch time: 4.09 s 
2023-10-26 15:47:03.062928:  
2023-10-26 15:47:03.063234: Epoch 274 
2023-10-26 15:47:03.063472: Current learning rate: 0.0075 
2023-10-26 15:47:07.197991: train_loss -0.8766 
2023-10-26 15:47:07.198364: val_loss -0.8445 
2023-10-26 15:47:07.198635: Pseudo dice [0.8645, 0.9012, 0.9675, 0.7023, 0.9075] 
2023-10-26 15:47:07.198881: Epoch time: 4.14 s 
2023-10-26 15:47:08.276529:  
2023-10-26 15:47:08.276844: Epoch 275 
2023-10-26 15:47:08.277101: Current learning rate: 0.00749 
2023-10-26 15:47:12.330769: train_loss -0.8639 
2023-10-26 15:47:12.331165: val_loss -0.7893 
2023-10-26 15:47:12.331427: Pseudo dice [0.8549, 0.8971, 0.9663, 0.0928, 0.8704] 
2023-10-26 15:47:12.331672: Epoch time: 4.05 s 
2023-10-26 15:47:13.446176:  
2023-10-26 15:47:13.446513: Epoch 276 
2023-10-26 15:47:13.446749: Current learning rate: 0.00748 
2023-10-26 15:47:17.488263: train_loss -0.8705 
2023-10-26 15:47:17.488661: val_loss -0.8282 
2023-10-26 15:47:17.488924: Pseudo dice [0.8696, 0.895, 0.9671, 0.6643, 0.8936] 
2023-10-26 15:47:17.489157: Epoch time: 4.04 s 
2023-10-26 15:47:18.592197:  
2023-10-26 15:47:18.592506: Epoch 277 
2023-10-26 15:47:18.592766: Current learning rate: 0.00747 
2023-10-26 15:47:22.700773: train_loss -0.8797 
2023-10-26 15:47:22.701244: val_loss -0.7582 
2023-10-26 15:47:22.701544: Pseudo dice [0.856, 0.9002, 0.9652, 0.0447, 0.8407] 
2023-10-26 15:47:22.701870: Epoch time: 4.11 s 
2023-10-26 15:47:23.993230:  
2023-10-26 15:47:23.993545: Epoch 278 
2023-10-26 15:47:23.993806: Current learning rate: 0.00746 
2023-10-26 15:47:28.111648: train_loss -0.8691 
2023-10-26 15:47:28.112047: val_loss -0.8215 
2023-10-26 15:47:28.112310: Pseudo dice [0.8654, 0.9039, 0.9622, 0.6983, 0.8766] 
2023-10-26 15:47:28.112544: Epoch time: 4.12 s 
2023-10-26 15:47:29.193037:  
2023-10-26 15:47:29.193359: Epoch 279 
2023-10-26 15:47:29.193613: Current learning rate: 0.00745 
2023-10-26 15:47:33.417856: train_loss -0.8763 
2023-10-26 15:47:33.418408: val_loss -0.8282 
2023-10-26 15:47:33.418746: Pseudo dice [0.8643, 0.8904, 0.9657, 0.6964, 0.8914] 
2023-10-26 15:47:33.419029: Epoch time: 4.23 s 
2023-10-26 15:47:34.659261:  
2023-10-26 15:47:34.659664: Epoch 280 
2023-10-26 15:47:34.659917: Current learning rate: 0.00744 
2023-10-26 15:47:38.698536: train_loss -0.8765 
2023-10-26 15:47:38.698963: val_loss -0.812 
2023-10-26 15:47:38.699227: Pseudo dice [0.8657, 0.9028, 0.9666, 0.68, 0.8481] 
2023-10-26 15:47:38.699479: Epoch time: 4.04 s 
2023-10-26 15:47:39.790792:  
2023-10-26 15:47:39.791091: Epoch 281 
2023-10-26 15:47:39.791347: Current learning rate: 0.00743 
2023-10-26 15:47:43.933161: train_loss -0.8784 
2023-10-26 15:47:43.933554: val_loss -0.8116 
2023-10-26 15:47:43.933815: Pseudo dice [0.8664, 0.8884, 0.9673, 0.5608, 0.8714] 
2023-10-26 15:47:43.934051: Epoch time: 4.14 s 
2023-10-26 15:47:45.022210:  
2023-10-26 15:47:45.022513: Epoch 282 
2023-10-26 15:47:45.022757: Current learning rate: 0.00742 
2023-10-26 15:47:49.128510: train_loss -0.8664 
2023-10-26 15:47:49.128935: val_loss -0.8009 
2023-10-26 15:47:49.129211: Pseudo dice [0.8573, 0.8939, 0.9632, 0.5418, 0.8962] 
2023-10-26 15:47:49.129440: Epoch time: 4.11 s 
2023-10-26 15:47:50.213528:  
2023-10-26 15:47:50.213840: Epoch 283 
2023-10-26 15:47:50.214099: Current learning rate: 0.00741 
2023-10-26 15:47:54.297919: train_loss -0.8602 
2023-10-26 15:47:54.298341: val_loss -0.8189 
2023-10-26 15:47:54.298609: Pseudo dice [0.8719, 0.8984, 0.9667, 0.765, 0.8628] 
2023-10-26 15:47:54.298850: Epoch time: 4.08 s 
2023-10-26 15:47:55.407934:  
2023-10-26 15:47:55.408242: Epoch 284 
2023-10-26 15:47:55.408500: Current learning rate: 0.0074 
2023-10-26 15:47:59.447405: train_loss -0.8676 
2023-10-26 15:47:59.447816: val_loss -0.8046 
2023-10-26 15:47:59.448091: Pseudo dice [0.867, 0.8955, 0.967, 0.5568, 0.8287] 
2023-10-26 15:47:59.448325: Epoch time: 4.04 s 
2023-10-26 15:48:00.710023:  
2023-10-26 15:48:00.710340: Epoch 285 
2023-10-26 15:48:00.710611: Current learning rate: 0.00739 
2023-10-26 15:48:04.922861: train_loss -0.8712 
2023-10-26 15:48:04.923396: val_loss -0.8122 
2023-10-26 15:48:04.923826: Pseudo dice [0.8665, 0.898, 0.9677, 0.655, 0.8741] 
2023-10-26 15:48:04.924089: Epoch time: 4.21 s 
2023-10-26 15:48:06.016226:  
2023-10-26 15:48:06.016541: Epoch 286 
2023-10-26 15:48:06.016788: Current learning rate: 0.00738 
2023-10-26 15:48:10.032178: train_loss -0.8768 
2023-10-26 15:48:10.032568: val_loss -0.8263 
2023-10-26 15:48:10.032831: Pseudo dice [0.8611, 0.8943, 0.9662, 0.6293, 0.9079] 
2023-10-26 15:48:10.033085: Epoch time: 4.02 s 
2023-10-26 15:48:11.136612:  
2023-10-26 15:48:11.136925: Epoch 287 
2023-10-26 15:48:11.137186: Current learning rate: 0.00738 
2023-10-26 15:48:15.213743: train_loss -0.8714 
2023-10-26 15:48:15.214176: val_loss -0.7969 
2023-10-26 15:48:15.214448: Pseudo dice [0.862, 0.8805, 0.9654, 0.4582, 0.876] 
2023-10-26 15:48:15.214689: Epoch time: 4.08 s 
2023-10-26 15:48:16.334198:  
2023-10-26 15:48:16.334554: Epoch 288 
2023-10-26 15:48:16.334882: Current learning rate: 0.00737 
2023-10-26 15:48:20.319631: train_loss -0.8689 
2023-10-26 15:48:20.320076: val_loss -0.8382 
2023-10-26 15:48:20.320335: Pseudo dice [0.859, 0.9048, 0.966, 0.774, 0.9081] 
2023-10-26 15:48:20.320572: Epoch time: 3.99 s 
2023-10-26 15:48:21.427144:  
2023-10-26 15:48:21.427445: Epoch 289 
2023-10-26 15:48:21.427687: Current learning rate: 0.00736 
2023-10-26 15:48:25.353828: train_loss -0.8793 
2023-10-26 15:48:25.354238: val_loss -0.8247 
2023-10-26 15:48:25.354499: Pseudo dice [0.8692, 0.8981, 0.9667, 0.6128, 0.9031] 
2023-10-26 15:48:25.354735: Epoch time: 3.93 s 
2023-10-26 15:48:26.462214:  
2023-10-26 15:48:26.462516: Epoch 290 
2023-10-26 15:48:26.462763: Current learning rate: 0.00735 
2023-10-26 15:48:30.552693: train_loss -0.8831 
2023-10-26 15:48:30.553136: val_loss -0.8223 
2023-10-26 15:48:30.553406: Pseudo dice [0.8629, 0.9032, 0.965, 0.7386, 0.8624] 
2023-10-26 15:48:30.553648: Epoch time: 4.09 s 
2023-10-26 15:48:31.834760:  
2023-10-26 15:48:31.835094: Epoch 291 
2023-10-26 15:48:31.835393: Current learning rate: 0.00734 
2023-10-26 15:48:35.889244: train_loss -0.8757 
2023-10-26 15:48:35.889786: val_loss -0.7934 
2023-10-26 15:48:35.890146: Pseudo dice [0.8659, 0.8997, 0.9638, 0.4767, 0.8605] 
2023-10-26 15:48:35.890427: Epoch time: 4.06 s 
2023-10-26 15:48:37.060449:  
2023-10-26 15:48:37.060736: Epoch 292 
2023-10-26 15:48:37.060989: Current learning rate: 0.00733 
2023-10-26 15:48:41.101707: train_loss -0.878 
2023-10-26 15:48:41.102097: val_loss -0.8206 
2023-10-26 15:48:41.102352: Pseudo dice [0.8694, 0.8936, 0.9669, 0.4245, 0.9099] 
2023-10-26 15:48:41.102584: Epoch time: 4.04 s 
2023-10-26 15:48:42.219937:  
2023-10-26 15:48:42.220261: Epoch 293 
2023-10-26 15:48:42.220522: Current learning rate: 0.00732 
2023-10-26 15:48:46.222216: train_loss -0.8703 
2023-10-26 15:48:46.222919: val_loss -0.8356 
2023-10-26 15:48:46.223318: Pseudo dice [0.8737, 0.906, 0.968, 0.7605, 0.8771] 
2023-10-26 15:48:46.223620: Epoch time: 4.0 s 
2023-10-26 15:48:47.337664:  
2023-10-26 15:48:47.338028: Epoch 294 
2023-10-26 15:48:47.338275: Current learning rate: 0.00731 
2023-10-26 15:48:51.381608: train_loss -0.8759 
2023-10-26 15:48:51.382178: val_loss -0.8318 
2023-10-26 15:48:51.382647: Pseudo dice [0.8669, 0.9046, 0.966, 0.696, 0.9023] 
2023-10-26 15:48:51.382941: Epoch time: 4.04 s 
2023-10-26 15:48:52.488271:  
2023-10-26 15:48:52.488583: Epoch 295 
2023-10-26 15:48:52.488829: Current learning rate: 0.0073 
2023-10-26 15:48:56.513894: train_loss -0.8754 
2023-10-26 15:48:56.514279: val_loss -0.8243 
2023-10-26 15:48:56.514542: Pseudo dice [0.8662, 0.8958, 0.9661, 0.6455, 0.8737] 
2023-10-26 15:48:56.514769: Epoch time: 4.03 s 
2023-10-26 15:48:57.614447:  
2023-10-26 15:48:57.614744: Epoch 296 
2023-10-26 15:48:57.615003: Current learning rate: 0.00729 
2023-10-26 15:49:01.756153: train_loss -0.8733 
2023-10-26 15:49:01.756721: val_loss -0.8253 
2023-10-26 15:49:01.757080: Pseudo dice [0.8698, 0.8992, 0.9666, 0.6218, 0.8904] 
2023-10-26 15:49:01.757358: Epoch time: 4.14 s 
2023-10-26 15:49:03.053473:  
2023-10-26 15:49:03.053782: Epoch 297 
2023-10-26 15:49:03.054104: Current learning rate: 0.00728 
2023-10-26 15:49:07.088716: train_loss -0.8763 
2023-10-26 15:49:07.089199: val_loss -0.8369 
2023-10-26 15:49:07.089587: Pseudo dice [0.8611, 0.8954, 0.9681, 0.691, 0.9151] 
2023-10-26 15:49:07.089859: Epoch time: 4.04 s 
2023-10-26 15:49:08.284177:  
2023-10-26 15:49:08.284481: Epoch 298 
2023-10-26 15:49:08.284747: Current learning rate: 0.00727 
2023-10-26 15:49:12.349952: train_loss -0.8797 
2023-10-26 15:49:12.350362: val_loss -0.8169 
2023-10-26 15:49:12.350618: Pseudo dice [0.8644, 0.897, 0.969, 0.6393, 0.8709] 
2023-10-26 15:49:12.350868: Epoch time: 4.07 s 
2023-10-26 15:49:13.457781:  
2023-10-26 15:49:13.458080: Epoch 299 
2023-10-26 15:49:13.458332: Current learning rate: 0.00726 
2023-10-26 15:49:17.562403: train_loss -0.8859 
2023-10-26 15:49:17.562767: val_loss -0.8239 
2023-10-26 15:49:17.563031: Pseudo dice [0.8618, 0.8944, 0.9682, 0.6958, 0.8867] 
2023-10-26 15:49:17.563254: Epoch time: 4.11 s 
2023-10-26 15:49:18.778184:  
2023-10-26 15:49:18.778525: Epoch 300 
2023-10-26 15:49:18.778808: Current learning rate: 0.00725 
2023-10-26 15:49:22.844582: train_loss -0.8811 
2023-10-26 15:49:22.844970: val_loss -0.8081 
2023-10-26 15:49:22.845235: Pseudo dice [0.8631, 0.907, 0.9668, 0.644, 0.8336] 
2023-10-26 15:49:22.845462: Epoch time: 4.07 s 
2023-10-26 15:49:23.946451:  
2023-10-26 15:49:23.946754: Epoch 301 
2023-10-26 15:49:23.947034: Current learning rate: 0.00724 
2023-10-26 15:49:27.785096: train_loss -0.8824 
2023-10-26 15:49:27.785461: val_loss -0.7998 
2023-10-26 15:49:27.785738: Pseudo dice [0.8672, 0.8994, 0.9659, 0.5343, 0.8605] 
2023-10-26 15:49:27.785980: Epoch time: 3.84 s 
2023-10-26 15:49:28.888309:  
2023-10-26 15:49:28.888597: Epoch 302 
2023-10-26 15:49:28.888841: Current learning rate: 0.00724 
2023-10-26 15:49:32.842788: train_loss -0.8832 
2023-10-26 15:49:32.843333: val_loss -0.832 
2023-10-26 15:49:32.843701: Pseudo dice [0.8631, 0.8969, 0.968, 0.7637, 0.883] 
2023-10-26 15:49:32.844066: Epoch time: 3.96 s 
2023-10-26 15:49:34.168047:  
2023-10-26 15:49:34.168331: Epoch 303 
2023-10-26 15:49:34.168576: Current learning rate: 0.00723 
2023-10-26 15:49:38.302494: train_loss -0.887 
2023-10-26 15:49:38.302858: val_loss -0.802 
2023-10-26 15:49:38.303123: Pseudo dice [0.8692, 0.9024, 0.9672, 0.6279, 0.8525] 
2023-10-26 15:49:38.303358: Epoch time: 4.14 s 
2023-10-26 15:49:39.439936:  
2023-10-26 15:49:39.440257: Epoch 304 
2023-10-26 15:49:39.440520: Current learning rate: 0.00722 
2023-10-26 15:49:43.501692: train_loss -0.8882 
2023-10-26 15:49:43.502143: val_loss -0.8276 
2023-10-26 15:49:43.502428: Pseudo dice [0.8698, 0.905, 0.9677, 0.6769, 0.8729] 
2023-10-26 15:49:43.502669: Epoch time: 4.06 s 
2023-10-26 15:49:44.599861:  
2023-10-26 15:49:44.600173: Epoch 305 
2023-10-26 15:49:44.600420: Current learning rate: 0.00721 
2023-10-26 15:49:48.611101: train_loss -0.891 
2023-10-26 15:49:48.611487: val_loss -0.8209 
2023-10-26 15:49:48.611755: Pseudo dice [0.8654, 0.9016, 0.9662, 0.7398, 0.8685] 
2023-10-26 15:49:48.611997: Epoch time: 4.01 s 
2023-10-26 15:49:49.725148:  
2023-10-26 15:49:49.725451: Epoch 306 
2023-10-26 15:49:49.725699: Current learning rate: 0.0072 
2023-10-26 15:49:53.904654: train_loss -0.8854 
2023-10-26 15:49:53.905478: val_loss -0.8253 
2023-10-26 15:49:53.905787: Pseudo dice [0.8577, 0.8995, 0.9662, 0.6284, 0.8907] 
2023-10-26 15:49:53.906031: Epoch time: 4.18 s 
2023-10-26 15:49:54.999802:  
2023-10-26 15:49:55.000118: Epoch 307 
2023-10-26 15:49:55.000358: Current learning rate: 0.00719 
2023-10-26 15:49:59.030928: train_loss -0.886 
2023-10-26 15:49:59.031338: val_loss -0.8105 
2023-10-26 15:49:59.031587: Pseudo dice [0.8664, 0.904, 0.9676, 0.7386, 0.8429] 
2023-10-26 15:49:59.031823: Epoch time: 4.03 s 
2023-10-26 15:50:00.161473:  
2023-10-26 15:50:00.178784: Epoch 308 
2023-10-26 15:50:00.179048: Current learning rate: 0.00718 
2023-10-26 15:50:04.207406: train_loss -0.8841 
2023-10-26 15:50:04.207787: val_loss -0.8264 
2023-10-26 15:50:04.208057: Pseudo dice [0.8676, 0.9039, 0.9671, 0.7671, 0.8778] 
2023-10-26 15:50:04.208291: Epoch time: 4.05 s 
2023-10-26 15:50:05.487409:  
2023-10-26 15:50:05.487718: Epoch 309 
2023-10-26 15:50:05.487961: Current learning rate: 0.00717 
2023-10-26 15:50:09.659740: train_loss -0.8854 
2023-10-26 15:50:09.660194: val_loss -0.8262 
2023-10-26 15:50:09.660457: Pseudo dice [0.8707, 0.9065, 0.968, 0.6843, 0.8629] 
2023-10-26 15:50:09.660696: Epoch time: 4.17 s 
2023-10-26 15:50:10.759452:  
2023-10-26 15:50:10.759758: Epoch 310 
2023-10-26 15:50:10.760012: Current learning rate: 0.00716 
2023-10-26 15:50:14.811078: train_loss -0.8788 
2023-10-26 15:50:14.811472: val_loss -0.7992 
2023-10-26 15:50:14.811729: Pseudo dice [0.873, 0.9018, 0.9676, 0.7295, 0.8354] 
2023-10-26 15:50:14.811949: Epoch time: 4.05 s 
2023-10-26 15:50:15.944688:  
2023-10-26 15:50:15.944985: Epoch 311 
2023-10-26 15:50:15.945236: Current learning rate: 0.00715 
2023-10-26 15:50:20.030521: train_loss -0.8823 
2023-10-26 15:50:20.030992: val_loss -0.801 
2023-10-26 15:50:20.031271: Pseudo dice [0.859, 0.9027, 0.9641, 0.5943, 0.8571] 
2023-10-26 15:50:20.031508: Epoch time: 4.09 s 
2023-10-26 15:50:21.122535:  
2023-10-26 15:50:21.122832: Epoch 312 
2023-10-26 15:50:21.123080: Current learning rate: 0.00714 
2023-10-26 15:50:25.244309: train_loss -0.8797 
2023-10-26 15:50:25.244685: val_loss -0.8101 
2023-10-26 15:50:25.244951: Pseudo dice [0.8656, 0.9003, 0.9662, 0.6748, 0.8536] 
2023-10-26 15:50:25.245183: Epoch time: 4.12 s 
2023-10-26 15:50:26.348497:  
2023-10-26 15:50:26.348787: Epoch 313 
2023-10-26 15:50:26.349040: Current learning rate: 0.00713 
2023-10-26 15:50:30.407137: train_loss -0.8868 
2023-10-26 15:50:30.407554: val_loss -0.7942 
2023-10-26 15:50:30.407823: Pseudo dice [0.8743, 0.9105, 0.9687, 0.623, 0.8166] 
2023-10-26 15:50:30.408077: Epoch time: 4.06 s 
2023-10-26 15:50:31.526516:  
2023-10-26 15:50:31.526819: Epoch 314 
2023-10-26 15:50:31.527070: Current learning rate: 0.00712 
2023-10-26 15:50:35.659230: train_loss -0.8794 
2023-10-26 15:50:35.659622: val_loss -0.7808 
2023-10-26 15:50:35.659901: Pseudo dice [0.8569, 0.9045, 0.967, 0.7211, 0.8276] 
2023-10-26 15:50:35.660131: Epoch time: 4.13 s 
2023-10-26 15:50:36.841218:  
2023-10-26 15:50:36.841515: Epoch 315 
2023-10-26 15:50:36.841755: Current learning rate: 0.00711 
2023-10-26 15:50:40.940503: train_loss -0.8858 
2023-10-26 15:50:40.940927: val_loss -0.8289 
2023-10-26 15:50:40.941212: Pseudo dice [0.8615, 0.8925, 0.9656, 0.6997, 0.9006] 
2023-10-26 15:50:40.941453: Epoch time: 4.1 s 
2023-10-26 15:50:42.298453:  
2023-10-26 15:50:42.298780: Epoch 316 
2023-10-26 15:50:42.299046: Current learning rate: 0.0071 
2023-10-26 15:50:46.517916: train_loss -0.8703 
2023-10-26 15:50:46.518430: val_loss -0.7882 
2023-10-26 15:50:46.518754: Pseudo dice [0.8647, 0.9011, 0.9681, 0.5599, 0.8398] 
2023-10-26 15:50:46.519006: Epoch time: 4.22 s 
2023-10-26 15:50:47.633948:  
2023-10-26 15:50:47.634407: Epoch 317 
2023-10-26 15:50:47.634647: Current learning rate: 0.0071 
2023-10-26 15:50:51.892426: train_loss -0.8574 
2023-10-26 15:50:51.892840: val_loss -0.8352 
2023-10-26 15:50:51.893114: Pseudo dice [0.8618, 0.8939, 0.9659, 0.799, 0.9102] 
2023-10-26 15:50:51.893359: Epoch time: 4.26 s 
2023-10-26 15:50:53.004978:  
2023-10-26 15:50:53.005287: Epoch 318 
2023-10-26 15:50:53.005541: Current learning rate: 0.00709 
2023-10-26 15:50:57.052626: train_loss -0.8737 
2023-10-26 15:50:57.053010: val_loss -0.8246 
2023-10-26 15:50:57.053272: Pseudo dice [0.8652, 0.9006, 0.9643, 0.652, 0.8902] 
2023-10-26 15:50:57.053503: Epoch time: 4.05 s 
2023-10-26 15:50:58.146394:  
2023-10-26 15:50:58.146688: Epoch 319 
2023-10-26 15:50:58.146937: Current learning rate: 0.00708 
2023-10-26 15:51:02.002964: train_loss -0.8721 
2023-10-26 15:51:02.003371: val_loss -0.8033 
2023-10-26 15:51:02.003642: Pseudo dice [0.8699, 0.9017, 0.966, 0.5797, 0.8795] 
2023-10-26 15:51:02.003889: Epoch time: 3.86 s 
2023-10-26 15:51:03.126175:  
2023-10-26 15:51:03.126474: Epoch 320 
2023-10-26 15:51:03.126721: Current learning rate: 0.00707 
2023-10-26 15:51:07.240118: train_loss -0.8746 
2023-10-26 15:51:07.240505: val_loss -0.8183 
2023-10-26 15:51:07.240865: Pseudo dice [0.8556, 0.9021, 0.9652, 0.6827, 0.8776] 
2023-10-26 15:51:07.241201: Epoch time: 4.11 s 
2023-10-26 15:51:08.344188:  
2023-10-26 15:51:08.344481: Epoch 321 
2023-10-26 15:51:08.344739: Current learning rate: 0.00706 
2023-10-26 15:51:12.392193: train_loss -0.8751 
2023-10-26 15:51:12.392600: val_loss -0.8168 
2023-10-26 15:51:12.392881: Pseudo dice [0.8635, 0.8981, 0.9635, 0.5261, 0.9039] 
2023-10-26 15:51:12.393120: Epoch time: 4.05 s 
2023-10-26 15:51:13.704330:  
2023-10-26 15:51:13.704655: Epoch 322 
2023-10-26 15:51:13.704944: Current learning rate: 0.00705 
2023-10-26 15:51:17.764366: train_loss -0.8806 
2023-10-26 15:51:17.764735: val_loss -0.8189 
2023-10-26 15:51:17.765007: Pseudo dice [0.8685, 0.8988, 0.9657, 0.6076, 0.8834] 
2023-10-26 15:51:17.765243: Epoch time: 4.06 s 
2023-10-26 15:51:18.871148:  
2023-10-26 15:51:18.871490: Epoch 323 
2023-10-26 15:51:18.871732: Current learning rate: 0.00704 
2023-10-26 15:51:23.022619: train_loss -0.8826 
2023-10-26 15:51:23.023116: val_loss -0.8305 
2023-10-26 15:51:23.023399: Pseudo dice [0.8628, 0.9057, 0.9649, 0.5702, 0.8939] 
2023-10-26 15:51:23.023640: Epoch time: 4.15 s 
2023-10-26 15:51:24.130468:  
2023-10-26 15:51:24.130793: Epoch 324 
2023-10-26 15:51:24.131055: Current learning rate: 0.00703 
2023-10-26 15:51:28.222076: train_loss -0.8793 
2023-10-26 15:51:28.222484: val_loss -0.8101 
2023-10-26 15:51:28.222754: Pseudo dice [0.8631, 0.9041, 0.9616, 0.6616, 0.8658] 
2023-10-26 15:51:28.223231: Epoch time: 4.09 s 
2023-10-26 15:51:29.333410:  
2023-10-26 15:51:29.333712: Epoch 325 
2023-10-26 15:51:29.333971: Current learning rate: 0.00702 
2023-10-26 15:51:33.487007: train_loss -0.8785 
2023-10-26 15:51:33.487434: val_loss -0.8178 
2023-10-26 15:51:33.487691: Pseudo dice [0.8685, 0.902, 0.9645, 0.7135, 0.9036] 
2023-10-26 15:51:33.487931: Epoch time: 4.15 s 
2023-10-26 15:51:34.634457:  
2023-10-26 15:51:34.634829: Epoch 326 
2023-10-26 15:51:34.635165: Current learning rate: 0.00701 
2023-10-26 15:51:38.651545: train_loss -0.8792 
2023-10-26 15:51:38.651968: val_loss -0.8087 
2023-10-26 15:51:38.652248: Pseudo dice [0.8717, 0.9072, 0.9633, 0.6247, 0.8475] 
2023-10-26 15:51:38.652551: Epoch time: 4.02 s 
2023-10-26 15:51:39.765998:  
2023-10-26 15:51:39.766331: Epoch 327 
2023-10-26 15:51:39.766589: Current learning rate: 0.007 
2023-10-26 15:51:43.893350: train_loss -0.887 
2023-10-26 15:51:43.893759: val_loss -0.827 
2023-10-26 15:51:43.894060: Pseudo dice [0.8688, 0.9014, 0.9664, 0.638, 0.8871] 
2023-10-26 15:51:43.894315: Epoch time: 4.13 s 
2023-10-26 15:51:45.194429:  
2023-10-26 15:51:45.194781: Epoch 328 
2023-10-26 15:51:45.195097: Current learning rate: 0.00699 
2023-10-26 15:51:49.318501: train_loss -0.8651 
2023-10-26 15:51:49.318897: val_loss -0.8048 
2023-10-26 15:51:49.319160: Pseudo dice [0.8673, 0.8925, 0.9676, 0.6787, 0.8784] 
2023-10-26 15:51:49.319403: Epoch time: 4.12 s 
2023-10-26 15:51:50.412258:  
2023-10-26 15:51:50.412571: Epoch 329 
2023-10-26 15:51:50.412822: Current learning rate: 0.00698 
2023-10-26 15:51:54.650099: train_loss -0.8707 
2023-10-26 15:51:54.650507: val_loss -0.8176 
2023-10-26 15:51:54.650776: Pseudo dice [0.8604, 0.8938, 0.9674, 0.7153, 0.8738] 
2023-10-26 15:51:54.651028: Epoch time: 4.24 s 
2023-10-26 15:51:55.741687:  
2023-10-26 15:51:55.741976: Epoch 330 
2023-10-26 15:51:55.742223: Current learning rate: 0.00697 
2023-10-26 15:51:59.744417: train_loss -0.8747 
2023-10-26 15:51:59.745097: val_loss -0.8475 
2023-10-26 15:51:59.745401: Pseudo dice [0.8592, 0.8985, 0.9666, 0.7482, 0.9224] 
2023-10-26 15:51:59.745717: Epoch time: 4.0 s 
2023-10-26 15:52:00.835162:  
2023-10-26 15:52:00.835466: Epoch 331 
2023-10-26 15:52:00.835714: Current learning rate: 0.00696 
2023-10-26 15:52:04.736746: train_loss -0.8797 
2023-10-26 15:52:04.737155: val_loss -0.8093 
2023-10-26 15:52:04.737429: Pseudo dice [0.8677, 0.9057, 0.9648, 0.5893, 0.8564] 
2023-10-26 15:52:04.737670: Epoch time: 3.9 s 
2023-10-26 15:52:05.826159:  
2023-10-26 15:52:05.826464: Epoch 332 
2023-10-26 15:52:05.826716: Current learning rate: 0.00696 
2023-10-26 15:52:09.869318: train_loss -0.8784 
2023-10-26 15:52:09.869812: val_loss -0.8286 
2023-10-26 15:52:09.870190: Pseudo dice [0.8658, 0.8993, 0.9666, 0.5037, 0.914] 
2023-10-26 15:52:09.870470: Epoch time: 4.04 s 
2023-10-26 15:52:10.989700:  
2023-10-26 15:52:10.990064: Epoch 333 
2023-10-26 15:52:10.990330: Current learning rate: 0.00695 
2023-10-26 15:52:15.034829: train_loss -0.8636 
2023-10-26 15:52:15.035529: val_loss -0.7569 
2023-10-26 15:52:15.035845: Pseudo dice [0.8361, 0.8965, 0.9627, 0.0604, 0.8511] 
2023-10-26 15:52:15.036133: Epoch time: 4.05 s 
2023-10-26 15:52:16.294595:  
2023-10-26 15:52:16.294897: Epoch 334 
2023-10-26 15:52:16.295145: Current learning rate: 0.00694 
2023-10-26 15:52:20.506300: train_loss -0.8619 
2023-10-26 15:52:20.506748: val_loss -0.8034 
2023-10-26 15:52:20.507146: Pseudo dice [0.8551, 0.8908, 0.9657, 0.5181, 0.859] 
2023-10-26 15:52:20.507475: Epoch time: 4.21 s 
2023-10-26 15:52:21.616672:  
2023-10-26 15:52:21.616996: Epoch 335 
2023-10-26 15:52:21.617243: Current learning rate: 0.00693 
2023-10-26 15:52:25.831509: train_loss -0.8701 
2023-10-26 15:52:25.831942: val_loss -0.8095 
2023-10-26 15:52:25.832214: Pseudo dice [0.87, 0.9018, 0.9658, 0.4325, 0.8633] 
2023-10-26 15:52:25.832454: Epoch time: 4.22 s 
2023-10-26 15:52:26.937747:  
2023-10-26 15:52:26.938133: Epoch 336 
2023-10-26 15:52:26.938379: Current learning rate: 0.00692 
2023-10-26 15:52:31.025647: train_loss -0.8745 
2023-10-26 15:52:31.026040: val_loss -0.8246 
2023-10-26 15:52:31.026294: Pseudo dice [0.8642, 0.8947, 0.9676, 0.7285, 0.8952] 
2023-10-26 15:52:31.026516: Epoch time: 4.09 s 
2023-10-26 15:52:32.150401:  
2023-10-26 15:52:32.150704: Epoch 337 
2023-10-26 15:52:32.150946: Current learning rate: 0.00691 
2023-10-26 15:52:36.255470: train_loss -0.8808 
2023-10-26 15:52:36.255860: val_loss -0.8195 
2023-10-26 15:52:36.256241: Pseudo dice [0.8662, 0.8963, 0.9684, 0.503, 0.8912] 
2023-10-26 15:52:36.256594: Epoch time: 4.11 s 
2023-10-26 15:52:37.383309:  
2023-10-26 15:52:37.383623: Epoch 338 
2023-10-26 15:52:37.383883: Current learning rate: 0.0069 
2023-10-26 15:52:41.499556: train_loss -0.8797 
2023-10-26 15:52:41.499966: val_loss -0.7889 
2023-10-26 15:52:41.500228: Pseudo dice [0.8627, 0.8945, 0.968, 0.4701, 0.8777] 
2023-10-26 15:52:41.500465: Epoch time: 4.12 s 
2023-10-26 15:52:42.615806:  
2023-10-26 15:52:42.616101: Epoch 339 
2023-10-26 15:52:42.616349: Current learning rate: 0.00689 
2023-10-26 15:52:46.712622: train_loss -0.886 
2023-10-26 15:52:46.713027: val_loss -0.8363 
2023-10-26 15:52:46.713303: Pseudo dice [0.867, 0.9056, 0.9653, 0.6724, 0.8945] 
2023-10-26 15:52:46.713534: Epoch time: 4.1 s 
2023-10-26 15:52:48.009670:  
2023-10-26 15:52:48.009974: Epoch 340 
2023-10-26 15:52:48.010222: Current learning rate: 0.00688 
2023-10-26 15:52:52.287379: train_loss -0.8832 
2023-10-26 15:52:52.287803: val_loss -0.8413 
2023-10-26 15:52:52.288080: Pseudo dice [0.8652, 0.9025, 0.9666, 0.6454, 0.9146] 
2023-10-26 15:52:52.288324: Epoch time: 4.28 s 
2023-10-26 15:52:53.409892:  
2023-10-26 15:52:53.410196: Epoch 341 
2023-10-26 15:52:53.410439: Current learning rate: 0.00687 
2023-10-26 15:52:57.507771: train_loss -0.887 
2023-10-26 15:52:57.508149: val_loss -0.8279 
2023-10-26 15:52:57.508416: Pseudo dice [0.8621, 0.8981, 0.966, 0.6992, 0.8823] 
2023-10-26 15:52:57.508643: Epoch time: 4.1 s 
2023-10-26 15:52:58.625076:  
2023-10-26 15:52:58.625382: Epoch 342 
2023-10-26 15:52:58.625628: Current learning rate: 0.00686 
2023-10-26 15:53:02.866018: train_loss -0.8844 
2023-10-26 15:53:02.866427: val_loss -0.8356 
2023-10-26 15:53:02.866696: Pseudo dice [0.8684, 0.9014, 0.9659, 0.5862, 0.8939] 
2023-10-26 15:53:02.866945: Epoch time: 4.24 s 
2023-10-26 15:53:03.978997:  
2023-10-26 15:53:03.979300: Epoch 343 
2023-10-26 15:53:03.979546: Current learning rate: 0.00685 
2023-10-26 15:53:08.113558: train_loss -0.884 
2023-10-26 15:53:08.113962: val_loss -0.8262 
2023-10-26 15:53:08.114230: Pseudo dice [0.8572, 0.9041, 0.9659, 0.7022, 0.8668] 
2023-10-26 15:53:08.114473: Epoch time: 4.14 s 
2023-10-26 15:53:09.219620:  
2023-10-26 15:53:09.219932: Epoch 344 
2023-10-26 15:53:09.220187: Current learning rate: 0.00684 
2023-10-26 15:53:13.398301: train_loss -0.8878 
2023-10-26 15:53:13.398751: val_loss -0.8108 
2023-10-26 15:53:13.399037: Pseudo dice [0.8618, 0.9037, 0.9677, 0.6111, 0.8652] 
2023-10-26 15:53:13.399269: Epoch time: 4.18 s 
2023-10-26 15:53:14.510060:  
2023-10-26 15:53:14.510355: Epoch 345 
2023-10-26 15:53:14.510609: Current learning rate: 0.00683 
2023-10-26 15:53:18.648701: train_loss -0.8737 
2023-10-26 15:53:18.649114: val_loss -0.7946 
2023-10-26 15:53:18.649394: Pseudo dice [0.8643, 0.8987, 0.9654, 0.5634, 0.8384] 
2023-10-26 15:53:18.649631: Epoch time: 4.14 s 
2023-10-26 15:53:19.918262:  
2023-10-26 15:53:19.918564: Epoch 346 
2023-10-26 15:53:19.918804: Current learning rate: 0.00682 
2023-10-26 15:53:24.197573: train_loss -0.8772 
2023-10-26 15:53:24.197994: val_loss -0.7876 
2023-10-26 15:53:24.198259: Pseudo dice [0.8569, 0.882, 0.9671, 0.4862, 0.8993] 
2023-10-26 15:53:24.198487: Epoch time: 4.28 s 
2023-10-26 15:53:25.307727:  
2023-10-26 15:53:25.308047: Epoch 347 
2023-10-26 15:53:25.308306: Current learning rate: 0.00681 
2023-10-26 15:53:29.507556: train_loss -0.8719 
2023-10-26 15:53:29.507928: val_loss -0.8084 
2023-10-26 15:53:29.508299: Pseudo dice [0.8639, 0.8962, 0.9688, 0.449, 0.866] 
2023-10-26 15:53:29.508862: Epoch time: 4.2 s 
2023-10-26 15:53:30.610619:  
2023-10-26 15:53:30.610920: Epoch 348 
2023-10-26 15:53:30.611180: Current learning rate: 0.0068 
2023-10-26 15:53:34.875017: train_loss -0.8762 
2023-10-26 15:53:34.875573: val_loss -0.8087 
2023-10-26 15:53:34.875863: Pseudo dice [0.8632, 0.8974, 0.9681, 0.5423, 0.8885] 
2023-10-26 15:53:34.876103: Epoch time: 4.26 s 
2023-10-26 15:53:35.991348:  
2023-10-26 15:53:35.991654: Epoch 349 
2023-10-26 15:53:35.991900: Current learning rate: 0.0068 
2023-10-26 15:53:40.068470: train_loss -0.884 
2023-10-26 15:53:40.068959: val_loss -0.8327 
2023-10-26 15:53:40.069346: Pseudo dice [0.8632, 0.9024, 0.9677, 0.5812, 0.9069] 
2023-10-26 15:53:40.069625: Epoch time: 4.08 s 
2023-10-26 15:53:41.254135:  
2023-10-26 15:53:41.254441: Epoch 350 
2023-10-26 15:53:41.254687: Current learning rate: 0.00679 
2023-10-26 15:53:45.422844: train_loss -0.8876 
2023-10-26 15:53:45.423243: val_loss -0.7996 
2023-10-26 15:53:45.423512: Pseudo dice [0.8638, 0.9025, 0.9683, 0.5531, 0.8409] 
2023-10-26 15:53:45.423740: Epoch time: 4.17 s 
2023-10-26 15:53:46.530387:  
2023-10-26 15:53:46.530735: Epoch 351 
2023-10-26 15:53:46.531060: Current learning rate: 0.00678 
2023-10-26 15:53:50.715452: train_loss -0.8847 
2023-10-26 15:53:50.715910: val_loss -0.8176 
2023-10-26 15:53:50.716236: Pseudo dice [0.866, 0.8957, 0.9689, 0.6794, 0.8983] 
2023-10-26 15:53:50.716515: Epoch time: 4.19 s 
2023-10-26 15:53:51.967086:  
2023-10-26 15:53:51.967387: Epoch 352 
2023-10-26 15:53:51.967637: Current learning rate: 0.00677 
2023-10-26 15:53:56.191833: train_loss -0.8781 
2023-10-26 15:53:56.192209: val_loss -0.8095 
2023-10-26 15:53:56.192484: Pseudo dice [0.8649, 0.8983, 0.9678, 0.5898, 0.8621] 
2023-10-26 15:53:56.192718: Epoch time: 4.23 s 
2023-10-26 15:53:57.348493:  
2023-10-26 15:53:57.348805: Epoch 353 
2023-10-26 15:53:57.349062: Current learning rate: 0.00676 
2023-10-26 15:54:01.444468: train_loss -0.8835 
2023-10-26 15:54:01.444931: val_loss -0.7928 
2023-10-26 15:54:01.445293: Pseudo dice [0.8669, 0.9065, 0.9672, 0.548, 0.8282] 
2023-10-26 15:54:01.445562: Epoch time: 4.1 s 
2023-10-26 15:54:02.550612:  
2023-10-26 15:54:02.550920: Epoch 354 
2023-10-26 15:54:02.551167: Current learning rate: 0.00675 
2023-10-26 15:54:06.507996: train_loss -0.889 
2023-10-26 15:54:06.508355: val_loss -0.8046 
2023-10-26 15:54:06.508612: Pseudo dice [0.8588, 0.8976, 0.9675, 0.5104, 0.9076] 
2023-10-26 15:54:06.508844: Epoch time: 3.96 s 
2023-10-26 15:54:07.606144:  
2023-10-26 15:54:07.606451: Epoch 355 
2023-10-26 15:54:07.606700: Current learning rate: 0.00674 
2023-10-26 15:54:11.584455: train_loss -0.8842 
2023-10-26 15:54:11.584857: val_loss -0.8205 
2023-10-26 15:54:11.585130: Pseudo dice [0.8671, 0.9077, 0.9682, 0.6581, 0.8487] 
2023-10-26 15:54:11.585365: Epoch time: 3.98 s 
2023-10-26 15:54:12.718796:  
2023-10-26 15:54:12.719115: Epoch 356 
2023-10-26 15:54:12.719371: Current learning rate: 0.00673 
2023-10-26 15:54:16.982906: train_loss -0.886 
2023-10-26 15:54:16.983311: val_loss -0.8223 
2023-10-26 15:54:16.983833: Pseudo dice [0.8615, 0.9074, 0.9667, 0.6358, 0.8628] 
2023-10-26 15:54:16.984174: Epoch time: 4.26 s 
2023-10-26 15:54:18.093486:  
2023-10-26 15:54:18.093824: Epoch 357 
2023-10-26 15:54:18.094078: Current learning rate: 0.00672 
2023-10-26 15:54:22.266246: train_loss -0.8871 
2023-10-26 15:54:22.266675: val_loss -0.8135 
2023-10-26 15:54:22.266968: Pseudo dice [0.8598, 0.9072, 0.9663, 0.6412, 0.8586] 
2023-10-26 15:54:22.267219: Epoch time: 4.17 s 
2023-10-26 15:54:23.557849:  
2023-10-26 15:54:23.558173: Epoch 358 
2023-10-26 15:54:23.558423: Current learning rate: 0.00671 
2023-10-26 15:54:27.763255: train_loss -0.8754 
2023-10-26 15:54:27.763635: val_loss -0.8231 
2023-10-26 15:54:27.763908: Pseudo dice [0.8657, 0.8958, 0.9658, 0.7495, 0.8694] 
2023-10-26 15:54:27.764135: Epoch time: 4.21 s 
2023-10-26 15:54:28.868589:  
2023-10-26 15:54:28.868906: Epoch 359 
2023-10-26 15:54:28.869161: Current learning rate: 0.0067 
2023-10-26 15:54:32.975030: train_loss -0.8818 
2023-10-26 15:54:32.975456: val_loss -0.8145 
2023-10-26 15:54:32.975721: Pseudo dice [0.8674, 0.9027, 0.9658, 0.6239, 0.8728] 
2023-10-26 15:54:32.975960: Epoch time: 4.11 s 
2023-10-26 15:54:34.125554:  
2023-10-26 15:54:34.125852: Epoch 360 
2023-10-26 15:54:34.126109: Current learning rate: 0.00669 
2023-10-26 15:54:38.046921: train_loss -0.8768 
2023-10-26 15:54:38.047289: val_loss -0.7996 
2023-10-26 15:54:38.047544: Pseudo dice [0.8629, 0.8956, 0.9683, 0.4304, 0.8891] 
2023-10-26 15:54:38.047778: Epoch time: 3.92 s 
2023-10-26 15:54:39.184042:  
2023-10-26 15:54:39.184346: Epoch 361 
2023-10-26 15:54:39.184592: Current learning rate: 0.00668 
2023-10-26 15:54:43.340917: train_loss -0.8824 
2023-10-26 15:54:43.341335: val_loss -0.7997 
2023-10-26 15:54:43.341621: Pseudo dice [0.8647, 0.9015, 0.9671, 0.559, 0.8535] 
2023-10-26 15:54:43.341888: Epoch time: 4.16 s 
2023-10-26 15:54:44.455785:  
2023-10-26 15:54:44.456138: Epoch 362 
2023-10-26 15:54:44.456399: Current learning rate: 0.00667 
2023-10-26 15:54:48.324210: train_loss -0.8769 
2023-10-26 15:54:48.324637: val_loss -0.806 
2023-10-26 15:54:48.324911: Pseudo dice [0.8655, 0.8997, 0.9653, 0.6496, 0.8599] 
2023-10-26 15:54:48.325153: Epoch time: 3.87 s 
2023-10-26 15:54:49.435260:  
2023-10-26 15:54:49.435571: Epoch 363 
2023-10-26 15:54:49.435819: Current learning rate: 0.00666 
2023-10-26 15:54:53.600413: train_loss -0.8841 
2023-10-26 15:54:53.601223: val_loss -0.8275 
2023-10-26 15:54:53.601638: Pseudo dice [0.8641, 0.9042, 0.9674, 0.7519, 0.8774] 
2023-10-26 15:54:53.601891: Epoch time: 4.17 s 
2023-10-26 15:54:54.876088:  
2023-10-26 15:54:54.876385: Epoch 364 
2023-10-26 15:54:54.876632: Current learning rate: 0.00665 
2023-10-26 15:54:58.971244: train_loss -0.8867 
2023-10-26 15:54:58.971685: val_loss -0.827 
2023-10-26 15:54:58.971977: Pseudo dice [0.8549, 0.898, 0.9694, 0.7514, 0.8756] 
2023-10-26 15:54:58.972229: Epoch time: 4.1 s 
2023-10-26 15:55:00.082347:  
2023-10-26 15:55:00.082658: Epoch 365 
2023-10-26 15:55:00.082916: Current learning rate: 0.00665 
2023-10-26 15:55:04.232212: train_loss -0.8875 
2023-10-26 15:55:04.232617: val_loss -0.8096 
2023-10-26 15:55:04.232898: Pseudo dice [0.8701, 0.9005, 0.9682, 0.6456, 0.8658] 
2023-10-26 15:55:04.233146: Epoch time: 4.15 s 
2023-10-26 15:55:05.411015:  
2023-10-26 15:55:05.411326: Epoch 366 
2023-10-26 15:55:05.411588: Current learning rate: 0.00664 
2023-10-26 15:55:09.555517: train_loss -0.8857 
2023-10-26 15:55:09.555978: val_loss -0.8219 
2023-10-26 15:55:09.556343: Pseudo dice [0.8628, 0.8953, 0.9681, 0.6766, 0.8964] 
2023-10-26 15:55:09.556653: Epoch time: 4.15 s 
2023-10-26 15:55:10.673217:  
2023-10-26 15:55:10.673521: Epoch 367 
2023-10-26 15:55:10.673774: Current learning rate: 0.00663 
2023-10-26 15:55:14.843796: train_loss -0.8824 
2023-10-26 15:55:14.844157: val_loss -0.7892 
2023-10-26 15:55:14.844421: Pseudo dice [0.8635, 0.8954, 0.9688, 0.6127, 0.8141] 
2023-10-26 15:55:14.844658: Epoch time: 4.17 s 
2023-10-26 15:55:15.947124:  
2023-10-26 15:55:15.947438: Epoch 368 
2023-10-26 15:55:15.947699: Current learning rate: 0.00662 
2023-10-26 15:55:20.004374: train_loss -0.8724 
2023-10-26 15:55:20.004766: val_loss -0.7865 
2023-10-26 15:55:20.005032: Pseudo dice [0.8476, 0.8851, 0.9663, 0.15, 0.8852] 
2023-10-26 15:55:20.005257: Epoch time: 4.06 s 
2023-10-26 15:55:21.115239:  
2023-10-26 15:55:21.115548: Epoch 369 
2023-10-26 15:55:21.115791: Current learning rate: 0.00661 
2023-10-26 15:55:25.287948: train_loss -0.8702 
2023-10-26 15:55:25.288434: val_loss -0.8241 
2023-10-26 15:55:25.288791: Pseudo dice [0.8716, 0.9048, 0.9667, 0.5996, 0.8851] 
2023-10-26 15:55:25.289138: Epoch time: 4.17 s 
2023-10-26 15:55:26.612166:  
2023-10-26 15:55:26.612608: Epoch 370 
2023-10-26 15:55:26.612870: Current learning rate: 0.0066 
2023-10-26 15:55:30.837853: train_loss -0.8804 
2023-10-26 15:55:30.838308: val_loss -0.8044 
2023-10-26 15:55:30.838587: Pseudo dice [0.8647, 0.9001, 0.9658, 0.635, 0.8583] 
2023-10-26 15:55:30.838855: Epoch time: 4.23 s 
2023-10-26 15:55:32.033359:  
2023-10-26 15:55:32.033727: Epoch 371 
2023-10-26 15:55:32.034002: Current learning rate: 0.00659 
2023-10-26 15:55:36.041367: train_loss -0.8876 
2023-10-26 15:55:36.041756: val_loss -0.8151 
2023-10-26 15:55:36.042028: Pseudo dice [0.8628, 0.9037, 0.9655, 0.5942, 0.8898] 
2023-10-26 15:55:36.042257: Epoch time: 4.01 s 
2023-10-26 15:55:37.167320:  
2023-10-26 15:55:37.167623: Epoch 372 
2023-10-26 15:55:37.168014: Current learning rate: 0.00658 
2023-10-26 15:55:41.299280: train_loss -0.8771 
2023-10-26 15:55:41.299703: val_loss -0.8224 
2023-10-26 15:55:41.300042: Pseudo dice [0.8578, 0.8925, 0.9679, 0.7033, 0.8851] 
2023-10-26 15:55:41.300464: Epoch time: 4.13 s 
2023-10-26 15:55:42.489559:  
2023-10-26 15:55:42.489879: Epoch 373 
2023-10-26 15:55:42.490135: Current learning rate: 0.00657 
2023-10-26 15:55:46.529514: train_loss -0.879 
2023-10-26 15:55:46.529928: val_loss -0.8147 
2023-10-26 15:55:46.530195: Pseudo dice [0.8666, 0.9011, 0.9653, 0.5213, 0.8487] 
2023-10-26 15:55:46.530426: Epoch time: 4.04 s 
2023-10-26 15:55:47.639278:  
2023-10-26 15:55:47.639602: Epoch 374 
2023-10-26 15:55:47.639855: Current learning rate: 0.00656 
2023-10-26 15:55:51.759199: train_loss -0.8732 
2023-10-26 15:55:51.759709: val_loss -0.8351 
2023-10-26 15:55:51.760159: Pseudo dice [0.8656, 0.8901, 0.9663, 0.7125, 0.9049] 
2023-10-26 15:55:51.760481: Epoch time: 4.12 s 
2023-10-26 15:55:52.902155:  
2023-10-26 15:55:52.902448: Epoch 375 
2023-10-26 15:55:52.902687: Current learning rate: 0.00655 
2023-10-26 15:55:56.999840: train_loss -0.8649 
2023-10-26 15:55:57.000314: val_loss -0.8235 
2023-10-26 15:55:57.000864: Pseudo dice [0.8663, 0.9044, 0.9651, 0.6888, 0.8776] 
2023-10-26 15:55:57.001127: Epoch time: 4.1 s 
2023-10-26 15:55:58.295346:  
2023-10-26 15:55:58.295719: Epoch 376 
2023-10-26 15:55:58.296044: Current learning rate: 0.00654 
2023-10-26 15:56:02.355927: train_loss -0.8696 
2023-10-26 15:56:02.356406: val_loss -0.8027 
2023-10-26 15:56:02.356687: Pseudo dice [0.8741, 0.9019, 0.9663, 0.7099, 0.8379] 
2023-10-26 15:56:02.356943: Epoch time: 4.06 s 
2023-10-26 15:56:03.479069:  
2023-10-26 15:56:03.479424: Epoch 377 
2023-10-26 15:56:03.479734: Current learning rate: 0.00653 
2023-10-26 15:56:07.510566: train_loss -0.8635 
2023-10-26 15:56:07.510959: val_loss -0.8112 
2023-10-26 15:56:07.511226: Pseudo dice [0.8687, 0.901, 0.9655, 0.5004, 0.869] 
2023-10-26 15:56:07.511458: Epoch time: 4.03 s 
2023-10-26 15:56:08.688590:  
2023-10-26 15:56:08.688898: Epoch 378 
2023-10-26 15:56:08.689146: Current learning rate: 0.00652 
2023-10-26 15:56:12.742327: train_loss -0.8753 
2023-10-26 15:56:12.742737: val_loss -0.8383 
2023-10-26 15:56:12.743016: Pseudo dice [0.8636, 0.9007, 0.9668, 0.7152, 0.8982] 
2023-10-26 15:56:12.743270: Epoch time: 4.05 s 
2023-10-26 15:56:13.861996:  
2023-10-26 15:56:13.862312: Epoch 379 
2023-10-26 15:56:13.862574: Current learning rate: 0.00651 
2023-10-26 15:56:18.000717: train_loss -0.8825 
2023-10-26 15:56:18.001105: val_loss -0.7721 
2023-10-26 15:56:18.001390: Pseudo dice [0.8704, 0.9093, 0.9674, 0.745, 0.8334] 
2023-10-26 15:56:18.001641: Epoch time: 4.14 s 
2023-10-26 15:56:19.128364:  
2023-10-26 15:56:19.128665: Epoch 380 
2023-10-26 15:56:19.128920: Current learning rate: 0.0065 
2023-10-26 15:56:23.348096: train_loss -0.8832 
2023-10-26 15:56:23.348504: val_loss -0.796 
2023-10-26 15:56:23.348771: Pseudo dice [0.8565, 0.894, 0.9686, 0.696, 0.875] 
2023-10-26 15:56:23.349007: Epoch time: 4.22 s 
2023-10-26 15:56:24.466072:  
2023-10-26 15:56:24.466406: Epoch 381 
2023-10-26 15:56:24.466718: Current learning rate: 0.00649 
2023-10-26 15:56:28.651655: train_loss -0.8815 
2023-10-26 15:56:28.652060: val_loss -0.8168 
2023-10-26 15:56:28.652324: Pseudo dice [0.8565, 0.8992, 0.9667, 0.6938, 0.8614] 
2023-10-26 15:56:28.652552: Epoch time: 4.19 s 
2023-10-26 15:56:30.010541:  
2023-10-26 15:56:30.010845: Epoch 382 
2023-10-26 15:56:30.011098: Current learning rate: 0.00648 
2023-10-26 15:56:34.051404: train_loss -0.8806 
2023-10-26 15:56:34.051822: val_loss -0.8053 
2023-10-26 15:56:34.052096: Pseudo dice [0.8632, 0.8942, 0.9666, 0.377, 0.8906] 
2023-10-26 15:56:34.052349: Epoch time: 4.04 s 
2023-10-26 15:56:35.187656:  
2023-10-26 15:56:35.187965: Epoch 383 
2023-10-26 15:56:35.188212: Current learning rate: 0.00648 
2023-10-26 15:56:39.253238: train_loss -0.8788 
2023-10-26 15:56:39.253990: val_loss -0.8295 
2023-10-26 15:56:39.254260: Pseudo dice [0.8629, 0.9033, 0.9643, 0.717, 0.892] 
2023-10-26 15:56:39.254508: Epoch time: 4.07 s 
2023-10-26 15:56:40.401283:  
2023-10-26 15:56:40.401592: Epoch 384 
2023-10-26 15:56:40.401850: Current learning rate: 0.00647 
2023-10-26 15:56:44.281855: train_loss -0.8817 
2023-10-26 15:56:44.282279: val_loss -0.8168 
2023-10-26 15:56:44.282562: Pseudo dice [0.8694, 0.9022, 0.9685, 0.7301, 0.8738] 
2023-10-26 15:56:44.282799: Epoch time: 3.88 s 
2023-10-26 15:56:45.440174:  
2023-10-26 15:56:45.440507: Epoch 385 
2023-10-26 15:56:45.440759: Current learning rate: 0.00646 
2023-10-26 15:56:49.454855: train_loss -0.8843 
2023-10-26 15:56:49.455251: val_loss -0.8201 
2023-10-26 15:56:49.455514: Pseudo dice [0.8615, 0.889, 0.9682, 0.5347, 0.8912] 
2023-10-26 15:56:49.455755: Epoch time: 4.02 s 
2023-10-26 15:56:50.593353:  
2023-10-26 15:56:50.593659: Epoch 386 
2023-10-26 15:56:50.593920: Current learning rate: 0.00645 
2023-10-26 15:56:54.630065: train_loss -0.8673 
2023-10-26 15:56:54.630574: val_loss -0.8163 
2023-10-26 15:56:54.630982: Pseudo dice [0.8674, 0.8997, 0.9675, 0.5432, 0.8803] 
2023-10-26 15:56:54.631283: Epoch time: 4.04 s 
2023-10-26 15:56:56.016809:  
2023-10-26 15:56:56.017128: Epoch 387 
2023-10-26 15:56:56.017374: Current learning rate: 0.00644 
2023-10-26 15:56:59.990463: train_loss -0.8775 
2023-10-26 15:56:59.990949: val_loss -0.7973 
2023-10-26 15:56:59.991218: Pseudo dice [0.8711, 0.8994, 0.9662, 0.6132, 0.8358] 
2023-10-26 15:56:59.991457: Epoch time: 3.97 s 
2023-10-26 15:57:01.128606:  
2023-10-26 15:57:01.128923: Epoch 388 
2023-10-26 15:57:01.129183: Current learning rate: 0.00643 
2023-10-26 15:57:05.236459: train_loss -0.881 
2023-10-26 15:57:05.236931: val_loss -0.8144 
2023-10-26 15:57:05.237361: Pseudo dice [0.8487, 0.8847, 0.967, 0.5221, 0.8948] 
2023-10-26 15:57:05.237677: Epoch time: 4.11 s 
2023-10-26 15:57:06.379922:  
2023-10-26 15:57:06.380230: Epoch 389 
2023-10-26 15:57:06.380481: Current learning rate: 0.00642 
2023-10-26 15:57:10.542741: train_loss -0.8779 
2023-10-26 15:57:10.543148: val_loss -0.825 
2023-10-26 15:57:10.543422: Pseudo dice [0.8672, 0.8949, 0.9667, 0.748, 0.8932] 
2023-10-26 15:57:10.543659: Epoch time: 4.16 s 
2023-10-26 15:57:11.684368:  
2023-10-26 15:57:11.684706: Epoch 390 
2023-10-26 15:57:11.684973: Current learning rate: 0.00641 
2023-10-26 15:57:15.844846: train_loss -0.8792 
2023-10-26 15:57:15.845320: val_loss -0.7944 
2023-10-26 15:57:15.845685: Pseudo dice [0.8645, 0.9028, 0.9687, 0.7216, 0.8423] 
2023-10-26 15:57:15.846001: Epoch time: 4.16 s 
2023-10-26 15:57:17.127466:  
2023-10-26 15:57:17.127778: Epoch 391 
2023-10-26 15:57:17.128035: Current learning rate: 0.0064 
2023-10-26 15:57:21.143056: train_loss -0.8766 
2023-10-26 15:57:21.143456: val_loss -0.8368 
2023-10-26 15:57:21.143724: Pseudo dice [0.8644, 0.9003, 0.9677, 0.7474, 0.9009] 
2023-10-26 15:57:21.143964: Epoch time: 4.02 s 
2023-10-26 15:57:22.291959:  
2023-10-26 15:57:22.292249: Epoch 392 
2023-10-26 15:57:22.292500: Current learning rate: 0.00639 
2023-10-26 15:57:26.328792: train_loss -0.8851 
2023-10-26 15:57:26.329245: val_loss -0.8252 
2023-10-26 15:57:26.329516: Pseudo dice [0.8622, 0.8989, 0.9682, 0.6971, 0.8789] 
2023-10-26 15:57:26.329775: Epoch time: 4.04 s 
2023-10-26 15:57:27.647343:  
2023-10-26 15:57:27.647661: Epoch 393 
2023-10-26 15:57:27.647915: Current learning rate: 0.00638 
2023-10-26 15:57:31.799175: train_loss -0.8843 
2023-10-26 15:57:31.799570: val_loss -0.8185 
2023-10-26 15:57:31.799833: Pseudo dice [0.8663, 0.8951, 0.9677, 0.7684, 0.8562] 
2023-10-26 15:57:31.800068: Epoch time: 4.15 s 
2023-10-26 15:57:32.931713:  
2023-10-26 15:57:32.932017: Epoch 394 
2023-10-26 15:57:32.932268: Current learning rate: 0.00637 
2023-10-26 15:57:36.910340: train_loss -0.8883 
2023-10-26 15:57:36.910881: val_loss -0.8089 
2023-10-26 15:57:36.911246: Pseudo dice [0.8598, 0.9029, 0.9667, 0.6915, 0.8549] 
2023-10-26 15:57:36.911569: Epoch time: 3.98 s 
2023-10-26 15:57:38.050576:  
2023-10-26 15:57:38.050889: Epoch 395 
2023-10-26 15:57:38.051151: Current learning rate: 0.00636 
2023-10-26 15:57:42.050567: train_loss -0.8826 
2023-10-26 15:57:42.050979: val_loss -0.8234 
2023-10-26 15:57:42.051248: Pseudo dice [0.8644, 0.9072, 0.9668, 0.6318, 0.8719] 
2023-10-26 15:57:42.051669: Epoch time: 4.0 s 
2023-10-26 15:57:43.185017:  
2023-10-26 15:57:43.185330: Epoch 396 
2023-10-26 15:57:43.185575: Current learning rate: 0.00635 
2023-10-26 15:57:47.257496: train_loss -0.8846 
2023-10-26 15:57:47.257920: val_loss -0.8183 
2023-10-26 15:57:47.258188: Pseudo dice [0.8551, 0.9004, 0.9654, 0.6084, 0.8685] 
2023-10-26 15:57:47.258430: Epoch time: 4.07 s 
2023-10-26 15:57:48.404354:  
2023-10-26 15:57:48.404649: Epoch 397 
2023-10-26 15:57:48.404904: Current learning rate: 0.00634 
2023-10-26 15:57:52.544222: train_loss -0.8774 
2023-10-26 15:57:52.544606: val_loss -0.8108 
2023-10-26 15:57:52.544877: Pseudo dice [0.8618, 0.9085, 0.9665, 0.6787, 0.8614] 
2023-10-26 15:57:52.545119: Epoch time: 4.14 s 
2023-10-26 15:57:53.690638:  
2023-10-26 15:57:53.690963: Epoch 398 
2023-10-26 15:57:53.691219: Current learning rate: 0.00633 
2023-10-26 15:57:57.781938: train_loss -0.8843 
2023-10-26 15:57:57.782345: val_loss -0.818 
2023-10-26 15:57:57.782616: Pseudo dice [0.867, 0.897, 0.965, 0.6678, 0.8584] 
2023-10-26 15:57:57.782843: Epoch time: 4.09 s 
2023-10-26 15:57:59.124446:  
2023-10-26 15:57:59.124750: Epoch 399 
2023-10-26 15:57:59.125005: Current learning rate: 0.00632 
2023-10-26 15:58:03.151617: train_loss -0.8851 
2023-10-26 15:58:03.152040: val_loss -0.819 
2023-10-26 15:58:03.152311: Pseudo dice [0.8654, 0.9035, 0.9682, 0.6974, 0.8528] 
2023-10-26 15:58:03.152554: Epoch time: 4.03 s 
2023-10-26 15:58:04.403687:  
2023-10-26 15:58:04.404022: Epoch 400 
2023-10-26 15:58:04.404277: Current learning rate: 0.00631 
2023-10-26 15:58:08.492398: train_loss -0.8919 
2023-10-26 15:58:08.492775: val_loss -0.8239 
2023-10-26 15:58:08.493052: Pseudo dice [0.8631, 0.9007, 0.9679, 0.7231, 0.9099] 
2023-10-26 15:58:08.493280: Epoch time: 4.09 s 
2023-10-26 15:58:09.625427:  
2023-10-26 15:58:09.625735: Epoch 401 
2023-10-26 15:58:09.625986: Current learning rate: 0.0063 
2023-10-26 15:58:13.642291: train_loss -0.8812 
2023-10-26 15:58:13.642712: val_loss -0.8087 
2023-10-26 15:58:13.642977: Pseudo dice [0.8643, 0.9024, 0.9686, 0.5626, 0.8625] 
2023-10-26 15:58:13.643213: Epoch time: 4.02 s 
2023-10-26 15:58:14.784335:  
2023-10-26 15:58:14.784681: Epoch 402 
2023-10-26 15:58:14.784943: Current learning rate: 0.0063 
2023-10-26 15:58:18.640922: train_loss -0.8845 
2023-10-26 15:58:18.641336: val_loss -0.8349 
2023-10-26 15:58:18.641598: Pseudo dice [0.8667, 0.8993, 0.9681, 0.7149, 0.9038] 
2023-10-26 15:58:18.641841: Epoch time: 3.86 s 
2023-10-26 15:58:19.798571:  
2023-10-26 15:58:19.798976: Epoch 403 
2023-10-26 15:58:19.799300: Current learning rate: 0.00629 
2023-10-26 15:58:23.865979: train_loss -0.8875 
2023-10-26 15:58:23.866354: val_loss -0.8212 
2023-10-26 15:58:23.866643: Pseudo dice [0.8657, 0.9061, 0.968, 0.7545, 0.8439] 
2023-10-26 15:58:23.866897: Epoch time: 4.07 s 
2023-10-26 15:58:25.034271:  
2023-10-26 15:58:25.034577: Epoch 404 
2023-10-26 15:58:25.034828: Current learning rate: 0.00628 
2023-10-26 15:58:29.130699: train_loss -0.8877 
2023-10-26 15:58:29.131138: val_loss -0.8252 
2023-10-26 15:58:29.131415: Pseudo dice [0.8635, 0.8985, 0.9676, 0.7081, 0.8816] 
2023-10-26 15:58:29.131742: Epoch time: 4.1 s 
2023-10-26 15:58:30.448538:  
2023-10-26 15:58:30.448864: Epoch 405 
2023-10-26 15:58:30.449121: Current learning rate: 0.00627 
2023-10-26 15:58:34.658028: train_loss -0.8834 
2023-10-26 15:58:34.658401: val_loss -0.8164 
2023-10-26 15:58:34.658677: Pseudo dice [0.8677, 0.9, 0.9679, 0.6476, 0.8701] 
2023-10-26 15:58:34.658931: Epoch time: 4.21 s 
2023-10-26 15:58:35.794412:  
2023-10-26 15:58:35.794721: Epoch 406 
2023-10-26 15:58:35.794971: Current learning rate: 0.00626 
2023-10-26 15:58:39.869992: train_loss -0.8865 
2023-10-26 15:58:39.870480: val_loss -0.8251 
2023-10-26 15:58:39.871016: Pseudo dice [0.8635, 0.906, 0.9665, 0.7539, 0.878] 
2023-10-26 15:58:39.871296: Epoch time: 4.08 s 
2023-10-26 15:58:39.871595: Yayy! New best EMA pseudo Dice: 0.8574 
2023-10-26 15:58:41.109461:  
2023-10-26 15:58:41.109761: Epoch 407 
2023-10-26 15:58:41.110015: Current learning rate: 0.00625 
2023-10-26 15:58:45.193537: train_loss -0.8877 
2023-10-26 15:58:45.193947: val_loss -0.8139 
2023-10-26 15:58:45.194229: Pseudo dice [0.8625, 0.8934, 0.9688, 0.6518, 0.854] 
2023-10-26 15:58:45.194467: Epoch time: 4.08 s 
2023-10-26 15:58:46.333352:  
2023-10-26 15:58:46.333658: Epoch 408 
2023-10-26 15:58:46.333919: Current learning rate: 0.00624 
2023-10-26 15:58:50.301484: train_loss -0.8887 
2023-10-26 15:58:50.301891: val_loss -0.8213 
2023-10-26 15:58:50.302151: Pseudo dice [0.8661, 0.897, 0.9678, 0.7084, 0.873] 
2023-10-26 15:58:50.302407: Epoch time: 3.97 s 
2023-10-26 15:58:51.442281:  
2023-10-26 15:58:51.442587: Epoch 409 
2023-10-26 15:58:51.442828: Current learning rate: 0.00623 
2023-10-26 15:58:55.417321: train_loss -0.8902 
2023-10-26 15:58:55.417852: val_loss -0.8194 
2023-10-26 15:58:55.418184: Pseudo dice [0.8673, 0.9077, 0.9693, 0.7333, 0.8488] 
2023-10-26 15:58:55.418467: Epoch time: 3.98 s 
2023-10-26 15:58:55.418719: Yayy! New best EMA pseudo Dice: 0.8577 
2023-10-26 15:58:56.839118:  
2023-10-26 15:58:56.839404: Epoch 410 
2023-10-26 15:58:56.839644: Current learning rate: 0.00622 
2023-10-26 15:59:00.917439: train_loss -0.889 
2023-10-26 15:59:00.917899: val_loss -0.8146 
2023-10-26 15:59:00.918178: Pseudo dice [0.8674, 0.8963, 0.968, 0.6145, 0.879] 
2023-10-26 15:59:00.918425: Epoch time: 4.08 s 
2023-10-26 15:59:02.011145:  
2023-10-26 15:59:02.011435: Epoch 411 
2023-10-26 15:59:02.011671: Current learning rate: 0.00621 
2023-10-26 15:59:06.144574: train_loss -0.8908 
2023-10-26 15:59:06.145194: val_loss -0.8029 
2023-10-26 15:59:06.145559: Pseudo dice [0.8622, 0.9017, 0.9677, 0.5973, 0.8838] 
2023-10-26 15:59:06.145827: Epoch time: 4.13 s 
2023-10-26 15:59:07.238986:  
2023-10-26 15:59:07.239287: Epoch 412 
2023-10-26 15:59:07.239550: Current learning rate: 0.0062 
2023-10-26 15:59:11.210674: train_loss -0.8929 
2023-10-26 15:59:11.211095: val_loss -0.8346 
2023-10-26 15:59:11.211475: Pseudo dice [0.8625, 0.8994, 0.9666, 0.6577, 0.9094] 
2023-10-26 15:59:11.211775: Epoch time: 3.97 s 
2023-10-26 15:59:12.290168:  
2023-10-26 15:59:12.290466: Epoch 413 
2023-10-26 15:59:12.290709: Current learning rate: 0.00619 
2023-10-26 15:59:16.413548: train_loss -0.8871 
2023-10-26 15:59:16.413972: val_loss -0.8284 
2023-10-26 15:59:16.414243: Pseudo dice [0.8653, 0.9023, 0.967, 0.6683, 0.8832] 
2023-10-26 15:59:16.414494: Epoch time: 4.12 s 
2023-10-26 15:59:17.516516:  
2023-10-26 15:59:17.516804: Epoch 414 
2023-10-26 15:59:17.517051: Current learning rate: 0.00618 
2023-10-26 15:59:21.490701: train_loss -0.8851 
2023-10-26 15:59:21.491390: val_loss -0.7945 
2023-10-26 15:59:21.491811: Pseudo dice [0.8632, 0.898, 0.9689, 0.5818, 0.8254] 
2023-10-26 15:59:21.492189: Epoch time: 3.97 s 
2023-10-26 15:59:22.579512:  
2023-10-26 15:59:22.579805: Epoch 415 
2023-10-26 15:59:22.580060: Current learning rate: 0.00617 
2023-10-26 15:59:26.660574: train_loss -0.889 
2023-10-26 15:59:26.661186: val_loss -0.8296 
2023-10-26 15:59:26.661702: Pseudo dice [0.8571, 0.8938, 0.968, 0.7333, 0.8901] 
2023-10-26 15:59:26.662318: Epoch time: 4.08 s 
2023-10-26 15:59:27.852589:  
2023-10-26 15:59:27.852889: Epoch 416 
2023-10-26 15:59:27.853136: Current learning rate: 0.00616 
2023-10-26 15:59:31.999350: train_loss -0.8875 
2023-10-26 15:59:31.999757: val_loss -0.8012 
2023-10-26 15:59:32.000034: Pseudo dice [0.8695, 0.9059, 0.966, 0.6596, 0.8409] 
2023-10-26 15:59:32.000267: Epoch time: 4.15 s 
2023-10-26 15:59:33.313680:  
2023-10-26 15:59:33.313999: Epoch 417 
2023-10-26 15:59:33.314246: Current learning rate: 0.00615 
2023-10-26 15:59:37.482573: train_loss -0.884 
2023-10-26 15:59:37.483249: val_loss -0.8051 
2023-10-26 15:59:37.483516: Pseudo dice [0.8651, 0.908, 0.9662, 0.5462, 0.8847] 
2023-10-26 15:59:37.483741: Epoch time: 4.17 s 
2023-10-26 15:59:38.606598:  
2023-10-26 15:59:38.606939: Epoch 418 
2023-10-26 15:59:38.607199: Current learning rate: 0.00614 
2023-10-26 15:59:42.805064: train_loss -0.8826 
2023-10-26 15:59:42.805835: val_loss -0.7985 
2023-10-26 15:59:42.806144: Pseudo dice [0.8585, 0.8939, 0.9654, 0.3777, 0.879] 
2023-10-26 15:59:42.806395: Epoch time: 4.2 s 
2023-10-26 15:59:43.936962:  
2023-10-26 15:59:43.937259: Epoch 419 
2023-10-26 15:59:43.937699: Current learning rate: 0.00613 
2023-10-26 15:59:48.055264: train_loss -0.8805 
2023-10-26 15:59:48.056036: val_loss -0.8292 
2023-10-26 15:59:48.056315: Pseudo dice [0.8677, 0.9099, 0.9674, 0.7225, 0.8504] 
2023-10-26 15:59:48.056546: Epoch time: 4.12 s 
2023-10-26 15:59:49.173678:  
2023-10-26 15:59:49.173978: Epoch 420 
2023-10-26 15:59:49.174225: Current learning rate: 0.00612 
2023-10-26 15:59:53.396530: train_loss -0.8872 
2023-10-26 15:59:53.397362: val_loss -0.8185 
2023-10-26 15:59:53.397711: Pseudo dice [0.8721, 0.9035, 0.9655, 0.6215, 0.8404] 
2023-10-26 15:59:53.398022: Epoch time: 4.22 s 
2023-10-26 15:59:54.494152:  
2023-10-26 15:59:54.494487: Epoch 421 
2023-10-26 15:59:54.494733: Current learning rate: 0.00612 
2023-10-26 15:59:58.769902: train_loss -0.8873 
2023-10-26 15:59:58.770703: val_loss -0.8162 
2023-10-26 15:59:58.771130: Pseudo dice [0.8664, 0.899, 0.9681, 0.5896, 0.8819] 
2023-10-26 15:59:58.771519: Epoch time: 4.28 s 
2023-10-26 15:59:59.866982:  
2023-10-26 15:59:59.867332: Epoch 422 
2023-10-26 15:59:59.867640: Current learning rate: 0.00611 
2023-10-26 16:00:04.036367: train_loss -0.8939 
2023-10-26 16:00:04.036800: val_loss -0.8153 
2023-10-26 16:00:04.037079: Pseudo dice [0.8651, 0.8978, 0.967, 0.6111, 0.8835] 
2023-10-26 16:00:04.037318: Epoch time: 4.17 s 
2023-10-26 16:00:05.290367:  
2023-10-26 16:00:05.290737: Epoch 423 
2023-10-26 16:00:05.291051: Current learning rate: 0.0061 
2023-10-26 16:00:09.472241: train_loss -0.8887 
2023-10-26 16:00:09.472943: val_loss -0.8184 
2023-10-26 16:00:09.473220: Pseudo dice [0.867, 0.9017, 0.9678, 0.6203, 0.8847] 
2023-10-26 16:00:09.473459: Epoch time: 4.18 s 
2023-10-26 16:00:10.546156:  
2023-10-26 16:00:10.546485: Epoch 424 
2023-10-26 16:00:10.546739: Current learning rate: 0.00609 
2023-10-26 16:00:14.795247: train_loss -0.8843 
2023-10-26 16:00:14.795983: val_loss -0.8243 
2023-10-26 16:00:14.796265: Pseudo dice [0.8689, 0.8999, 0.9676, 0.633, 0.8762] 
2023-10-26 16:00:14.796537: Epoch time: 4.25 s 
2023-10-26 16:00:15.900864:  
2023-10-26 16:00:15.901237: Epoch 425 
2023-10-26 16:00:15.901653: Current learning rate: 0.00608 
2023-10-26 16:00:20.110008: train_loss -0.8879 
2023-10-26 16:00:20.110685: val_loss -0.8062 
2023-10-26 16:00:20.110968: Pseudo dice [0.86, 0.8972, 0.9671, 0.6757, 0.8696] 
2023-10-26 16:00:20.111204: Epoch time: 4.21 s 
2023-10-26 16:00:21.206086:  
2023-10-26 16:00:21.206441: Epoch 426 
2023-10-26 16:00:21.206697: Current learning rate: 0.00607 
2023-10-26 16:00:25.489250: train_loss -0.8871 
2023-10-26 16:00:25.489912: val_loss -0.7664 
2023-10-26 16:00:25.490185: Pseudo dice [0.8706, 0.8834, 0.9651, 0.0, 0.8415] 
2023-10-26 16:00:25.490414: Epoch time: 4.28 s 
2023-10-26 16:00:26.585493:  
2023-10-26 16:00:26.585886: Epoch 427 
2023-10-26 16:00:26.586235: Current learning rate: 0.00606 
2023-10-26 16:00:30.822696: train_loss -0.8717 
2023-10-26 16:00:30.823434: val_loss -0.8065 
2023-10-26 16:00:30.823715: Pseudo dice [0.8634, 0.8849, 0.9686, 0.5037, 0.885] 
2023-10-26 16:00:30.823952: Epoch time: 4.24 s 
2023-10-26 16:00:31.909925:  
2023-10-26 16:00:31.910708: Epoch 428 
2023-10-26 16:00:31.910954: Current learning rate: 0.00605 
2023-10-26 16:00:36.002097: train_loss -0.8763 
2023-10-26 16:00:36.002480: val_loss -0.7808 
2023-10-26 16:00:36.002782: Pseudo dice [0.855, 0.8838, 0.9639, 0.6288, 0.875] 
2023-10-26 16:00:36.003024: Epoch time: 4.09 s 
2023-10-26 16:00:37.283888:  
2023-10-26 16:00:37.284201: Epoch 429 
2023-10-26 16:00:37.284454: Current learning rate: 0.00604 
2023-10-26 16:00:41.397819: train_loss -0.882 
2023-10-26 16:00:41.398573: val_loss -0.8106 
2023-10-26 16:00:41.398918: Pseudo dice [0.8721, 0.8925, 0.9674, 0.586, 0.9036] 
2023-10-26 16:00:41.399186: Epoch time: 4.11 s 
2023-10-26 16:00:42.487494:  
2023-10-26 16:00:42.487897: Epoch 430 
2023-10-26 16:00:42.488262: Current learning rate: 0.00603 
2023-10-26 16:00:46.757826: train_loss -0.8942 
2023-10-26 16:00:46.758756: val_loss -0.8201 
2023-10-26 16:00:46.759217: Pseudo dice [0.8652, 0.8914, 0.968, 0.6716, 0.8771] 
2023-10-26 16:00:46.759698: Epoch time: 4.27 s 
2023-10-26 16:00:47.881887:  
2023-10-26 16:00:47.882215: Epoch 431 
2023-10-26 16:00:47.882467: Current learning rate: 0.00602 
2023-10-26 16:00:52.179385: train_loss -0.8926 
2023-10-26 16:00:52.180030: val_loss -0.8147 
2023-10-26 16:00:52.180297: Pseudo dice [0.8705, 0.9042, 0.9651, 0.5607, 0.891] 
2023-10-26 16:00:52.180526: Epoch time: 4.3 s 
2023-10-26 16:00:53.265369:  
2023-10-26 16:00:53.265678: Epoch 432 
2023-10-26 16:00:53.265935: Current learning rate: 0.00601 
2023-10-26 16:00:57.483157: train_loss -0.889 
2023-10-26 16:00:57.483837: val_loss -0.8215 
2023-10-26 16:00:57.484121: Pseudo dice [0.8623, 0.895, 0.965, 0.6862, 0.8859] 
2023-10-26 16:00:57.484365: Epoch time: 4.22 s 
2023-10-26 16:00:58.587957:  
2023-10-26 16:00:58.588271: Epoch 433 
2023-10-26 16:00:58.588527: Current learning rate: 0.006 
2023-10-26 16:01:02.761412: train_loss -0.8889 
2023-10-26 16:01:02.762167: val_loss -0.8205 
2023-10-26 16:01:02.762456: Pseudo dice [0.8691, 0.8993, 0.9667, 0.5568, 0.8867] 
2023-10-26 16:01:02.762705: Epoch time: 4.17 s 
2023-10-26 16:01:03.863000:  
2023-10-26 16:01:03.863308: Epoch 434 
2023-10-26 16:01:03.863560: Current learning rate: 0.00599 
2023-10-26 16:01:07.899842: train_loss -0.8861 
2023-10-26 16:01:07.900234: val_loss -0.8063 
2023-10-26 16:01:07.900502: Pseudo dice [0.868, 0.8926, 0.9673, 0.6792, 0.8592] 
2023-10-26 16:01:07.900726: Epoch time: 4.04 s 
2023-10-26 16:01:09.043770:  
2023-10-26 16:01:09.044079: Epoch 435 
2023-10-26 16:01:09.044340: Current learning rate: 0.00598 
2023-10-26 16:01:13.163194: train_loss -0.8895 
2023-10-26 16:01:13.163659: val_loss -0.8113 
2023-10-26 16:01:13.163981: Pseudo dice [0.8588, 0.8903, 0.9658, 0.4985, 0.9094] 
2023-10-26 16:01:13.164258: Epoch time: 4.12 s 
2023-10-26 16:01:14.459181:  
2023-10-26 16:01:14.459495: Epoch 436 
2023-10-26 16:01:14.459744: Current learning rate: 0.00597 
2023-10-26 16:01:18.421850: train_loss -0.8882 
2023-10-26 16:01:18.422215: val_loss -0.8019 
2023-10-26 16:01:18.422499: Pseudo dice [0.8554, 0.9024, 0.9664, 0.4908, 0.8965] 
2023-10-26 16:01:18.422734: Epoch time: 3.96 s 
2023-10-26 16:01:19.587996:  
2023-10-26 16:01:19.588326: Epoch 437 
2023-10-26 16:01:19.588585: Current learning rate: 0.00596 
2023-10-26 16:01:23.745073: train_loss -0.8787 
2023-10-26 16:01:23.745560: val_loss -0.8176 
2023-10-26 16:01:23.745901: Pseudo dice [0.8657, 0.8946, 0.9681, 0.6511, 0.8841] 
2023-10-26 16:01:23.746161: Epoch time: 4.16 s 
2023-10-26 16:01:24.836223:  
2023-10-26 16:01:24.836549: Epoch 438 
2023-10-26 16:01:24.836804: Current learning rate: 0.00595 
2023-10-26 16:01:28.901231: train_loss -0.8764 
2023-10-26 16:01:28.901627: val_loss -0.8047 
2023-10-26 16:01:28.901896: Pseudo dice [0.8685, 0.9047, 0.9672, 0.3406, 0.874] 
2023-10-26 16:01:28.902139: Epoch time: 4.07 s 
2023-10-26 16:01:29.992039:  
2023-10-26 16:01:29.992348: Epoch 439 
2023-10-26 16:01:29.992594: Current learning rate: 0.00594 
2023-10-26 16:01:34.030463: train_loss -0.8743 
2023-10-26 16:01:34.030882: val_loss -0.8121 
2023-10-26 16:01:34.031158: Pseudo dice [0.8643, 0.895, 0.9673, 0.5893, 0.8858] 
2023-10-26 16:01:34.031409: Epoch time: 4.04 s 
2023-10-26 16:01:35.155853:  
2023-10-26 16:01:35.156162: Epoch 440 
2023-10-26 16:01:35.156407: Current learning rate: 0.00593 
2023-10-26 16:01:39.123311: train_loss -0.8777 
2023-10-26 16:01:39.123692: val_loss -0.8195 
2023-10-26 16:01:39.123963: Pseudo dice [0.8629, 0.8914, 0.9666, 0.5944, 0.8937] 
2023-10-26 16:01:39.124196: Epoch time: 3.97 s 
2023-10-26 16:01:40.210866:  
2023-10-26 16:01:40.211193: Epoch 441 
2023-10-26 16:01:40.211445: Current learning rate: 0.00592 
2023-10-26 16:01:44.119094: train_loss -0.8831 
2023-10-26 16:01:44.119449: val_loss -0.8347 
2023-10-26 16:01:44.119704: Pseudo dice [0.8667, 0.897, 0.9677, 0.7015, 0.9067] 
2023-10-26 16:01:44.119929: Epoch time: 3.91 s 
2023-10-26 16:01:45.332045:  
2023-10-26 16:01:45.332359: Epoch 442 
2023-10-26 16:01:45.332613: Current learning rate: 0.00592 
2023-10-26 16:01:49.419742: train_loss -0.8899 
2023-10-26 16:01:49.420439: val_loss -0.8286 
2023-10-26 16:01:49.420721: Pseudo dice [0.8556, 0.9021, 0.9673, 0.7631, 0.8868] 
2023-10-26 16:01:49.421278: Epoch time: 4.09 s 
2023-10-26 16:01:50.502212:  
2023-10-26 16:01:50.502591: Epoch 443 
2023-10-26 16:01:50.502922: Current learning rate: 0.00591 
2023-10-26 16:01:54.670520: train_loss -0.8899 
2023-10-26 16:01:54.671491: val_loss -0.8027 
2023-10-26 16:01:54.672025: Pseudo dice [0.8684, 0.8995, 0.9677, 0.5596, 0.8848] 
2023-10-26 16:01:54.672404: Epoch time: 4.17 s 
2023-10-26 16:01:55.817570:  
2023-10-26 16:01:55.817893: Epoch 444 
2023-10-26 16:01:55.818153: Current learning rate: 0.0059 
2023-10-26 16:01:59.925207: train_loss -0.8946 
2023-10-26 16:01:59.925880: val_loss -0.8307 
2023-10-26 16:01:59.926436: Pseudo dice [0.8691, 0.9012, 0.9669, 0.7195, 0.8776] 
2023-10-26 16:01:59.926682: Epoch time: 4.11 s 
2023-10-26 16:02:00.989953:  
2023-10-26 16:02:00.990262: Epoch 445 
2023-10-26 16:02:00.990520: Current learning rate: 0.00589 
2023-10-26 16:02:05.235707: train_loss -0.8919 
2023-10-26 16:02:05.236467: val_loss -0.8123 
2023-10-26 16:02:05.236743: Pseudo dice [0.8714, 0.9023, 0.9684, 0.586, 0.8734] 
2023-10-26 16:02:05.236993: Epoch time: 4.25 s 
2023-10-26 16:02:06.353515:  
2023-10-26 16:02:06.353856: Epoch 446 
2023-10-26 16:02:06.354187: Current learning rate: 0.00588 
2023-10-26 16:02:10.519001: train_loss -0.8897 
2023-10-26 16:02:10.519682: val_loss -0.8136 
2023-10-26 16:02:10.519959: Pseudo dice [0.8696, 0.9074, 0.9657, 0.6168, 0.8828] 
2023-10-26 16:02:10.520196: Epoch time: 4.17 s 
2023-10-26 16:02:11.589509:  
2023-10-26 16:02:11.589813: Epoch 447 
2023-10-26 16:02:11.590064: Current learning rate: 0.00587 
2023-10-26 16:02:15.459341: train_loss -0.8883 
2023-10-26 16:02:15.459764: val_loss -0.8294 
2023-10-26 16:02:15.460066: Pseudo dice [0.8594, 0.8993, 0.9681, 0.6954, 0.8912] 
2023-10-26 16:02:15.460326: Epoch time: 3.87 s 
2023-10-26 16:02:16.714636:  
2023-10-26 16:02:16.714951: Epoch 448 
2023-10-26 16:02:16.715208: Current learning rate: 0.00586 
2023-10-26 16:02:20.992989: train_loss -0.8864 
2023-10-26 16:02:20.993685: val_loss -0.8188 
2023-10-26 16:02:20.993960: Pseudo dice [0.8696, 0.9005, 0.9699, 0.7547, 0.8646] 
2023-10-26 16:02:20.994199: Epoch time: 4.28 s 
2023-10-26 16:02:22.108262:  
2023-10-26 16:02:22.108571: Epoch 449 
2023-10-26 16:02:22.108824: Current learning rate: 0.00585 
2023-10-26 16:02:26.286724: train_loss -0.8893 
2023-10-26 16:02:26.287471: val_loss -0.8059 
2023-10-26 16:02:26.287811: Pseudo dice [0.8588, 0.8878, 0.966, 0.5301, 0.8905] 
2023-10-26 16:02:26.288101: Epoch time: 4.18 s 
2023-10-26 16:02:27.508757:  
2023-10-26 16:02:27.509067: Epoch 450 
2023-10-26 16:02:27.509313: Current learning rate: 0.00584 
2023-10-26 16:02:31.716491: train_loss -0.8902 
2023-10-26 16:02:31.717241: val_loss -0.8097 
2023-10-26 16:02:31.717516: Pseudo dice [0.8616, 0.8962, 0.9682, 0.648, 0.8901] 
2023-10-26 16:02:31.717753: Epoch time: 4.21 s 
2023-10-26 16:02:32.809658:  
2023-10-26 16:02:32.809959: Epoch 451 
2023-10-26 16:02:32.810205: Current learning rate: 0.00583 
2023-10-26 16:02:36.906857: train_loss -0.8923 
2023-10-26 16:02:36.907561: val_loss -0.7943 
2023-10-26 16:02:36.907967: Pseudo dice [0.8646, 0.8856, 0.9682, 0.5419, 0.8702] 
2023-10-26 16:02:36.908315: Epoch time: 4.1 s 
2023-10-26 16:02:38.023258:  
2023-10-26 16:02:38.023582: Epoch 452 
2023-10-26 16:02:38.023823: Current learning rate: 0.00582 
2023-10-26 16:02:42.062329: train_loss -0.8933 
2023-10-26 16:02:42.062832: val_loss -0.8202 
2023-10-26 16:02:42.063168: Pseudo dice [0.8705, 0.906, 0.9663, 0.737, 0.9051] 
2023-10-26 16:02:42.063460: Epoch time: 4.04 s 
2023-10-26 16:02:43.240252:  
2023-10-26 16:02:43.240564: Epoch 453 
2023-10-26 16:02:43.240807: Current learning rate: 0.00581 
2023-10-26 16:02:47.212412: train_loss -0.893 
2023-10-26 16:02:47.212911: val_loss -0.8178 
2023-10-26 16:02:47.213285: Pseudo dice [0.8708, 0.8967, 0.9686, 0.6263, 0.8685] 
2023-10-26 16:02:47.213552: Epoch time: 3.97 s 
2023-10-26 16:02:48.417250:  
2023-10-26 16:02:48.417561: Epoch 454 
2023-10-26 16:02:48.417817: Current learning rate: 0.0058 
2023-10-26 16:02:52.373896: train_loss -0.8937 
2023-10-26 16:02:52.374809: val_loss -0.7969 
2023-10-26 16:02:52.375290: Pseudo dice [0.8607, 0.9042, 0.9692, 0.6259, 0.831] 
2023-10-26 16:02:52.375899: Epoch time: 3.96 s 
2023-10-26 16:02:53.704418:  
2023-10-26 16:02:53.704774: Epoch 455 
2023-10-26 16:02:53.705048: Current learning rate: 0.00579 
2023-10-26 16:02:57.876340: train_loss -0.8955 
2023-10-26 16:02:57.877021: val_loss -0.8007 
2023-10-26 16:02:57.877304: Pseudo dice [0.8646, 0.8923, 0.9689, 0.5765, 0.883] 
2023-10-26 16:02:57.877540: Epoch time: 4.17 s 
2023-10-26 16:02:58.974911:  
2023-10-26 16:02:58.975223: Epoch 456 
2023-10-26 16:02:58.975492: Current learning rate: 0.00578 
2023-10-26 16:03:03.205268: train_loss -0.8979 
2023-10-26 16:03:03.206034: val_loss -0.8241 
2023-10-26 16:03:03.206316: Pseudo dice [0.8637, 0.8966, 0.966, 0.7031, 0.8703] 
2023-10-26 16:03:03.206571: Epoch time: 4.23 s 
2023-10-26 16:03:04.293108:  
2023-10-26 16:03:04.293406: Epoch 457 
2023-10-26 16:03:04.293660: Current learning rate: 0.00577 
2023-10-26 16:03:08.564895: train_loss -0.8844 
2023-10-26 16:03:08.565582: val_loss -0.8161 
2023-10-26 16:03:08.565859: Pseudo dice [0.8657, 0.9039, 0.9675, 0.4583, 0.8971] 
2023-10-26 16:03:08.566129: Epoch time: 4.27 s 
2023-10-26 16:03:09.665248:  
2023-10-26 16:03:09.665574: Epoch 458 
2023-10-26 16:03:09.665825: Current learning rate: 0.00576 
2023-10-26 16:03:13.802650: train_loss -0.8797 
2023-10-26 16:03:13.803225: val_loss -0.8166 
2023-10-26 16:03:13.803601: Pseudo dice [0.8693, 0.8997, 0.9654, 0.6993, 0.883] 
2023-10-26 16:03:13.804125: Epoch time: 4.14 s 
2023-10-26 16:03:14.898719:  
2023-10-26 16:03:14.899040: Epoch 459 
2023-10-26 16:03:14.899299: Current learning rate: 0.00575 
2023-10-26 16:03:19.022142: train_loss -0.8909 
2023-10-26 16:03:19.022574: val_loss -0.8164 
2023-10-26 16:03:19.022843: Pseudo dice [0.8667, 0.8997, 0.9695, 0.6135, 0.8777] 
2023-10-26 16:03:19.023104: Epoch time: 4.12 s 
2023-10-26 16:03:20.154536:  
2023-10-26 16:03:20.154867: Epoch 460 
2023-10-26 16:03:20.155129: Current learning rate: 0.00574 
2023-10-26 16:03:24.196908: train_loss -0.8932 
2023-10-26 16:03:24.197331: val_loss -0.803 
2023-10-26 16:03:24.197595: Pseudo dice [0.8685, 0.9079, 0.9693, 0.6214, 0.8429] 
2023-10-26 16:03:24.197855: Epoch time: 4.04 s 
2023-10-26 16:03:25.281625:  
2023-10-26 16:03:25.282222: Epoch 461 
2023-10-26 16:03:25.282521: Current learning rate: 0.00573 
2023-10-26 16:03:29.646300: train_loss -0.8847 
2023-10-26 16:03:29.646730: val_loss -0.8123 
2023-10-26 16:03:29.647014: Pseudo dice [0.865, 0.9011, 0.9688, 0.5952, 0.8704] 
2023-10-26 16:03:29.647254: Epoch time: 4.37 s 
2023-10-26 16:03:30.725222:  
2023-10-26 16:03:30.725540: Epoch 462 
2023-10-26 16:03:30.725788: Current learning rate: 0.00572 
2023-10-26 16:03:34.819818: train_loss -0.8941 
2023-10-26 16:03:34.820219: val_loss -0.8326 
2023-10-26 16:03:34.820505: Pseudo dice [0.8702, 0.9099, 0.9675, 0.7018, 0.8709] 
2023-10-26 16:03:34.820738: Epoch time: 4.1 s 
2023-10-26 16:03:35.919780:  
2023-10-26 16:03:35.920090: Epoch 463 
2023-10-26 16:03:35.920340: Current learning rate: 0.00571 
2023-10-26 16:03:40.184736: train_loss -0.8957 
2023-10-26 16:03:40.185112: val_loss -0.8134 
2023-10-26 16:03:40.185362: Pseudo dice [0.8645, 0.9, 0.9681, 0.6286, 0.8937] 
2023-10-26 16:03:40.185588: Epoch time: 4.27 s 
2023-10-26 16:03:41.347636:  
2023-10-26 16:03:41.347977: Epoch 464 
2023-10-26 16:03:41.348228: Current learning rate: 0.0057 
2023-10-26 16:03:45.394102: train_loss -0.8924 
2023-10-26 16:03:45.394558: val_loss -0.8026 
2023-10-26 16:03:45.394847: Pseudo dice [0.8547, 0.9012, 0.966, 0.5843, 0.8766] 
2023-10-26 16:03:45.395103: Epoch time: 4.05 s 
2023-10-26 16:03:46.483074:  
2023-10-26 16:03:46.483361: Epoch 465 
2023-10-26 16:03:46.483610: Current learning rate: 0.0057 
2023-10-26 16:03:50.598745: train_loss -0.8939 
2023-10-26 16:03:50.599177: val_loss -0.8048 
2023-10-26 16:03:50.599693: Pseudo dice [0.8642, 0.8924, 0.9678, 0.5056, 0.8749] 
2023-10-26 16:03:50.600059: Epoch time: 4.12 s 
2023-10-26 16:03:51.716419:  
2023-10-26 16:03:51.716738: Epoch 466 
2023-10-26 16:03:51.717015: Current learning rate: 0.00569 
2023-10-26 16:03:55.747627: train_loss -0.8905 
2023-10-26 16:03:55.748137: val_loss -0.8194 
2023-10-26 16:03:55.748463: Pseudo dice [0.8628, 0.8982, 0.9685, 0.6359, 0.8969] 
2023-10-26 16:03:55.748718: Epoch time: 4.03 s 
2023-10-26 16:03:56.852573:  
2023-10-26 16:03:56.852955: Epoch 467 
2023-10-26 16:03:56.853285: Current learning rate: 0.00568 
2023-10-26 16:04:00.951352: train_loss -0.8797 
2023-10-26 16:04:00.965162: val_loss -0.8323 
2023-10-26 16:04:00.965487: Pseudo dice [0.8658, 0.9062, 0.964, 0.675, 0.9] 
2023-10-26 16:04:00.965746: Epoch time: 4.1 s 
2023-10-26 16:04:02.245162:  
2023-10-26 16:04:02.245489: Epoch 468 
2023-10-26 16:04:02.245740: Current learning rate: 0.00567 
2023-10-26 16:04:06.338302: train_loss -0.8819 
2023-10-26 16:04:06.339108: val_loss -0.7967 
2023-10-26 16:04:06.339408: Pseudo dice [0.8658, 0.8988, 0.9683, 0.5987, 0.826] 
2023-10-26 16:04:06.339644: Epoch time: 4.09 s 
2023-10-26 16:04:07.418597:  
2023-10-26 16:04:07.418914: Epoch 469 
2023-10-26 16:04:07.419167: Current learning rate: 0.00566 
2023-10-26 16:04:11.410919: train_loss -0.8858 
2023-10-26 16:04:11.411678: val_loss -0.8245 
2023-10-26 16:04:11.412144: Pseudo dice [0.8602, 0.8992, 0.9664, 0.6974, 0.8959] 
2023-10-26 16:04:11.412405: Epoch time: 3.99 s 
2023-10-26 16:04:12.506467:  
2023-10-26 16:04:12.506762: Epoch 470 
2023-10-26 16:04:12.507012: Current learning rate: 0.00565 
2023-10-26 16:04:16.525128: train_loss -0.8829 
2023-10-26 16:04:16.525938: val_loss -0.7876 
2023-10-26 16:04:16.526222: Pseudo dice [0.8709, 0.8909, 0.9685, 0.4305, 0.8628] 
2023-10-26 16:04:16.526543: Epoch time: 4.02 s 
2023-10-26 16:04:17.616356:  
2023-10-26 16:04:17.616663: Epoch 471 
2023-10-26 16:04:17.616914: Current learning rate: 0.00564 
2023-10-26 16:04:21.788069: train_loss -0.875 
2023-10-26 16:04:21.788659: val_loss -0.7531 
2023-10-26 16:04:21.789071: Pseudo dice [0.8577, 0.8867, 0.9666, 0.0216, 0.8471] 
2023-10-26 16:04:21.789337: Epoch time: 4.17 s 
2023-10-26 16:04:22.876184:  
2023-10-26 16:04:22.876493: Epoch 472 
2023-10-26 16:04:22.876743: Current learning rate: 0.00563 
2023-10-26 16:04:26.965069: train_loss -0.8737 
2023-10-26 16:04:26.965466: val_loss -0.8273 
2023-10-26 16:04:26.965730: Pseudo dice [0.8614, 0.8979, 0.9653, 0.7559, 0.8834] 
2023-10-26 16:04:26.965963: Epoch time: 4.09 s 
2023-10-26 16:04:28.052437:  
2023-10-26 16:04:28.052732: Epoch 473 
2023-10-26 16:04:28.052984: Current learning rate: 0.00562 
2023-10-26 16:04:32.104895: train_loss -0.8767 
2023-10-26 16:04:32.105327: val_loss -0.8269 
2023-10-26 16:04:32.105608: Pseudo dice [0.8645, 0.9029, 0.9695, 0.7187, 0.8781] 
2023-10-26 16:04:32.105846: Epoch time: 4.05 s 
2023-10-26 16:04:33.223851:  
2023-10-26 16:04:33.224163: Epoch 474 
2023-10-26 16:04:33.224418: Current learning rate: 0.00561 
2023-10-26 16:04:37.158097: train_loss -0.8862 
2023-10-26 16:04:37.158524: val_loss -0.788 
2023-10-26 16:04:37.158792: Pseudo dice [0.8697, 0.902, 0.9654, 0.6483, 0.8801] 
2023-10-26 16:04:37.159041: Epoch time: 3.93 s 
2023-10-26 16:04:38.424413:  
2023-10-26 16:04:38.424767: Epoch 475 
2023-10-26 16:04:38.425036: Current learning rate: 0.0056 
2023-10-26 16:04:42.454964: train_loss -0.8909 
2023-10-26 16:04:42.455699: val_loss -0.817 
2023-10-26 16:04:42.455992: Pseudo dice [0.8603, 0.8925, 0.9676, 0.6385, 0.9125] 
2023-10-26 16:04:42.456261: Epoch time: 4.03 s 
2023-10-26 16:04:43.538608:  
2023-10-26 16:04:43.538958: Epoch 476 
2023-10-26 16:04:43.539223: Current learning rate: 0.00559 
2023-10-26 16:04:47.578076: train_loss -0.8919 
2023-10-26 16:04:47.578779: val_loss -0.8247 
2023-10-26 16:04:47.579055: Pseudo dice [0.8693, 0.9024, 0.9688, 0.7688, 0.8899] 
2023-10-26 16:04:47.579300: Epoch time: 4.04 s 
2023-10-26 16:04:48.713335:  
2023-10-26 16:04:48.713637: Epoch 477 
2023-10-26 16:04:48.713890: Current learning rate: 0.00558 
2023-10-26 16:04:52.748044: train_loss -0.879 
2023-10-26 16:04:52.748906: val_loss -0.7962 
2023-10-26 16:04:52.749253: Pseudo dice [0.8615, 0.8742, 0.9631, 0.5609, 0.8875] 
2023-10-26 16:04:52.749524: Epoch time: 4.04 s 
2023-10-26 16:04:53.888074:  
2023-10-26 16:04:53.888433: Epoch 478 
2023-10-26 16:04:53.888733: Current learning rate: 0.00557 
2023-10-26 16:04:57.724308: train_loss -0.879 
2023-10-26 16:04:57.724709: val_loss -0.8011 
2023-10-26 16:04:57.724974: Pseudo dice [0.8684, 0.8994, 0.9655, 0.5577, 0.8444] 
2023-10-26 16:04:57.725216: Epoch time: 3.84 s 
2023-10-26 16:04:58.841637:  
2023-10-26 16:04:58.841953: Epoch 479 
2023-10-26 16:04:58.842197: Current learning rate: 0.00556 
2023-10-26 16:05:02.732027: train_loss -0.8861 
2023-10-26 16:05:02.732448: val_loss -0.816 
2023-10-26 16:05:02.732716: Pseudo dice [0.8708, 0.9039, 0.9631, 0.713, 0.8553] 
2023-10-26 16:05:02.732954: Epoch time: 3.89 s 
2023-10-26 16:05:03.868556:  
2023-10-26 16:05:03.868854: Epoch 480 
2023-10-26 16:05:03.869119: Current learning rate: 0.00555 
2023-10-26 16:05:07.947581: train_loss -0.8726 
2023-10-26 16:05:07.948344: val_loss -0.815 
2023-10-26 16:05:07.948721: Pseudo dice [0.8584, 0.8967, 0.9653, 0.7964, 0.8595] 
2023-10-26 16:05:07.949070: Epoch time: 4.08 s 
2023-10-26 16:05:09.301623:  
2023-10-26 16:05:09.301951: Epoch 481 
2023-10-26 16:05:09.302206: Current learning rate: 0.00554 
2023-10-26 16:05:13.357524: train_loss -0.874 
2023-10-26 16:05:13.357921: val_loss -0.8171 
2023-10-26 16:05:13.358187: Pseudo dice [0.8665, 0.9051, 0.9647, 0.7531, 0.8526] 
2023-10-26 16:05:13.358430: Epoch time: 4.06 s 
2023-10-26 16:05:14.454313:  
2023-10-26 16:05:14.454622: Epoch 482 
2023-10-26 16:05:14.454864: Current learning rate: 0.00553 
2023-10-26 16:05:18.474690: train_loss -0.8818 
2023-10-26 16:05:18.475124: val_loss -0.8176 
2023-10-26 16:05:18.475553: Pseudo dice [0.8595, 0.8948, 0.9676, 0.6838, 0.8733] 
2023-10-26 16:05:18.475803: Epoch time: 4.02 s 
2023-10-26 16:05:19.617473:  
2023-10-26 16:05:19.617805: Epoch 483 
2023-10-26 16:05:19.618066: Current learning rate: 0.00552 
2023-10-26 16:05:23.750475: train_loss -0.8818 
2023-10-26 16:05:23.750870: val_loss -0.807 
2023-10-26 16:05:23.751158: Pseudo dice [0.8705, 0.899, 0.9685, 0.6616, 0.8608] 
2023-10-26 16:05:23.751397: Epoch time: 4.13 s 
2023-10-26 16:05:24.868088:  
2023-10-26 16:05:24.868491: Epoch 484 
2023-10-26 16:05:24.868777: Current learning rate: 0.00551 
2023-10-26 16:05:28.901209: train_loss -0.8906 
2023-10-26 16:05:28.901693: val_loss -0.8236 
2023-10-26 16:05:28.902087: Pseudo dice [0.8719, 0.9093, 0.9675, 0.778, 0.851] 
2023-10-26 16:05:28.902403: Epoch time: 4.03 s 
2023-10-26 16:05:30.008743:  
2023-10-26 16:05:30.009081: Epoch 485 
2023-10-26 16:05:30.009365: Current learning rate: 0.0055 
2023-10-26 16:05:34.088828: train_loss -0.8916 
2023-10-26 16:05:34.089544: val_loss -0.8033 
2023-10-26 16:05:34.089806: Pseudo dice [0.8699, 0.9029, 0.9658, 0.5537, 0.8795] 
2023-10-26 16:05:34.090048: Epoch time: 4.08 s 
2023-10-26 16:05:35.188149:  
2023-10-26 16:05:35.188457: Epoch 486 
2023-10-26 16:05:35.188705: Current learning rate: 0.00549 
2023-10-26 16:05:39.323471: train_loss -0.8926 
2023-10-26 16:05:39.323984: val_loss -0.8084 
2023-10-26 16:05:39.324310: Pseudo dice [0.862, 0.9044, 0.9653, 0.5879, 0.8766] 
2023-10-26 16:05:39.324615: Epoch time: 4.14 s 
2023-10-26 16:05:40.604239:  
2023-10-26 16:05:40.604550: Epoch 487 
2023-10-26 16:05:40.604797: Current learning rate: 0.00548 
2023-10-26 16:05:44.671178: train_loss -0.8946 
2023-10-26 16:05:44.671627: val_loss -0.8249 
2023-10-26 16:05:44.671905: Pseudo dice [0.8665, 0.897, 0.9675, 0.6587, 0.8974] 
2023-10-26 16:05:44.672156: Epoch time: 4.07 s 
2023-10-26 16:05:45.765073:  
2023-10-26 16:05:45.765381: Epoch 488 
2023-10-26 16:05:45.765631: Current learning rate: 0.00547 
2023-10-26 16:05:49.908242: train_loss -0.8912 
2023-10-26 16:05:49.908705: val_loss -0.8252 
2023-10-26 16:05:49.909049: Pseudo dice [0.8681, 0.8943, 0.9687, 0.6379, 0.8963] 
2023-10-26 16:05:49.909386: Epoch time: 4.14 s 
2023-10-26 16:05:51.005380:  
2023-10-26 16:05:51.005681: Epoch 489 
2023-10-26 16:05:51.005929: Current learning rate: 0.00546 
2023-10-26 16:05:55.098658: train_loss -0.8935 
2023-10-26 16:05:55.099122: val_loss -0.8069 
2023-10-26 16:05:55.099399: Pseudo dice [0.8698, 0.9069, 0.9663, 0.5826, 0.864] 
2023-10-26 16:05:55.099644: Epoch time: 4.09 s 
2023-10-26 16:05:56.194571:  
2023-10-26 16:05:56.194864: Epoch 490 
2023-10-26 16:05:56.195108: Current learning rate: 0.00546 
2023-10-26 16:06:00.311285: train_loss -0.8919 
2023-10-26 16:06:00.311665: val_loss -0.8302 
2023-10-26 16:06:00.311928: Pseudo dice [0.8681, 0.9045, 0.9674, 0.7457, 0.8925] 
2023-10-26 16:06:00.312164: Epoch time: 4.12 s 
2023-10-26 16:06:01.422212:  
2023-10-26 16:06:01.422527: Epoch 491 
2023-10-26 16:06:01.422780: Current learning rate: 0.00545 
2023-10-26 16:06:05.402132: train_loss -0.8938 
2023-10-26 16:06:05.402552: val_loss -0.8173 
2023-10-26 16:06:05.402813: Pseudo dice [0.8674, 0.8988, 0.9691, 0.7096, 0.853] 
2023-10-26 16:06:05.403057: Epoch time: 3.98 s 
2023-10-26 16:06:06.586776:  
2023-10-26 16:06:06.587075: Epoch 492 
2023-10-26 16:06:06.587320: Current learning rate: 0.00544 
2023-10-26 16:06:10.588660: train_loss -0.8932 
2023-10-26 16:06:10.589318: val_loss -0.8087 
2023-10-26 16:06:10.589597: Pseudo dice [0.8599, 0.8924, 0.9646, 0.5231, 0.8653] 
2023-10-26 16:06:10.589824: Epoch time: 4.0 s 
2023-10-26 16:06:11.767465:  
2023-10-26 16:06:11.767760: Epoch 493 
2023-10-26 16:06:11.768005: Current learning rate: 0.00543 
2023-10-26 16:06:15.881364: train_loss -0.8811 
2023-10-26 16:06:15.881810: val_loss -0.7962 
2023-10-26 16:06:15.882159: Pseudo dice [0.8609, 0.9023, 0.9667, 0.4603, 0.8531] 
2023-10-26 16:06:15.882484: Epoch time: 4.11 s 
2023-10-26 16:06:17.168618:  
2023-10-26 16:06:17.168931: Epoch 494 
2023-10-26 16:06:17.169188: Current learning rate: 0.00542 
2023-10-26 16:06:21.277316: train_loss -0.8852 
2023-10-26 16:06:21.277791: val_loss -0.7926 
2023-10-26 16:06:21.278069: Pseudo dice [0.8617, 0.9009, 0.9642, 0.6536, 0.8298] 
2023-10-26 16:06:21.278312: Epoch time: 4.11 s 
2023-10-26 16:06:22.379173:  
2023-10-26 16:06:22.379477: Epoch 495 
2023-10-26 16:06:22.379723: Current learning rate: 0.00541 
2023-10-26 16:06:26.398819: train_loss -0.8811 
2023-10-26 16:06:26.399202: val_loss -0.7998 
2023-10-26 16:06:26.399464: Pseudo dice [0.8684, 0.8982, 0.9644, 0.5486, 0.8627] 
2023-10-26 16:06:26.399706: Epoch time: 4.02 s 
2023-10-26 16:06:27.500845:  
2023-10-26 16:06:27.501167: Epoch 496 
2023-10-26 16:06:27.501415: Current learning rate: 0.0054 
2023-10-26 16:06:31.637140: train_loss -0.8819 
2023-10-26 16:06:31.637513: val_loss -0.7863 
2023-10-26 16:06:31.637777: Pseudo dice [0.8752, 0.9012, 0.9691, 0.579, 0.8477] 
2023-10-26 16:06:31.638013: Epoch time: 4.14 s 
2023-10-26 16:06:32.736649:  
2023-10-26 16:06:32.736984: Epoch 497 
2023-10-26 16:06:32.737242: Current learning rate: 0.00539 
2023-10-26 16:06:36.894204: train_loss -0.8844 
2023-10-26 16:06:36.894564: val_loss -0.797 
2023-10-26 16:06:36.894828: Pseudo dice [0.8704, 0.8975, 0.9663, 0.5712, 0.8773] 
2023-10-26 16:06:36.895055: Epoch time: 4.16 s 
2023-10-26 16:06:37.991799:  
2023-10-26 16:06:37.992100: Epoch 498 
2023-10-26 16:06:37.992352: Current learning rate: 0.00538 
2023-10-26 16:06:42.136549: train_loss -0.8841 
2023-10-26 16:06:42.137258: val_loss -0.7924 
2023-10-26 16:06:42.137530: Pseudo dice [0.8735, 0.8969, 0.9663, 0.595, 0.8669] 
2023-10-26 16:06:42.137762: Epoch time: 4.15 s 
2023-10-26 16:06:43.238943:  
2023-10-26 16:06:43.239247: Epoch 499 
2023-10-26 16:06:43.239497: Current learning rate: 0.00537 
2023-10-26 16:06:47.249686: train_loss -0.891 
2023-10-26 16:06:47.250093: val_loss -0.7967 
2023-10-26 16:06:47.250370: Pseudo dice [0.8654, 0.9082, 0.9642, 0.6029, 0.8536] 
2023-10-26 16:06:47.250606: Epoch time: 4.01 s 
2023-10-26 16:06:48.639738:  
2023-10-26 16:06:48.640052: Epoch 500 
2023-10-26 16:06:48.640327: Current learning rate: 0.00536 
2023-10-26 16:06:52.762229: train_loss -0.8915 
2023-10-26 16:06:52.762860: val_loss -0.8082 
2023-10-26 16:06:52.763160: Pseudo dice [0.8681, 0.9048, 0.9668, 0.5799, 0.874] 
2023-10-26 16:06:52.763495: Epoch time: 4.12 s 
2023-10-26 16:06:53.874778:  
2023-10-26 16:06:53.875118: Epoch 501 
2023-10-26 16:06:53.875375: Current learning rate: 0.00535 
2023-10-26 16:06:57.925494: train_loss -0.8892 
2023-10-26 16:06:57.925894: val_loss -0.7798 
2023-10-26 16:06:57.926167: Pseudo dice [0.8634, 0.9024, 0.9645, 0.0432, 0.8748] 
2023-10-26 16:06:57.926403: Epoch time: 4.05 s 
2023-10-26 16:06:59.033128:  
2023-10-26 16:06:59.033435: Epoch 502 
2023-10-26 16:06:59.033682: Current learning rate: 0.00534 
2023-10-26 16:07:03.014016: train_loss -0.8833 
2023-10-26 16:07:03.014480: val_loss -0.8017 
2023-10-26 16:07:03.014855: Pseudo dice [0.8718, 0.9089, 0.9658, 0.6078, 0.8538] 
2023-10-26 16:07:03.015164: Epoch time: 3.98 s 
2023-10-26 16:07:04.193151:  
2023-10-26 16:07:04.193466: Epoch 503 
2023-10-26 16:07:04.193709: Current learning rate: 0.00533 
2023-10-26 16:07:08.227884: train_loss -0.8911 
2023-10-26 16:07:08.228322: val_loss -0.81 
2023-10-26 16:07:08.228587: Pseudo dice [0.8595, 0.8995, 0.9668, 0.6867, 0.8746] 
2023-10-26 16:07:08.228825: Epoch time: 4.04 s 
2023-10-26 16:07:09.328037:  
2023-10-26 16:07:09.328345: Epoch 504 
2023-10-26 16:07:09.328601: Current learning rate: 0.00532 
2023-10-26 16:07:13.438644: train_loss -0.8929 
2023-10-26 16:07:13.439077: val_loss -0.7795 
2023-10-26 16:07:13.439352: Pseudo dice [0.87, 0.8988, 0.9667, 0.6255, 0.8639] 
2023-10-26 16:07:13.439626: Epoch time: 4.11 s 
2023-10-26 16:07:14.571066:  
2023-10-26 16:07:14.571375: Epoch 505 
2023-10-26 16:07:14.571643: Current learning rate: 0.00531 
2023-10-26 16:07:18.640624: train_loss -0.89 
2023-10-26 16:07:18.641458: val_loss -0.7953 
2023-10-26 16:07:18.641733: Pseudo dice [0.8649, 0.8985, 0.9662, 0.661, 0.8687] 
2023-10-26 16:07:18.641976: Epoch time: 4.07 s 
2023-10-26 16:07:19.937788:  
2023-10-26 16:07:19.938175: Epoch 506 
2023-10-26 16:07:19.938434: Current learning rate: 0.0053 
2023-10-26 16:07:24.121391: train_loss -0.8864 
2023-10-26 16:07:24.121975: val_loss -0.8108 
2023-10-26 16:07:24.122361: Pseudo dice [0.8607, 0.9008, 0.9669, 0.5279, 0.8828] 
2023-10-26 16:07:24.122677: Epoch time: 4.18 s 
2023-10-26 16:07:25.235401:  
2023-10-26 16:07:25.235714: Epoch 507 
2023-10-26 16:07:25.235961: Current learning rate: 0.00529 
2023-10-26 16:07:29.245337: train_loss -0.8893 
2023-10-26 16:07:29.245785: val_loss -0.7993 
2023-10-26 16:07:29.246097: Pseudo dice [0.8604, 0.8978, 0.9666, 0.559, 0.8645] 
2023-10-26 16:07:29.246359: Epoch time: 4.01 s 
2023-10-26 16:07:30.346582:  
2023-10-26 16:07:30.346880: Epoch 508 
2023-10-26 16:07:30.347126: Current learning rate: 0.00528 
2023-10-26 16:07:34.383388: train_loss -0.8952 
2023-10-26 16:07:34.383824: val_loss -0.8297 
2023-10-26 16:07:34.384116: Pseudo dice [0.8701, 0.899, 0.9649, 0.6302, 0.8957] 
2023-10-26 16:07:34.384379: Epoch time: 4.04 s 
2023-10-26 16:07:35.488843:  
2023-10-26 16:07:35.489155: Epoch 509 
2023-10-26 16:07:35.489404: Current learning rate: 0.00527 
2023-10-26 16:07:39.602905: train_loss -0.8929 
2023-10-26 16:07:39.603291: val_loss -0.8 
2023-10-26 16:07:39.603550: Pseudo dice [0.8714, 0.9006, 0.9665, 0.6386, 0.8463] 
2023-10-26 16:07:39.603783: Epoch time: 4.11 s 
2023-10-26 16:07:40.704708:  
2023-10-26 16:07:40.705019: Epoch 510 
2023-10-26 16:07:40.705261: Current learning rate: 0.00526 
2023-10-26 16:07:44.787621: train_loss -0.8891 
2023-10-26 16:07:44.788071: val_loss -0.8014 
2023-10-26 16:07:44.788343: Pseudo dice [0.8645, 0.905, 0.9659, 0.5935, 0.8431] 
2023-10-26 16:07:44.788585: Epoch time: 4.08 s 
2023-10-26 16:07:45.895751:  
2023-10-26 16:07:45.896059: Epoch 511 
2023-10-26 16:07:45.896302: Current learning rate: 0.00525 
2023-10-26 16:07:49.895723: train_loss -0.8915 
2023-10-26 16:07:49.896163: val_loss -0.8255 
2023-10-26 16:07:49.896436: Pseudo dice [0.8633, 0.8975, 0.9647, 0.6828, 0.8887] 
2023-10-26 16:07:49.896681: Epoch time: 4.0 s 
2023-10-26 16:07:50.996463:  
2023-10-26 16:07:50.996757: Epoch 512 
2023-10-26 16:07:50.997028: Current learning rate: 0.00524 
2023-10-26 16:07:55.136918: train_loss -0.8853 
2023-10-26 16:07:55.137352: val_loss -0.782 
2023-10-26 16:07:55.137622: Pseudo dice [0.8619, 0.8886, 0.967, 0.2409, 0.8657] 
2023-10-26 16:07:55.137879: Epoch time: 4.14 s 
2023-10-26 16:07:56.463450:  
2023-10-26 16:07:56.463746: Epoch 513 
2023-10-26 16:07:56.464011: Current learning rate: 0.00523 
2023-10-26 16:08:00.603952: train_loss -0.8873 
2023-10-26 16:08:00.604326: val_loss -0.8101 
2023-10-26 16:08:00.604586: Pseudo dice [0.8687, 0.9086, 0.9674, 0.6882, 0.85] 
2023-10-26 16:08:00.604817: Epoch time: 4.14 s 
2023-10-26 16:08:01.708356:  
2023-10-26 16:08:01.708659: Epoch 514 
2023-10-26 16:08:01.708906: Current learning rate: 0.00522 
2023-10-26 16:08:05.914885: train_loss -0.8884 
2023-10-26 16:08:05.915287: val_loss -0.8206 
2023-10-26 16:08:05.915547: Pseudo dice [0.8602, 0.9051, 0.9651, 0.7209, 0.8596] 
2023-10-26 16:08:05.915781: Epoch time: 4.21 s 
2023-10-26 16:08:07.053043:  
2023-10-26 16:08:07.053354: Epoch 515 
2023-10-26 16:08:07.053601: Current learning rate: 0.00521 
2023-10-26 16:08:11.240041: train_loss -0.8734 
2023-10-26 16:08:11.240422: val_loss -0.8118 
2023-10-26 16:08:11.240677: Pseudo dice [0.8638, 0.9029, 0.9653, 0.6642, 0.8716] 
2023-10-26 16:08:11.240902: Epoch time: 4.19 s 
2023-10-26 16:08:12.357467:  
2023-10-26 16:08:12.357764: Epoch 516 
2023-10-26 16:08:12.358027: Current learning rate: 0.0052 
2023-10-26 16:08:16.601666: train_loss -0.8824 
2023-10-26 16:08:16.602060: val_loss -0.8286 
2023-10-26 16:08:16.602329: Pseudo dice [0.8696, 0.8969, 0.9681, 0.7142, 0.8821] 
2023-10-26 16:08:16.602567: Epoch time: 4.24 s 
2023-10-26 16:08:17.695379:  
2023-10-26 16:08:17.695686: Epoch 517 
2023-10-26 16:08:17.695941: Current learning rate: 0.00519 
2023-10-26 16:08:21.986548: train_loss -0.8626 
2023-10-26 16:08:21.986940: val_loss -0.8118 
2023-10-26 16:08:21.987202: Pseudo dice [0.8667, 0.8996, 0.9657, 0.6758, 0.842] 
2023-10-26 16:08:21.987435: Epoch time: 4.29 s 
2023-10-26 16:08:23.081633:  
2023-10-26 16:08:23.081933: Epoch 518 
2023-10-26 16:08:23.082183: Current learning rate: 0.00518 
2023-10-26 16:08:27.358962: train_loss -0.8719 
2023-10-26 16:08:27.359618: val_loss -0.8392 
2023-10-26 16:08:27.359968: Pseudo dice [0.8654, 0.9021, 0.9669, 0.6651, 0.904] 
2023-10-26 16:08:27.360347: Epoch time: 4.28 s 
2023-10-26 16:08:28.595109:  
2023-10-26 16:08:28.595443: Epoch 519 
2023-10-26 16:08:28.595693: Current learning rate: 0.00518 
2023-10-26 16:08:32.835938: train_loss -0.871 
2023-10-26 16:08:32.836296: val_loss -0.8259 
2023-10-26 16:08:32.836546: Pseudo dice [0.8693, 0.9059, 0.9638, 0.7012, 0.8811] 
2023-10-26 16:08:32.836767: Epoch time: 4.24 s 
2023-10-26 16:08:33.919384:  
2023-10-26 16:08:33.919698: Epoch 520 
2023-10-26 16:08:33.919953: Current learning rate: 0.00517 
2023-10-26 16:08:38.156293: train_loss -0.8784 
2023-10-26 16:08:38.156682: val_loss -0.8106 
2023-10-26 16:08:38.156953: Pseudo dice [0.8751, 0.9004, 0.9681, 0.6856, 0.8521] 
2023-10-26 16:08:38.157190: Epoch time: 4.24 s 
2023-10-26 16:08:39.248924:  
2023-10-26 16:08:39.249217: Epoch 521 
2023-10-26 16:08:39.249462: Current learning rate: 0.00516 
2023-10-26 16:08:43.457627: train_loss -0.8817 
2023-10-26 16:08:43.458102: val_loss -0.8264 
2023-10-26 16:08:43.458502: Pseudo dice [0.8715, 0.899, 0.9689, 0.71, 0.8693] 
2023-10-26 16:08:43.458942: Epoch time: 4.21 s 
2023-10-26 16:08:44.552117:  
2023-10-26 16:08:44.552504: Epoch 522 
2023-10-26 16:08:44.552751: Current learning rate: 0.00515 
2023-10-26 16:08:48.849040: train_loss -0.8841 
2023-10-26 16:08:48.849539: val_loss -0.8072 
2023-10-26 16:08:48.849860: Pseudo dice [0.8659, 0.9026, 0.965, 0.6286, 0.8637] 
2023-10-26 16:08:48.850197: Epoch time: 4.3 s 
2023-10-26 16:08:49.988502:  
2023-10-26 16:08:49.988860: Epoch 523 
2023-10-26 16:08:49.989136: Current learning rate: 0.00514 
2023-10-26 16:08:54.235545: train_loss -0.8748 
2023-10-26 16:08:54.235946: val_loss -0.8074 
2023-10-26 16:08:54.236220: Pseudo dice [0.858, 0.8968, 0.9668, 0.6302, 0.8729] 
2023-10-26 16:08:54.236460: Epoch time: 4.25 s 
2023-10-26 16:08:55.344419:  
2023-10-26 16:08:55.344715: Epoch 524 
2023-10-26 16:08:55.344962: Current learning rate: 0.00513 
2023-10-26 16:08:59.477609: train_loss -0.8825 
2023-10-26 16:08:59.478029: val_loss -0.794 
2023-10-26 16:08:59.478301: Pseudo dice [0.8635, 0.9015, 0.9674, 0.4027, 0.8539] 
2023-10-26 16:08:59.478543: Epoch time: 4.13 s 
2023-10-26 16:09:00.759201:  
2023-10-26 16:09:00.759504: Epoch 525 
2023-10-26 16:09:00.759754: Current learning rate: 0.00512 
2023-10-26 16:09:05.007980: train_loss -0.8682 
2023-10-26 16:09:05.008403: val_loss -0.7852 
2023-10-26 16:09:05.008677: Pseudo dice [0.8674, 0.8962, 0.9656, 0.2419, 0.8649] 
2023-10-26 16:09:05.008926: Epoch time: 4.25 s 
2023-10-26 16:09:06.083649:  
2023-10-26 16:09:06.083960: Epoch 526 
2023-10-26 16:09:06.084207: Current learning rate: 0.00511 
2023-10-26 16:09:10.226469: train_loss -0.8778 
2023-10-26 16:09:10.226838: val_loss -0.8046 
2023-10-26 16:09:10.227096: Pseudo dice [0.865, 0.8995, 0.9662, 0.503, 0.868] 
2023-10-26 16:09:10.227319: Epoch time: 4.14 s 
2023-10-26 16:09:11.319124:  
2023-10-26 16:09:11.319452: Epoch 527 
2023-10-26 16:09:11.319702: Current learning rate: 0.0051 
2023-10-26 16:09:15.511184: train_loss -0.8807 
2023-10-26 16:09:15.511600: val_loss -0.8242 
2023-10-26 16:09:15.511860: Pseudo dice [0.8682, 0.8997, 0.967, 0.5611, 0.8778] 
2023-10-26 16:09:15.512114: Epoch time: 4.19 s 
2023-10-26 16:09:16.713192:  
2023-10-26 16:09:16.713502: Epoch 528 
2023-10-26 16:09:16.713748: Current learning rate: 0.00509 
2023-10-26 16:09:20.875110: train_loss -0.8897 
2023-10-26 16:09:20.875532: val_loss -0.8317 
2023-10-26 16:09:20.875787: Pseudo dice [0.856, 0.8983, 0.9674, 0.693, 0.8901] 
2023-10-26 16:09:20.876026: Epoch time: 4.16 s 
2023-10-26 16:09:21.971761:  
2023-10-26 16:09:21.972149: Epoch 529 
2023-10-26 16:09:21.972398: Current learning rate: 0.00508 
2023-10-26 16:09:26.147956: train_loss -0.8918 
2023-10-26 16:09:26.148368: val_loss -0.8315 
2023-10-26 16:09:26.148644: Pseudo dice [0.8608, 0.8919, 0.9669, 0.5956, 0.9072] 
2023-10-26 16:09:26.148889: Epoch time: 4.18 s 
2023-10-26 16:09:27.282847:  
2023-10-26 16:09:27.283167: Epoch 530 
2023-10-26 16:09:27.283413: Current learning rate: 0.00507 
2023-10-26 16:09:31.345546: train_loss -0.8906 
2023-10-26 16:09:31.345942: val_loss -0.7788 
2023-10-26 16:09:31.346205: Pseudo dice [0.8627, 0.8973, 0.969, 0.518, 0.8925] 
2023-10-26 16:09:31.346461: Epoch time: 4.06 s 
2023-10-26 16:09:32.466530:  
2023-10-26 16:09:32.466841: Epoch 531 
2023-10-26 16:09:32.467085: Current learning rate: 0.00506 
2023-10-26 16:09:36.651866: train_loss -0.8791 
2023-10-26 16:09:36.652340: val_loss -0.8163 
2023-10-26 16:09:36.652864: Pseudo dice [0.8714, 0.8968, 0.9677, 0.5, 0.8786] 
2023-10-26 16:09:36.653228: Epoch time: 4.19 s 
2023-10-26 16:09:37.905276:  
2023-10-26 16:09:37.905580: Epoch 532 
2023-10-26 16:09:37.905824: Current learning rate: 0.00505 
2023-10-26 16:09:41.983398: train_loss -0.8749 
2023-10-26 16:09:41.983831: val_loss -0.7919 
2023-10-26 16:09:41.984452: Pseudo dice [0.8709, 0.8963, 0.9667, 0.5415, 0.8308] 
2023-10-26 16:09:41.984782: Epoch time: 4.08 s 
2023-10-26 16:09:43.064816:  
2023-10-26 16:09:43.065128: Epoch 533 
2023-10-26 16:09:43.065370: Current learning rate: 0.00504 
2023-10-26 16:09:47.143690: train_loss -0.8849 
2023-10-26 16:09:47.144058: val_loss -0.8026 
2023-10-26 16:09:47.144326: Pseudo dice [0.8638, 0.8979, 0.9679, 0.2996, 0.8825] 
2023-10-26 16:09:47.144560: Epoch time: 4.08 s 
2023-10-26 16:09:48.256270:  
2023-10-26 16:09:48.256568: Epoch 534 
2023-10-26 16:09:48.256812: Current learning rate: 0.00503 
2023-10-26 16:09:52.229740: train_loss -0.8851 
2023-10-26 16:09:52.230141: val_loss -0.8144 
2023-10-26 16:09:52.230410: Pseudo dice [0.8573, 0.9036, 0.9676, 0.6355, 0.8716] 
2023-10-26 16:09:52.230642: Epoch time: 3.97 s 
2023-10-26 16:09:53.387691:  
2023-10-26 16:09:53.388004: Epoch 535 
2023-10-26 16:09:53.388259: Current learning rate: 0.00502 
2023-10-26 16:09:57.504817: train_loss -0.8923 
2023-10-26 16:09:57.505224: val_loss -0.8218 
2023-10-26 16:09:57.505488: Pseudo dice [0.8741, 0.9048, 0.9681, 0.6366, 0.8932] 
2023-10-26 16:09:57.505726: Epoch time: 4.12 s 
2023-10-26 16:09:58.615129:  
2023-10-26 16:09:58.615438: Epoch 536 
2023-10-26 16:09:58.615683: Current learning rate: 0.00501 
2023-10-26 16:10:02.701562: train_loss -0.8851 
2023-10-26 16:10:02.701983: val_loss -0.8078 
2023-10-26 16:10:02.702434: Pseudo dice [0.8663, 0.8975, 0.966, 0.6926, 0.8703] 
2023-10-26 16:10:02.702734: Epoch time: 4.09 s 
2023-10-26 16:10:03.861858:  
2023-10-26 16:10:03.862188: Epoch 537 
2023-10-26 16:10:03.862432: Current learning rate: 0.005 
2023-10-26 16:10:07.841614: train_loss -0.8744 
2023-10-26 16:10:07.842042: val_loss -0.8224 
2023-10-26 16:10:07.842351: Pseudo dice [0.8634, 0.9007, 0.9653, 0.6133, 0.908] 
2023-10-26 16:10:07.842854: Epoch time: 3.98 s 
2023-10-26 16:10:09.171274:  
2023-10-26 16:10:09.171633: Epoch 538 
2023-10-26 16:10:09.171953: Current learning rate: 0.00499 
2023-10-26 16:10:13.231457: train_loss -0.8824 
2023-10-26 16:10:13.231878: val_loss -0.816 
2023-10-26 16:10:13.232242: Pseudo dice [0.8733, 0.9092, 0.9686, 0.6747, 0.8361] 
2023-10-26 16:10:13.232584: Epoch time: 4.06 s 
2023-10-26 16:10:14.338944:  
2023-10-26 16:10:14.339243: Epoch 539 
2023-10-26 16:10:14.339487: Current learning rate: 0.00498 
2023-10-26 16:10:18.442464: train_loss -0.8823 
2023-10-26 16:10:18.444996: val_loss -0.8087 
2023-10-26 16:10:18.445277: Pseudo dice [0.8733, 0.8977, 0.9677, 0.6231, 0.8772] 
2023-10-26 16:10:18.445522: Epoch time: 4.1 s 
2023-10-26 16:10:19.581087:  
2023-10-26 16:10:19.581399: Epoch 540 
2023-10-26 16:10:19.581651: Current learning rate: 0.00497 
2023-10-26 16:10:23.755405: train_loss -0.8897 
2023-10-26 16:10:23.755792: val_loss -0.8096 
2023-10-26 16:10:23.756083: Pseudo dice [0.8625, 0.9008, 0.9672, 0.7158, 0.8808] 
2023-10-26 16:10:23.756315: Epoch time: 4.17 s 
2023-10-26 16:10:24.859603:  
2023-10-26 16:10:24.859922: Epoch 541 
2023-10-26 16:10:24.860171: Current learning rate: 0.00496 
2023-10-26 16:10:28.946849: train_loss -0.888 
2023-10-26 16:10:28.947266: val_loss -0.8047 
2023-10-26 16:10:28.947539: Pseudo dice [0.8685, 0.9057, 0.9678, 0.6953, 0.8333] 
2023-10-26 16:10:28.947785: Epoch time: 4.09 s 
2023-10-26 16:10:30.059895:  
2023-10-26 16:10:30.060208: Epoch 542 
2023-10-26 16:10:30.060469: Current learning rate: 0.00495 
2023-10-26 16:10:34.141005: train_loss -0.8859 
2023-10-26 16:10:34.141426: val_loss -0.8045 
2023-10-26 16:10:34.141697: Pseudo dice [0.8743, 0.9001, 0.9657, 0.5873, 0.8595] 
2023-10-26 16:10:34.141933: Epoch time: 4.08 s 
2023-10-26 16:10:35.250008:  
2023-10-26 16:10:35.250319: Epoch 543 
2023-10-26 16:10:35.250566: Current learning rate: 0.00494 
2023-10-26 16:10:39.191489: train_loss -0.8891 
2023-10-26 16:10:39.191907: val_loss -0.7976 
2023-10-26 16:10:39.192178: Pseudo dice [0.8706, 0.9047, 0.968, 0.7286, 0.816] 
2023-10-26 16:10:39.192414: Epoch time: 3.94 s 
2023-10-26 16:10:40.365264:  
2023-10-26 16:10:40.365637: Epoch 544 
2023-10-26 16:10:40.365930: Current learning rate: 0.00493 
2023-10-26 16:10:44.678500: train_loss -0.892 
2023-10-26 16:10:44.678941: val_loss -0.804 
2023-10-26 16:10:44.679368: Pseudo dice [0.8658, 0.9051, 0.9689, 0.6747, 0.8213] 
2023-10-26 16:10:44.679689: Epoch time: 4.31 s 
2023-10-26 16:10:45.791098:  
2023-10-26 16:10:45.791424: Epoch 545 
2023-10-26 16:10:45.791690: Current learning rate: 0.00492 
2023-10-26 16:10:49.917502: train_loss -0.8886 
2023-10-26 16:10:49.924580: val_loss -0.8269 
2023-10-26 16:10:49.924848: Pseudo dice [0.8675, 0.9016, 0.967, 0.7208, 0.8762] 
2023-10-26 16:10:49.925128: Epoch time: 4.13 s 
2023-10-26 16:10:51.039041:  
2023-10-26 16:10:51.039374: Epoch 546 
2023-10-26 16:10:51.039646: Current learning rate: 0.00491 
2023-10-26 16:10:55.129820: train_loss -0.891 
2023-10-26 16:10:55.130247: val_loss -0.8278 
2023-10-26 16:10:55.130518: Pseudo dice [0.8731, 0.9091, 0.9647, 0.5936, 0.8892] 
2023-10-26 16:10:55.130757: Epoch time: 4.09 s 
2023-10-26 16:10:56.234110:  
2023-10-26 16:10:56.234411: Epoch 547 
2023-10-26 16:10:56.234664: Current learning rate: 0.0049 
2023-10-26 16:11:00.346052: train_loss -0.8918 
2023-10-26 16:11:00.346498: val_loss -0.8081 
2023-10-26 16:11:00.346766: Pseudo dice [0.8646, 0.9019, 0.9676, 0.7061, 0.8513] 
2023-10-26 16:11:00.347007: Epoch time: 4.11 s 
2023-10-26 16:11:01.504651:  
2023-10-26 16:11:01.504979: Epoch 548 
2023-10-26 16:11:01.505231: Current learning rate: 0.00489 
2023-10-26 16:11:05.517759: train_loss -0.8787 
2023-10-26 16:11:05.518159: val_loss -0.8092 
2023-10-26 16:11:05.518428: Pseudo dice [0.8667, 0.8938, 0.9668, 0.3683, 0.8867] 
2023-10-26 16:11:05.518656: Epoch time: 4.01 s 
2023-10-26 16:11:06.617301:  
2023-10-26 16:11:06.617614: Epoch 549 
2023-10-26 16:11:06.617866: Current learning rate: 0.00488 
2023-10-26 16:11:10.577888: train_loss -0.8817 
2023-10-26 16:11:10.578312: val_loss -0.82 
2023-10-26 16:11:10.578576: Pseudo dice [0.8619, 0.8954, 0.9675, 0.7123, 0.8953] 
2023-10-26 16:11:10.578818: Epoch time: 3.96 s 
2023-10-26 16:11:11.814767:  
2023-10-26 16:11:11.815076: Epoch 550 
2023-10-26 16:11:11.815317: Current learning rate: 0.00487 
2023-10-26 16:11:15.979740: train_loss -0.8881 
2023-10-26 16:11:15.980129: val_loss -0.814 
2023-10-26 16:11:15.980394: Pseudo dice [0.8739, 0.9004, 0.9673, 0.5783, 0.8952] 
2023-10-26 16:11:15.980624: Epoch time: 4.17 s 
2023-10-26 16:11:17.273453:  
2023-10-26 16:11:17.273771: Epoch 551 
2023-10-26 16:11:17.274024: Current learning rate: 0.00486 
2023-10-26 16:11:21.378239: train_loss -0.8937 
2023-10-26 16:11:21.378619: val_loss -0.8311 
2023-10-26 16:11:21.378897: Pseudo dice [0.8651, 0.9018, 0.9684, 0.6896, 0.8875] 
2023-10-26 16:11:21.379136: Epoch time: 4.11 s 
2023-10-26 16:11:22.481354:  
2023-10-26 16:11:22.481663: Epoch 552 
2023-10-26 16:11:22.481925: Current learning rate: 0.00485 
2023-10-26 16:11:26.485042: train_loss -0.8809 
2023-10-26 16:11:26.485412: val_loss -0.8282 
2023-10-26 16:11:26.485680: Pseudo dice [0.863, 0.8951, 0.9684, 0.6813, 0.8792] 
2023-10-26 16:11:26.485920: Epoch time: 4.0 s 
2023-10-26 16:11:27.590355:  
2023-10-26 16:11:27.590675: Epoch 553 
2023-10-26 16:11:27.591003: Current learning rate: 0.00484 
2023-10-26 16:11:31.574237: train_loss -0.8887 
2023-10-26 16:11:31.574657: val_loss -0.8289 
2023-10-26 16:11:31.574924: Pseudo dice [0.8691, 0.9014, 0.9694, 0.6892, 0.8838] 
2023-10-26 16:11:31.575158: Epoch time: 3.98 s 
2023-10-26 16:11:32.687906:  
2023-10-26 16:11:32.688225: Epoch 554 
2023-10-26 16:11:32.688523: Current learning rate: 0.00484 
2023-10-26 16:11:36.735210: train_loss -0.8891 
2023-10-26 16:11:36.735586: val_loss -0.8083 
2023-10-26 16:11:36.735847: Pseudo dice [0.8651, 0.9016, 0.9663, 0.5313, 0.9023] 
2023-10-26 16:11:36.736082: Epoch time: 4.05 s 
2023-10-26 16:11:37.847140:  
2023-10-26 16:11:37.847443: Epoch 555 
2023-10-26 16:11:37.847713: Current learning rate: 0.00483 
2023-10-26 16:11:41.881851: train_loss -0.8946 
2023-10-26 16:11:41.882336: val_loss -0.8278 
2023-10-26 16:11:41.882812: Pseudo dice [0.8625, 0.9038, 0.9664, 0.6451, 0.8758] 
2023-10-26 16:11:41.883075: Epoch time: 4.04 s 
2023-10-26 16:11:43.086824:  
2023-10-26 16:11:43.087131: Epoch 556 
2023-10-26 16:11:43.087379: Current learning rate: 0.00482 
2023-10-26 16:11:47.140819: train_loss -0.8895 
2023-10-26 16:11:47.141249: val_loss -0.7731 
2023-10-26 16:11:47.141516: Pseudo dice [0.8629, 0.9013, 0.9673, 0.0, 0.8446] 
2023-10-26 16:11:47.141766: Epoch time: 4.05 s 
2023-10-26 16:11:48.477113:  
2023-10-26 16:11:48.477428: Epoch 557 
2023-10-26 16:11:48.477672: Current learning rate: 0.00481 
2023-10-26 16:11:52.558120: train_loss -0.8781 
2023-10-26 16:11:52.558505: val_loss -0.8196 
2023-10-26 16:11:52.558777: Pseudo dice [0.8704, 0.9043, 0.9668, 0.6872, 0.8806] 
2023-10-26 16:11:52.559021: Epoch time: 4.08 s 
2023-10-26 16:11:53.693039:  
2023-10-26 16:11:53.693374: Epoch 558 
2023-10-26 16:11:53.693624: Current learning rate: 0.0048 
2023-10-26 16:11:57.792103: train_loss -0.8885 
2023-10-26 16:11:57.792475: val_loss -0.8018 
2023-10-26 16:11:57.792743: Pseudo dice [0.8612, 0.9047, 0.9672, 0.5215, 0.8746] 
2023-10-26 16:11:57.792982: Epoch time: 4.1 s 
2023-10-26 16:11:58.938867:  
2023-10-26 16:11:58.939166: Epoch 559 
2023-10-26 16:11:58.939415: Current learning rate: 0.00479 
2023-10-26 16:12:03.040192: train_loss -0.8897 
2023-10-26 16:12:03.040584: val_loss -0.7685 
2023-10-26 16:12:03.040842: Pseudo dice [0.8783, 0.9054, 0.9669, 0.5564, 0.8406] 
2023-10-26 16:12:03.041079: Epoch time: 4.1 s 
2023-10-26 16:12:04.140067:  
2023-10-26 16:12:04.140380: Epoch 560 
2023-10-26 16:12:04.140627: Current learning rate: 0.00478 
2023-10-26 16:12:08.218261: train_loss -0.8884 
2023-10-26 16:12:08.218703: val_loss -0.8282 
2023-10-26 16:12:08.218994: Pseudo dice [0.8687, 0.8993, 0.9664, 0.617, 0.9107] 
2023-10-26 16:12:08.219267: Epoch time: 4.08 s 
2023-10-26 16:12:09.354759:  
2023-10-26 16:12:09.355067: Epoch 561 
2023-10-26 16:12:09.355310: Current learning rate: 0.00477 
2023-10-26 16:12:13.496668: train_loss -0.888 
2023-10-26 16:12:13.497312: val_loss -0.7905 
2023-10-26 16:12:13.497590: Pseudo dice [0.8578, 0.8975, 0.9669, 0.6173, 0.8495] 
2023-10-26 16:12:13.497837: Epoch time: 4.14 s 
2023-10-26 16:12:14.652950:  
2023-10-26 16:12:14.653245: Epoch 562 
2023-10-26 16:12:14.653493: Current learning rate: 0.00476 
2023-10-26 16:12:18.648553: train_loss -0.894 
2023-10-26 16:12:18.648942: val_loss -0.8143 
2023-10-26 16:12:18.649220: Pseudo dice [0.8606, 0.904, 0.9666, 0.5875, 0.8621] 
2023-10-26 16:12:18.649447: Epoch time: 4.0 s 
2023-10-26 16:12:19.962523:  
2023-10-26 16:12:19.962863: Epoch 563 
2023-10-26 16:12:19.963124: Current learning rate: 0.00475 
2023-10-26 16:12:24.097834: train_loss -0.8935 
2023-10-26 16:12:24.098259: val_loss -0.8048 
2023-10-26 16:12:24.098531: Pseudo dice [0.8693, 0.902, 0.9682, 0.7122, 0.8429] 
2023-10-26 16:12:24.098768: Epoch time: 4.14 s 
2023-10-26 16:12:25.243670:  
2023-10-26 16:12:25.243989: Epoch 564 
2023-10-26 16:12:25.244239: Current learning rate: 0.00474 
2023-10-26 16:12:29.380106: train_loss -0.8866 
2023-10-26 16:12:29.380535: val_loss -0.829 
2023-10-26 16:12:29.380924: Pseudo dice [0.8701, 0.9033, 0.9681, 0.6458, 0.8775] 
2023-10-26 16:12:29.381313: Epoch time: 4.14 s 
2023-10-26 16:12:30.520749:  
2023-10-26 16:12:30.521067: Epoch 565 
2023-10-26 16:12:30.521314: Current learning rate: 0.00473 
2023-10-26 16:12:34.614106: train_loss -0.858 
2023-10-26 16:12:34.614516: val_loss -0.7759 
2023-10-26 16:12:34.614791: Pseudo dice [0.8684, 0.8882, 0.9633, 0.0, 0.8769] 
2023-10-26 16:12:34.615063: Epoch time: 4.09 s 
2023-10-26 16:12:35.736450:  
2023-10-26 16:12:35.736789: Epoch 566 
2023-10-26 16:12:35.737067: Current learning rate: 0.00472 
2023-10-26 16:12:39.789789: train_loss -0.8745 
2023-10-26 16:12:39.790210: val_loss -0.7883 
2023-10-26 16:12:39.790481: Pseudo dice [0.8709, 0.9074, 0.964, 0.1265, 0.8681] 
2023-10-26 16:12:39.790727: Epoch time: 4.05 s 
2023-10-26 16:12:40.899974:  
2023-10-26 16:12:40.900293: Epoch 567 
2023-10-26 16:12:40.900537: Current learning rate: 0.00471 
2023-10-26 16:12:45.021343: train_loss -0.8857 
2023-10-26 16:12:45.022113: val_loss -0.7911 
2023-10-26 16:12:45.022592: Pseudo dice [0.8681, 0.9052, 0.9693, 0.2014, 0.8815] 
2023-10-26 16:12:45.022907: Epoch time: 4.12 s 
2023-10-26 16:12:46.164069:  
2023-10-26 16:12:46.164371: Epoch 568 
2023-10-26 16:12:46.164619: Current learning rate: 0.0047 
2023-10-26 16:12:50.191367: train_loss -0.8884 
2023-10-26 16:12:50.191790: val_loss -0.7929 
2023-10-26 16:12:50.192078: Pseudo dice [0.8693, 0.9115, 0.9646, 0.3861, 0.8931] 
2023-10-26 16:12:50.192319: Epoch time: 4.03 s 
2023-10-26 16:12:51.298799:  
2023-10-26 16:12:51.299136: Epoch 569 
2023-10-26 16:12:51.299442: Current learning rate: 0.00469 
2023-10-26 16:12:55.361426: train_loss -0.8902 
2023-10-26 16:12:55.362307: val_loss -0.7889 
2023-10-26 16:12:55.362668: Pseudo dice [0.8592, 0.902, 0.9688, 0.1022, 0.8986] 
2023-10-26 16:12:55.363014: Epoch time: 4.06 s 
2023-10-26 16:12:56.664828:  
2023-10-26 16:12:56.665153: Epoch 570 
2023-10-26 16:12:56.665407: Current learning rate: 0.00468 
2023-10-26 16:13:00.673150: train_loss -0.892 
2023-10-26 16:13:00.673660: val_loss -0.7786 
2023-10-26 16:13:00.674111: Pseudo dice [0.8624, 0.9034, 0.9678, 0.2336, 0.8671] 
2023-10-26 16:13:00.674396: Epoch time: 4.01 s 
2023-10-26 16:13:01.803761:  
2023-10-26 16:13:01.804088: Epoch 571 
2023-10-26 16:13:01.804340: Current learning rate: 0.00467 
2023-10-26 16:13:05.814838: train_loss -0.893 
2023-10-26 16:13:05.815212: val_loss -0.7987 
2023-10-26 16:13:05.815477: Pseudo dice [0.8645, 0.909, 0.966, 0.5383, 0.9012] 
2023-10-26 16:13:05.815705: Epoch time: 4.01 s 
2023-10-26 16:13:06.938336:  
2023-10-26 16:13:06.938647: Epoch 572 
2023-10-26 16:13:06.938897: Current learning rate: 0.00466 
2023-10-26 16:13:11.095379: train_loss -0.8905 
2023-10-26 16:13:11.095777: val_loss -0.7667 
2023-10-26 16:13:11.096059: Pseudo dice [0.8546, 0.8904, 0.9669, 0.0091, 0.8802] 
2023-10-26 16:13:11.096309: Epoch time: 4.16 s 
2023-10-26 16:13:12.215934:  
2023-10-26 16:13:12.216248: Epoch 573 
2023-10-26 16:13:12.216515: Current learning rate: 0.00465 
2023-10-26 16:13:16.246318: train_loss -0.8886 
2023-10-26 16:13:16.246819: val_loss -0.8145 
2023-10-26 16:13:16.247116: Pseudo dice [0.8645, 0.8985, 0.967, 0.5451, 0.8921] 
2023-10-26 16:13:16.247518: Epoch time: 4.03 s 
2023-10-26 16:13:17.373474:  
2023-10-26 16:13:17.373761: Epoch 574 
2023-10-26 16:13:17.374008: Current learning rate: 0.00464 
2023-10-26 16:13:21.426275: train_loss -0.8834 
2023-10-26 16:13:21.426721: val_loss -0.814 
2023-10-26 16:13:21.427068: Pseudo dice [0.8613, 0.8997, 0.967, 0.6033, 0.8741] 
2023-10-26 16:13:21.427570: Epoch time: 4.05 s 
2023-10-26 16:13:22.559477:  
2023-10-26 16:13:22.559816: Epoch 575 
2023-10-26 16:13:22.560083: Current learning rate: 0.00463 
2023-10-26 16:13:26.553411: train_loss -0.8865 
2023-10-26 16:13:26.553788: val_loss -0.8198 
2023-10-26 16:13:26.554061: Pseudo dice [0.8645, 0.8982, 0.969, 0.4868, 0.8913] 
2023-10-26 16:13:26.554309: Epoch time: 3.99 s 
2023-10-26 16:13:27.857830:  
2023-10-26 16:13:27.858155: Epoch 576 
2023-10-26 16:13:27.858402: Current learning rate: 0.00462 
2023-10-26 16:13:31.889791: train_loss -0.8895 
2023-10-26 16:13:31.890215: val_loss -0.7905 
2023-10-26 16:13:31.890482: Pseudo dice [0.8672, 0.8952, 0.969, 0.4916, 0.8833] 
2023-10-26 16:13:31.890733: Epoch time: 4.03 s 
2023-10-26 16:13:33.015358:  
2023-10-26 16:13:33.015659: Epoch 577 
2023-10-26 16:13:33.015909: Current learning rate: 0.00461 
2023-10-26 16:13:37.188007: train_loss -0.8917 
2023-10-26 16:13:37.188389: val_loss -0.8014 
2023-10-26 16:13:37.188649: Pseudo dice [0.8634, 0.901, 0.9679, 0.4836, 0.8681] 
2023-10-26 16:13:37.188905: Epoch time: 4.17 s 
2023-10-26 16:13:38.303151:  
2023-10-26 16:13:38.303439: Epoch 578 
2023-10-26 16:13:38.303682: Current learning rate: 0.0046 
2023-10-26 16:13:42.468849: train_loss -0.8911 
2023-10-26 16:13:42.469263: val_loss -0.8308 
2023-10-26 16:13:42.469541: Pseudo dice [0.8616, 0.9006, 0.9673, 0.6069, 0.8891] 
2023-10-26 16:13:42.469820: Epoch time: 4.17 s 
2023-10-26 16:13:43.628695:  
2023-10-26 16:13:43.629011: Epoch 579 
2023-10-26 16:13:43.629258: Current learning rate: 0.00459 
2023-10-26 16:13:47.527546: train_loss -0.8955 
2023-10-26 16:13:47.528046: val_loss -0.817 
2023-10-26 16:13:47.528313: Pseudo dice [0.8661, 0.8931, 0.9676, 0.6276, 0.8858] 
2023-10-26 16:13:47.528558: Epoch time: 3.9 s 
2023-10-26 16:13:48.652768:  
2023-10-26 16:13:48.653106: Epoch 580 
2023-10-26 16:13:48.653357: Current learning rate: 0.00458 
2023-10-26 16:13:52.753706: train_loss -0.8919 
2023-10-26 16:13:52.754129: val_loss -0.8021 
2023-10-26 16:13:52.754398: Pseudo dice [0.8615, 0.8933, 0.9677, 0.5589, 0.8731] 
2023-10-26 16:13:52.754635: Epoch time: 4.1 s 
2023-10-26 16:13:53.875743:  
2023-10-26 16:13:53.876060: Epoch 581 
2023-10-26 16:13:53.876316: Current learning rate: 0.00457 
2023-10-26 16:13:58.006177: train_loss -0.8939 
2023-10-26 16:13:58.006611: val_loss -0.824 
2023-10-26 16:13:58.006961: Pseudo dice [0.8654, 0.8951, 0.9669, 0.5919, 0.9016] 
2023-10-26 16:13:58.007429: Epoch time: 4.13 s 
2023-10-26 16:13:59.313162:  
2023-10-26 16:13:59.313528: Epoch 582 
2023-10-26 16:13:59.313797: Current learning rate: 0.00456 
2023-10-26 16:14:03.644560: train_loss -0.8934 
2023-10-26 16:14:03.645026: val_loss -0.8213 
2023-10-26 16:14:03.645292: Pseudo dice [0.8634, 0.9022, 0.9684, 0.726, 0.8987] 
2023-10-26 16:14:03.645534: Epoch time: 4.33 s 
2023-10-26 16:14:04.763202:  
2023-10-26 16:14:04.763523: Epoch 583 
2023-10-26 16:14:04.763765: Current learning rate: 0.00455 
2023-10-26 16:14:08.889245: train_loss -0.8958 
2023-10-26 16:14:08.889628: val_loss -0.832 
2023-10-26 16:14:08.889898: Pseudo dice [0.8637, 0.9055, 0.967, 0.6088, 0.9041] 
2023-10-26 16:14:08.890152: Epoch time: 4.13 s 
2023-10-26 16:14:10.017592:  
2023-10-26 16:14:10.017899: Epoch 584 
2023-10-26 16:14:10.018146: Current learning rate: 0.00454 
2023-10-26 16:14:14.099078: train_loss -0.8959 
2023-10-26 16:14:14.099597: val_loss -0.8069 
2023-10-26 16:14:14.100009: Pseudo dice [0.8673, 0.8972, 0.9687, 0.6146, 0.8507] 
2023-10-26 16:14:14.100344: Epoch time: 4.08 s 
2023-10-26 16:14:15.300512:  
2023-10-26 16:14:15.300839: Epoch 585 
2023-10-26 16:14:15.301289: Current learning rate: 0.00453 
2023-10-26 16:14:19.434709: train_loss -0.8955 
2023-10-26 16:14:19.435129: val_loss -0.8153 
2023-10-26 16:14:19.435435: Pseudo dice [0.8739, 0.8978, 0.9679, 0.705, 0.8853] 
2023-10-26 16:14:19.435673: Epoch time: 4.13 s 
2023-10-26 16:14:20.592962:  
2023-10-26 16:14:20.593364: Epoch 586 
2023-10-26 16:14:20.593777: Current learning rate: 0.00452 
2023-10-26 16:14:24.662804: train_loss -0.8997 
2023-10-26 16:14:24.663223: val_loss -0.8025 
2023-10-26 16:14:24.663492: Pseudo dice [0.8649, 0.8985, 0.9669, 0.6386, 0.8861] 
2023-10-26 16:14:24.663729: Epoch time: 4.07 s 
2023-10-26 16:14:25.776917:  
2023-10-26 16:14:25.777212: Epoch 587 
2023-10-26 16:14:25.777478: Current learning rate: 0.00451 
2023-10-26 16:14:29.825294: train_loss -0.8988 
2023-10-26 16:14:29.825958: val_loss -0.8217 
2023-10-26 16:14:29.826234: Pseudo dice [0.8666, 0.8989, 0.9688, 0.7685, 0.849] 
2023-10-26 16:14:29.826455: Epoch time: 4.05 s 
2023-10-26 16:14:31.127897:  
2023-10-26 16:14:31.128271: Epoch 588 
2023-10-26 16:14:31.128516: Current learning rate: 0.0045 
2023-10-26 16:14:35.157508: train_loss -0.8981 
2023-10-26 16:14:35.158039: val_loss -0.8071 
2023-10-26 16:14:35.158399: Pseudo dice [0.8632, 0.8948, 0.9696, 0.64, 0.8829] 
2023-10-26 16:14:35.158730: Epoch time: 4.03 s 
2023-10-26 16:14:36.292566:  
2023-10-26 16:14:36.292869: Epoch 589 
2023-10-26 16:14:36.293121: Current learning rate: 0.00449 
2023-10-26 16:14:40.398419: train_loss -0.8945 
2023-10-26 16:14:40.398814: val_loss -0.7961 
2023-10-26 16:14:40.399075: Pseudo dice [0.8698, 0.909, 0.9647, 0.6935, 0.864] 
2023-10-26 16:14:40.399302: Epoch time: 4.11 s 
2023-10-26 16:14:41.541590:  
2023-10-26 16:14:41.541888: Epoch 590 
2023-10-26 16:14:41.542138: Current learning rate: 0.00448 
2023-10-26 16:14:45.591073: train_loss -0.8964 
2023-10-26 16:14:45.591467: val_loss -0.8138 
2023-10-26 16:14:45.591755: Pseudo dice [0.8632, 0.8973, 0.9663, 0.699, 0.8853] 
2023-10-26 16:14:45.592000: Epoch time: 4.05 s 
2023-10-26 16:14:46.738904:  
2023-10-26 16:14:46.739228: Epoch 591 
2023-10-26 16:14:46.739476: Current learning rate: 0.00447 
2023-10-26 16:14:50.806513: train_loss -0.8936 
2023-10-26 16:14:50.806944: val_loss -0.7822 
2023-10-26 16:14:50.807211: Pseudo dice [0.8626, 0.8926, 0.9674, 0.5263, 0.8919] 
2023-10-26 16:14:50.807470: Epoch time: 4.07 s 
2023-10-26 16:14:51.929928:  
2023-10-26 16:14:51.930228: Epoch 592 
2023-10-26 16:14:51.930477: Current learning rate: 0.00446 
2023-10-26 16:14:56.089645: train_loss -0.8983 
2023-10-26 16:14:56.090183: val_loss -0.7885 
2023-10-26 16:14:56.090538: Pseudo dice [0.8674, 0.8977, 0.965, 0.5725, 0.8975] 
2023-10-26 16:14:56.090818: Epoch time: 4.16 s 
2023-10-26 16:14:57.210126:  
2023-10-26 16:14:57.210442: Epoch 593 
2023-10-26 16:14:57.210692: Current learning rate: 0.00445 
2023-10-26 16:15:01.361828: train_loss -0.8939 
2023-10-26 16:15:01.362227: val_loss -0.8128 
2023-10-26 16:15:01.362535: Pseudo dice [0.8626, 0.9046, 0.9666, 0.6363, 0.897] 
2023-10-26 16:15:01.362778: Epoch time: 4.15 s 
2023-10-26 16:15:02.700113:  
2023-10-26 16:15:02.700447: Epoch 594 
2023-10-26 16:15:02.700696: Current learning rate: 0.00444 
2023-10-26 16:15:06.866239: train_loss -0.8903 
2023-10-26 16:15:06.866669: val_loss -0.7923 
2023-10-26 16:15:06.866947: Pseudo dice [0.8675, 0.8995, 0.9697, 0.6103, 0.8383] 
2023-10-26 16:15:06.867192: Epoch time: 4.17 s 
2023-10-26 16:15:08.064446:  
2023-10-26 16:15:08.064789: Epoch 595 
2023-10-26 16:15:08.065118: Current learning rate: 0.00443 
2023-10-26 16:15:12.193126: train_loss -0.8915 
2023-10-26 16:15:12.193615: val_loss -0.8086 
2023-10-26 16:15:12.193931: Pseudo dice [0.8682, 0.8965, 0.9684, 0.5957, 0.8692] 
2023-10-26 16:15:12.194205: Epoch time: 4.13 s 
2023-10-26 16:15:13.316534:  
2023-10-26 16:15:13.316842: Epoch 596 
2023-10-26 16:15:13.317102: Current learning rate: 0.00442 
2023-10-26 16:15:17.194497: train_loss -0.887 
2023-10-26 16:15:17.194927: val_loss -0.8106 
2023-10-26 16:15:17.195194: Pseudo dice [0.865, 0.9087, 0.9687, 0.7401, 0.8372] 
2023-10-26 16:15:17.195450: Epoch time: 3.88 s 
2023-10-26 16:15:18.393569:  
2023-10-26 16:15:18.393888: Epoch 597 
2023-10-26 16:15:18.394147: Current learning rate: 0.00441 
2023-10-26 16:15:22.551337: train_loss -0.8929 
2023-10-26 16:15:22.551806: val_loss -0.7893 
2023-10-26 16:15:22.552168: Pseudo dice [0.8712, 0.8943, 0.9679, 0.0018, 0.8948] 
2023-10-26 16:15:22.552485: Epoch time: 4.16 s 
2023-10-26 16:15:23.727123:  
2023-10-26 16:15:23.727413: Epoch 598 
2023-10-26 16:15:23.727656: Current learning rate: 0.0044 
2023-10-26 16:15:27.798532: train_loss -0.8925 
2023-10-26 16:15:27.798947: val_loss -0.8137 
2023-10-26 16:15:27.799277: Pseudo dice [0.8621, 0.8965, 0.9676, 0.5369, 0.8751] 
2023-10-26 16:15:27.799530: Epoch time: 4.07 s 
2023-10-26 16:15:28.977362:  
2023-10-26 16:15:28.977674: Epoch 599 
2023-10-26 16:15:28.977925: Current learning rate: 0.00439 
2023-10-26 16:15:33.053243: train_loss -0.8955 
2023-10-26 16:15:33.053622: val_loss -0.8306 
2023-10-26 16:15:33.053901: Pseudo dice [0.8597, 0.8977, 0.9687, 0.5781, 0.9092] 
2023-10-26 16:15:33.054140: Epoch time: 4.08 s 
2023-10-26 16:15:34.484953:  
2023-10-26 16:15:34.485256: Epoch 600 
2023-10-26 16:15:34.485499: Current learning rate: 0.00438 
2023-10-26 16:15:38.608267: train_loss -0.8944 
2023-10-26 16:15:38.608635: val_loss -0.8165 
2023-10-26 16:15:38.608913: Pseudo dice [0.8687, 0.9064, 0.9677, 0.7256, 0.8537] 
2023-10-26 16:15:38.609140: Epoch time: 4.12 s 
2023-10-26 16:15:39.733901:  
2023-10-26 16:15:39.734195: Epoch 601 
2023-10-26 16:15:39.734432: Current learning rate: 0.00437 
2023-10-26 16:15:43.994773: train_loss -0.877 
2023-10-26 16:15:43.995199: val_loss -0.8027 
2023-10-26 16:15:43.995490: Pseudo dice [0.8633, 0.8929, 0.9644, 0.4992, 0.8484] 
2023-10-26 16:15:43.995733: Epoch time: 4.26 s 
2023-10-26 16:15:45.153524:  
2023-10-26 16:15:45.153833: Epoch 602 
2023-10-26 16:15:45.154081: Current learning rate: 0.00436 
2023-10-26 16:15:49.362273: train_loss -0.88 
2023-10-26 16:15:49.362672: val_loss -0.8213 
2023-10-26 16:15:49.363269: Pseudo dice [0.8622, 0.902, 0.9634, 0.6978, 0.8679] 
2023-10-26 16:15:49.363702: Epoch time: 4.21 s 
2023-10-26 16:15:50.481309:  
2023-10-26 16:15:50.481610: Epoch 603 
2023-10-26 16:15:50.481855: Current learning rate: 0.00435 
2023-10-26 16:15:54.568630: train_loss -0.879 
2023-10-26 16:15:54.569497: val_loss -0.8163 
2023-10-26 16:15:54.569930: Pseudo dice [0.8643, 0.8836, 0.9669, 0.7417, 0.8995] 
2023-10-26 16:15:54.570299: Epoch time: 4.09 s 
2023-10-26 16:15:55.687344:  
2023-10-26 16:15:55.687647: Epoch 604 
2023-10-26 16:15:55.687901: Current learning rate: 0.00434 
2023-10-26 16:15:59.893962: train_loss -0.8852 
2023-10-26 16:15:59.894359: val_loss -0.8186 
2023-10-26 16:15:59.894620: Pseudo dice [0.8639, 0.9, 0.9671, 0.7567, 0.8739] 
2023-10-26 16:15:59.894865: Epoch time: 4.21 s 
2023-10-26 16:16:01.001734:  
2023-10-26 16:16:01.002035: Epoch 605 
2023-10-26 16:16:01.002285: Current learning rate: 0.00433 
2023-10-26 16:16:05.041011: train_loss -0.8862 
2023-10-26 16:16:05.041389: val_loss -0.7988 
2023-10-26 16:16:05.041642: Pseudo dice [0.8672, 0.9007, 0.9667, 0.775, 0.8467] 
2023-10-26 16:16:05.041869: Epoch time: 4.04 s 
2023-10-26 16:16:06.317858:  
2023-10-26 16:16:06.318170: Epoch 606 
2023-10-26 16:16:06.318412: Current learning rate: 0.00432 
2023-10-26 16:16:10.592346: train_loss -0.8902 
2023-10-26 16:16:10.592774: val_loss -0.7852 
2023-10-26 16:16:10.593051: Pseudo dice [0.8713, 0.8982, 0.9695, 0.5487, 0.8393] 
2023-10-26 16:16:10.593289: Epoch time: 4.28 s 
2023-10-26 16:16:11.699901:  
2023-10-26 16:16:11.700192: Epoch 607 
2023-10-26 16:16:11.700440: Current learning rate: 0.00431 
2023-10-26 16:16:15.960722: train_loss -0.8931 
2023-10-26 16:16:15.961187: val_loss -0.7978 
2023-10-26 16:16:15.961546: Pseudo dice [0.8693, 0.9039, 0.9684, 0.6955, 0.8626] 
2023-10-26 16:16:15.961822: Epoch time: 4.26 s 
2023-10-26 16:16:17.060618:  
2023-10-26 16:16:17.060921: Epoch 608 
2023-10-26 16:16:17.061169: Current learning rate: 0.0043 
2023-10-26 16:16:21.278370: train_loss -0.8875 
2023-10-26 16:16:21.278760: val_loss -0.7958 
2023-10-26 16:16:21.279040: Pseudo dice [0.8757, 0.903, 0.9683, 0.5644, 0.8725] 
2023-10-26 16:16:21.279385: Epoch time: 4.22 s 
2023-10-26 16:16:22.386376:  
2023-10-26 16:16:22.386682: Epoch 609 
2023-10-26 16:16:22.386941: Current learning rate: 0.00429 
2023-10-26 16:16:26.505777: train_loss -0.8969 
2023-10-26 16:16:26.506191: val_loss -0.7585 
2023-10-26 16:16:26.506453: Pseudo dice [0.8669, 0.8963, 0.968, 0.0, 0.8534] 
2023-10-26 16:16:26.506710: Epoch time: 4.12 s 
2023-10-26 16:16:27.608639:  
2023-10-26 16:16:27.608953: Epoch 610 
2023-10-26 16:16:27.609207: Current learning rate: 0.00429 
2023-10-26 16:16:31.876822: train_loss -0.8884 
2023-10-26 16:16:31.877568: val_loss -0.7827 
2023-10-26 16:16:31.877888: Pseudo dice [0.8668, 0.903, 0.9665, 0.6551, 0.8544] 
2023-10-26 16:16:31.878130: Epoch time: 4.27 s 
2023-10-26 16:16:32.981725:  
2023-10-26 16:16:32.982044: Epoch 611 
2023-10-26 16:16:32.982299: Current learning rate: 0.00428 
2023-10-26 16:16:37.173748: train_loss -0.8939 
2023-10-26 16:16:37.174120: val_loss -0.7834 
2023-10-26 16:16:37.174374: Pseudo dice [0.867, 0.8894, 0.9683, 0.5345, 0.8605] 
2023-10-26 16:16:37.174599: Epoch time: 4.19 s 
2023-10-26 16:16:38.289582:  
2023-10-26 16:16:38.289889: Epoch 612 
2023-10-26 16:16:38.290147: Current learning rate: 0.00427 
2023-10-26 16:16:42.529239: train_loss -0.9009 
2023-10-26 16:16:42.529668: val_loss -0.8244 
2023-10-26 16:16:42.529946: Pseudo dice [0.8591, 0.8928, 0.9675, 0.5655, 0.8951] 
2023-10-26 16:16:42.530196: Epoch time: 4.24 s 
2023-10-26 16:16:43.820378:  
2023-10-26 16:16:43.820673: Epoch 613 
2023-10-26 16:16:43.820921: Current learning rate: 0.00426 
2023-10-26 16:16:48.139313: train_loss -0.8899 
2023-10-26 16:16:48.139670: val_loss -0.7959 
2023-10-26 16:16:48.139933: Pseudo dice [0.8589, 0.8891, 0.9639, 0.5265, 0.8553] 
2023-10-26 16:16:48.140163: Epoch time: 4.32 s 
2023-10-26 16:16:49.240072:  
2023-10-26 16:16:49.240372: Epoch 614 
2023-10-26 16:16:49.240624: Current learning rate: 0.00425 
2023-10-26 16:16:53.383216: train_loss -0.8879 
2023-10-26 16:16:53.383676: val_loss -0.8225 
2023-10-26 16:16:53.383950: Pseudo dice [0.8602, 0.9043, 0.9673, 0.633, 0.884] 
2023-10-26 16:16:53.384205: Epoch time: 4.14 s 
2023-10-26 16:16:54.492099:  
2023-10-26 16:16:54.492386: Epoch 615 
2023-10-26 16:16:54.492628: Current learning rate: 0.00424 
2023-10-26 16:16:58.386683: train_loss -0.8922 
2023-10-26 16:16:58.387060: val_loss -0.7929 
2023-10-26 16:16:58.387321: Pseudo dice [0.8629, 0.8994, 0.9677, 0.4977, 0.8234] 
2023-10-26 16:16:58.387559: Epoch time: 3.9 s 
2023-10-26 16:16:59.501460:  
2023-10-26 16:16:59.501769: Epoch 616 
2023-10-26 16:16:59.502021: Current learning rate: 0.00423 
2023-10-26 16:17:03.570224: train_loss -0.8909 
2023-10-26 16:17:03.570632: val_loss -0.8278 
2023-10-26 16:17:03.570907: Pseudo dice [0.8654, 0.8987, 0.9676, 0.6007, 0.8968] 
2023-10-26 16:17:03.571154: Epoch time: 4.07 s 
2023-10-26 16:17:04.701380:  
2023-10-26 16:17:04.701700: Epoch 617 
2023-10-26 16:17:04.701952: Current learning rate: 0.00422 
2023-10-26 16:17:08.754086: train_loss -0.8974 
2023-10-26 16:17:08.754532: val_loss -0.809 
2023-10-26 16:17:08.754913: Pseudo dice [0.857, 0.8998, 0.9667, 0.5697, 0.865] 
2023-10-26 16:17:08.755160: Epoch time: 4.05 s 
2023-10-26 16:17:09.888423:  
2023-10-26 16:17:09.888724: Epoch 618 
2023-10-26 16:17:09.888979: Current learning rate: 0.00421 
2023-10-26 16:17:14.017612: train_loss -0.8955 
2023-10-26 16:17:14.018020: val_loss -0.8418 
2023-10-26 16:17:14.018285: Pseudo dice [0.8628, 0.9021, 0.9667, 0.6791, 0.9165] 
2023-10-26 16:17:14.018527: Epoch time: 4.13 s 
2023-10-26 16:17:15.300033:  
2023-10-26 16:17:15.300356: Epoch 619 
2023-10-26 16:17:15.300602: Current learning rate: 0.0042 
2023-10-26 16:17:19.393842: train_loss -0.8986 
2023-10-26 16:17:19.394241: val_loss -0.7894 
2023-10-26 16:17:19.394501: Pseudo dice [0.8623, 0.9022, 0.9673, 0.5702, 0.8415] 
2023-10-26 16:17:19.394727: Epoch time: 4.09 s 
2023-10-26 16:17:20.499635:  
2023-10-26 16:17:20.499952: Epoch 620 
2023-10-26 16:17:20.500205: Current learning rate: 0.00419 
2023-10-26 16:17:24.717248: train_loss -0.8941 
2023-10-26 16:17:24.717654: val_loss -0.8134 
2023-10-26 16:17:24.717913: Pseudo dice [0.8701, 0.8983, 0.9673, 0.6527, 0.8705] 
2023-10-26 16:17:24.718141: Epoch time: 4.22 s 
2023-10-26 16:17:25.827976:  
2023-10-26 16:17:25.828275: Epoch 621 
2023-10-26 16:17:25.828533: Current learning rate: 0.00418 
2023-10-26 16:17:29.878986: train_loss -0.8998 
2023-10-26 16:17:29.879338: val_loss -0.8267 
2023-10-26 16:17:29.879600: Pseudo dice [0.8646, 0.9003, 0.9671, 0.7274, 0.8649] 
2023-10-26 16:17:29.879839: Epoch time: 4.05 s 
2023-10-26 16:17:30.998189:  
2023-10-26 16:17:30.998500: Epoch 622 
2023-10-26 16:17:30.998744: Current learning rate: 0.00417 
2023-10-26 16:17:35.249065: train_loss -0.8932 
2023-10-26 16:17:35.249424: val_loss -0.8143 
2023-10-26 16:17:35.249679: Pseudo dice [0.8637, 0.8973, 0.9689, 0.5996, 0.8842] 
2023-10-26 16:17:35.249913: Epoch time: 4.25 s 
2023-10-26 16:17:36.370373:  
2023-10-26 16:17:36.370668: Epoch 623 
2023-10-26 16:17:36.370916: Current learning rate: 0.00416 
2023-10-26 16:17:40.500512: train_loss -0.9017 
2023-10-26 16:17:40.500987: val_loss -0.7966 
2023-10-26 16:17:40.501338: Pseudo dice [0.8662, 0.8935, 0.9682, 0.6594, 0.8163] 
2023-10-26 16:17:40.501820: Epoch time: 4.13 s 
2023-10-26 16:17:41.631232:  
2023-10-26 16:17:41.631535: Epoch 624 
2023-10-26 16:17:41.631787: Current learning rate: 0.00415 
2023-10-26 16:17:45.571588: train_loss -0.9012 
2023-10-26 16:17:45.571981: val_loss -0.8195 
2023-10-26 16:17:45.572248: Pseudo dice [0.8654, 0.8998, 0.9673, 0.6071, 0.8768] 
2023-10-26 16:17:45.572477: Epoch time: 3.94 s 
2023-10-26 16:17:46.856700:  
2023-10-26 16:17:46.857015: Epoch 625 
2023-10-26 16:17:46.857269: Current learning rate: 0.00414 
2023-10-26 16:17:51.069685: train_loss -0.9026 
2023-10-26 16:17:51.070122: val_loss -0.8327 
2023-10-26 16:17:51.070386: Pseudo dice [0.8651, 0.8988, 0.9687, 0.6893, 0.8762] 
2023-10-26 16:17:51.070624: Epoch time: 4.21 s 
2023-10-26 16:17:52.191720:  
2023-10-26 16:17:52.192101: Epoch 626 
2023-10-26 16:17:52.192447: Current learning rate: 0.00413 
2023-10-26 16:17:56.382325: train_loss -0.8958 
2023-10-26 16:17:56.382751: val_loss -0.8276 
2023-10-26 16:17:56.383032: Pseudo dice [0.8609, 0.8992, 0.9673, 0.6268, 0.9084] 
2023-10-26 16:17:56.383273: Epoch time: 4.19 s 
2023-10-26 16:17:57.498846:  
2023-10-26 16:17:57.499160: Epoch 627 
2023-10-26 16:17:57.499407: Current learning rate: 0.00412 
2023-10-26 16:18:01.665446: train_loss -0.8943 
2023-10-26 16:18:01.665842: val_loss -0.8161 
2023-10-26 16:18:01.666136: Pseudo dice [0.8691, 0.8976, 0.9659, 0.6277, 0.8871] 
2023-10-26 16:18:01.666384: Epoch time: 4.17 s 
2023-10-26 16:18:02.794078:  
2023-10-26 16:18:02.794453: Epoch 628 
2023-10-26 16:18:02.794710: Current learning rate: 0.00411 
2023-10-26 16:18:06.888864: train_loss -0.8898 
2023-10-26 16:18:06.889808: val_loss -0.7998 
2023-10-26 16:18:06.890090: Pseudo dice [0.8655, 0.9027, 0.9682, 0.7126, 0.8512] 
2023-10-26 16:18:06.890333: Epoch time: 4.1 s 
2023-10-26 16:18:08.002295:  
2023-10-26 16:18:08.002598: Epoch 629 
2023-10-26 16:18:08.002850: Current learning rate: 0.0041 
2023-10-26 16:18:12.061854: train_loss -0.8862 
2023-10-26 16:18:12.062359: val_loss -0.789 
2023-10-26 16:18:12.062634: Pseudo dice [0.8649, 0.8986, 0.9659, 0.594, 0.8869] 
2023-10-26 16:18:12.062870: Epoch time: 4.06 s 
2023-10-26 16:18:13.188335:  
2023-10-26 16:18:13.188627: Epoch 630 
2023-10-26 16:18:13.188879: Current learning rate: 0.00409 
2023-10-26 16:18:17.333545: train_loss -0.8912 
2023-10-26 16:18:17.334087: val_loss -0.8212 
2023-10-26 16:18:17.334444: Pseudo dice [0.8642, 0.8964, 0.9645, 0.696, 0.8897] 
2023-10-26 16:18:17.334855: Epoch time: 4.15 s 
2023-10-26 16:18:18.675272:  
2023-10-26 16:18:18.675637: Epoch 631 
2023-10-26 16:18:18.675934: Current learning rate: 0.00408 
2023-10-26 16:18:22.796565: train_loss -0.8956 
2023-10-26 16:18:22.796998: val_loss -0.8115 
2023-10-26 16:18:22.797274: Pseudo dice [0.8648, 0.9014, 0.9676, 0.5403, 0.8666] 
2023-10-26 16:18:22.797542: Epoch time: 4.12 s 
2023-10-26 16:18:23.925308:  
2023-10-26 16:18:23.925615: Epoch 632 
2023-10-26 16:18:23.925877: Current learning rate: 0.00407 
2023-10-26 16:18:28.130686: train_loss -0.8985 
2023-10-26 16:18:28.131240: val_loss -0.8024 
2023-10-26 16:18:28.131528: Pseudo dice [0.8579, 0.8955, 0.9686, 0.7039, 0.8332] 
2023-10-26 16:18:28.131764: Epoch time: 4.21 s 
2023-10-26 16:18:29.274570:  
2023-10-26 16:18:29.274885: Epoch 633 
2023-10-26 16:18:29.275141: Current learning rate: 0.00406 
2023-10-26 16:18:33.478714: train_loss -0.9017 
2023-10-26 16:18:33.479099: val_loss -0.8094 
2023-10-26 16:18:33.479361: Pseudo dice [0.8617, 0.9045, 0.9686, 0.7721, 0.8452] 
2023-10-26 16:18:33.479600: Epoch time: 4.2 s 
2023-10-26 16:18:34.593932:  
2023-10-26 16:18:34.594233: Epoch 634 
2023-10-26 16:18:34.594481: Current learning rate: 0.00405 
2023-10-26 16:18:38.506688: train_loss -0.8927 
2023-10-26 16:18:38.507083: val_loss -0.7635 
2023-10-26 16:18:38.507344: Pseudo dice [0.8687, 0.8986, 0.9673, 0.5375, 0.8695] 
2023-10-26 16:18:38.507589: Epoch time: 3.91 s 
2023-10-26 16:18:39.645823:  
2023-10-26 16:18:39.646142: Epoch 635 
2023-10-26 16:18:39.646388: Current learning rate: 0.00404 
2023-10-26 16:18:43.755336: train_loss -0.8879 
2023-10-26 16:18:43.755789: val_loss -0.7927 
2023-10-26 16:18:43.756071: Pseudo dice [0.8615, 0.9004, 0.9666, 0.6607, 0.8729] 
2023-10-26 16:18:43.756306: Epoch time: 4.11 s 
2023-10-26 16:18:44.877141:  
2023-10-26 16:18:44.877540: Epoch 636 
2023-10-26 16:18:44.877783: Current learning rate: 0.00403 
2023-10-26 16:18:48.971408: train_loss -0.8902 
2023-10-26 16:18:48.971803: val_loss -0.8078 
2023-10-26 16:18:48.972084: Pseudo dice [0.871, 0.9114, 0.9645, 0.7184, 0.8327] 
2023-10-26 16:18:48.972330: Epoch time: 4.09 s 
2023-10-26 16:18:50.311483:  
2023-10-26 16:18:50.311786: Epoch 637 
2023-10-26 16:18:50.312040: Current learning rate: 0.00402 
2023-10-26 16:18:54.430554: train_loss -0.8923 
2023-10-26 16:18:54.430936: val_loss -0.8226 
2023-10-26 16:18:54.431201: Pseudo dice [0.8668, 0.8926, 0.9686, 0.5865, 0.8982] 
2023-10-26 16:18:54.431432: Epoch time: 4.12 s 
2023-10-26 16:18:55.553826:  
2023-10-26 16:18:55.554135: Epoch 638 
2023-10-26 16:18:55.554379: Current learning rate: 0.00401 
2023-10-26 16:18:59.546333: train_loss -0.9019 
2023-10-26 16:18:59.546727: val_loss -0.8056 
2023-10-26 16:18:59.546993: Pseudo dice [0.8628, 0.8934, 0.9665, 0.5907, 0.8717] 
2023-10-26 16:18:59.547227: Epoch time: 3.99 s 
2023-10-26 16:19:00.675866:  
2023-10-26 16:19:00.676275: Epoch 639 
2023-10-26 16:19:00.676522: Current learning rate: 0.004 
2023-10-26 16:19:04.855981: train_loss -0.9011 
2023-10-26 16:19:04.856407: val_loss -0.7996 
2023-10-26 16:19:04.856674: Pseudo dice [0.8657, 0.9006, 0.9667, 0.6608, 0.873] 
2023-10-26 16:19:04.856916: Epoch time: 4.18 s 
2023-10-26 16:19:05.976313:  
2023-10-26 16:19:05.976614: Epoch 640 
2023-10-26 16:19:05.976863: Current learning rate: 0.00399 
2023-10-26 16:19:10.187767: train_loss -0.8932 
2023-10-26 16:19:10.188178: val_loss -0.8055 
2023-10-26 16:19:10.188492: Pseudo dice [0.8702, 0.9018, 0.9669, 0.663, 0.8314] 
2023-10-26 16:19:10.188775: Epoch time: 4.21 s 
2023-10-26 16:19:11.302141:  
2023-10-26 16:19:11.302435: Epoch 641 
2023-10-26 16:19:11.302692: Current learning rate: 0.00398 
2023-10-26 16:19:15.405007: train_loss -0.8944 
2023-10-26 16:19:15.405397: val_loss -0.8233 
2023-10-26 16:19:15.405658: Pseudo dice [0.8609, 0.8989, 0.967, 0.6613, 0.8957] 
2023-10-26 16:19:15.405969: Epoch time: 4.1 s 
2023-10-26 16:19:16.560659:  
2023-10-26 16:19:16.560968: Epoch 642 
2023-10-26 16:19:16.561211: Current learning rate: 0.00397 
2023-10-26 16:19:20.575140: train_loss -0.9008 
2023-10-26 16:19:20.575528: val_loss -0.813 
2023-10-26 16:19:20.575794: Pseudo dice [0.8631, 0.8941, 0.9687, 0.6601, 0.9044] 
2023-10-26 16:19:20.576048: Epoch time: 4.02 s 
2023-10-26 16:19:21.861571:  
2023-10-26 16:19:21.861955: Epoch 643 
2023-10-26 16:19:21.862206: Current learning rate: 0.00396 
2023-10-26 16:19:25.994511: train_loss -0.9008 
2023-10-26 16:19:25.994954: val_loss -0.8044 
2023-10-26 16:19:25.995226: Pseudo dice [0.8692, 0.9012, 0.9678, 0.7183, 0.8702] 
2023-10-26 16:19:25.995476: Epoch time: 4.13 s 
2023-10-26 16:19:27.118616:  
2023-10-26 16:19:27.118912: Epoch 644 
2023-10-26 16:19:27.119163: Current learning rate: 0.00395 
2023-10-26 16:19:31.268902: train_loss -0.9 
2023-10-26 16:19:31.269290: val_loss -0.7778 
2023-10-26 16:19:31.269554: Pseudo dice [0.8715, 0.907, 0.9677, 0.6282, 0.8331] 
2023-10-26 16:19:31.269789: Epoch time: 4.15 s 
2023-10-26 16:19:32.385342:  
2023-10-26 16:19:32.385637: Epoch 645 
2023-10-26 16:19:32.385887: Current learning rate: 0.00394 
2023-10-26 16:19:36.423462: train_loss -0.8962 
2023-10-26 16:19:36.423830: val_loss -0.7942 
2023-10-26 16:19:36.424088: Pseudo dice [0.8556, 0.8946, 0.9657, 0.1524, 0.8924] 
2023-10-26 16:19:36.424317: Epoch time: 4.04 s 
2023-10-26 16:19:37.549417:  
2023-10-26 16:19:37.549704: Epoch 646 
2023-10-26 16:19:37.549945: Current learning rate: 0.00393 
2023-10-26 16:19:41.745136: train_loss -0.8934 
2023-10-26 16:19:41.745559: val_loss -0.8146 
2023-10-26 16:19:41.745826: Pseudo dice [0.8636, 0.9038, 0.9659, 0.704, 0.8813] 
2023-10-26 16:19:41.746065: Epoch time: 4.2 s 
2023-10-26 16:19:42.856887:  
2023-10-26 16:19:42.857212: Epoch 647 
2023-10-26 16:19:42.857457: Current learning rate: 0.00392 
2023-10-26 16:19:47.009574: train_loss -0.8989 
2023-10-26 16:19:47.009947: val_loss -0.8025 
2023-10-26 16:19:47.010205: Pseudo dice [0.8668, 0.9045, 0.9662, 0.6308, 0.867] 
2023-10-26 16:19:47.010426: Epoch time: 4.15 s 
2023-10-26 16:19:48.132120:  
2023-10-26 16:19:48.132417: Epoch 648 
2023-10-26 16:19:48.132657: Current learning rate: 0.00391 
2023-10-26 16:19:52.183265: train_loss -0.8992 
2023-10-26 16:19:52.183667: val_loss -0.823 
2023-10-26 16:19:52.183950: Pseudo dice [0.8695, 0.905, 0.9655, 0.6492, 0.8668] 
2023-10-26 16:19:52.184201: Epoch time: 4.05 s 
2023-10-26 16:19:53.513297:  
2023-10-26 16:19:53.513586: Epoch 649 
2023-10-26 16:19:53.513830: Current learning rate: 0.0039 
2023-10-26 16:19:57.562740: train_loss -0.9004 
2023-10-26 16:19:57.563139: val_loss -0.7994 
2023-10-26 16:19:57.563418: Pseudo dice [0.8704, 0.9021, 0.9676, 0.6471, 0.856] 
2023-10-26 16:19:57.563638: Epoch time: 4.05 s 
2023-10-26 16:19:58.812402:  
2023-10-26 16:19:58.812699: Epoch 650 
2023-10-26 16:19:58.812953: Current learning rate: 0.00389 
2023-10-26 16:20:03.012569: train_loss -0.9017 
2023-10-26 16:20:03.012957: val_loss -0.8118 
2023-10-26 16:20:03.013221: Pseudo dice [0.8633, 0.8952, 0.9671, 0.6691, 0.8849] 
2023-10-26 16:20:03.013461: Epoch time: 4.2 s 
2023-10-26 16:20:04.128744:  
2023-10-26 16:20:04.129067: Epoch 651 
2023-10-26 16:20:04.129314: Current learning rate: 0.00388 
2023-10-26 16:20:08.339301: train_loss -0.8979 
2023-10-26 16:20:08.339686: val_loss -0.8124 
2023-10-26 16:20:08.339950: Pseudo dice [0.8624, 0.9034, 0.9642, 0.5164, 0.8809] 
2023-10-26 16:20:08.340193: Epoch time: 4.21 s 
2023-10-26 16:20:09.481441:  
2023-10-26 16:20:09.481748: Epoch 652 
2023-10-26 16:20:09.481995: Current learning rate: 0.00387 
2023-10-26 16:20:13.578167: train_loss -0.8933 
2023-10-26 16:20:13.578541: val_loss -0.8073 
2023-10-26 16:20:13.578799: Pseudo dice [0.8678, 0.9019, 0.9661, 0.5112, 0.9019] 
2023-10-26 16:20:13.579032: Epoch time: 4.1 s 
2023-10-26 16:20:14.688140:  
2023-10-26 16:20:14.688425: Epoch 653 
2023-10-26 16:20:14.688665: Current learning rate: 0.00386 
2023-10-26 16:20:18.845543: train_loss -0.9007 
2023-10-26 16:20:18.845926: val_loss -0.8046 
2023-10-26 16:20:18.846199: Pseudo dice [0.8677, 0.897, 0.9684, 0.5993, 0.8586] 
2023-10-26 16:20:18.846431: Epoch time: 4.16 s 
2023-10-26 16:20:20.018122:  
2023-10-26 16:20:20.018436: Epoch 654 
2023-10-26 16:20:20.018707: Current learning rate: 0.00385 
2023-10-26 16:20:24.164903: train_loss -0.896 
2023-10-26 16:20:24.165266: val_loss -0.7744 
2023-10-26 16:20:24.165531: Pseudo dice [0.8663, 0.9093, 0.9655, 0.6371, 0.828] 
2023-10-26 16:20:24.165768: Epoch time: 4.15 s 
2023-10-26 16:20:25.474406:  
2023-10-26 16:20:25.474719: Epoch 655 
2023-10-26 16:20:25.474963: Current learning rate: 0.00384 
2023-10-26 16:20:29.664430: train_loss -0.9037 
2023-10-26 16:20:29.664829: val_loss -0.7967 
2023-10-26 16:20:29.665100: Pseudo dice [0.8609, 0.9034, 0.9669, 0.6791, 0.8707] 
2023-10-26 16:20:29.665333: Epoch time: 4.19 s 
2023-10-26 16:20:30.789073:  
2023-10-26 16:20:30.789394: Epoch 656 
2023-10-26 16:20:30.789642: Current learning rate: 0.00383 
2023-10-26 16:20:34.996900: train_loss -0.8972 
2023-10-26 16:20:34.997338: val_loss -0.8091 
2023-10-26 16:20:34.997602: Pseudo dice [0.8605, 0.9018, 0.9658, 0.6299, 0.8833] 
2023-10-26 16:20:34.997843: Epoch time: 4.21 s 
2023-10-26 16:20:36.112434:  
2023-10-26 16:20:36.112744: Epoch 657 
2023-10-26 16:20:36.112996: Current learning rate: 0.00382 
2023-10-26 16:20:40.353263: train_loss -0.8998 
2023-10-26 16:20:40.353719: val_loss -0.799 
2023-10-26 16:20:40.354047: Pseudo dice [0.8679, 0.9071, 0.9665, 0.6936, 0.8565] 
2023-10-26 16:20:40.354332: Epoch time: 4.24 s 
2023-10-26 16:20:41.476818:  
2023-10-26 16:20:41.477141: Epoch 658 
2023-10-26 16:20:41.477389: Current learning rate: 0.00381 
2023-10-26 16:20:45.631869: train_loss -0.9007 
2023-10-26 16:20:45.632303: val_loss -0.8219 
2023-10-26 16:20:45.632629: Pseudo dice [0.8618, 0.8948, 0.965, 0.6931, 0.9051] 
2023-10-26 16:20:45.632896: Epoch time: 4.16 s 
2023-10-26 16:20:46.746505:  
2023-10-26 16:20:46.746787: Epoch 659 
2023-10-26 16:20:46.747029: Current learning rate: 0.0038 
2023-10-26 16:20:50.808402: train_loss -0.896 
2023-10-26 16:20:50.808814: val_loss -0.8308 
2023-10-26 16:20:50.809082: Pseudo dice [0.8621, 0.8954, 0.9686, 0.6401, 0.9118] 
2023-10-26 16:20:50.809315: Epoch time: 4.06 s 
2023-10-26 16:20:51.940847:  
2023-10-26 16:20:51.941159: Epoch 660 
2023-10-26 16:20:51.941430: Current learning rate: 0.00379 
2023-10-26 16:20:56.057378: train_loss -0.903 
2023-10-26 16:20:56.057850: val_loss -0.8073 
2023-10-26 16:20:56.058132: Pseudo dice [0.8679, 0.9051, 0.9679, 0.6402, 0.8494] 
2023-10-26 16:20:56.058356: Epoch time: 4.12 s 
2023-10-26 16:20:57.367761:  
2023-10-26 16:20:57.368073: Epoch 661 
2023-10-26 16:20:57.368319: Current learning rate: 0.00378 
2023-10-26 16:21:01.525097: train_loss -0.906 
2023-10-26 16:21:01.525494: val_loss -0.81 
2023-10-26 16:21:01.525767: Pseudo dice [0.864, 0.9065, 0.9682, 0.6471, 0.8539] 
2023-10-26 16:21:01.526021: Epoch time: 4.16 s 
2023-10-26 16:21:02.668115:  
2023-10-26 16:21:02.668429: Epoch 662 
2023-10-26 16:21:02.668682: Current learning rate: 0.00377 
2023-10-26 16:21:06.656541: train_loss -0.8988 
2023-10-26 16:21:06.656959: val_loss -0.8209 
2023-10-26 16:21:06.657229: Pseudo dice [0.8677, 0.8985, 0.9692, 0.652, 0.8654] 
2023-10-26 16:21:06.657457: Epoch time: 3.99 s 
2023-10-26 16:21:07.779335:  
2023-10-26 16:21:07.779626: Epoch 663 
2023-10-26 16:21:07.779890: Current learning rate: 0.00376 
2023-10-26 16:21:11.839527: train_loss -0.9017 
2023-10-26 16:21:11.839921: val_loss -0.8001 
2023-10-26 16:21:11.840206: Pseudo dice [0.8672, 0.8996, 0.966, 0.6518, 0.8506] 
2023-10-26 16:21:11.840439: Epoch time: 4.06 s 
2023-10-26 16:21:12.972416:  
2023-10-26 16:21:12.972742: Epoch 664 
2023-10-26 16:21:12.973003: Current learning rate: 0.00375 
2023-10-26 16:21:16.994446: train_loss -0.8949 
2023-10-26 16:21:16.994852: val_loss -0.8225 
2023-10-26 16:21:16.995137: Pseudo dice [0.8715, 0.9051, 0.9675, 0.6649, 0.8907] 
2023-10-26 16:21:16.995384: Epoch time: 4.02 s 
2023-10-26 16:21:18.135623:  
2023-10-26 16:21:18.135915: Epoch 665 
2023-10-26 16:21:18.136159: Current learning rate: 0.00374 
2023-10-26 16:21:22.174424: train_loss -0.9054 
2023-10-26 16:21:22.174856: val_loss -0.7926 
2023-10-26 16:21:22.175132: Pseudo dice [0.8693, 0.9042, 0.967, 0.6296, 0.8505] 
2023-10-26 16:21:22.175380: Epoch time: 4.04 s 
2023-10-26 16:21:23.315992:  
2023-10-26 16:21:23.316299: Epoch 666 
2023-10-26 16:21:23.316549: Current learning rate: 0.00373 
2023-10-26 16:21:27.330539: train_loss -0.8995 
2023-10-26 16:21:27.330976: val_loss -0.7949 
2023-10-26 16:21:27.331271: Pseudo dice [0.8656, 0.9021, 0.9675, 0.6624, 0.8705] 
2023-10-26 16:21:27.331515: Epoch time: 4.02 s 
2023-10-26 16:21:28.679963:  
2023-10-26 16:21:28.680274: Epoch 667 
2023-10-26 16:21:28.680522: Current learning rate: 0.00372 
2023-10-26 16:21:32.842076: train_loss -0.8971 
2023-10-26 16:21:32.842479: val_loss -0.8288 
2023-10-26 16:21:32.842739: Pseudo dice [0.8688, 0.9002, 0.9667, 0.6442, 0.9056] 
2023-10-26 16:21:32.842988: Epoch time: 4.16 s 
2023-10-26 16:21:33.985702:  
2023-10-26 16:21:33.986016: Epoch 668 
2023-10-26 16:21:33.986270: Current learning rate: 0.00371 
2023-10-26 16:21:38.102262: train_loss -0.8941 
2023-10-26 16:21:38.103117: val_loss -0.8181 
2023-10-26 16:21:38.103441: Pseudo dice [0.8701, 0.9066, 0.9669, 0.7345, 0.8621] 
2023-10-26 16:21:38.103744: Epoch time: 4.12 s 
2023-10-26 16:21:39.236182:  
2023-10-26 16:21:39.236470: Epoch 669 
2023-10-26 16:21:39.236733: Current learning rate: 0.0037 
2023-10-26 16:21:43.302635: train_loss -0.8948 
2023-10-26 16:21:43.303035: val_loss -0.7809 
2023-10-26 16:21:43.303324: Pseudo dice [0.8724, 0.9035, 0.9682, 0.6057, 0.8768] 
2023-10-26 16:21:43.303558: Epoch time: 4.07 s 
2023-10-26 16:21:44.463516:  
2023-10-26 16:21:44.463822: Epoch 670 
2023-10-26 16:21:44.464067: Current learning rate: 0.00369 
2023-10-26 16:21:48.300354: train_loss -0.8994 
2023-10-26 16:21:48.300735: val_loss -0.7937 
2023-10-26 16:21:48.301204: Pseudo dice [0.8731, 0.9031, 0.9649, 0.6297, 0.8762] 
2023-10-26 16:21:48.301512: Epoch time: 3.84 s 
2023-10-26 16:21:49.441244:  
2023-10-26 16:21:49.441554: Epoch 671 
2023-10-26 16:21:49.441810: Current learning rate: 0.00368 
2023-10-26 16:21:53.490076: train_loss -0.8978 
2023-10-26 16:21:53.490475: val_loss -0.7949 
2023-10-26 16:21:53.490742: Pseudo dice [0.8716, 0.9043, 0.9678, 0.7302, 0.8238] 
2023-10-26 16:21:53.490985: Epoch time: 4.05 s 
2023-10-26 16:21:54.686338:  
2023-10-26 16:21:54.686652: Epoch 672 
2023-10-26 16:21:54.686922: Current learning rate: 0.00367 
2023-10-26 16:21:58.754457: train_loss -0.9014 
2023-10-26 16:21:58.754842: val_loss -0.8109 
2023-10-26 16:21:58.755133: Pseudo dice [0.8656, 0.9022, 0.9653, 0.6798, 0.8507] 
2023-10-26 16:21:58.755366: Epoch time: 4.07 s 
2023-10-26 16:22:00.083517:  
2023-10-26 16:22:00.083826: Epoch 673 
2023-10-26 16:22:00.084092: Current learning rate: 0.00366 
2023-10-26 16:22:04.053473: train_loss -0.8952 
2023-10-26 16:22:04.053899: val_loss -0.7883 
2023-10-26 16:22:04.054173: Pseudo dice [0.8648, 0.9018, 0.9652, 0.6324, 0.9006] 
2023-10-26 16:22:04.054417: Epoch time: 3.97 s 
2023-10-26 16:22:05.195215:  
2023-10-26 16:22:05.195509: Epoch 674 
2023-10-26 16:22:05.195751: Current learning rate: 0.00365 
2023-10-26 16:22:09.204298: train_loss -0.9001 
2023-10-26 16:22:09.204683: val_loss -0.8204 
2023-10-26 16:22:09.204958: Pseudo dice [0.863, 0.8934, 0.9661, 0.5501, 0.895] 
2023-10-26 16:22:09.205201: Epoch time: 4.01 s 
2023-10-26 16:22:10.368004:  
2023-10-26 16:22:10.368298: Epoch 675 
2023-10-26 16:22:10.368548: Current learning rate: 0.00364 
2023-10-26 16:22:14.417264: train_loss -0.8959 
2023-10-26 16:22:14.417670: val_loss -0.807 
2023-10-26 16:22:14.417951: Pseudo dice [0.8657, 0.8974, 0.9657, 0.6803, 0.8852] 
2023-10-26 16:22:14.418190: Epoch time: 4.05 s 
2023-10-26 16:22:15.819134:  
2023-10-26 16:22:15.819448: Epoch 676 
2023-10-26 16:22:15.819696: Current learning rate: 0.00363 
2023-10-26 16:22:19.984114: train_loss -0.8962 
2023-10-26 16:22:19.984517: val_loss -0.7968 
2023-10-26 16:22:19.984797: Pseudo dice [0.8649, 0.8998, 0.9672, 0.6192, 0.8838] 
2023-10-26 16:22:19.985063: Epoch time: 4.17 s 
2023-10-26 16:22:21.142409:  
2023-10-26 16:22:21.142704: Epoch 677 
2023-10-26 16:22:21.142970: Current learning rate: 0.00362 
2023-10-26 16:22:25.160250: train_loss -0.8966 
2023-10-26 16:22:25.160656: val_loss -0.817 
2023-10-26 16:22:25.160932: Pseudo dice [0.8668, 0.904, 0.965, 0.6172, 0.8849] 
2023-10-26 16:22:25.161163: Epoch time: 4.02 s 
2023-10-26 16:22:26.304235:  
2023-10-26 16:22:26.304531: Epoch 678 
2023-10-26 16:22:26.304792: Current learning rate: 0.00361 
2023-10-26 16:22:30.397159: train_loss -0.9034 
2023-10-26 16:22:30.397639: val_loss -0.8166 
2023-10-26 16:22:30.397971: Pseudo dice [0.865, 0.9073, 0.9647, 0.5801, 0.9064] 
2023-10-26 16:22:30.398226: Epoch time: 4.09 s 
2023-10-26 16:22:31.747745:  
2023-10-26 16:22:31.748074: Epoch 679 
2023-10-26 16:22:31.748332: Current learning rate: 0.0036 
2023-10-26 16:22:35.781155: train_loss -0.9018 
2023-10-26 16:22:35.781542: val_loss -0.7981 
2023-10-26 16:22:35.781814: Pseudo dice [0.8719, 0.8972, 0.9659, 0.6513, 0.8618] 
2023-10-26 16:22:35.782050: Epoch time: 4.03 s 
2023-10-26 16:22:36.984965:  
2023-10-26 16:22:36.985272: Epoch 680 
2023-10-26 16:22:36.985519: Current learning rate: 0.00359 
2023-10-26 16:22:41.064248: train_loss -0.901 
2023-10-26 16:22:41.064649: val_loss -0.8126 
2023-10-26 16:22:41.064928: Pseudo dice [0.8587, 0.899, 0.966, 0.7198, 0.8764] 
2023-10-26 16:22:41.065165: Epoch time: 4.08 s 
2023-10-26 16:22:42.224363:  
2023-10-26 16:22:42.224650: Epoch 681 
2023-10-26 16:22:42.224901: Current learning rate: 0.00358 
2023-10-26 16:22:46.417429: train_loss -0.907 
2023-10-26 16:22:46.417803: val_loss -0.8122 
2023-10-26 16:22:46.418070: Pseudo dice [0.8729, 0.9004, 0.966, 0.5821, 0.883] 
2023-10-26 16:22:46.418301: Epoch time: 4.19 s 
2023-10-26 16:22:47.598352:  
2023-10-26 16:22:47.598654: Epoch 682 
2023-10-26 16:22:47.598905: Current learning rate: 0.00357 
2023-10-26 16:22:51.685360: train_loss -0.8971 
2023-10-26 16:22:51.685736: val_loss -0.7609 
2023-10-26 16:22:51.686003: Pseudo dice [0.8473, 0.8862, 0.9677, 0.0, 0.911] 
2023-10-26 16:22:51.686244: Epoch time: 4.09 s 
2023-10-26 16:22:52.819900:  
2023-10-26 16:22:52.820194: Epoch 683 
2023-10-26 16:22:52.820441: Current learning rate: 0.00356 
2023-10-26 16:22:56.663903: train_loss -0.8953 
2023-10-26 16:22:56.664310: val_loss -0.8161 
2023-10-26 16:22:56.664586: Pseudo dice [0.8689, 0.9069, 0.9645, 0.5758, 0.8806] 
2023-10-26 16:22:56.664836: Epoch time: 3.84 s 
2023-10-26 16:22:57.799662:  
2023-10-26 16:22:57.799962: Epoch 684 
2023-10-26 16:22:57.800204: Current learning rate: 0.00355 
2023-10-26 16:23:01.955154: train_loss -0.903 
2023-10-26 16:23:01.955549: val_loss -0.7835 
2023-10-26 16:23:01.955812: Pseudo dice [0.8691, 0.9026, 0.967, 0.4986, 0.839] 
2023-10-26 16:23:01.956051: Epoch time: 4.16 s 
2023-10-26 16:23:03.271313:  
2023-10-26 16:23:03.271620: Epoch 685 
2023-10-26 16:23:03.271865: Current learning rate: 0.00354 
2023-10-26 16:23:07.335270: train_loss -0.8973 
2023-10-26 16:23:07.335631: val_loss -0.7782 
2023-10-26 16:23:07.335907: Pseudo dice [0.8735, 0.9018, 0.9674, 0.3664, 0.8336] 
2023-10-26 16:23:07.336139: Epoch time: 4.06 s 
2023-10-26 16:23:08.482656:  
2023-10-26 16:23:08.482997: Epoch 686 
2023-10-26 16:23:08.483239: Current learning rate: 0.00353 
2023-10-26 16:23:12.590015: train_loss -0.8991 
2023-10-26 16:23:12.590407: val_loss -0.8103 
2023-10-26 16:23:12.590679: Pseudo dice [0.8743, 0.9083, 0.9665, 0.5586, 0.8635] 
2023-10-26 16:23:12.590928: Epoch time: 4.11 s 
2023-10-26 16:23:13.721893:  
2023-10-26 16:23:13.722185: Epoch 687 
2023-10-26 16:23:13.722449: Current learning rate: 0.00352 
2023-10-26 16:23:17.844565: train_loss -0.9005 
2023-10-26 16:23:17.845042: val_loss -0.8189 
2023-10-26 16:23:17.845320: Pseudo dice [0.8687, 0.892, 0.9687, 0.603, 0.8964] 
2023-10-26 16:23:17.845568: Epoch time: 4.12 s 
2023-10-26 16:23:19.024840:  
2023-10-26 16:23:19.025158: Epoch 688 
2023-10-26 16:23:19.025410: Current learning rate: 0.00351 
2023-10-26 16:23:23.120136: train_loss -0.9044 
2023-10-26 16:23:23.120519: val_loss -0.8036 
2023-10-26 16:23:23.120779: Pseudo dice [0.8669, 0.8963, 0.9681, 0.6849, 0.8501] 
2023-10-26 16:23:23.121017: Epoch time: 4.1 s 
2023-10-26 16:23:24.251903:  
2023-10-26 16:23:24.252186: Epoch 689 
2023-10-26 16:23:24.252427: Current learning rate: 0.0035 
2023-10-26 16:23:28.421809: train_loss -0.9029 
2023-10-26 16:23:28.422278: val_loss -0.8208 
2023-10-26 16:23:28.422560: Pseudo dice [0.8683, 0.9063, 0.9668, 0.6991, 0.8939] 
2023-10-26 16:23:28.422816: Epoch time: 4.17 s 
2023-10-26 16:23:29.558910:  
2023-10-26 16:23:29.559206: Epoch 690 
2023-10-26 16:23:29.559457: Current learning rate: 0.00349 
2023-10-26 16:23:33.665008: train_loss -0.9013 
2023-10-26 16:23:33.665444: val_loss -0.8121 
2023-10-26 16:23:33.665719: Pseudo dice [0.8648, 0.9063, 0.9666, 0.6232, 0.8723] 
2023-10-26 16:23:33.665972: Epoch time: 4.11 s 
2023-10-26 16:23:35.003971:  
2023-10-26 16:23:35.004281: Epoch 691 
2023-10-26 16:23:35.004539: Current learning rate: 0.00348 
2023-10-26 16:23:39.163007: train_loss -0.9003 
2023-10-26 16:23:39.163455: val_loss -0.797 
2023-10-26 16:23:39.163729: Pseudo dice [0.8666, 0.8957, 0.9659, 0.5407, 0.8688] 
2023-10-26 16:23:39.163977: Epoch time: 4.16 s 
2023-10-26 16:23:40.362272:  
2023-10-26 16:23:40.362596: Epoch 692 
2023-10-26 16:23:40.362849: Current learning rate: 0.00346 
2023-10-26 16:23:44.378644: train_loss -0.9049 
2023-10-26 16:23:44.379047: val_loss -0.8162 
2023-10-26 16:23:44.379304: Pseudo dice [0.8653, 0.9043, 0.9666, 0.655, 0.8979] 
2023-10-26 16:23:44.379540: Epoch time: 4.02 s 
2023-10-26 16:23:45.531035:  
2023-10-26 16:23:45.531334: Epoch 693 
2023-10-26 16:23:45.531574: Current learning rate: 0.00345 
2023-10-26 16:23:49.665206: train_loss -0.8993 
2023-10-26 16:23:49.665616: val_loss -0.8111 
2023-10-26 16:23:49.665910: Pseudo dice [0.8658, 0.9018, 0.9668, 0.6063, 0.8897] 
2023-10-26 16:23:49.666139: Epoch time: 4.13 s 
2023-10-26 16:23:50.831461:  
2023-10-26 16:23:50.831750: Epoch 694 
2023-10-26 16:23:50.831995: Current learning rate: 0.00344 
2023-10-26 16:23:54.833469: train_loss -0.9043 
2023-10-26 16:23:54.833889: val_loss -0.7899 
2023-10-26 16:23:54.834157: Pseudo dice [0.8698, 0.9089, 0.962, 0.2301, 0.8535] 
2023-10-26 16:23:54.834402: Epoch time: 4.0 s 
2023-10-26 16:23:55.980673:  
2023-10-26 16:23:55.980979: Epoch 695 
2023-10-26 16:23:55.981231: Current learning rate: 0.00343 
2023-10-26 16:23:59.987638: train_loss -0.8966 
2023-10-26 16:23:59.988047: val_loss -0.8247 
2023-10-26 16:23:59.988329: Pseudo dice [0.872, 0.9072, 0.9662, 0.5919, 0.8818] 
2023-10-26 16:23:59.988557: Epoch time: 4.01 s 
2023-10-26 16:24:01.163808:  
2023-10-26 16:24:01.164160: Epoch 696 
2023-10-26 16:24:01.164477: Current learning rate: 0.00342 
2023-10-26 16:24:05.280980: train_loss -0.897 
2023-10-26 16:24:05.281380: val_loss -0.8099 
2023-10-26 16:24:05.281645: Pseudo dice [0.8612, 0.8973, 0.9649, 0.6186, 0.8831] 
2023-10-26 16:24:05.281882: Epoch time: 4.12 s 
2023-10-26 16:24:06.648936:  
2023-10-26 16:24:06.649309: Epoch 697 
2023-10-26 16:24:06.649634: Current learning rate: 0.00341 
2023-10-26 16:24:10.768647: train_loss -0.8998 
2023-10-26 16:24:10.769062: val_loss -0.8007 
2023-10-26 16:24:10.769348: Pseudo dice [0.8609, 0.8917, 0.9662, 0.5217, 0.8616] 
2023-10-26 16:24:10.769587: Epoch time: 4.12 s 
2023-10-26 16:24:11.948747:  
2023-10-26 16:24:11.949061: Epoch 698 
2023-10-26 16:24:11.949308: Current learning rate: 0.0034 
2023-10-26 16:24:15.956732: train_loss -0.8989 
2023-10-26 16:24:15.957144: val_loss -0.8068 
2023-10-26 16:24:15.957411: Pseudo dice [0.8664, 0.898, 0.968, 0.5102, 0.8841] 
2023-10-26 16:24:15.957639: Epoch time: 4.01 s 
2023-10-26 16:24:17.106290:  
2023-10-26 16:24:17.106602: Epoch 699 
2023-10-26 16:24:17.106861: Current learning rate: 0.00339 
2023-10-26 16:24:21.209847: train_loss -0.9046 
2023-10-26 16:24:21.210300: val_loss -0.8025 
2023-10-26 16:24:21.210569: Pseudo dice [0.8645, 0.903, 0.9667, 0.5609, 0.8736] 
2023-10-26 16:24:21.210817: Epoch time: 4.1 s 
2023-10-26 16:24:22.500594:  
2023-10-26 16:24:22.500991: Epoch 700 
2023-10-26 16:24:22.501250: Current learning rate: 0.00338 
2023-10-26 16:24:26.713763: train_loss -0.897 
2023-10-26 16:24:26.714157: val_loss -0.8192 
2023-10-26 16:24:26.714435: Pseudo dice [0.8717, 0.9057, 0.966, 0.6268, 0.873] 
2023-10-26 16:24:26.714669: Epoch time: 4.21 s 
2023-10-26 16:24:27.918250:  
2023-10-26 16:24:27.918621: Epoch 701 
2023-10-26 16:24:27.918950: Current learning rate: 0.00337 
2023-10-26 16:24:32.044777: train_loss -0.9026 
2023-10-26 16:24:32.045263: val_loss -0.8021 
2023-10-26 16:24:32.045558: Pseudo dice [0.8692, 0.8973, 0.9675, 0.5398, 0.89] 
2023-10-26 16:24:32.045830: Epoch time: 4.13 s 
2023-10-26 16:24:33.194248:  
2023-10-26 16:24:33.194565: Epoch 702 
2023-10-26 16:24:33.194832: Current learning rate: 0.00336 
2023-10-26 16:24:37.400172: train_loss -0.9048 
2023-10-26 16:24:37.400605: val_loss -0.7956 
2023-10-26 16:24:37.400894: Pseudo dice [0.8728, 0.8929, 0.9674, 0.4546, 0.873] 
2023-10-26 16:24:37.401156: Epoch time: 4.21 s 
2023-10-26 16:24:38.734044:  
2023-10-26 16:24:38.734353: Epoch 703 
2023-10-26 16:24:38.734612: Current learning rate: 0.00335 
2023-10-26 16:24:42.919027: train_loss -0.9092 
2023-10-26 16:24:42.919495: val_loss -0.8027 
2023-10-26 16:24:42.919754: Pseudo dice [0.8686, 0.9018, 0.9679, 0.6426, 0.8661] 
2023-10-26 16:24:42.920217: Epoch time: 4.19 s 
2023-10-26 16:24:44.067806:  
2023-10-26 16:24:44.068116: Epoch 704 
2023-10-26 16:24:44.068369: Current learning rate: 0.00334 
2023-10-26 16:24:48.325377: train_loss -0.9033 
2023-10-26 16:24:48.325934: val_loss -0.8189 
2023-10-26 16:24:48.326351: Pseudo dice [0.8688, 0.8992, 0.9666, 0.6652, 0.8781] 
2023-10-26 16:24:48.326698: Epoch time: 4.26 s 
2023-10-26 16:24:49.493350:  
2023-10-26 16:24:49.493659: Epoch 705 
2023-10-26 16:24:49.493933: Current learning rate: 0.00333 
2023-10-26 16:24:53.699087: train_loss -0.9026 
2023-10-26 16:24:53.699552: val_loss -0.8404 
2023-10-26 16:24:53.699821: Pseudo dice [0.8616, 0.9026, 0.9681, 0.6952, 0.9037] 
2023-10-26 16:24:53.700060: Epoch time: 4.21 s 
2023-10-26 16:24:54.840922:  
2023-10-26 16:24:54.841237: Epoch 706 
2023-10-26 16:24:54.841496: Current learning rate: 0.00332 
2023-10-26 16:24:58.843787: train_loss -0.9056 
2023-10-26 16:24:58.844202: val_loss -0.8104 
2023-10-26 16:24:58.844460: Pseudo dice [0.8668, 0.9, 0.9668, 0.6048, 0.8683] 
2023-10-26 16:24:58.844687: Epoch time: 4.0 s 
2023-10-26 16:24:59.985183:  
2023-10-26 16:24:59.985474: Epoch 707 
2023-10-26 16:24:59.985713: Current learning rate: 0.00331 
2023-10-26 16:25:04.096820: train_loss -0.9075 
2023-10-26 16:25:04.097271: val_loss -0.794 
2023-10-26 16:25:04.097547: Pseudo dice [0.8676, 0.8986, 0.9654, 0.5166, 0.8575] 
2023-10-26 16:25:04.097807: Epoch time: 4.11 s 
2023-10-26 16:25:05.252630:  
2023-10-26 16:25:05.252924: Epoch 708 
2023-10-26 16:25:05.253171: Current learning rate: 0.0033 
2023-10-26 16:25:09.200254: train_loss -0.9055 
2023-10-26 16:25:09.200672: val_loss -0.8075 
2023-10-26 16:25:09.200957: Pseudo dice [0.868, 0.9042, 0.9662, 0.6507, 0.87] 
2023-10-26 16:25:09.201220: Epoch time: 3.95 s 
2023-10-26 16:25:10.597573:  
2023-10-26 16:25:10.598021: Epoch 709 
2023-10-26 16:25:10.598271: Current learning rate: 0.00329 
2023-10-26 16:25:14.488467: train_loss -0.8999 
2023-10-26 16:25:14.488881: val_loss -0.8057 
2023-10-26 16:25:14.489145: Pseudo dice [0.8614, 0.8968, 0.9671, 0.6386, 0.8524] 
2023-10-26 16:25:14.489373: Epoch time: 3.89 s 
2023-10-26 16:25:15.625804:  
2023-10-26 16:25:15.626140: Epoch 710 
2023-10-26 16:25:15.626392: Current learning rate: 0.00328 
2023-10-26 16:25:19.734911: train_loss -0.8999 
2023-10-26 16:25:19.735347: val_loss -0.8167 
2023-10-26 16:25:19.735626: Pseudo dice [0.8692, 0.9042, 0.969, 0.7017, 0.8647] 
2023-10-26 16:25:19.735869: Epoch time: 4.11 s 
2023-10-26 16:25:20.895310:  
2023-10-26 16:25:20.895613: Epoch 711 
2023-10-26 16:25:20.895878: Current learning rate: 0.00327 
2023-10-26 16:25:25.080491: train_loss -0.9094 
2023-10-26 16:25:25.080887: val_loss -0.8213 
2023-10-26 16:25:25.081155: Pseudo dice [0.8675, 0.9022, 0.9677, 0.6496, 0.882] 
2023-10-26 16:25:25.081394: Epoch time: 4.19 s 
2023-10-26 16:25:26.241859:  
2023-10-26 16:25:26.242304: Epoch 712 
2023-10-26 16:25:26.242549: Current learning rate: 0.00326 
2023-10-26 16:25:30.399332: train_loss -0.9058 
2023-10-26 16:25:30.400062: val_loss -0.8263 
2023-10-26 16:25:30.400371: Pseudo dice [0.867, 0.8955, 0.9661, 0.5804, 0.8974] 
2023-10-26 16:25:30.400644: Epoch time: 4.16 s 
2023-10-26 16:25:31.548672:  
2023-10-26 16:25:31.548992: Epoch 713 
2023-10-26 16:25:31.549241: Current learning rate: 0.00325 
2023-10-26 16:25:35.675250: train_loss -0.9038 
2023-10-26 16:25:35.675651: val_loss -0.8041 
2023-10-26 16:25:35.675931: Pseudo dice [0.8705, 0.9042, 0.968, 0.7054, 0.8209] 
2023-10-26 16:25:35.676280: Epoch time: 4.13 s 
2023-10-26 16:25:36.820200:  
2023-10-26 16:25:36.820517: Epoch 714 
2023-10-26 16:25:36.820771: Current learning rate: 0.00324 
2023-10-26 16:25:41.030545: train_loss -0.9049 
2023-10-26 16:25:41.030998: val_loss -0.7683 
2023-10-26 16:25:41.031277: Pseudo dice [0.8661, 0.8981, 0.9656, 0.6028, 0.8833] 
2023-10-26 16:25:41.031516: Epoch time: 4.21 s 
2023-10-26 16:25:42.375439:  
2023-10-26 16:25:42.375746: Epoch 715 
2023-10-26 16:25:42.376012: Current learning rate: 0.00323 
2023-10-26 16:25:46.522331: train_loss -0.9046 
2023-10-26 16:25:46.522704: val_loss -0.8088 
2023-10-26 16:25:46.523000: Pseudo dice [0.8708, 0.8997, 0.9666, 0.6956, 0.8586] 
2023-10-26 16:25:46.523239: Epoch time: 4.15 s 
2023-10-26 16:25:47.661305:  
2023-10-26 16:25:47.661619: Epoch 716 
2023-10-26 16:25:47.661911: Current learning rate: 0.00322 
2023-10-26 16:25:51.763420: train_loss -0.9062 
2023-10-26 16:25:51.763891: val_loss -0.7959 
2023-10-26 16:25:51.764163: Pseudo dice [0.8643, 0.9053, 0.9653, 0.6619, 0.8776] 
2023-10-26 16:25:51.764412: Epoch time: 4.1 s 
2023-10-26 16:25:52.918502:  
2023-10-26 16:25:52.918826: Epoch 717 
2023-10-26 16:25:52.919102: Current learning rate: 0.00321 
2023-10-26 16:25:57.112545: train_loss -0.9098 
2023-10-26 16:25:57.112994: val_loss -0.8006 
2023-10-26 16:25:57.113283: Pseudo dice [0.8651, 0.8953, 0.9681, 0.7246, 0.8693] 
2023-10-26 16:25:57.113547: Epoch time: 4.19 s 
2023-10-26 16:25:58.259961:  
2023-10-26 16:25:58.260280: Epoch 718 
2023-10-26 16:25:58.260547: Current learning rate: 0.0032 
2023-10-26 16:26:02.434899: train_loss -0.9061 
2023-10-26 16:26:02.435324: val_loss -0.8081 
2023-10-26 16:26:02.435601: Pseudo dice [0.8646, 0.906, 0.9653, 0.6291, 0.8873] 
2023-10-26 16:26:02.435911: Epoch time: 4.18 s 
2023-10-26 16:26:03.596459:  
2023-10-26 16:26:03.596779: Epoch 719 
2023-10-26 16:26:03.597052: Current learning rate: 0.00319 
2023-10-26 16:26:07.794897: train_loss -0.9024 
2023-10-26 16:26:07.795295: val_loss -0.8113 
2023-10-26 16:26:07.795625: Pseudo dice [0.866, 0.8952, 0.9678, 0.6557, 0.8833] 
2023-10-26 16:26:07.795866: Epoch time: 4.2 s 
2023-10-26 16:26:08.931534:  
2023-10-26 16:26:08.931829: Epoch 720 
2023-10-26 16:26:08.932081: Current learning rate: 0.00318 
2023-10-26 16:26:13.033060: train_loss -0.9043 
2023-10-26 16:26:13.033483: val_loss -0.7932 
2023-10-26 16:26:13.033749: Pseudo dice [0.8616, 0.9001, 0.9659, 0.6984, 0.904] 
2023-10-26 16:26:13.034002: Epoch time: 4.1 s 
2023-10-26 16:26:14.357449:  
2023-10-26 16:26:14.357750: Epoch 721 
2023-10-26 16:26:14.358013: Current learning rate: 0.00317 
2023-10-26 16:26:18.365572: train_loss -0.8945 
2023-10-26 16:26:18.366068: val_loss -0.7646 
2023-10-26 16:26:18.366388: Pseudo dice [0.8692, 0.8993, 0.9683, 0.0295, 0.8696] 
2023-10-26 16:26:18.366747: Epoch time: 4.01 s 
2023-10-26 16:26:19.565063:  
2023-10-26 16:26:19.565367: Epoch 722 
2023-10-26 16:26:19.565615: Current learning rate: 0.00316 
2023-10-26 16:26:23.715464: train_loss -0.8987 
2023-10-26 16:26:23.715867: val_loss -0.7861 
2023-10-26 16:26:23.716142: Pseudo dice [0.8676, 0.8952, 0.9656, 0.5301, 0.9005] 
2023-10-26 16:26:23.716370: Epoch time: 4.15 s 
2023-10-26 16:26:24.875953:  
2023-10-26 16:26:24.876262: Epoch 723 
2023-10-26 16:26:24.876520: Current learning rate: 0.00315 
2023-10-26 16:26:29.060011: train_loss -0.9055 
2023-10-26 16:26:29.060443: val_loss -0.7699 
2023-10-26 16:26:29.060749: Pseudo dice [0.8647, 0.9029, 0.9667, 0.6579, 0.8722] 
2023-10-26 16:26:29.061062: Epoch time: 4.18 s 
2023-10-26 16:26:30.213356:  
2023-10-26 16:26:30.213665: Epoch 724 
2023-10-26 16:26:30.213925: Current learning rate: 0.00314 
2023-10-26 16:26:34.478421: train_loss -0.8931 
2023-10-26 16:26:34.478816: val_loss -0.8113 
2023-10-26 16:26:34.479084: Pseudo dice [0.8621, 0.9012, 0.9646, 0.6536, 0.8662] 
2023-10-26 16:26:34.479316: Epoch time: 4.27 s 
2023-10-26 16:26:35.619721:  
2023-10-26 16:26:35.620023: Epoch 725 
2023-10-26 16:26:35.620265: Current learning rate: 0.00313 
2023-10-26 16:26:39.838073: train_loss -0.8985 
2023-10-26 16:26:39.838484: val_loss -0.815 
2023-10-26 16:26:39.838754: Pseudo dice [0.8721, 0.8996, 0.9679, 0.7082, 0.8672] 
2023-10-26 16:26:39.838998: Epoch time: 4.22 s 
2023-10-26 16:26:40.979758:  
2023-10-26 16:26:40.980131: Epoch 726 
2023-10-26 16:26:40.980451: Current learning rate: 0.00312 
2023-10-26 16:26:45.145788: train_loss -0.9 
2023-10-26 16:26:45.146418: val_loss -0.806 
2023-10-26 16:26:45.146923: Pseudo dice [0.8636, 0.8987, 0.9664, 0.5441, 0.8942] 
2023-10-26 16:26:45.147324: Epoch time: 4.17 s 
2023-10-26 16:26:46.450335:  
2023-10-26 16:26:46.450675: Epoch 727 
2023-10-26 16:26:46.450953: Current learning rate: 0.00311 
2023-10-26 16:26:50.647323: train_loss -0.9058 
2023-10-26 16:26:50.647926: val_loss -0.8148 
2023-10-26 16:26:50.648212: Pseudo dice [0.8677, 0.9047, 0.9678, 0.5975, 0.8754] 
2023-10-26 16:26:50.648450: Epoch time: 4.2 s 
2023-10-26 16:26:51.775387:  
2023-10-26 16:26:51.775687: Epoch 728 
2023-10-26 16:26:51.775968: Current learning rate: 0.0031 
2023-10-26 16:26:56.074438: train_loss -0.9048 
2023-10-26 16:26:56.074836: val_loss -0.8175 
2023-10-26 16:26:56.075148: Pseudo dice [0.8727, 0.8966, 0.9691, 0.6135, 0.8817] 
2023-10-26 16:26:56.075392: Epoch time: 4.3 s 
2023-10-26 16:26:57.216845:  
2023-10-26 16:26:57.217190: Epoch 729 
2023-10-26 16:26:57.217491: Current learning rate: 0.00309 
2023-10-26 16:27:01.304582: train_loss -0.901 
2023-10-26 16:27:01.304965: val_loss -0.8282 
2023-10-26 16:27:01.305234: Pseudo dice [0.8678, 0.8988, 0.9671, 0.7148, 0.9034] 
2023-10-26 16:27:01.305472: Epoch time: 4.09 s 
2023-10-26 16:27:02.440407:  
2023-10-26 16:27:02.440715: Epoch 730 
2023-10-26 16:27:02.440963: Current learning rate: 0.00308 
2023-10-26 16:27:06.536631: train_loss -0.9023 
2023-10-26 16:27:06.537012: val_loss -0.7877 
2023-10-26 16:27:06.537314: Pseudo dice [0.8714, 0.8983, 0.9681, 0.6355, 0.8556] 
2023-10-26 16:27:06.537558: Epoch time: 4.1 s 
2023-10-26 16:27:07.707155:  
2023-10-26 16:27:07.707449: Epoch 731 
2023-10-26 16:27:07.707691: Current learning rate: 0.00307 
2023-10-26 16:27:11.867638: train_loss -0.9038 
2023-10-26 16:27:11.868064: val_loss -0.7867 
2023-10-26 16:27:11.868330: Pseudo dice [0.8668, 0.8951, 0.9686, 0.5729, 0.8589] 
2023-10-26 16:27:11.868567: Epoch time: 4.16 s 
2023-10-26 16:27:13.013363:  
2023-10-26 16:27:13.013666: Epoch 732 
2023-10-26 16:27:13.013917: Current learning rate: 0.00306 
2023-10-26 16:27:17.077101: train_loss -0.9052 
2023-10-26 16:27:17.077481: val_loss -0.815 
2023-10-26 16:27:17.077745: Pseudo dice [0.8676, 0.9101, 0.9667, 0.6216, 0.8616] 
2023-10-26 16:27:17.077984: Epoch time: 4.06 s 
2023-10-26 16:27:18.411958:  
2023-10-26 16:27:18.412259: Epoch 733 
2023-10-26 16:27:18.412499: Current learning rate: 0.00305 
2023-10-26 16:27:22.525579: train_loss -0.9037 
2023-10-26 16:27:22.525978: val_loss -0.8093 
2023-10-26 16:27:22.526271: Pseudo dice [0.8634, 0.8955, 0.9647, 0.6217, 0.9146] 
2023-10-26 16:27:22.526512: Epoch time: 4.11 s 
2023-10-26 16:27:23.752376:  
2023-10-26 16:27:23.752685: Epoch 734 
2023-10-26 16:27:23.752947: Current learning rate: 0.00304 
2023-10-26 16:27:27.942503: train_loss -0.9063 
2023-10-26 16:27:27.942960: val_loss -0.7948 
2023-10-26 16:27:27.943226: Pseudo dice [0.8698, 0.8998, 0.9669, 0.6303, 0.8495] 
2023-10-26 16:27:27.943478: Epoch time: 4.19 s 
2023-10-26 16:27:29.071607:  
2023-10-26 16:27:29.071906: Epoch 735 
2023-10-26 16:27:29.072151: Current learning rate: 0.00303 
2023-10-26 16:27:33.299176: train_loss -0.9125 
2023-10-26 16:27:33.299740: val_loss -0.7957 
2023-10-26 16:27:33.300004: Pseudo dice [0.8666, 0.8984, 0.9672, 0.653, 0.8526] 
2023-10-26 16:27:33.300228: Epoch time: 4.23 s 
2023-10-26 16:27:34.438557:  
2023-10-26 16:27:34.438880: Epoch 736 
2023-10-26 16:27:34.439130: Current learning rate: 0.00302 
2023-10-26 16:27:38.596506: train_loss -0.8975 
2023-10-26 16:27:38.597047: val_loss -0.7967 
2023-10-26 16:27:38.597320: Pseudo dice [0.8643, 0.8944, 0.9679, 0.6454, 0.8778] 
2023-10-26 16:27:38.597620: Epoch time: 4.16 s 
2023-10-26 16:27:39.760929:  
2023-10-26 16:27:39.761226: Epoch 737 
2023-10-26 16:27:39.761488: Current learning rate: 0.00301 
2023-10-26 16:27:43.912555: train_loss -0.9056 
2023-10-26 16:27:43.912926: val_loss -0.7982 
2023-10-26 16:27:43.913211: Pseudo dice [0.8686, 0.9047, 0.9649, 0.6192, 0.8627] 
2023-10-26 16:27:43.913467: Epoch time: 4.15 s 
2023-10-26 16:27:45.043817:  
2023-10-26 16:27:45.044126: Epoch 738 
2023-10-26 16:27:45.044374: Current learning rate: 0.003 
2023-10-26 16:27:49.173356: train_loss -0.8931 
2023-10-26 16:27:49.173804: val_loss -0.8154 
2023-10-26 16:27:49.174069: Pseudo dice [0.86, 0.891, 0.9673, 0.4738, 0.9064] 
2023-10-26 16:27:49.174305: Epoch time: 4.13 s 
2023-10-26 16:27:50.473272:  
2023-10-26 16:27:50.473574: Epoch 739 
2023-10-26 16:27:50.473832: Current learning rate: 0.00299 
2023-10-26 16:27:54.550684: train_loss -0.8935 
2023-10-26 16:27:54.551636: val_loss -0.8202 
2023-10-26 16:27:54.551904: Pseudo dice [0.8661, 0.8929, 0.9667, 0.5771, 0.9084] 
2023-10-26 16:27:54.552130: Epoch time: 4.08 s 
2023-10-26 16:27:55.689982:  
2023-10-26 16:27:55.690280: Epoch 740 
2023-10-26 16:27:55.690562: Current learning rate: 0.00297 
2023-10-26 16:27:59.861615: train_loss -0.9031 
2023-10-26 16:27:59.862001: val_loss -0.8136 
2023-10-26 16:27:59.862261: Pseudo dice [0.8703, 0.9021, 0.9656, 0.5447, 0.8667] 
2023-10-26 16:27:59.862484: Epoch time: 4.17 s 
2023-10-26 16:28:00.990379:  
2023-10-26 16:28:00.990678: Epoch 741 
2023-10-26 16:28:00.990935: Current learning rate: 0.00296 
2023-10-26 16:28:05.093782: train_loss -0.9023 
2023-10-26 16:28:05.094199: val_loss -0.8065 
2023-10-26 16:28:05.094479: Pseudo dice [0.8669, 0.8944, 0.9654, 0.4676, 0.8812] 
2023-10-26 16:28:05.094723: Epoch time: 4.1 s 
2023-10-26 16:28:06.234428:  
2023-10-26 16:28:06.234726: Epoch 742 
2023-10-26 16:28:06.234989: Current learning rate: 0.00295 
2023-10-26 16:28:10.400570: train_loss -0.8993 
2023-10-26 16:28:10.401082: val_loss -0.8139 
2023-10-26 16:28:10.401460: Pseudo dice [0.8638, 0.8904, 0.9664, 0.5683, 0.8936] 
2023-10-26 16:28:10.401737: Epoch time: 4.17 s 
2023-10-26 16:28:11.560683:  
2023-10-26 16:28:11.560980: Epoch 743 
2023-10-26 16:28:11.561228: Current learning rate: 0.00294 
2023-10-26 16:28:17.022676: train_loss -0.9069 
2023-10-26 16:28:17.023108: val_loss -0.7876 
2023-10-26 16:28:17.023378: Pseudo dice [0.8719, 0.904, 0.9666, 0.38, 0.8448] 
2023-10-26 16:28:17.023615: Epoch time: 5.46 s 
2023-10-26 16:28:18.160540:  
2023-10-26 16:28:18.160833: Epoch 744 
2023-10-26 16:28:18.161077: Current learning rate: 0.00293 
2023-10-26 16:28:22.182486: train_loss -0.9007 
2023-10-26 16:28:22.182866: val_loss -0.7986 
2023-10-26 16:28:22.183151: Pseudo dice [0.8708, 0.8966, 0.9688, 0.612, 0.8607] 
2023-10-26 16:28:22.183383: Epoch time: 4.02 s 
2023-10-26 16:28:23.493906:  
2023-10-26 16:28:23.494212: Epoch 745 
2023-10-26 16:28:23.494464: Current learning rate: 0.00292 
2023-10-26 16:28:27.380418: train_loss -0.9033 
2023-10-26 16:28:27.380806: val_loss -0.8047 
2023-10-26 16:28:27.381072: Pseudo dice [0.8652, 0.9008, 0.9673, 0.5514, 0.862] 
2023-10-26 16:28:27.381326: Epoch time: 3.89 s 
2023-10-26 16:28:28.522430:  
2023-10-26 16:28:28.522746: Epoch 746 
2023-10-26 16:28:28.523003: Current learning rate: 0.00291 
2023-10-26 16:28:32.695371: train_loss -0.9094 
2023-10-26 16:28:32.695805: val_loss -0.7708 
2023-10-26 16:28:32.696070: Pseudo dice [0.8602, 0.8932, 0.9667, 0.3222, 0.8374] 
2023-10-26 16:28:32.696313: Epoch time: 4.17 s 
2023-10-26 16:28:33.839423:  
2023-10-26 16:28:33.839720: Epoch 747 
2023-10-26 16:28:33.839974: Current learning rate: 0.0029 
2023-10-26 16:28:37.852357: train_loss -0.9059 
2023-10-26 16:28:37.852749: val_loss -0.7952 
2023-10-26 16:28:37.853020: Pseudo dice [0.8722, 0.8965, 0.9681, 0.3609, 0.8591] 
2023-10-26 16:28:37.853252: Epoch time: 4.01 s 
2023-10-26 16:28:38.996567:  
2023-10-26 16:28:38.996849: Epoch 748 
2023-10-26 16:28:38.997108: Current learning rate: 0.00289 
2023-10-26 16:28:43.154829: train_loss -0.9082 
2023-10-26 16:28:43.155219: val_loss -0.7773 
2023-10-26 16:28:43.155485: Pseudo dice [0.8644, 0.8917, 0.9661, 0.1822, 0.8596] 
2023-10-26 16:28:43.155714: Epoch time: 4.16 s 
2023-10-26 16:28:44.328781:  
2023-10-26 16:28:44.329083: Epoch 749 
2023-10-26 16:28:44.329334: Current learning rate: 0.00288 
2023-10-26 16:28:48.487799: train_loss -0.9104 
2023-10-26 16:28:48.488172: val_loss -0.8099 
2023-10-26 16:28:48.488437: Pseudo dice [0.8673, 0.8937, 0.9694, 0.5511, 0.8648] 
2023-10-26 16:28:48.488676: Epoch time: 4.16 s 
2023-10-26 16:28:49.725694:  
2023-10-26 16:28:49.725999: Epoch 750 
2023-10-26 16:28:49.726272: Current learning rate: 0.00287 
2023-10-26 16:28:53.751260: train_loss -0.9041 
2023-10-26 16:28:53.751812: val_loss -0.7932 
2023-10-26 16:28:53.752209: Pseudo dice [0.8709, 0.8966, 0.9668, 0.2531, 0.8699] 
2023-10-26 16:28:53.752566: Epoch time: 4.03 s 
2023-10-26 16:28:55.093492:  
2023-10-26 16:28:55.093857: Epoch 751 
2023-10-26 16:28:55.094485: Current learning rate: 0.00286 
2023-10-26 16:28:59.091627: train_loss -0.9032 
2023-10-26 16:28:59.092034: val_loss -0.7859 
2023-10-26 16:28:59.092318: Pseudo dice [0.8648, 0.8936, 0.9668, 0.2512, 0.8774] 
2023-10-26 16:28:59.092571: Epoch time: 4.0 s 
2023-10-26 16:29:00.269969:  
2023-10-26 16:29:00.270321: Epoch 752 
2023-10-26 16:29:00.270575: Current learning rate: 0.00285 
2023-10-26 16:29:04.356804: train_loss -0.8953 
2023-10-26 16:29:04.357212: val_loss -0.7975 
2023-10-26 16:29:04.357479: Pseudo dice [0.8646, 0.8856, 0.9685, 0.3725, 0.8764] 
2023-10-26 16:29:04.357729: Epoch time: 4.09 s 
2023-10-26 16:29:05.509179:  
2023-10-26 16:29:05.509524: Epoch 753 
2023-10-26 16:29:05.509779: Current learning rate: 0.00284 
2023-10-26 16:29:09.658916: train_loss -0.8988 
2023-10-26 16:29:09.659308: val_loss -0.8307 
2023-10-26 16:29:09.659581: Pseudo dice [0.8807, 0.9099, 0.9672, 0.7203, 0.8804] 
2023-10-26 16:29:09.659836: Epoch time: 4.15 s 
2023-10-26 16:29:10.801588:  
2023-10-26 16:29:10.801898: Epoch 754 
2023-10-26 16:29:10.802147: Current learning rate: 0.00283 
2023-10-26 16:29:14.882494: train_loss -0.9078 
2023-10-26 16:29:14.882914: val_loss -0.8111 
2023-10-26 16:29:14.883183: Pseudo dice [0.8765, 0.8954, 0.9673, 0.5489, 0.8763] 
2023-10-26 16:29:14.883422: Epoch time: 4.08 s 
2023-10-26 16:29:16.125017:  
2023-10-26 16:29:16.125316: Epoch 755 
2023-10-26 16:29:16.125580: Current learning rate: 0.00282 
2023-10-26 16:29:20.250394: train_loss -0.9022 
2023-10-26 16:29:20.250791: val_loss -0.7924 
2023-10-26 16:29:20.251058: Pseudo dice [0.8663, 0.9015, 0.9667, 0.5658, 0.8442] 
2023-10-26 16:29:20.251281: Epoch time: 4.13 s 
2023-10-26 16:29:21.434418:  
2023-10-26 16:29:21.434716: Epoch 756 
2023-10-26 16:29:21.434960: Current learning rate: 0.00281 
2023-10-26 16:29:25.521544: train_loss -0.9002 
2023-10-26 16:29:25.521954: val_loss -0.8143 
2023-10-26 16:29:25.522224: Pseudo dice [0.8709, 0.8958, 0.9673, 0.6088, 0.8725] 
2023-10-26 16:29:25.522466: Epoch time: 4.09 s 
2023-10-26 16:29:26.900025:  
2023-10-26 16:29:26.900352: Epoch 757 
2023-10-26 16:29:26.900611: Current learning rate: 0.0028 
2023-10-26 16:29:30.971101: train_loss -0.8961 
2023-10-26 16:29:30.971512: val_loss -0.7753 
2023-10-26 16:29:30.971774: Pseudo dice [0.8707, 0.9068, 0.9686, 0.1571, 0.8185] 
2023-10-26 16:29:30.972014: Epoch time: 4.07 s 
2023-10-26 16:29:32.111716:  
2023-10-26 16:29:32.112025: Epoch 758 
2023-10-26 16:29:32.112279: Current learning rate: 0.00279 
2023-10-26 16:29:36.262315: train_loss -0.8948 
2023-10-26 16:29:36.262776: val_loss -0.7819 
2023-10-26 16:29:36.263062: Pseudo dice [0.8793, 0.8982, 0.9695, 0.5534, 0.8483] 
2023-10-26 16:29:36.263310: Epoch time: 4.15 s 
2023-10-26 16:29:37.411148:  
2023-10-26 16:29:37.411459: Epoch 759 
2023-10-26 16:29:37.411722: Current learning rate: 0.00278 
2023-10-26 16:29:41.560911: train_loss -0.8929 
2023-10-26 16:29:41.561305: val_loss -0.8205 
2023-10-26 16:29:41.561607: Pseudo dice [0.8682, 0.9016, 0.9664, 0.56, 0.904] 
2023-10-26 16:29:41.561842: Epoch time: 4.15 s 
2023-10-26 16:29:42.708470:  
2023-10-26 16:29:42.708823: Epoch 760 
2023-10-26 16:29:42.709136: Current learning rate: 0.00277 
2023-10-26 16:29:46.893770: train_loss -0.902 
2023-10-26 16:29:46.894199: val_loss -0.795 
2023-10-26 16:29:46.894469: Pseudo dice [0.8688, 0.8996, 0.9681, 0.5814, 0.8657] 
2023-10-26 16:29:46.894720: Epoch time: 4.19 s 
2023-10-26 16:29:48.094512:  
2023-10-26 16:29:48.094846: Epoch 761 
2023-10-26 16:29:48.095166: Current learning rate: 0.00276 
2023-10-26 16:29:52.204645: train_loss -0.9057 
2023-10-26 16:29:52.205059: val_loss -0.7917 
2023-10-26 16:29:52.205324: Pseudo dice [0.8728, 0.9018, 0.9671, 0.5475, 0.8392] 
2023-10-26 16:29:52.205582: Epoch time: 4.11 s 
2023-10-26 16:29:53.344793:  
2023-10-26 16:29:53.345277: Epoch 762 
2023-10-26 16:29:53.345533: Current learning rate: 0.00275 
2023-10-26 16:29:57.405426: train_loss -0.9066 
2023-10-26 16:29:57.405840: val_loss -0.8074 
2023-10-26 16:29:57.406118: Pseudo dice [0.8667, 0.9024, 0.9637, 0.5075, 0.898] 
2023-10-26 16:29:57.406357: Epoch time: 4.06 s 
2023-10-26 16:29:58.738342:  
2023-10-26 16:29:58.738644: Epoch 763 
2023-10-26 16:29:58.738899: Current learning rate: 0.00274 
2023-10-26 16:30:02.916579: train_loss -0.9056 
2023-10-26 16:30:02.916989: val_loss -0.7842 
2023-10-26 16:30:02.917251: Pseudo dice [0.8709, 0.9031, 0.9651, 0.325, 0.8497] 
2023-10-26 16:30:02.917482: Epoch time: 4.18 s 
2023-10-26 16:30:04.096101:  
2023-10-26 16:30:04.096414: Epoch 764 
2023-10-26 16:30:04.096673: Current learning rate: 0.00273 
2023-10-26 16:30:08.286899: train_loss -0.905 
2023-10-26 16:30:08.287318: val_loss -0.8063 
2023-10-26 16:30:08.287602: Pseudo dice [0.8705, 0.8951, 0.9676, 0.5065, 0.8639] 
2023-10-26 16:30:08.287851: Epoch time: 4.19 s 
2023-10-26 16:30:09.454297:  
2023-10-26 16:30:09.454585: Epoch 765 
2023-10-26 16:30:09.454829: Current learning rate: 0.00272 
2023-10-26 16:30:13.660885: train_loss -0.9091 
2023-10-26 16:30:13.661294: val_loss -0.7934 
2023-10-26 16:30:13.661566: Pseudo dice [0.8693, 0.9012, 0.967, 0.4939, 0.864] 
2023-10-26 16:30:13.661816: Epoch time: 4.21 s 
2023-10-26 16:30:14.811888:  
2023-10-26 16:30:14.812181: Epoch 766 
2023-10-26 16:30:14.812433: Current learning rate: 0.00271 
2023-10-26 16:30:18.990216: train_loss -0.9023 
2023-10-26 16:30:18.990616: val_loss -0.7931 
2023-10-26 16:30:18.990893: Pseudo dice [0.873, 0.8989, 0.9692, 0.3165, 0.8521] 
2023-10-26 16:30:18.991127: Epoch time: 4.18 s 
2023-10-26 16:30:20.159487:  
2023-10-26 16:30:20.159791: Epoch 767 
2023-10-26 16:30:20.160057: Current learning rate: 0.0027 
2023-10-26 16:30:24.221861: train_loss -0.9058 
2023-10-26 16:30:24.222311: val_loss -0.7945 
2023-10-26 16:30:24.222584: Pseudo dice [0.8758, 0.9036, 0.9656, 0.437, 0.8647] 
2023-10-26 16:30:24.222913: Epoch time: 4.06 s 
2023-10-26 16:30:25.392035:  
2023-10-26 16:30:25.392347: Epoch 768 
2023-10-26 16:30:25.392633: Current learning rate: 0.00268 
2023-10-26 16:30:29.596043: train_loss -0.9022 
2023-10-26 16:30:29.596442: val_loss -0.7884 
2023-10-26 16:30:29.596704: Pseudo dice [0.872, 0.9009, 0.9654, 0.5782, 0.8356] 
2023-10-26 16:30:29.596940: Epoch time: 4.2 s 
2023-10-26 16:30:30.898977:  
2023-10-26 16:30:30.899272: Epoch 769 
2023-10-26 16:30:30.899520: Current learning rate: 0.00267 
2023-10-26 16:30:34.990348: train_loss -0.9032 
2023-10-26 16:30:34.990769: val_loss -0.8014 
2023-10-26 16:30:34.991055: Pseudo dice [0.8649, 0.9067, 0.9656, 0.6488, 0.8567] 
2023-10-26 16:30:34.991300: Epoch time: 4.09 s 
2023-10-26 16:30:36.151852:  
2023-10-26 16:30:36.152158: Epoch 770 
2023-10-26 16:30:36.152402: Current learning rate: 0.00266 
2023-10-26 16:30:40.293544: train_loss -0.9112 
2023-10-26 16:30:40.294009: val_loss -0.7893 
2023-10-26 16:30:40.294301: Pseudo dice [0.8668, 0.8978, 0.9673, 0.5318, 0.8797] 
2023-10-26 16:30:40.294762: Epoch time: 4.14 s 
2023-10-26 16:30:41.464175:  
2023-10-26 16:30:41.464470: Epoch 771 
2023-10-26 16:30:41.464718: Current learning rate: 0.00265 
2023-10-26 16:30:45.638794: train_loss -0.9014 
2023-10-26 16:30:45.639185: val_loss -0.7669 
2023-10-26 16:30:45.639448: Pseudo dice [0.8723, 0.9111, 0.9644, 0.6303, 0.8225] 
2023-10-26 16:30:45.639693: Epoch time: 4.18 s 
2023-10-26 16:30:46.784333:  
2023-10-26 16:30:46.784628: Epoch 772 
2023-10-26 16:30:46.784882: Current learning rate: 0.00264 
2023-10-26 16:30:50.941271: train_loss -0.9091 
2023-10-26 16:30:50.941822: val_loss -0.8098 
2023-10-26 16:30:50.942177: Pseudo dice [0.8648, 0.8987, 0.9677, 0.4816, 0.8955] 
2023-10-26 16:30:50.942426: Epoch time: 4.16 s 
2023-10-26 16:30:52.085344:  
2023-10-26 16:30:52.085631: Epoch 773 
2023-10-26 16:30:52.085878: Current learning rate: 0.00263 
2023-10-26 16:30:56.189730: train_loss -0.9072 
2023-10-26 16:30:56.190173: val_loss -0.8252 
2023-10-26 16:30:56.190453: Pseudo dice [0.8697, 0.9004, 0.9661, 0.5952, 0.8941] 
2023-10-26 16:30:56.190696: Epoch time: 4.1 s 
2023-10-26 16:30:57.492153:  
2023-10-26 16:30:57.492479: Epoch 774 
2023-10-26 16:30:57.492736: Current learning rate: 0.00262 
2023-10-26 16:31:01.550376: train_loss -0.905 
2023-10-26 16:31:01.550773: val_loss -0.8031 
2023-10-26 16:31:01.551049: Pseudo dice [0.8743, 0.9041, 0.9667, 0.4685, 0.8552] 
2023-10-26 16:31:01.551351: Epoch time: 4.06 s 
2023-10-26 16:31:02.694676:  
2023-10-26 16:31:02.695001: Epoch 775 
2023-10-26 16:31:02.695252: Current learning rate: 0.00261 
2023-10-26 16:31:06.763760: train_loss -0.9093 
2023-10-26 16:31:06.764199: val_loss -0.8056 
2023-10-26 16:31:06.764485: Pseudo dice [0.8722, 0.8931, 0.9675, 0.5342, 0.8792] 
2023-10-26 16:31:06.764737: Epoch time: 4.07 s 
2023-10-26 16:31:07.922459:  
2023-10-26 16:31:07.922840: Epoch 776 
2023-10-26 16:31:07.923165: Current learning rate: 0.0026 
2023-10-26 16:31:12.145324: train_loss -0.8987 
2023-10-26 16:31:12.145778: val_loss -0.8271 
2023-10-26 16:31:12.146060: Pseudo dice [0.8699, 0.8998, 0.9644, 0.6746, 0.8895] 
2023-10-26 16:31:12.146345: Epoch time: 4.22 s 
2023-10-26 16:31:13.299885:  
2023-10-26 16:31:13.300190: Epoch 777 
2023-10-26 16:31:13.300437: Current learning rate: 0.00259 
2023-10-26 16:31:17.506010: train_loss -0.9042 
2023-10-26 16:31:17.506427: val_loss -0.7921 
2023-10-26 16:31:17.506755: Pseudo dice [0.8752, 0.8979, 0.9671, 0.5367, 0.8411] 
2023-10-26 16:31:17.507002: Epoch time: 4.21 s 
2023-10-26 16:31:18.657724:  
2023-10-26 16:31:18.658074: Epoch 778 
2023-10-26 16:31:18.658380: Current learning rate: 0.00258 
2023-10-26 16:31:22.890016: train_loss -0.9028 
2023-10-26 16:31:22.890471: val_loss -0.8091 
2023-10-26 16:31:22.890844: Pseudo dice [0.8691, 0.9035, 0.9664, 0.5564, 0.8895] 
2023-10-26 16:31:22.891200: Epoch time: 4.23 s 
2023-10-26 16:31:24.031569:  
2023-10-26 16:31:24.031868: Epoch 779 
2023-10-26 16:31:24.032124: Current learning rate: 0.00257 
2023-10-26 16:31:28.360635: train_loss -0.9041 
2023-10-26 16:31:28.361063: val_loss -0.8098 
2023-10-26 16:31:28.361330: Pseudo dice [0.8687, 0.8956, 0.9663, 0.5514, 0.8734] 
2023-10-26 16:31:28.361567: Epoch time: 4.33 s 
2023-10-26 16:31:29.690076:  
2023-10-26 16:31:29.690403: Epoch 780 
2023-10-26 16:31:29.690653: Current learning rate: 0.00256 
2023-10-26 16:31:33.876382: train_loss -0.9048 
2023-10-26 16:31:33.876752: val_loss -0.7982 
2023-10-26 16:31:33.877019: Pseudo dice [0.8662, 0.8979, 0.9642, 0.4454, 0.8798] 
2023-10-26 16:31:33.877250: Epoch time: 4.19 s 
2023-10-26 16:31:35.015948:  
2023-10-26 16:31:35.016283: Epoch 781 
2023-10-26 16:31:35.016558: Current learning rate: 0.00255 
2023-10-26 16:31:39.176512: train_loss -0.9054 
2023-10-26 16:31:39.176937: val_loss -0.8148 
2023-10-26 16:31:39.177223: Pseudo dice [0.8668, 0.8934, 0.9674, 0.6078, 0.8879] 
2023-10-26 16:31:39.177473: Epoch time: 4.16 s 
2023-10-26 16:31:40.379945:  
2023-10-26 16:31:40.380297: Epoch 782 
2023-10-26 16:31:40.380596: Current learning rate: 0.00254 
2023-10-26 16:31:44.551912: train_loss -0.9042 
2023-10-26 16:31:44.552302: val_loss -0.813 
2023-10-26 16:31:44.552594: Pseudo dice [0.8707, 0.8995, 0.9674, 0.5407, 0.8958] 
2023-10-26 16:31:44.552833: Epoch time: 4.17 s 
2023-10-26 16:31:45.722620:  
2023-10-26 16:31:45.722935: Epoch 783 
2023-10-26 16:31:45.723191: Current learning rate: 0.00253 
2023-10-26 16:31:49.806784: train_loss -0.907 
2023-10-26 16:31:49.807145: val_loss -0.7922 
2023-10-26 16:31:49.807408: Pseudo dice [0.8707, 0.8986, 0.9656, 0.5543, 0.8536] 
2023-10-26 16:31:49.807645: Epoch time: 4.08 s 
2023-10-26 16:31:50.965557:  
2023-10-26 16:31:50.965849: Epoch 784 
2023-10-26 16:31:50.966100: Current learning rate: 0.00252 
2023-10-26 16:31:55.003320: train_loss -0.9052 
2023-10-26 16:31:55.004045: val_loss -0.7952 
2023-10-26 16:31:55.004372: Pseudo dice [0.8573, 0.8895, 0.9646, 0.4015, 0.8868] 
2023-10-26 16:31:55.004637: Epoch time: 4.04 s 
2023-10-26 16:31:56.150611:  
2023-10-26 16:31:56.150927: Epoch 785 
2023-10-26 16:31:56.151181: Current learning rate: 0.00251 
2023-10-26 16:32:00.207205: train_loss -0.8973 
2023-10-26 16:32:00.207604: val_loss -0.7898 
2023-10-26 16:32:00.207891: Pseudo dice [0.8736, 0.9006, 0.9668, 0.6401, 0.8776] 
2023-10-26 16:32:00.208137: Epoch time: 4.06 s 
2023-10-26 16:32:01.508200:  
2023-10-26 16:32:01.508499: Epoch 786 
2023-10-26 16:32:01.508755: Current learning rate: 0.0025 
2023-10-26 16:32:05.663286: train_loss -0.8999 
2023-10-26 16:32:05.663662: val_loss -0.7843 
2023-10-26 16:32:05.663939: Pseudo dice [0.8664, 0.8986, 0.9645, 0.5854, 0.8328] 
2023-10-26 16:32:05.664225: Epoch time: 4.16 s 
2023-10-26 16:32:06.849835:  
2023-10-26 16:32:06.850171: Epoch 787 
2023-10-26 16:32:06.850424: Current learning rate: 0.00249 
2023-10-26 16:32:10.986329: train_loss -0.913 
2023-10-26 16:32:10.986699: val_loss -0.8056 
2023-10-26 16:32:10.986959: Pseudo dice [0.8613, 0.8935, 0.9669, 0.6313, 0.8777] 
2023-10-26 16:32:10.987190: Epoch time: 4.14 s 
2023-10-26 16:32:12.151626:  
2023-10-26 16:32:12.151921: Epoch 788 
2023-10-26 16:32:12.152178: Current learning rate: 0.00248 
2023-10-26 16:32:16.296854: train_loss -0.9074 
2023-10-26 16:32:16.297276: val_loss -0.7925 
2023-10-26 16:32:16.297535: Pseudo dice [0.8667, 0.9016, 0.9669, 0.5586, 0.838] 
2023-10-26 16:32:16.297774: Epoch time: 4.15 s 
2023-10-26 16:32:17.444551:  
2023-10-26 16:32:17.444860: Epoch 789 
2023-10-26 16:32:17.445118: Current learning rate: 0.00247 
2023-10-26 16:32:21.606733: train_loss -0.9094 
2023-10-26 16:32:21.607230: val_loss -0.7885 
2023-10-26 16:32:21.607521: Pseudo dice [0.8673, 0.8976, 0.9687, 0.4439, 0.8456] 
2023-10-26 16:32:21.607762: Epoch time: 4.16 s 
2023-10-26 16:32:22.740864:  
2023-10-26 16:32:22.741157: Epoch 790 
2023-10-26 16:32:22.741407: Current learning rate: 0.00245 
2023-10-26 16:32:26.934494: train_loss -0.9061 
2023-10-26 16:32:26.934920: val_loss -0.7938 
2023-10-26 16:32:26.935185: Pseudo dice [0.8638, 0.8955, 0.9647, 0.6511, 0.8478] 
2023-10-26 16:32:26.935415: Epoch time: 4.19 s 
2023-10-26 16:32:28.124072:  
2023-10-26 16:32:28.124390: Epoch 791 
2023-10-26 16:32:28.124648: Current learning rate: 0.00244 
2023-10-26 16:32:32.272385: train_loss -0.9066 
2023-10-26 16:32:32.272764: val_loss -0.8142 
2023-10-26 16:32:32.273039: Pseudo dice [0.8614, 0.8895, 0.9673, 0.6532, 0.8876] 
2023-10-26 16:32:32.273281: Epoch time: 4.15 s 
2023-10-26 16:32:33.615714:  
2023-10-26 16:32:33.616040: Epoch 792 
2023-10-26 16:32:33.616293: Current learning rate: 0.00243 
2023-10-26 16:32:37.768561: train_loss -0.9021 
2023-10-26 16:32:37.768981: val_loss -0.7974 
2023-10-26 16:32:37.769241: Pseudo dice [0.8647, 0.8962, 0.9672, 0.553, 0.8498] 
2023-10-26 16:32:37.769531: Epoch time: 4.15 s 
2023-10-26 16:32:38.967877:  
2023-10-26 16:32:38.968196: Epoch 793 
2023-10-26 16:32:38.968455: Current learning rate: 0.00242 
2023-10-26 16:32:42.969708: train_loss -0.9067 
2023-10-26 16:32:42.970224: val_loss -0.8175 
2023-10-26 16:32:42.970565: Pseudo dice [0.8615, 0.8947, 0.9666, 0.6069, 0.9026] 
2023-10-26 16:32:42.970954: Epoch time: 4.0 s 
2023-10-26 16:32:44.117801:  
2023-10-26 16:32:44.118111: Epoch 794 
2023-10-26 16:32:44.118356: Current learning rate: 0.00241 
2023-10-26 16:32:48.114097: train_loss -0.9086 
2023-10-26 16:32:48.114465: val_loss -0.8133 
2023-10-26 16:32:48.114727: Pseudo dice [0.8657, 0.8957, 0.9655, 0.6262, 0.8966] 
2023-10-26 16:32:48.114972: Epoch time: 4.0 s 
2023-10-26 16:32:49.243527:  
2023-10-26 16:32:49.243844: Epoch 795 
2023-10-26 16:32:49.244097: Current learning rate: 0.0024 
2023-10-26 16:32:53.382456: train_loss -0.906 
2023-10-26 16:32:53.382954: val_loss -0.7995 
2023-10-26 16:32:53.383318: Pseudo dice [0.8662, 0.8974, 0.9645, 0.4756, 0.88] 
2023-10-26 16:32:53.383640: Epoch time: 4.14 s 
2023-10-26 16:32:54.634309:  
2023-10-26 16:32:54.634627: Epoch 796 
2023-10-26 16:32:54.634889: Current learning rate: 0.00239 
2023-10-26 16:32:58.749294: train_loss -0.9067 
2023-10-26 16:32:58.749697: val_loss -0.7868 
2023-10-26 16:32:58.749975: Pseudo dice [0.8667, 0.8946, 0.9667, 0.5923, 0.8521] 
2023-10-26 16:32:58.750210: Epoch time: 4.12 s 
2023-10-26 16:32:59.914573:  
2023-10-26 16:32:59.914892: Epoch 797 
2023-10-26 16:32:59.915378: Current learning rate: 0.00238 
2023-10-26 16:33:03.986054: train_loss -0.906 
2023-10-26 16:33:03.986417: val_loss -0.7984 
2023-10-26 16:33:03.986678: Pseudo dice [0.8694, 0.8969, 0.9664, 0.4926, 0.8771] 
2023-10-26 16:33:03.986909: Epoch time: 4.07 s 
2023-10-26 16:33:05.269320:  
2023-10-26 16:33:05.269641: Epoch 798 
2023-10-26 16:33:05.269897: Current learning rate: 0.00237 
2023-10-26 16:33:09.425146: train_loss -0.9121 
2023-10-26 16:33:09.425615: val_loss -0.7987 
2023-10-26 16:33:09.425987: Pseudo dice [0.8676, 0.8959, 0.9643, 0.3271, 0.8801] 
2023-10-26 16:33:09.426280: Epoch time: 4.16 s 
2023-10-26 16:33:10.561263:  
2023-10-26 16:33:10.561589: Epoch 799 
2023-10-26 16:33:10.561835: Current learning rate: 0.00236 
2023-10-26 16:33:14.665310: train_loss -0.9056 
2023-10-26 16:33:14.665673: val_loss -0.7858 
2023-10-26 16:33:14.665943: Pseudo dice [0.8666, 0.8968, 0.965, 0.5395, 0.8726] 
2023-10-26 16:33:14.666176: Epoch time: 4.1 s 
2023-10-26 16:33:15.919359:  
2023-10-26 16:33:15.919664: Epoch 800 
2023-10-26 16:33:15.919927: Current learning rate: 0.00235 
2023-10-26 16:33:20.052988: train_loss -0.9044 
2023-10-26 16:33:20.053361: val_loss -0.7627 
2023-10-26 16:33:20.053622: Pseudo dice [0.8666, 0.8989, 0.9667, 0.5491, 0.8518] 
2023-10-26 16:33:20.053847: Epoch time: 4.13 s 
2023-10-26 16:33:21.197226:  
2023-10-26 16:33:21.197542: Epoch 801 
2023-10-26 16:33:21.197805: Current learning rate: 0.00234 
2023-10-26 16:33:25.268280: train_loss -0.9082 
2023-10-26 16:33:25.268692: val_loss -0.7683 
2023-10-26 16:33:25.268962: Pseudo dice [0.8697, 0.8993, 0.9652, 0.3898, 0.8573] 
2023-10-26 16:33:25.269195: Epoch time: 4.07 s 
2023-10-26 16:33:26.404580:  
2023-10-26 16:33:26.404876: Epoch 802 
2023-10-26 16:33:26.405126: Current learning rate: 0.00233 
2023-10-26 16:33:30.508390: train_loss -0.9066 
2023-10-26 16:33:30.508770: val_loss -0.7861 
2023-10-26 16:33:30.509044: Pseudo dice [0.8629, 0.8947, 0.9662, 0.4724, 0.8696] 
2023-10-26 16:33:30.509279: Epoch time: 4.1 s 
2023-10-26 16:33:31.806048:  
2023-10-26 16:33:31.806357: Epoch 803 
2023-10-26 16:33:31.806602: Current learning rate: 0.00232 
2023-10-26 16:33:35.978060: train_loss -0.9123 
2023-10-26 16:33:35.978445: val_loss -0.7664 
2023-10-26 16:33:35.978719: Pseudo dice [0.8678, 0.8932, 0.9659, 0.4986, 0.8221] 
2023-10-26 16:33:35.978948: Epoch time: 4.17 s 
2023-10-26 16:33:37.118234:  
2023-10-26 16:33:37.118525: Epoch 804 
2023-10-26 16:33:37.118768: Current learning rate: 0.00231 
2023-10-26 16:33:41.293420: train_loss -0.9098 
2023-10-26 16:33:41.293826: val_loss -0.7969 
2023-10-26 16:33:41.294102: Pseudo dice [0.8685, 0.8956, 0.9669, 0.5782, 0.8933] 
2023-10-26 16:33:41.294346: Epoch time: 4.18 s 
2023-10-26 16:33:42.436062:  
2023-10-26 16:33:42.436378: Epoch 805 
2023-10-26 16:33:42.436638: Current learning rate: 0.0023 
2023-10-26 16:33:46.755727: train_loss -0.9061 
2023-10-26 16:33:46.756111: val_loss -0.7947 
2023-10-26 16:33:46.756376: Pseudo dice [0.8646, 0.8994, 0.9665, 0.5666, 0.8706] 
2023-10-26 16:33:46.756608: Epoch time: 4.32 s 
2023-10-26 16:33:47.959498:  
2023-10-26 16:33:47.959799: Epoch 806 
2023-10-26 16:33:47.960055: Current learning rate: 0.00229 
2023-10-26 16:33:52.009654: train_loss -0.9119 
2023-10-26 16:33:52.010068: val_loss -0.7785 
2023-10-26 16:33:52.010342: Pseudo dice [0.8679, 0.9, 0.9643, 0.4959, 0.8672] 
2023-10-26 16:33:52.010589: Epoch time: 4.05 s 
2023-10-26 16:33:53.157865:  
2023-10-26 16:33:53.158176: Epoch 807 
2023-10-26 16:33:53.158421: Current learning rate: 0.00228 
2023-10-26 16:33:57.331194: train_loss -0.9135 
2023-10-26 16:33:57.331564: val_loss -0.8035 
2023-10-26 16:33:57.331824: Pseudo dice [0.8701, 0.8963, 0.967, 0.6378, 0.8521] 
2023-10-26 16:33:57.332050: Epoch time: 4.17 s 
2023-10-26 16:33:58.526789:  
2023-10-26 16:33:58.527103: Epoch 808 
2023-10-26 16:33:58.527362: Current learning rate: 0.00226 
2023-10-26 16:34:02.635827: train_loss -0.9114 
2023-10-26 16:34:02.636203: val_loss -0.7867 
2023-10-26 16:34:02.636469: Pseudo dice [0.8641, 0.8952, 0.9647, 0.5879, 0.8731] 
2023-10-26 16:34:02.636716: Epoch time: 4.11 s 
2023-10-26 16:34:03.975210:  
2023-10-26 16:34:03.975517: Epoch 809 
2023-10-26 16:34:03.975768: Current learning rate: 0.00225 
2023-10-26 16:34:08.110460: train_loss -0.9124 
2023-10-26 16:34:08.110852: val_loss -0.7897 
2023-10-26 16:34:08.111144: Pseudo dice [0.8638, 0.9041, 0.9648, 0.6018, 0.8767] 
2023-10-26 16:34:08.111387: Epoch time: 4.14 s 
2023-10-26 16:34:09.263264:  
2023-10-26 16:34:09.263588: Epoch 810 
2023-10-26 16:34:09.263847: Current learning rate: 0.00224 
2023-10-26 16:34:13.439113: train_loss -0.9063 
2023-10-26 16:34:13.439468: val_loss -0.8012 
2023-10-26 16:34:13.439729: Pseudo dice [0.8671, 0.9004, 0.965, 0.4812, 0.8959] 
2023-10-26 16:34:13.439981: Epoch time: 4.18 s 
2023-10-26 16:34:14.584394:  
2023-10-26 16:34:14.584699: Epoch 811 
2023-10-26 16:34:14.584955: Current learning rate: 0.00223 
2023-10-26 16:34:18.740100: train_loss -0.9095 
2023-10-26 16:34:18.740476: val_loss -0.8047 
2023-10-26 16:34:18.740740: Pseudo dice [0.8634, 0.8992, 0.9653, 0.447, 0.9128] 
2023-10-26 16:34:18.740982: Epoch time: 4.16 s 
2023-10-26 16:34:19.911030:  
2023-10-26 16:34:19.911379: Epoch 812 
2023-10-26 16:34:19.911738: Current learning rate: 0.00222 
2023-10-26 16:34:24.000086: train_loss -0.9076 
2023-10-26 16:34:24.000523: val_loss -0.7788 
2023-10-26 16:34:24.000784: Pseudo dice [0.8696, 0.9001, 0.9658, 0.6304, 0.821] 
2023-10-26 16:34:24.001037: Epoch time: 4.09 s 
2023-10-26 16:34:25.145445:  
2023-10-26 16:34:25.145737: Epoch 813 
2023-10-26 16:34:25.145994: Current learning rate: 0.00221 
2023-10-26 16:34:29.147883: train_loss -0.9007 
2023-10-26 16:34:29.148269: val_loss -0.8 
2023-10-26 16:34:29.148535: Pseudo dice [0.871, 0.8958, 0.9657, 0.5556, 0.8848] 
2023-10-26 16:34:29.148771: Epoch time: 4.0 s 
2023-10-26 16:34:30.311901:  
2023-10-26 16:34:30.312229: Epoch 814 
2023-10-26 16:34:30.312477: Current learning rate: 0.0022 
2023-10-26 16:34:34.463184: train_loss -0.9099 
2023-10-26 16:34:34.463545: val_loss -0.8075 
2023-10-26 16:34:34.463802: Pseudo dice [0.8731, 0.9028, 0.9677, 0.5076, 0.8823] 
2023-10-26 16:34:34.464034: Epoch time: 4.15 s 
2023-10-26 16:34:35.758162:  
2023-10-26 16:34:35.758477: Epoch 815 
2023-10-26 16:34:35.758728: Current learning rate: 0.00219 
2023-10-26 16:34:39.873279: train_loss -0.9114 
2023-10-26 16:34:39.873651: val_loss -0.8047 
2023-10-26 16:34:39.873924: Pseudo dice [0.8631, 0.8971, 0.9663, 0.6017, 0.9056] 
2023-10-26 16:34:39.874162: Epoch time: 4.12 s 
2023-10-26 16:34:41.021568:  
2023-10-26 16:34:41.021891: Epoch 816 
2023-10-26 16:34:41.022157: Current learning rate: 0.00218 
2023-10-26 16:34:45.208153: train_loss -0.9114 
2023-10-26 16:34:45.208523: val_loss -0.7867 
2023-10-26 16:34:45.208792: Pseudo dice [0.8623, 0.9019, 0.966, 0.5271, 0.8608] 
2023-10-26 16:34:45.209036: Epoch time: 4.19 s 
2023-10-26 16:34:46.372555:  
2023-10-26 16:34:46.372885: Epoch 817 
2023-10-26 16:34:46.373174: Current learning rate: 0.00217 
2023-10-26 16:34:50.541962: train_loss -0.9038 
2023-10-26 16:34:50.542399: val_loss -0.7851 
2023-10-26 16:34:50.542678: Pseudo dice [0.8648, 0.8922, 0.9645, 0.0, 0.8647] 
2023-10-26 16:34:50.542933: Epoch time: 4.17 s 
2023-10-26 16:34:51.764508:  
2023-10-26 16:34:51.764821: Epoch 818 
2023-10-26 16:34:51.765086: Current learning rate: 0.00216 
2023-10-26 16:34:55.859861: train_loss -0.8992 
2023-10-26 16:34:55.860262: val_loss -0.7991 
2023-10-26 16:34:55.860526: Pseudo dice [0.8618, 0.8991, 0.9674, 0.6254, 0.8573] 
2023-10-26 16:34:55.860762: Epoch time: 4.1 s 
2023-10-26 16:34:57.018708:  
2023-10-26 16:34:57.019035: Epoch 819 
2023-10-26 16:34:57.019650: Current learning rate: 0.00215 
2023-10-26 16:35:01.132238: train_loss -0.9015 
2023-10-26 16:35:01.132679: val_loss -0.7829 
2023-10-26 16:35:01.132989: Pseudo dice [0.8703, 0.8994, 0.9668, 0.441, 0.8532] 
2023-10-26 16:35:01.133464: Epoch time: 4.11 s 
2023-10-26 16:35:02.498548:  
2023-10-26 16:35:02.498854: Epoch 820 
2023-10-26 16:35:02.499203: Current learning rate: 0.00214 
2023-10-26 16:35:06.550750: train_loss -0.9042 
2023-10-26 16:35:06.551197: val_loss -0.8154 
2023-10-26 16:35:06.551505: Pseudo dice [0.8647, 0.8955, 0.9683, 0.5309, 0.8948] 
2023-10-26 16:35:06.551756: Epoch time: 4.05 s 
2023-10-26 16:35:07.745189:  
2023-10-26 16:35:07.745505: Epoch 821 
2023-10-26 16:35:07.745758: Current learning rate: 0.00213 
2023-10-26 16:35:11.893104: train_loss -0.9036 
2023-10-26 16:35:11.893489: val_loss -0.7968 
2023-10-26 16:35:11.893784: Pseudo dice [0.872, 0.9024, 0.9662, 0.657, 0.8334] 
2023-10-26 16:35:11.894032: Epoch time: 4.15 s 
2023-10-26 16:35:13.001156:  
2023-10-26 16:35:13.001454: Epoch 822 
2023-10-26 16:35:13.001822: Current learning rate: 0.00212 
2023-10-26 16:35:16.929627: train_loss -0.9099 
2023-10-26 16:35:16.930073: val_loss -0.7908 
2023-10-26 16:35:16.930350: Pseudo dice [0.868, 0.8988, 0.9668, 0.5634, 0.8624] 
2023-10-26 16:35:16.930600: Epoch time: 3.93 s 
2023-10-26 16:35:18.068472:  
2023-10-26 16:35:18.068781: Epoch 823 
2023-10-26 16:35:18.069052: Current learning rate: 0.0021 
2023-10-26 16:35:22.177921: train_loss -0.9103 
2023-10-26 16:35:22.178355: val_loss -0.7888 
2023-10-26 16:35:22.178623: Pseudo dice [0.86, 0.897, 0.9666, 0.5831, 0.8859] 
2023-10-26 16:35:22.178860: Epoch time: 4.11 s 
2023-10-26 16:35:23.299417:  
2023-10-26 16:35:23.299733: Epoch 824 
2023-10-26 16:35:23.300004: Current learning rate: 0.00209 
2023-10-26 16:35:27.420520: train_loss -0.9078 
2023-10-26 16:35:27.420945: val_loss -0.8068 
2023-10-26 16:35:27.421214: Pseudo dice [0.8672, 0.9001, 0.9665, 0.5714, 0.8815] 
2023-10-26 16:35:27.421457: Epoch time: 4.12 s 
2023-10-26 16:35:28.554554:  
2023-10-26 16:35:28.554870: Epoch 825 
2023-10-26 16:35:28.555171: Current learning rate: 0.00208 
2023-10-26 16:35:32.554368: train_loss -0.9086 
2023-10-26 16:35:32.554750: val_loss -0.8044 
2023-10-26 16:35:32.555052: Pseudo dice [0.8631, 0.8909, 0.9666, 0.7389, 0.8567] 
2023-10-26 16:35:32.555288: Epoch time: 4.0 s 
2023-10-26 16:35:33.723853:  
2023-10-26 16:35:33.724174: Epoch 826 
2023-10-26 16:35:33.724428: Current learning rate: 0.00207 
2023-10-26 16:35:37.773195: train_loss -0.9141 
2023-10-26 16:35:37.773610: val_loss -0.806 
2023-10-26 16:35:37.773891: Pseudo dice [0.8683, 0.8949, 0.9674, 0.5618, 0.8799] 
2023-10-26 16:35:37.774135: Epoch time: 4.05 s 
2023-10-26 16:35:39.154106:  
2023-10-26 16:35:39.154419: Epoch 827 
2023-10-26 16:35:39.154663: Current learning rate: 0.00206 
2023-10-26 16:35:43.319831: train_loss -0.91 
2023-10-26 16:35:43.320204: val_loss -0.7935 
2023-10-26 16:35:43.320472: Pseudo dice [0.8672, 0.8959, 0.966, 0.0493, 0.8888] 
2023-10-26 16:35:43.320708: Epoch time: 4.17 s 
2023-10-26 16:35:44.426578:  
2023-10-26 16:35:44.427011: Epoch 828 
2023-10-26 16:35:44.427261: Current learning rate: 0.00205 
2023-10-26 16:35:48.517259: train_loss -0.91 
2023-10-26 16:35:48.517637: val_loss -0.8024 
2023-10-26 16:35:48.517942: Pseudo dice [0.8706, 0.9006, 0.9661, 0.4754, 0.8696] 
2023-10-26 16:35:48.518183: Epoch time: 4.09 s 
2023-10-26 16:35:49.630412:  
2023-10-26 16:35:49.630733: Epoch 829 
2023-10-26 16:35:49.630998: Current learning rate: 0.00204 
2023-10-26 16:35:53.772690: train_loss -0.9044 
2023-10-26 16:35:53.773122: val_loss -0.7734 
2023-10-26 16:35:53.773400: Pseudo dice [0.8646, 0.8952, 0.9673, 0.2008, 0.8779] 
2023-10-26 16:35:53.773652: Epoch time: 4.14 s 
2023-10-26 16:35:54.896168:  
2023-10-26 16:35:54.896499: Epoch 830 
2023-10-26 16:35:54.896764: Current learning rate: 0.00203 
2023-10-26 16:35:59.060035: train_loss -0.9082 
2023-10-26 16:35:59.060475: val_loss -0.8266 
2023-10-26 16:35:59.060751: Pseudo dice [0.8631, 0.8945, 0.9674, 0.5161, 0.9328] 
2023-10-26 16:35:59.061055: Epoch time: 4.16 s 
2023-10-26 16:36:00.197763:  
2023-10-26 16:36:00.198121: Epoch 831 
2023-10-26 16:36:00.198448: Current learning rate: 0.00202 
2023-10-26 16:36:04.379943: train_loss -0.9055 
2023-10-26 16:36:04.380358: val_loss -0.8149 
2023-10-26 16:36:04.380626: Pseudo dice [0.8646, 0.9029, 0.9663, 0.6434, 0.8877] 
2023-10-26 16:36:04.380862: Epoch time: 4.18 s 
2023-10-26 16:36:05.547661:  
2023-10-26 16:36:05.548259: Epoch 832 
2023-10-26 16:36:05.548531: Current learning rate: 0.00201 
2023-10-26 16:36:09.782300: train_loss -0.9118 
2023-10-26 16:36:09.782795: val_loss -0.7813 
2023-10-26 16:36:09.783071: Pseudo dice [0.8671, 0.8887, 0.9664, 0.5272, 0.8687] 
2023-10-26 16:36:09.783343: Epoch time: 4.24 s 
2023-10-26 16:36:11.123822:  
2023-10-26 16:36:11.124188: Epoch 833 
2023-10-26 16:36:11.124575: Current learning rate: 0.002 
2023-10-26 16:36:15.354029: train_loss -0.9105 
2023-10-26 16:36:15.354457: val_loss -0.7915 
2023-10-26 16:36:15.354717: Pseudo dice [0.8656, 0.8913, 0.9671, 0.6365, 0.8665] 
2023-10-26 16:36:15.354952: Epoch time: 4.23 s 
2023-10-26 16:36:16.476503:  
2023-10-26 16:36:16.476834: Epoch 834 
2023-10-26 16:36:16.477095: Current learning rate: 0.00199 
2023-10-26 16:36:20.560295: train_loss -0.9152 
2023-10-26 16:36:20.560741: val_loss -0.7937 
2023-10-26 16:36:20.561033: Pseudo dice [0.8595, 0.9005, 0.9646, 0.5346, 0.8966] 
2023-10-26 16:36:20.561314: Epoch time: 4.08 s 
2023-10-26 16:36:21.761723:  
2023-10-26 16:36:21.762289: Epoch 835 
2023-10-26 16:36:21.762562: Current learning rate: 0.00198 
2023-10-26 16:36:25.827794: train_loss -0.9069 
2023-10-26 16:36:25.828254: val_loss -0.8026 
2023-10-26 16:36:25.828528: Pseudo dice [0.8721, 0.897, 0.9658, 0.5503, 0.8814] 
2023-10-26 16:36:25.828784: Epoch time: 4.07 s 
2023-10-26 16:36:26.964103:  
2023-10-26 16:36:26.964426: Epoch 836 
2023-10-26 16:36:26.964682: Current learning rate: 0.00196 
2023-10-26 16:36:30.875173: train_loss -0.9129 
2023-10-26 16:36:30.875566: val_loss -0.7906 
2023-10-26 16:36:30.875854: Pseudo dice [0.8681, 0.898, 0.9656, 0.5725, 0.8687] 
2023-10-26 16:36:30.876111: Epoch time: 3.91 s 
2023-10-26 16:36:31.998151:  
2023-10-26 16:36:31.998595: Epoch 837 
2023-10-26 16:36:31.999058: Current learning rate: 0.00195 
2023-10-26 16:36:36.091380: train_loss -0.907 
2023-10-26 16:36:36.091798: val_loss -0.7961 
2023-10-26 16:36:36.092071: Pseudo dice [0.8681, 0.8938, 0.9651, 0.5897, 0.8876] 
2023-10-26 16:36:36.092314: Epoch time: 4.09 s 
2023-10-26 16:36:37.210910:  
2023-10-26 16:36:37.211238: Epoch 838 
2023-10-26 16:36:37.211494: Current learning rate: 0.00194 
2023-10-26 16:36:41.342681: train_loss -0.9078 
2023-10-26 16:36:41.343206: val_loss -0.764 
2023-10-26 16:36:41.343615: Pseudo dice [0.8741, 0.9039, 0.9662, 0.5702, 0.8182] 
2023-10-26 16:36:41.343914: Epoch time: 4.13 s 
2023-10-26 16:36:42.644150:  
2023-10-26 16:36:42.644496: Epoch 839 
2023-10-26 16:36:42.644751: Current learning rate: 0.00193 
2023-10-26 16:36:46.685069: train_loss -0.9099 
2023-10-26 16:36:46.685633: val_loss -0.8 
2023-10-26 16:36:46.686010: Pseudo dice [0.8572, 0.8976, 0.9651, 0.6228, 0.8893] 
2023-10-26 16:36:46.686382: Epoch time: 4.04 s 
2023-10-26 16:36:47.850577:  
2023-10-26 16:36:47.850914: Epoch 840 
2023-10-26 16:36:47.851175: Current learning rate: 0.00192 
2023-10-26 16:36:51.963270: train_loss -0.9108 
2023-10-26 16:36:51.963684: val_loss -0.7964 
2023-10-26 16:36:51.963959: Pseudo dice [0.8669, 0.8997, 0.9664, 0.5871, 0.8988] 
2023-10-26 16:36:51.964200: Epoch time: 4.11 s 
2023-10-26 16:36:53.080465:  
2023-10-26 16:36:53.080765: Epoch 841 
2023-10-26 16:36:53.081020: Current learning rate: 0.00191 
2023-10-26 16:36:57.117259: train_loss -0.9064 
2023-10-26 16:36:57.117650: val_loss -0.7586 
2023-10-26 16:36:57.117930: Pseudo dice [0.8542, 0.8966, 0.9637, 0.0396, 0.9004] 
2023-10-26 16:36:57.118175: Epoch time: 4.04 s 
2023-10-26 16:36:58.225537:  
2023-10-26 16:36:58.225888: Epoch 842 
2023-10-26 16:36:58.226222: Current learning rate: 0.0019 
2023-10-26 16:37:02.155713: train_loss -0.9032 
2023-10-26 16:37:02.156106: val_loss -0.7727 
2023-10-26 16:37:02.156371: Pseudo dice [0.8602, 0.889, 0.9638, 0.0261, 0.8824] 
2023-10-26 16:37:02.156610: Epoch time: 3.93 s 
2023-10-26 16:37:03.270494:  
2023-10-26 16:37:03.270790: Epoch 843 
2023-10-26 16:37:03.271044: Current learning rate: 0.00189 
2023-10-26 16:37:07.321568: train_loss -0.8982 
2023-10-26 16:37:07.321963: val_loss -0.7582 
2023-10-26 16:37:07.322228: Pseudo dice [0.8609, 0.8995, 0.966, 0.0, 0.8446] 
2023-10-26 16:37:07.322458: Epoch time: 4.05 s 
2023-10-26 16:37:08.520975:  
2023-10-26 16:37:08.521305: Epoch 844 
2023-10-26 16:37:08.521568: Current learning rate: 0.00188 
2023-10-26 16:37:12.480280: train_loss -0.9003 
2023-10-26 16:37:12.480839: val_loss -0.8059 
2023-10-26 16:37:12.481109: Pseudo dice [0.8633, 0.8952, 0.9644, 0.5375, 0.8685] 
2023-10-26 16:37:12.481352: Epoch time: 3.96 s 
2023-10-26 16:37:13.593307:  
2023-10-26 16:37:13.593629: Epoch 845 
2023-10-26 16:37:13.593895: Current learning rate: 0.00187 
2023-10-26 16:37:17.746575: train_loss -0.9041 
2023-10-26 16:37:17.746984: val_loss -0.8045 
2023-10-26 16:37:17.747253: Pseudo dice [0.8664, 0.8997, 0.9652, 0.6095, 0.8783] 
2023-10-26 16:37:17.747491: Epoch time: 4.15 s 
2023-10-26 16:37:19.076203:  
2023-10-26 16:37:19.076512: Epoch 846 
2023-10-26 16:37:19.076768: Current learning rate: 0.00186 
2023-10-26 16:37:23.108767: train_loss -0.9007 
2023-10-26 16:37:23.109176: val_loss -0.7929 
2023-10-26 16:37:23.109447: Pseudo dice [0.8714, 0.8983, 0.9654, 0.3881, 0.8704] 
2023-10-26 16:37:23.109691: Epoch time: 4.03 s 
2023-10-26 16:37:24.258191:  
2023-10-26 16:37:24.258517: Epoch 847 
2023-10-26 16:37:24.258770: Current learning rate: 0.00185 
2023-10-26 16:37:28.287996: train_loss -0.899 
2023-10-26 16:37:28.288428: val_loss -0.7815 
2023-10-26 16:37:28.288693: Pseudo dice [0.8675, 0.9088, 0.9654, 0.168, 0.8414] 
2023-10-26 16:37:28.288935: Epoch time: 4.03 s 
2023-10-26 16:37:29.464150:  
2023-10-26 16:37:29.464486: Epoch 848 
2023-10-26 16:37:29.464766: Current learning rate: 0.00184 
2023-10-26 16:37:33.483381: train_loss -0.9106 
2023-10-26 16:37:33.483867: val_loss -0.7945 
2023-10-26 16:37:33.484563: Pseudo dice [0.8661, 0.8966, 0.9664, 0.6167, 0.8302] 
2023-10-26 16:37:33.484838: Epoch time: 4.02 s 
2023-10-26 16:37:34.600665:  
2023-10-26 16:37:34.600997: Epoch 849 
2023-10-26 16:37:34.601260: Current learning rate: 0.00182 
2023-10-26 16:37:38.664275: train_loss -0.9065 
2023-10-26 16:37:38.664808: val_loss -0.8074 
2023-10-26 16:37:38.665119: Pseudo dice [0.8694, 0.9014, 0.9646, 0.4805, 0.8801] 
2023-10-26 16:37:38.665461: Epoch time: 4.06 s 
2023-10-26 16:37:39.928097:  
2023-10-26 16:37:39.928403: Epoch 850 
2023-10-26 16:37:39.928653: Current learning rate: 0.00181 
2023-10-26 16:37:44.020287: train_loss -0.9047 
2023-10-26 16:37:44.020715: val_loss -0.8154 
2023-10-26 16:37:44.021050: Pseudo dice [0.8715, 0.8937, 0.9656, 0.5297, 0.8881] 
2023-10-26 16:37:44.021305: Epoch time: 4.09 s 
2023-10-26 16:37:45.163360:  
2023-10-26 16:37:45.163686: Epoch 851 
2023-10-26 16:37:45.163935: Current learning rate: 0.0018 
2023-10-26 16:37:49.408056: train_loss -0.9088 
2023-10-26 16:37:49.408461: val_loss -0.8082 
2023-10-26 16:37:49.408716: Pseudo dice [0.8685, 0.8993, 0.9664, 0.5758, 0.8784] 
2023-10-26 16:37:49.408941: Epoch time: 4.25 s 
2023-10-26 16:37:50.611097:  
2023-10-26 16:37:50.611411: Epoch 852 
2023-10-26 16:37:50.611658: Current learning rate: 0.00179 
2023-10-26 16:37:54.819587: train_loss -0.9115 
2023-10-26 16:37:54.820014: val_loss -0.7953 
2023-10-26 16:37:54.820303: Pseudo dice [0.8702, 0.9024, 0.966, 0.625, 0.8494] 
2023-10-26 16:37:54.820555: Epoch time: 4.21 s 
2023-10-26 16:37:55.935752:  
2023-10-26 16:37:55.936087: Epoch 853 
2023-10-26 16:37:55.936341: Current learning rate: 0.00178 
2023-10-26 16:38:00.027035: train_loss -0.9151 
2023-10-26 16:38:00.027468: val_loss -0.8207 
2023-10-26 16:38:00.027751: Pseudo dice [0.8759, 0.8983, 0.9658, 0.714, 0.8544] 
2023-10-26 16:38:00.028012: Epoch time: 4.09 s 
2023-10-26 16:38:01.129401:  
2023-10-26 16:38:01.129705: Epoch 854 
2023-10-26 16:38:01.129956: Current learning rate: 0.00177 
2023-10-26 16:38:05.234413: train_loss -0.9105 
2023-10-26 16:38:05.234857: val_loss -0.7903 
2023-10-26 16:38:05.235132: Pseudo dice [0.8657, 0.9029, 0.9657, 0.6132, 0.8947] 
2023-10-26 16:38:05.235387: Epoch time: 4.11 s 
2023-10-26 16:38:06.339747:  
2023-10-26 16:38:06.340132: Epoch 855 
2023-10-26 16:38:06.340495: Current learning rate: 0.00176 
2023-10-26 16:38:10.342574: train_loss -0.9119 
2023-10-26 16:38:10.342987: val_loss -0.7903 
2023-10-26 16:38:10.343298: Pseudo dice [0.8768, 0.9046, 0.9665, 0.6716, 0.8344] 
2023-10-26 16:38:10.343575: Epoch time: 4.0 s 
2023-10-26 16:38:11.480016:  
2023-10-26 16:38:11.480304: Epoch 856 
2023-10-26 16:38:11.480549: Current learning rate: 0.00175 
2023-10-26 16:38:15.562430: train_loss -0.9094 
2023-10-26 16:38:15.562880: val_loss -0.7983 
2023-10-26 16:38:15.563149: Pseudo dice [0.8707, 0.9008, 0.9662, 0.668, 0.8462] 
2023-10-26 16:38:15.563387: Epoch time: 4.08 s 
2023-10-26 16:38:16.666231:  
2023-10-26 16:38:16.666552: Epoch 857 
2023-10-26 16:38:16.666810: Current learning rate: 0.00174 
2023-10-26 16:38:20.756557: train_loss -0.909 
2023-10-26 16:38:20.757278: val_loss -0.7982 
2023-10-26 16:38:20.757590: Pseudo dice [0.8683, 0.8934, 0.9674, 0.5713, 0.8593] 
2023-10-26 16:38:20.757954: Epoch time: 4.09 s 
2023-10-26 16:38:22.084766:  
2023-10-26 16:38:22.085089: Epoch 858 
2023-10-26 16:38:22.085337: Current learning rate: 0.00173 
2023-10-26 16:38:26.275622: train_loss -0.9129 
2023-10-26 16:38:26.276051: val_loss -0.7783 
2023-10-26 16:38:26.276315: Pseudo dice [0.8684, 0.8996, 0.9657, 0.652, 0.8601] 
2023-10-26 16:38:26.276553: Epoch time: 4.19 s 
2023-10-26 16:38:27.386972:  
2023-10-26 16:38:27.387269: Epoch 859 
2023-10-26 16:38:27.387523: Current learning rate: 0.00172 
2023-10-26 16:38:31.571963: train_loss -0.9123 
2023-10-26 16:38:31.572473: val_loss -0.8033 
2023-10-26 16:38:31.572785: Pseudo dice [0.8677, 0.9028, 0.964, 0.5506, 0.8811] 
2023-10-26 16:38:31.573112: Epoch time: 4.19 s 
2023-10-26 16:38:32.666141:  
2023-10-26 16:38:32.666436: Epoch 860 
2023-10-26 16:38:32.666685: Current learning rate: 0.0017 
2023-10-26 16:38:36.844779: train_loss -0.9114 
2023-10-26 16:38:36.845178: val_loss -0.811 
2023-10-26 16:38:36.845442: Pseudo dice [0.8681, 0.9036, 0.9642, 0.6413, 0.8603] 
2023-10-26 16:38:36.845688: Epoch time: 4.18 s 
2023-10-26 16:38:37.970378:  
2023-10-26 16:38:37.970682: Epoch 861 
2023-10-26 16:38:37.970934: Current learning rate: 0.00169 
2023-10-26 16:38:42.140892: train_loss -0.9096 
2023-10-26 16:38:42.141309: val_loss -0.8041 
2023-10-26 16:38:42.141575: Pseudo dice [0.869, 0.8997, 0.9655, 0.6983, 0.8596] 
2023-10-26 16:38:42.141828: Epoch time: 4.17 s 
2023-10-26 16:38:43.233773:  
2023-10-26 16:38:43.234065: Epoch 862 
2023-10-26 16:38:43.234320: Current learning rate: 0.00168 
2023-10-26 16:38:47.372658: train_loss -0.913 
2023-10-26 16:38:47.373026: val_loss -0.8111 
2023-10-26 16:38:47.373293: Pseudo dice [0.8711, 0.9016, 0.9668, 0.5519, 0.8718] 
2023-10-26 16:38:47.373590: Epoch time: 4.14 s 
2023-10-26 16:38:48.481244:  
2023-10-26 16:38:48.481552: Epoch 863 
2023-10-26 16:38:48.481797: Current learning rate: 0.00167 
2023-10-26 16:38:52.515757: train_loss -0.9101 
2023-10-26 16:38:52.516351: val_loss -0.8088 
2023-10-26 16:38:52.516888: Pseudo dice [0.8679, 0.8978, 0.9659, 0.5565, 0.8906] 
2023-10-26 16:38:52.517145: Epoch time: 4.04 s 
2023-10-26 16:38:53.830221:  
2023-10-26 16:38:53.830532: Epoch 864 
2023-10-26 16:38:53.830782: Current learning rate: 0.00166 
2023-10-26 16:38:57.922253: train_loss -0.9112 
2023-10-26 16:38:57.922679: val_loss -0.8035 
2023-10-26 16:38:57.923157: Pseudo dice [0.8722, 0.9045, 0.9659, 0.5362, 0.8582] 
2023-10-26 16:38:57.923453: Epoch time: 4.09 s 
2023-10-26 16:38:59.023283:  
2023-10-26 16:38:59.023593: Epoch 865 
2023-10-26 16:38:59.023839: Current learning rate: 0.00165 
2023-10-26 16:39:03.183553: train_loss -0.9132 
2023-10-26 16:39:03.184038: val_loss -0.7988 
2023-10-26 16:39:03.184406: Pseudo dice [0.8712, 0.8899, 0.9673, 0.5521, 0.8607] 
2023-10-26 16:39:03.184688: Epoch time: 4.16 s 
2023-10-26 16:39:04.289863:  
2023-10-26 16:39:04.290181: Epoch 866 
2023-10-26 16:39:04.290437: Current learning rate: 0.00164 
2023-10-26 16:39:08.391292: train_loss -0.9178 
2023-10-26 16:39:08.391690: val_loss -0.8018 
2023-10-26 16:39:08.392014: Pseudo dice [0.8684, 0.9012, 0.9657, 0.5641, 0.8704] 
2023-10-26 16:39:08.392249: Epoch time: 4.1 s 
2023-10-26 16:39:09.494980:  
2023-10-26 16:39:09.495282: Epoch 867 
2023-10-26 16:39:09.495525: Current learning rate: 0.00163 
2023-10-26 16:39:13.544020: train_loss -0.9066 
2023-10-26 16:39:13.544407: val_loss -0.7847 
2023-10-26 16:39:13.544668: Pseudo dice [0.8742, 0.9034, 0.9667, 0.6608, 0.8352] 
2023-10-26 16:39:13.544909: Epoch time: 4.05 s 
2023-10-26 16:39:14.708721:  
2023-10-26 16:39:14.709027: Epoch 868 
2023-10-26 16:39:14.709280: Current learning rate: 0.00162 
2023-10-26 16:39:18.556083: train_loss -0.913 
2023-10-26 16:39:18.556467: val_loss -0.7914 
2023-10-26 16:39:18.556746: Pseudo dice [0.8695, 0.8999, 0.9657, 0.5972, 0.8506] 
2023-10-26 16:39:18.556998: Epoch time: 3.85 s 
2023-10-26 16:39:19.701091:  
2023-10-26 16:39:19.701404: Epoch 869 
2023-10-26 16:39:19.701668: Current learning rate: 0.00161 
2023-10-26 16:39:23.886619: train_loss -0.9128 
2023-10-26 16:39:23.887138: val_loss -0.8142 
2023-10-26 16:39:23.887456: Pseudo dice [0.8748, 0.9041, 0.9663, 0.6646, 0.8633] 
2023-10-26 16:39:23.887992: Epoch time: 4.19 s 
2023-10-26 16:39:24.996882:  
2023-10-26 16:39:24.997197: Epoch 870 
2023-10-26 16:39:24.997447: Current learning rate: 0.00159 
2023-10-26 16:39:29.067731: train_loss -0.9119 
2023-10-26 16:39:29.068098: val_loss -0.7906 
2023-10-26 16:39:29.068396: Pseudo dice [0.866, 0.9007, 0.9654, 0.6236, 0.8531] 
2023-10-26 16:39:29.068630: Epoch time: 4.07 s 
2023-10-26 16:39:30.349527:  
2023-10-26 16:39:30.349829: Epoch 871 
2023-10-26 16:39:30.350078: Current learning rate: 0.00158 
2023-10-26 16:39:34.400398: train_loss -0.9145 
2023-10-26 16:39:34.400791: val_loss -0.8033 
2023-10-26 16:39:34.401110: Pseudo dice [0.8706, 0.8983, 0.9661, 0.5033, 0.8631] 
2023-10-26 16:39:34.401365: Epoch time: 4.05 s 
2023-10-26 16:39:35.544485:  
2023-10-26 16:39:35.544788: Epoch 872 
2023-10-26 16:39:35.545059: Current learning rate: 0.00157 
2023-10-26 16:39:39.644285: train_loss -0.9138 
2023-10-26 16:39:39.644713: val_loss -0.8224 
2023-10-26 16:39:39.645011: Pseudo dice [0.8747, 0.8957, 0.9661, 0.6695, 0.881] 
2023-10-26 16:39:39.645269: Epoch time: 4.1 s 
2023-10-26 16:39:40.789371:  
2023-10-26 16:39:40.789738: Epoch 873 
2023-10-26 16:39:40.790009: Current learning rate: 0.00156 
2023-10-26 16:39:44.860597: train_loss -0.9163 
2023-10-26 16:39:44.861008: val_loss -0.8194 
2023-10-26 16:39:44.861281: Pseudo dice [0.868, 0.9032, 0.9654, 0.6426, 0.9024] 
2023-10-26 16:39:44.861522: Epoch time: 4.07 s 
2023-10-26 16:39:46.123390:  
2023-10-26 16:39:46.123728: Epoch 874 
2023-10-26 16:39:46.123975: Current learning rate: 0.00155 
2023-10-26 16:39:50.310476: train_loss -0.9113 
2023-10-26 16:39:50.310869: val_loss -0.8009 
2023-10-26 16:39:50.311162: Pseudo dice [0.8728, 0.8985, 0.9664, 0.5609, 0.8758] 
2023-10-26 16:39:50.311423: Epoch time: 4.19 s 
2023-10-26 16:39:51.419162:  
2023-10-26 16:39:51.419478: Epoch 875 
2023-10-26 16:39:51.419730: Current learning rate: 0.00154 
2023-10-26 16:39:55.490665: train_loss -0.9153 
2023-10-26 16:39:55.491082: val_loss -0.7873 
2023-10-26 16:39:55.491350: Pseudo dice [0.8626, 0.8985, 0.9658, 0.5457, 0.8665] 
2023-10-26 16:39:55.491585: Epoch time: 4.07 s 
2023-10-26 16:39:56.662508:  
2023-10-26 16:39:56.662842: Epoch 876 
2023-10-26 16:39:56.663116: Current learning rate: 0.00153 
2023-10-26 16:40:00.579282: train_loss -0.9065 
2023-10-26 16:40:00.579686: val_loss -0.8303 
2023-10-26 16:40:00.579954: Pseudo dice [0.858, 0.894, 0.9671, 0.751, 0.9201] 
2023-10-26 16:40:00.580203: Epoch time: 3.92 s 
2023-10-26 16:40:01.690044:  
2023-10-26 16:40:01.690349: Epoch 877 
2023-10-26 16:40:01.690612: Current learning rate: 0.00152 
2023-10-26 16:40:05.795703: train_loss -0.9156 
2023-10-26 16:40:05.796468: val_loss -0.8092 
2023-10-26 16:40:05.796746: Pseudo dice [0.866, 0.9028, 0.9649, 0.6991, 0.9095] 
2023-10-26 16:40:05.797001: Epoch time: 4.11 s 
2023-10-26 16:40:06.945459:  
2023-10-26 16:40:06.945755: Epoch 878 
2023-10-26 16:40:06.946036: Current learning rate: 0.00151 
2023-10-26 16:40:10.999065: train_loss -0.909 
2023-10-26 16:40:10.999813: val_loss -0.7953 
2023-10-26 16:40:11.000104: Pseudo dice [0.8699, 0.9058, 0.9643, 0.6373, 0.8569] 
2023-10-26 16:40:11.000529: Epoch time: 4.05 s 
2023-10-26 16:40:12.095072:  
2023-10-26 16:40:12.095378: Epoch 879 
2023-10-26 16:40:12.095643: Current learning rate: 0.00149 
2023-10-26 16:40:16.213622: train_loss -0.9178 
2023-10-26 16:40:16.214284: val_loss -0.7973 
2023-10-26 16:40:16.214546: Pseudo dice [0.8673, 0.9011, 0.9659, 0.6052, 0.8776] 
2023-10-26 16:40:16.214791: Epoch time: 4.12 s 
2023-10-26 16:40:17.331341:  
2023-10-26 16:40:17.331649: Epoch 880 
2023-10-26 16:40:17.331903: Current learning rate: 0.00148 
2023-10-26 16:40:21.410414: train_loss -0.913 
2023-10-26 16:40:21.411077: val_loss -0.8118 
2023-10-26 16:40:21.411355: Pseudo dice [0.8696, 0.8969, 0.9665, 0.6469, 0.8768] 
2023-10-26 16:40:21.411586: Epoch time: 4.08 s 
2023-10-26 16:40:22.534175:  
2023-10-26 16:40:22.534481: Epoch 881 
2023-10-26 16:40:22.534731: Current learning rate: 0.00147 
2023-10-26 16:40:26.660212: train_loss -0.9146 
2023-10-26 16:40:26.660954: val_loss -0.8053 
2023-10-26 16:40:26.661287: Pseudo dice [0.8672, 0.8981, 0.9655, 0.6789, 0.8755] 
2023-10-26 16:40:26.661697: Epoch time: 4.13 s 
2023-10-26 16:40:27.816860:  
2023-10-26 16:40:27.817180: Epoch 882 
2023-10-26 16:40:27.817426: Current learning rate: 0.00146 
2023-10-26 16:40:31.877634: train_loss -0.9183 
2023-10-26 16:40:31.878162: val_loss -0.7982 
2023-10-26 16:40:31.878515: Pseudo dice [0.8673, 0.9003, 0.9652, 0.6732, 0.8651] 
2023-10-26 16:40:31.878812: Epoch time: 4.06 s 
2023-10-26 16:40:33.001368:  
2023-10-26 16:40:33.001671: Epoch 883 
2023-10-26 16:40:33.001945: Current learning rate: 0.00145 
2023-10-26 16:40:37.041022: train_loss -0.9126 
2023-10-26 16:40:37.041408: val_loss -0.8267 
2023-10-26 16:40:37.041687: Pseudo dice [0.868, 0.904, 0.9659, 0.7365, 0.8929] 
2023-10-26 16:40:37.042014: Epoch time: 4.04 s 
2023-10-26 16:40:38.353451:  
2023-10-26 16:40:38.353754: Epoch 884 
2023-10-26 16:40:38.354011: Current learning rate: 0.00144 
2023-10-26 16:40:42.492003: train_loss -0.9161 
2023-10-26 16:40:42.492469: val_loss -0.8107 
2023-10-26 16:40:42.492738: Pseudo dice [0.8676, 0.9025, 0.9652, 0.7058, 0.8687] 
2023-10-26 16:40:42.492998: Epoch time: 4.14 s 
2023-10-26 16:40:43.641238:  
2023-10-26 16:40:43.641591: Epoch 885 
2023-10-26 16:40:43.641914: Current learning rate: 0.00143 
2023-10-26 16:40:47.849041: train_loss -0.9062 
2023-10-26 16:40:47.849455: val_loss -0.7997 
2023-10-26 16:40:47.849720: Pseudo dice [0.8675, 0.8996, 0.9654, 0.6301, 0.8753] 
2023-10-26 16:40:47.849959: Epoch time: 4.21 s 
2023-10-26 16:40:48.946754:  
2023-10-26 16:40:48.947068: Epoch 886 
2023-10-26 16:40:48.947314: Current learning rate: 0.00142 
2023-10-26 16:40:53.029724: train_loss -0.913 
2023-10-26 16:40:53.030156: val_loss -0.8002 
2023-10-26 16:40:53.030448: Pseudo dice [0.8662, 0.902, 0.9646, 0.6679, 0.8853] 
2023-10-26 16:40:53.030730: Epoch time: 4.08 s 
2023-10-26 16:40:54.131911:  
2023-10-26 16:40:54.132229: Epoch 887 
2023-10-26 16:40:54.132490: Current learning rate: 0.00141 
2023-10-26 16:40:58.288141: train_loss -0.9128 
2023-10-26 16:40:58.288563: val_loss -0.8077 
2023-10-26 16:40:58.288832: Pseudo dice [0.867, 0.8947, 0.9658, 0.5656, 0.8929] 
2023-10-26 16:40:58.289085: Epoch time: 4.16 s 
2023-10-26 16:40:59.398147:  
2023-10-26 16:40:59.398467: Epoch 888 
2023-10-26 16:40:59.398725: Current learning rate: 0.00139 
2023-10-26 16:41:03.473429: train_loss -0.9096 
2023-10-26 16:41:03.473826: val_loss -0.786 
2023-10-26 16:41:03.474100: Pseudo dice [0.8679, 0.8958, 0.9656, 0.5538, 0.8718] 
2023-10-26 16:41:03.474339: Epoch time: 4.08 s 
2023-10-26 16:41:04.574323:  
2023-10-26 16:41:04.574621: Epoch 889 
2023-10-26 16:41:04.574866: Current learning rate: 0.00138 
2023-10-26 16:41:08.700590: train_loss -0.9127 
2023-10-26 16:41:08.701037: val_loss -0.8026 
2023-10-26 16:41:08.701344: Pseudo dice [0.8704, 0.9045, 0.9646, 0.5306, 0.8699] 
2023-10-26 16:41:08.701604: Epoch time: 4.13 s 
2023-10-26 16:41:09.815096:  
2023-10-26 16:41:09.815472: Epoch 890 
2023-10-26 16:41:09.815754: Current learning rate: 0.00137 
2023-10-26 16:41:13.847564: train_loss -0.9086 
2023-10-26 16:41:13.847998: val_loss -0.799 
2023-10-26 16:41:13.848272: Pseudo dice [0.8697, 0.9022, 0.9661, 0.6243, 0.8317] 
2023-10-26 16:41:13.848508: Epoch time: 4.03 s 
2023-10-26 16:41:15.172588:  
2023-10-26 16:41:15.172904: Epoch 891 
2023-10-26 16:41:15.173153: Current learning rate: 0.00136 
2023-10-26 16:41:19.327961: train_loss -0.9129 
2023-10-26 16:41:19.328351: val_loss -0.822 
2023-10-26 16:41:19.328611: Pseudo dice [0.869, 0.9039, 0.964, 0.6516, 0.8916] 
2023-10-26 16:41:19.328837: Epoch time: 4.16 s 
2023-10-26 16:41:20.426909:  
2023-10-26 16:41:20.427299: Epoch 892 
2023-10-26 16:41:20.427555: Current learning rate: 0.00135 
2023-10-26 16:41:24.446550: train_loss -0.91 
2023-10-26 16:41:24.446962: val_loss -0.7964 
2023-10-26 16:41:24.447231: Pseudo dice [0.8627, 0.8913, 0.9647, 0.6113, 0.8779] 
2023-10-26 16:41:24.447477: Epoch time: 4.02 s 
2023-10-26 16:41:25.552966:  
2023-10-26 16:41:25.553272: Epoch 893 
2023-10-26 16:41:25.553534: Current learning rate: 0.00134 
2023-10-26 16:41:29.756941: train_loss -0.9171 
2023-10-26 16:41:29.757881: val_loss -0.8097 
2023-10-26 16:41:29.758199: Pseudo dice [0.8654, 0.8957, 0.9656, 0.5537, 0.8935] 
2023-10-26 16:41:29.758433: Epoch time: 4.2 s 
2023-10-26 16:41:30.860476:  
2023-10-26 16:41:30.860774: Epoch 894 
2023-10-26 16:41:30.861019: Current learning rate: 0.00133 
2023-10-26 16:41:34.877662: train_loss -0.9146 
2023-10-26 16:41:34.878051: val_loss -0.8124 
2023-10-26 16:41:34.878318: Pseudo dice [0.8625, 0.8909, 0.9652, 0.6023, 0.9008] 
2023-10-26 16:41:34.878550: Epoch time: 4.02 s 
2023-10-26 16:41:36.009554:  
2023-10-26 16:41:36.009850: Epoch 895 
2023-10-26 16:41:36.010103: Current learning rate: 0.00132 
2023-10-26 16:41:40.127223: train_loss -0.9175 
2023-10-26 16:41:40.127597: val_loss -0.7918 
2023-10-26 16:41:40.127933: Pseudo dice [0.8666, 0.9023, 0.9655, 0.6467, 0.856] 
2023-10-26 16:41:40.128223: Epoch time: 4.12 s 
2023-10-26 16:41:41.250492:  
2023-10-26 16:41:41.250781: Epoch 896 
2023-10-26 16:41:41.251042: Current learning rate: 0.0013 
2023-10-26 16:41:45.476876: train_loss -0.9155 
2023-10-26 16:41:45.477327: val_loss -0.8142 
2023-10-26 16:41:45.477792: Pseudo dice [0.8619, 0.8998, 0.9669, 0.638, 0.9054] 
2023-10-26 16:41:45.478064: Epoch time: 4.23 s 
2023-10-26 16:41:46.755510:  
2023-10-26 16:41:46.755842: Epoch 897 
2023-10-26 16:41:46.756160: Current learning rate: 0.00129 
2023-10-26 16:41:51.013557: train_loss -0.9177 
2023-10-26 16:41:51.014104: val_loss -0.7887 
2023-10-26 16:41:51.014494: Pseudo dice [0.8715, 0.9006, 0.9636, 0.6264, 0.8321] 
2023-10-26 16:41:51.014835: Epoch time: 4.26 s 
2023-10-26 16:41:52.118767:  
2023-10-26 16:41:52.119101: Epoch 898 
2023-10-26 16:41:52.119356: Current learning rate: 0.00128 
2023-10-26 16:41:56.221263: train_loss -0.9156 
2023-10-26 16:41:56.221655: val_loss -0.798 
2023-10-26 16:41:56.221926: Pseudo dice [0.8744, 0.8969, 0.9675, 0.5466, 0.8678] 
2023-10-26 16:41:56.222158: Epoch time: 4.1 s 
2023-10-26 16:41:57.332935:  
2023-10-26 16:41:57.333240: Epoch 899 
2023-10-26 16:41:57.333494: Current learning rate: 0.00127 
2023-10-26 16:42:01.550029: train_loss -0.9147 
2023-10-26 16:42:01.550448: val_loss -0.8053 
2023-10-26 16:42:01.550737: Pseudo dice [0.8704, 0.8994, 0.966, 0.6204, 0.8704] 
2023-10-26 16:42:01.550994: Epoch time: 4.22 s 
2023-10-26 16:42:02.785474:  
2023-10-26 16:42:02.785810: Epoch 900 
2023-10-26 16:42:02.786075: Current learning rate: 0.00126 
2023-10-26 16:42:06.899273: train_loss -0.9129 
2023-10-26 16:42:06.899747: val_loss -0.7842 
2023-10-26 16:42:06.900249: Pseudo dice [0.8637, 0.8909, 0.9653, 0.5072, 0.8721] 
2023-10-26 16:42:06.900559: Epoch time: 4.11 s 
2023-10-26 16:42:08.004882:  
2023-10-26 16:42:08.005232: Epoch 901 
2023-10-26 16:42:08.005545: Current learning rate: 0.00125 
2023-10-26 16:42:12.190467: train_loss -0.9185 
2023-10-26 16:42:12.190899: val_loss -0.8021 
2023-10-26 16:42:12.191170: Pseudo dice [0.8625, 0.8966, 0.9668, 0.5901, 0.8668] 
2023-10-26 16:42:12.191403: Epoch time: 4.19 s 
2023-10-26 16:42:13.315069:  
2023-10-26 16:42:13.315367: Epoch 902 
2023-10-26 16:42:13.315614: Current learning rate: 0.00124 
2023-10-26 16:42:17.513758: train_loss -0.916 
2023-10-26 16:42:17.514236: val_loss -0.7864 
2023-10-26 16:42:17.514504: Pseudo dice [0.864, 0.8969, 0.9639, 0.5693, 0.8612] 
2023-10-26 16:42:17.514754: Epoch time: 4.2 s 
2023-10-26 16:42:18.619721:  
2023-10-26 16:42:18.620032: Epoch 903 
2023-10-26 16:42:18.620280: Current learning rate: 0.00122 
2023-10-26 16:42:22.632726: train_loss -0.9139 
2023-10-26 16:42:22.633169: val_loss -0.8031 
2023-10-26 16:42:22.633455: Pseudo dice [0.8585, 0.898, 0.9639, 0.6114, 0.8779] 
2023-10-26 16:42:22.633705: Epoch time: 4.01 s 
2023-10-26 16:42:24.031567:  
2023-10-26 16:42:24.031897: Epoch 904 
2023-10-26 16:42:24.032145: Current learning rate: 0.00121 
2023-10-26 16:42:28.165483: train_loss -0.9165 
2023-10-26 16:42:28.165917: val_loss -0.8003 
2023-10-26 16:42:28.166194: Pseudo dice [0.8599, 0.8942, 0.9654, 0.5611, 0.8834] 
2023-10-26 16:42:28.166435: Epoch time: 4.13 s 
2023-10-26 16:42:29.269218:  
2023-10-26 16:42:29.269547: Epoch 905 
2023-10-26 16:42:29.269806: Current learning rate: 0.0012 
2023-10-26 16:42:33.448635: train_loss -0.9149 
2023-10-26 16:42:33.449075: val_loss -0.7986 
2023-10-26 16:42:33.449347: Pseudo dice [0.8659, 0.8998, 0.9641, 0.6805, 0.8468] 
2023-10-26 16:42:33.449603: Epoch time: 4.18 s 
2023-10-26 16:42:34.543370:  
2023-10-26 16:42:34.543671: Epoch 906 
2023-10-26 16:42:34.543925: Current learning rate: 0.00119 
2023-10-26 16:42:38.701462: train_loss -0.9146 
2023-10-26 16:42:38.701848: val_loss -0.7844 
2023-10-26 16:42:38.702127: Pseudo dice [0.8697, 0.8979, 0.9647, 0.4647, 0.8689] 
2023-10-26 16:42:38.702357: Epoch time: 4.16 s 
2023-10-26 16:42:39.799416:  
2023-10-26 16:42:39.799736: Epoch 907 
2023-10-26 16:42:39.799998: Current learning rate: 0.00118 
2023-10-26 16:42:43.947623: train_loss -0.9149 
2023-10-26 16:42:43.948061: val_loss -0.7963 
2023-10-26 16:42:43.948336: Pseudo dice [0.8647, 0.8903, 0.9658, 0.5329, 0.8868] 
2023-10-26 16:42:43.948589: Epoch time: 4.15 s 
2023-10-26 16:42:45.073295:  
2023-10-26 16:42:45.073616: Epoch 908 
2023-10-26 16:42:45.073889: Current learning rate: 0.00117 
2023-10-26 16:42:49.123348: train_loss -0.9129 
2023-10-26 16:42:49.123744: val_loss -0.8114 
2023-10-26 16:42:49.124034: Pseudo dice [0.8714, 0.8991, 0.9644, 0.5441, 0.8824] 
2023-10-26 16:42:49.124264: Epoch time: 4.05 s 
2023-10-26 16:42:50.221480:  
2023-10-26 16:42:50.221775: Epoch 909 
2023-10-26 16:42:50.222037: Current learning rate: 0.00116 
2023-10-26 16:42:54.388101: train_loss -0.915 
2023-10-26 16:42:54.388559: val_loss -0.7898 
2023-10-26 16:42:54.388830: Pseudo dice [0.8694, 0.8932, 0.9649, 0.5286, 0.859] 
2023-10-26 16:42:54.389071: Epoch time: 4.17 s 
2023-10-26 16:42:55.644375:  
2023-10-26 16:42:55.644667: Epoch 910 
2023-10-26 16:42:55.644920: Current learning rate: 0.00115 
2023-10-26 16:42:59.900795: train_loss -0.9148 
2023-10-26 16:42:59.901278: val_loss -0.8006 
2023-10-26 16:42:59.901644: Pseudo dice [0.865, 0.8944, 0.9661, 0.6104, 0.8812] 
2023-10-26 16:42:59.901966: Epoch time: 4.26 s 
2023-10-26 16:43:00.990160:  
2023-10-26 16:43:00.990484: Epoch 911 
2023-10-26 16:43:00.990728: Current learning rate: 0.00113 
2023-10-26 16:43:05.011126: train_loss -0.9123 
2023-10-26 16:43:05.011726: val_loss -0.8187 
2023-10-26 16:43:05.012139: Pseudo dice [0.8666, 0.9001, 0.9638, 0.6382, 0.8933] 
2023-10-26 16:43:05.012399: Epoch time: 4.02 s 
2023-10-26 16:43:06.095690:  
2023-10-26 16:43:06.096007: Epoch 912 
2023-10-26 16:43:06.096259: Current learning rate: 0.00112 
2023-10-26 16:43:10.169287: train_loss -0.9134 
2023-10-26 16:43:10.169718: val_loss -0.7998 
2023-10-26 16:43:10.169991: Pseudo dice [0.8621, 0.8978, 0.9646, 0.6334, 0.9024] 
2023-10-26 16:43:10.170226: Epoch time: 4.07 s 
2023-10-26 16:43:11.263356:  
2023-10-26 16:43:11.263651: Epoch 913 
2023-10-26 16:43:11.263916: Current learning rate: 0.00111 
2023-10-26 16:43:15.450647: train_loss -0.9143 
2023-10-26 16:43:15.451040: val_loss -0.8005 
2023-10-26 16:43:15.451306: Pseudo dice [0.8629, 0.8948, 0.9652, 0.6602, 0.8942] 
2023-10-26 16:43:15.451541: Epoch time: 4.19 s 
2023-10-26 16:43:16.532785:  
2023-10-26 16:43:16.533083: Epoch 914 
2023-10-26 16:43:16.533331: Current learning rate: 0.0011 
2023-10-26 16:43:20.717070: train_loss -0.9129 
2023-10-26 16:43:20.717519: val_loss -0.7862 
2023-10-26 16:43:20.717780: Pseudo dice [0.8683, 0.8995, 0.9661, 0.6036, 0.8232] 
2023-10-26 16:43:20.718064: Epoch time: 4.18 s 
2023-10-26 16:43:21.806728:  
2023-10-26 16:43:21.807158: Epoch 915 
2023-10-26 16:43:21.807425: Current learning rate: 0.00109 
2023-10-26 16:43:25.967463: train_loss -0.9183 
2023-10-26 16:43:25.968240: val_loss -0.795 
2023-10-26 16:43:25.968610: Pseudo dice [0.8617, 0.8943, 0.9652, 0.5882, 0.8667] 
2023-10-26 16:43:25.968935: Epoch time: 4.16 s 
2023-10-26 16:43:27.088756:  
2023-10-26 16:43:27.089057: Epoch 916 
2023-10-26 16:43:27.089313: Current learning rate: 0.00108 
2023-10-26 16:43:31.343601: train_loss -0.9165 
2023-10-26 16:43:31.344082: val_loss -0.782 
2023-10-26 16:43:31.344440: Pseudo dice [0.8693, 0.9027, 0.9637, 0.6305, 0.8194] 
2023-10-26 16:43:31.344719: Epoch time: 4.26 s 
2023-10-26 16:43:32.650646:  
2023-10-26 16:43:32.650934: Epoch 917 
2023-10-26 16:43:32.651213: Current learning rate: 0.00106 
2023-10-26 16:43:36.804689: train_loss -0.9161 
2023-10-26 16:43:36.805073: val_loss -0.8086 
2023-10-26 16:43:36.805346: Pseudo dice [0.8621, 0.8946, 0.9652, 0.6527, 0.8795] 
2023-10-26 16:43:36.805578: Epoch time: 4.15 s 
2023-10-26 16:43:37.880889:  
2023-10-26 16:43:37.881182: Epoch 918 
2023-10-26 16:43:37.881430: Current learning rate: 0.00105 
2023-10-26 16:43:42.125983: train_loss -0.9228 
2023-10-26 16:43:42.126382: val_loss -0.8008 
2023-10-26 16:43:42.126642: Pseudo dice [0.868, 0.9, 0.967, 0.6451, 0.8573] 
2023-10-26 16:43:42.126881: Epoch time: 4.25 s 
2023-10-26 16:43:43.215057:  
2023-10-26 16:43:43.215373: Epoch 919 
2023-10-26 16:43:43.215629: Current learning rate: 0.00104 
2023-10-26 16:43:47.517230: train_loss -0.9193 
2023-10-26 16:43:47.517590: val_loss -0.7805 
2023-10-26 16:43:47.517854: Pseudo dice [0.8735, 0.897, 0.9673, 0.5598, 0.8237] 
2023-10-26 16:43:47.518084: Epoch time: 4.3 s 
2023-10-26 16:43:48.603488:  
2023-10-26 16:43:48.603844: Epoch 920 
2023-10-26 16:43:48.604132: Current learning rate: 0.00103 
2023-10-26 16:43:52.769036: train_loss -0.9134 
2023-10-26 16:43:52.769452: val_loss -0.8053 
2023-10-26 16:43:52.769710: Pseudo dice [0.864, 0.9005, 0.9647, 0.6429, 0.8727] 
2023-10-26 16:43:52.769942: Epoch time: 4.17 s 
2023-10-26 16:43:53.861352:  
2023-10-26 16:43:53.861647: Epoch 921 
2023-10-26 16:43:53.861911: Current learning rate: 0.00102 
2023-10-26 16:43:58.010946: train_loss -0.918 
2023-10-26 16:43:58.011330: val_loss -0.8018 
2023-10-26 16:43:58.011591: Pseudo dice [0.8737, 0.8996, 0.964, 0.5824, 0.864] 
2023-10-26 16:43:58.011818: Epoch time: 4.15 s 
2023-10-26 16:43:59.093677:  
2023-10-26 16:43:59.093970: Epoch 922 
2023-10-26 16:43:59.094207: Current learning rate: 0.00101 
2023-10-26 16:44:03.352919: train_loss -0.9142 
2023-10-26 16:44:03.353302: val_loss -0.7988 
2023-10-26 16:44:03.353569: Pseudo dice [0.8613, 0.9001, 0.964, 0.6646, 0.873] 
2023-10-26 16:44:03.353801: Epoch time: 4.26 s 
2023-10-26 16:44:04.603185:  
2023-10-26 16:44:04.603572: Epoch 923 
2023-10-26 16:44:04.603976: Current learning rate: 0.001 
2023-10-26 16:44:08.838837: train_loss -0.9103 
2023-10-26 16:44:08.839511: val_loss -0.7904 
2023-10-26 16:44:08.839907: Pseudo dice [0.8728, 0.9014, 0.9659, 0.6434, 0.8235] 
2023-10-26 16:44:08.840161: Epoch time: 4.24 s 
2023-10-26 16:44:09.926362:  
2023-10-26 16:44:09.926672: Epoch 924 
2023-10-26 16:44:09.926937: Current learning rate: 0.00098 
2023-10-26 16:44:14.222185: train_loss -0.9158 
2023-10-26 16:44:14.222570: val_loss -0.8007 
2023-10-26 16:44:14.222836: Pseudo dice [0.8693, 0.9003, 0.9644, 0.6225, 0.8657] 
2023-10-26 16:44:14.223071: Epoch time: 4.3 s 
2023-10-26 16:44:15.357321:  
2023-10-26 16:44:15.357636: Epoch 925 
2023-10-26 16:44:15.357887: Current learning rate: 0.00097 
2023-10-26 16:44:19.609803: train_loss -0.9096 
2023-10-26 16:44:19.610232: val_loss -0.7821 
2023-10-26 16:44:19.610572: Pseudo dice [0.8686, 0.8963, 0.9638, 0.6909, 0.8489] 
2023-10-26 16:44:19.610857: Epoch time: 4.25 s 
2023-10-26 16:44:20.700758:  
2023-10-26 16:44:20.701134: Epoch 926 
2023-10-26 16:44:20.701408: Current learning rate: 0.00096 
2023-10-26 16:44:24.770594: train_loss -0.9187 
2023-10-26 16:44:24.771042: val_loss -0.7974 
2023-10-26 16:44:24.771310: Pseudo dice [0.867, 0.8892, 0.9665, 0.6172, 0.8679] 
2023-10-26 16:44:24.771540: Epoch time: 4.07 s 
2023-10-26 16:44:25.906228:  
2023-10-26 16:44:25.906563: Epoch 927 
2023-10-26 16:44:25.906823: Current learning rate: 0.00095 
2023-10-26 16:44:30.003493: train_loss -0.912 
2023-10-26 16:44:30.004114: val_loss -0.8169 
2023-10-26 16:44:30.004489: Pseudo dice [0.8684, 0.9004, 0.9642, 0.7301, 0.8722] 
2023-10-26 16:44:30.004830: Epoch time: 4.1 s 
2023-10-26 16:44:31.153025:  
2023-10-26 16:44:31.153332: Epoch 928 
2023-10-26 16:44:31.153589: Current learning rate: 0.00094 
2023-10-26 16:44:35.411437: train_loss -0.9167 
2023-10-26 16:44:35.412146: val_loss -0.7857 
2023-10-26 16:44:35.412424: Pseudo dice [0.8679, 0.8996, 0.9641, 0.5977, 0.8618] 
2023-10-26 16:44:35.412661: Epoch time: 4.26 s 
2023-10-26 16:44:36.581062:  
2023-10-26 16:44:36.581376: Epoch 929 
2023-10-26 16:44:36.581638: Current learning rate: 0.00092 
2023-10-26 16:44:40.831778: train_loss -0.9123 
2023-10-26 16:44:40.832328: val_loss -0.7894 
2023-10-26 16:44:40.832713: Pseudo dice [0.8688, 0.8938, 0.9646, 0.5798, 0.8702] 
2023-10-26 16:44:40.832963: Epoch time: 4.25 s 
2023-10-26 16:44:42.091701:  
2023-10-26 16:44:42.092010: Epoch 930 
2023-10-26 16:44:42.092260: Current learning rate: 0.00091 
2023-10-26 16:44:46.127789: train_loss -0.9146 
2023-10-26 16:44:46.128575: val_loss -0.7939 
2023-10-26 16:44:46.128831: Pseudo dice [0.8564, 0.8957, 0.966, 0.5766, 0.8757] 
2023-10-26 16:44:46.129066: Epoch time: 4.04 s 
2023-10-26 16:44:47.215264:  
2023-10-26 16:44:47.215572: Epoch 931 
2023-10-26 16:44:47.215834: Current learning rate: 0.0009 
2023-10-26 16:44:51.439933: train_loss -0.9103 
2023-10-26 16:44:51.440324: val_loss -0.7854 
2023-10-26 16:44:51.440593: Pseudo dice [0.8664, 0.8941, 0.9669, 0.5232, 0.8648] 
2023-10-26 16:44:51.440822: Epoch time: 4.23 s 
2023-10-26 16:44:52.549327:  
2023-10-26 16:44:52.549640: Epoch 932 
2023-10-26 16:44:52.549909: Current learning rate: 0.00089 
2023-10-26 16:44:56.673804: train_loss -0.9112 
2023-10-26 16:44:56.674209: val_loss -0.7993 
2023-10-26 16:44:56.674481: Pseudo dice [0.8667, 0.899, 0.9657, 0.6563, 0.8714] 
2023-10-26 16:44:56.674711: Epoch time: 4.13 s 
2023-10-26 16:44:57.759560:  
2023-10-26 16:44:57.759882: Epoch 933 
2023-10-26 16:44:57.760134: Current learning rate: 0.00088 
2023-10-26 16:45:01.804006: train_loss -0.9154 
2023-10-26 16:45:01.804606: val_loss -0.7953 
2023-10-26 16:45:01.804994: Pseudo dice [0.8686, 0.8928, 0.9654, 0.6215, 0.8974] 
2023-10-26 16:45:01.805548: Epoch time: 4.05 s 
2023-10-26 16:45:02.901150:  
2023-10-26 16:45:02.901469: Epoch 934 
2023-10-26 16:45:02.901727: Current learning rate: 0.00087 
2023-10-26 16:45:06.768297: train_loss -0.9134 
2023-10-26 16:45:06.768670: val_loss -0.7966 
2023-10-26 16:45:06.768940: Pseudo dice [0.8646, 0.9038, 0.9653, 0.6741, 0.8498] 
2023-10-26 16:45:06.769172: Epoch time: 3.87 s 
2023-10-26 16:45:07.852877:  
2023-10-26 16:45:07.853179: Epoch 935 
2023-10-26 16:45:07.853419: Current learning rate: 0.00085 
2023-10-26 16:45:11.980425: train_loss -0.9174 
2023-10-26 16:45:11.980801: val_loss -0.8162 
2023-10-26 16:45:11.981066: Pseudo dice [0.8691, 0.8888, 0.9669, 0.6608, 0.8949] 
2023-10-26 16:45:11.981295: Epoch time: 4.13 s 
2023-10-26 16:45:13.246365:  
2023-10-26 16:45:13.246685: Epoch 936 
2023-10-26 16:45:13.246937: Current learning rate: 0.00084 
2023-10-26 16:45:17.480063: train_loss -0.9123 
2023-10-26 16:45:17.480452: val_loss -0.7998 
2023-10-26 16:45:17.480716: Pseudo dice [0.8657, 0.8962, 0.9657, 0.6528, 0.8553] 
2023-10-26 16:45:17.480973: Epoch time: 4.23 s 
2023-10-26 16:45:18.602012:  
2023-10-26 16:45:18.602333: Epoch 937 
2023-10-26 16:45:18.602590: Current learning rate: 0.00083 
2023-10-26 16:45:22.747438: train_loss -0.9183 
2023-10-26 16:45:22.747845: val_loss -0.8081 
2023-10-26 16:45:22.748126: Pseudo dice [0.8647, 0.8942, 0.9662, 0.6093, 0.874] 
2023-10-26 16:45:22.748370: Epoch time: 4.15 s 
2023-10-26 16:45:23.841443:  
2023-10-26 16:45:23.841753: Epoch 938 
2023-10-26 16:45:23.842010: Current learning rate: 0.00082 
2023-10-26 16:45:27.829726: train_loss -0.9181 
2023-10-26 16:45:27.830127: val_loss -0.8114 
2023-10-26 16:45:27.830389: Pseudo dice [0.8714, 0.8979, 0.9649, 0.6766, 0.8866] 
2023-10-26 16:45:27.830617: Epoch time: 3.99 s 
2023-10-26 16:45:28.913558:  
2023-10-26 16:45:28.915702: Epoch 939 
2023-10-26 16:45:28.915952: Current learning rate: 0.00081 
2023-10-26 16:45:32.899111: train_loss -0.9141 
2023-10-26 16:45:32.899508: val_loss -0.7915 
2023-10-26 16:45:32.899775: Pseudo dice [0.8695, 0.9003, 0.9656, 0.5914, 0.8613] 
2023-10-26 16:45:32.900025: Epoch time: 3.99 s 
2023-10-26 16:45:33.986225:  
2023-10-26 16:45:33.986526: Epoch 940 
2023-10-26 16:45:33.986789: Current learning rate: 0.00079 
2023-10-26 16:45:38.167415: train_loss -0.9168 
2023-10-26 16:45:38.167819: val_loss -0.7873 
2023-10-26 16:45:38.168090: Pseudo dice [0.8719, 0.9001, 0.9669, 0.6771, 0.835] 
2023-10-26 16:45:38.168482: Epoch time: 4.18 s 
2023-10-26 16:45:39.246602:  
2023-10-26 16:45:39.246930: Epoch 941 
2023-10-26 16:45:39.247184: Current learning rate: 0.00078 
2023-10-26 16:45:43.574096: train_loss -0.9167 
2023-10-26 16:45:43.574503: val_loss -0.8006 
2023-10-26 16:45:43.574765: Pseudo dice [0.8656, 0.9003, 0.9656, 0.6585, 0.87] 
2023-10-26 16:45:43.575009: Epoch time: 4.33 s 
2023-10-26 16:45:44.722119:  
2023-10-26 16:45:44.722439: Epoch 942 
2023-10-26 16:45:44.722705: Current learning rate: 0.00077 
2023-10-26 16:45:48.901036: train_loss -0.915 
2023-10-26 16:45:48.901460: val_loss -0.8037 
2023-10-26 16:45:48.906217: Pseudo dice [0.8713, 0.9063, 0.9627, 0.632, 0.872] 
2023-10-26 16:45:48.906484: Epoch time: 4.18 s 
2023-10-26 16:45:50.195867:  
2023-10-26 16:45:50.196213: Epoch 943 
2023-10-26 16:45:50.196497: Current learning rate: 0.00076 
2023-10-26 16:45:54.313252: train_loss -0.908 
2023-10-26 16:45:54.313628: val_loss -0.7862 
2023-10-26 16:45:54.313900: Pseudo dice [0.8683, 0.9038, 0.9645, 0.3399, 0.8481] 
2023-10-26 16:45:54.314135: Epoch time: 4.12 s 
2023-10-26 16:45:55.396926:  
2023-10-26 16:45:55.397614: Epoch 944 
2023-10-26 16:45:55.397938: Current learning rate: 0.00075 
2023-10-26 16:45:59.309677: train_loss -0.9167 
2023-10-26 16:45:59.310091: val_loss -0.8053 
2023-10-26 16:45:59.310359: Pseudo dice [0.8586, 0.8916, 0.9651, 0.6332, 0.8948] 
2023-10-26 16:45:59.310597: Epoch time: 3.91 s 
2023-10-26 16:46:00.392707:  
2023-10-26 16:46:00.393080: Epoch 945 
2023-10-26 16:46:00.393403: Current learning rate: 0.00074 
2023-10-26 16:46:04.729384: train_loss -0.9187 
2023-10-26 16:46:04.729750: val_loss -0.7936 
2023-10-26 16:46:04.730025: Pseudo dice [0.8615, 0.8989, 0.9657, 0.6264, 0.8474] 
2023-10-26 16:46:04.730264: Epoch time: 4.34 s 
2023-10-26 16:46:05.809843:  
2023-10-26 16:46:05.810216: Epoch 946 
2023-10-26 16:46:05.810510: Current learning rate: 0.00072 
2023-10-26 16:46:09.984334: train_loss -0.9168 
2023-10-26 16:46:09.984731: val_loss -0.8052 
2023-10-26 16:46:09.985000: Pseudo dice [0.8675, 0.8884, 0.9666, 0.6218, 0.8835] 
2023-10-26 16:46:09.985237: Epoch time: 4.18 s 
2023-10-26 16:46:11.060206:  
2023-10-26 16:46:11.060520: Epoch 947 
2023-10-26 16:46:11.060778: Current learning rate: 0.00071 
2023-10-26 16:46:15.237444: train_loss -0.9189 
2023-10-26 16:46:15.237857: val_loss -0.803 
2023-10-26 16:46:15.238156: Pseudo dice [0.8687, 0.8901, 0.9669, 0.6388, 0.8645] 
2023-10-26 16:46:15.238386: Epoch time: 4.18 s 
2023-10-26 16:46:16.312371:  
2023-10-26 16:46:16.312665: Epoch 948 
2023-10-26 16:46:16.312915: Current learning rate: 0.0007 
2023-10-26 16:46:20.457884: train_loss -0.9199 
2023-10-26 16:46:20.458365: val_loss -0.7952 
2023-10-26 16:46:20.458624: Pseudo dice [0.8659, 0.8903, 0.9657, 0.6727, 0.8824] 
2023-10-26 16:46:20.458859: Epoch time: 4.15 s 
2023-10-26 16:46:21.557338:  
2023-10-26 16:46:21.557641: Epoch 949 
2023-10-26 16:46:21.557903: Current learning rate: 0.00069 
2023-10-26 16:46:25.905157: train_loss -0.9147 
2023-10-26 16:46:25.905536: val_loss -0.7849 
2023-10-26 16:46:25.905810: Pseudo dice [0.8699, 0.8976, 0.9679, 0.6363, 0.8275] 
2023-10-26 16:46:25.906044: Epoch time: 4.35 s 
2023-10-26 16:46:27.109167:  
2023-10-27 09:02:17.143784: Epoch 950 
2023-10-27 09:02:17.144108: Current learning rate: 0.00067 
2023-10-27 09:02:24.773576: train_loss -0.9142 
2023-10-27 09:02:24.774145: val_loss -0.8016 
2023-10-27 09:02:24.774447: Pseudo dice [0.864, 0.9043, 0.963, 0.6267, 0.8959] 
2023-10-27 09:02:24.774724: Epoch time: 7.62 s 
2023-10-27 09:02:26.239630:  
2023-10-27 09:02:26.240383: Epoch 951 
2023-10-27 09:02:26.241097: Current learning rate: 0.00066 
2023-10-27 09:02:30.946557: train_loss -0.9191 
2023-10-27 09:02:30.947115: val_loss -0.7947 
2023-10-27 09:02:30.947440: Pseudo dice [0.8642, 0.8979, 0.9653, 0.5912, 0.8599] 
2023-10-27 09:02:30.947750: Epoch time: 4.71 s 
2023-10-27 09:02:32.096747:  
2023-10-27 09:02:32.097275: Epoch 952 
2023-10-27 09:02:32.097655: Current learning rate: 0.00065 
2023-10-27 09:02:36.757614: train_loss -0.9188 
2023-10-27 09:02:36.758117: val_loss -0.7923 
2023-10-27 09:02:36.758529: Pseudo dice [0.8611, 0.8955, 0.9658, 0.6179, 0.8574] 
2023-10-27 09:02:36.758841: Epoch time: 4.66 s 
2023-10-27 09:02:38.068546:  
2023-10-27 09:02:38.069316: Epoch 953 
2023-10-27 09:02:38.070002: Current learning rate: 0.00064 
2023-10-27 09:02:42.746768: train_loss -0.9205 
2023-10-27 09:02:42.747825: val_loss -0.793 
2023-10-27 09:02:42.748276: Pseudo dice [0.8673, 0.8993, 0.9641, 0.5845, 0.88] 
2023-10-27 09:02:42.748709: Epoch time: 4.68 s 
2023-10-27 09:02:43.909691:  
2023-10-27 09:02:43.910461: Epoch 954 
2023-10-27 09:02:43.910756: Current learning rate: 0.00063 
2023-10-27 09:02:48.509149: train_loss -0.9177 
2023-10-27 09:02:48.509718: val_loss -0.7567 
2023-10-27 09:02:48.510070: Pseudo dice [0.8646, 0.9031, 0.9647, 0.6424, 0.8117] 
2023-10-27 09:02:48.510334: Epoch time: 4.6 s 
2023-10-27 09:02:49.664568:  
2023-10-27 09:02:49.665079: Epoch 955 
2023-10-27 09:02:49.665854: Current learning rate: 0.00061 
2023-10-27 09:02:54.252075: train_loss -0.9202 
2023-10-27 09:02:54.252674: val_loss -0.8071 
2023-10-27 09:02:54.253113: Pseudo dice [0.8684, 0.8945, 0.9659, 0.6044, 0.8899] 
2023-10-27 09:02:54.253543: Epoch time: 4.59 s 
2023-10-27 09:02:55.410208:  
2023-10-27 09:02:55.410658: Epoch 956 
2023-10-27 09:02:55.410927: Current learning rate: 0.0006 
2023-10-27 09:02:59.989462: train_loss -0.9179 
2023-10-27 09:02:59.990036: val_loss -0.7762 
2023-10-27 09:02:59.990562: Pseudo dice [0.8721, 0.905, 0.9636, 0.5599, 0.8757] 
2023-10-27 09:02:59.991470: Epoch time: 4.58 s 
2023-10-27 09:03:01.148465:  
2023-10-27 09:03:01.148869: Epoch 957 
2023-10-27 09:03:01.149195: Current learning rate: 0.00059 
2023-10-27 09:03:05.862796: train_loss -0.9203 
2023-10-27 09:03:05.863395: val_loss -0.787 
2023-10-27 09:03:05.863754: Pseudo dice [0.8656, 0.8971, 0.9652, 0.5995, 0.8731] 
2023-10-27 09:03:05.864167: Epoch time: 4.71 s 
2023-10-27 09:03:07.033701:  
2023-10-27 09:03:07.034520: Epoch 958 
2023-10-27 09:03:07.034868: Current learning rate: 0.00058 
2023-10-27 09:03:11.750815: train_loss -0.9115 
2023-10-27 09:03:11.751295: val_loss -0.813 
2023-10-27 09:03:11.751612: Pseudo dice [0.8676, 0.8919, 0.9669, 0.6151, 0.884] 
2023-10-27 09:03:11.751879: Epoch time: 4.72 s 
2023-10-27 09:03:13.082582:  
2023-10-27 09:03:13.083103: Epoch 959 
2023-10-27 09:03:13.083394: Current learning rate: 0.00056 
2023-10-27 09:03:17.808165: train_loss -0.9204 
2023-10-27 09:03:17.808775: val_loss -0.8096 
2023-10-27 09:03:17.809095: Pseudo dice [0.863, 0.8951, 0.965, 0.6204, 0.887] 
2023-10-27 09:03:17.809380: Epoch time: 4.73 s 
2023-10-27 09:03:18.972939:  
2023-10-27 09:03:18.973449: Epoch 960 
2023-10-27 09:03:18.974220: Current learning rate: 0.00055 
2023-10-27 09:03:23.704690: train_loss -0.9183 
2023-10-27 09:03:23.705218: val_loss -0.8065 
2023-10-27 09:03:23.705663: Pseudo dice [0.8642, 0.8933, 0.9658, 0.6252, 0.8837] 
2023-10-27 09:03:23.706028: Epoch time: 4.73 s 
2023-10-27 09:03:24.871801:  
2023-10-27 09:03:24.872319: Epoch 961 
2023-10-27 09:03:24.873041: Current learning rate: 0.00054 
2023-10-27 09:03:29.497278: train_loss -0.9189 
2023-10-27 09:03:29.497755: val_loss -0.7889 
2023-10-27 09:03:29.498101: Pseudo dice [0.8676, 0.8999, 0.9646, 0.5773, 0.852] 
2023-10-27 09:03:29.498385: Epoch time: 4.63 s 
2023-10-27 09:03:30.643400:  
2023-10-27 09:03:30.643864: Epoch 962 
2023-10-27 09:03:30.644176: Current learning rate: 0.00053 
2023-10-27 09:03:35.345365: train_loss -0.922 
2023-10-27 09:03:35.345957: val_loss -0.7918 
2023-10-27 09:03:35.346293: Pseudo dice [0.8674, 0.8995, 0.9647, 0.5165, 0.8692] 
2023-10-27 09:03:35.346708: Epoch time: 4.7 s 
2023-10-27 09:03:36.498958:  
2023-10-27 09:03:36.499416: Epoch 963 
2023-10-27 09:03:36.499741: Current learning rate: 0.00051 
2023-10-27 09:03:41.063180: train_loss -0.9203 
2023-10-27 09:03:41.063770: val_loss -0.7864 
2023-10-27 09:03:41.064119: Pseudo dice [0.8674, 0.8908, 0.9667, 0.5989, 0.8593] 
2023-10-27 09:03:41.064440: Epoch time: 4.56 s 
2023-10-27 09:03:42.221603:  
2023-10-27 09:03:42.222112: Epoch 964 
2023-10-27 09:03:42.222950: Current learning rate: 0.0005 
2023-10-27 09:03:46.853944: train_loss -0.9177 
2023-10-27 09:03:46.854370: val_loss -0.777 
2023-10-27 09:03:46.854689: Pseudo dice [0.8622, 0.8925, 0.9653, 0.5109, 0.866] 
2023-10-27 09:03:46.855000: Epoch time: 4.63 s 
2023-10-27 09:03:47.997654:  
2023-10-27 09:03:47.998039: Epoch 965 
2023-10-27 09:03:47.998341: Current learning rate: 0.00049 
2023-10-27 09:03:52.611690: train_loss -0.9173 
2023-10-27 09:03:52.612159: val_loss -0.8014 
2023-10-27 09:03:52.612499: Pseudo dice [0.8698, 0.8899, 0.9671, 0.5867, 0.8803] 
2023-10-27 09:03:52.612791: Epoch time: 4.61 s 
2023-10-27 09:03:53.937395:  
2023-10-27 09:03:53.937982: Epoch 966 
2023-10-27 09:03:53.938655: Current learning rate: 0.00048 
2023-10-27 09:03:58.587718: train_loss -0.921 
2023-10-27 09:03:58.588322: val_loss -0.7807 
2023-10-27 09:03:58.588648: Pseudo dice [0.8726, 0.905, 0.964, 0.5005, 0.8447] 
2023-10-27 09:03:58.589434: Epoch time: 4.65 s 
2023-10-27 09:03:59.736414:  
2023-10-27 09:03:59.736779: Epoch 967 
2023-10-27 09:03:59.737090: Current learning rate: 0.00046 
2023-10-27 09:04:04.143831: train_loss -0.9175 
2023-10-27 09:04:04.144395: val_loss -0.7917 
2023-10-27 09:04:04.144734: Pseudo dice [0.8712, 0.8997, 0.9646, 0.6467, 0.8453] 
2023-10-27 09:04:04.145032: Epoch time: 4.41 s 
2023-10-27 09:04:05.304805:  
2023-10-27 09:04:05.305202: Epoch 968 
2023-10-27 09:04:05.305485: Current learning rate: 0.00045 
2023-10-27 09:04:09.642112: train_loss -0.9211 
2023-10-27 09:04:09.642642: val_loss -0.7856 
2023-10-27 09:04:09.642924: Pseudo dice [0.8678, 0.8978, 0.9628, 0.565, 0.8707] 
2023-10-27 09:04:09.643200: Epoch time: 4.34 s 
2023-10-27 09:04:10.792772:  
2023-10-27 09:04:10.793253: Epoch 969 
2023-10-27 09:04:10.793828: Current learning rate: 0.00044 
2023-10-27 09:04:15.167228: train_loss -0.9178 
2023-10-27 09:04:15.167727: val_loss -0.7998 
2023-10-27 09:04:15.168419: Pseudo dice [0.8655, 0.9022, 0.9642, 0.628, 0.8677] 
2023-10-27 09:04:15.168698: Epoch time: 4.38 s 
2023-10-27 09:04:16.317663:  
2023-10-27 09:04:16.318096: Epoch 970 
2023-10-27 09:04:16.318383: Current learning rate: 0.00043 
2023-10-27 09:04:20.713525: train_loss -0.9161 
2023-10-27 09:04:20.714301: val_loss -0.7721 
2023-10-27 09:04:20.714628: Pseudo dice [0.8692, 0.9013, 0.9645, 0.5873, 0.8235] 
2023-10-27 09:04:20.714967: Epoch time: 4.4 s 
2023-10-27 09:04:21.893350:  
2023-10-27 09:04:21.893825: Epoch 971 
2023-10-27 09:04:21.894106: Current learning rate: 0.00041 
2023-10-27 09:04:26.246673: train_loss -0.9189 
2023-10-27 09:04:26.247201: val_loss -0.7937 
2023-10-27 09:04:26.247509: Pseudo dice [0.8672, 0.897, 0.965, 0.5219, 0.8833] 
2023-10-27 09:04:26.247782: Epoch time: 4.35 s 
2023-10-27 09:04:27.620277:  
2023-10-27 09:04:27.620621: Epoch 972 
2023-10-27 09:04:27.620881: Current learning rate: 0.0004 
2023-10-27 09:04:31.954996: train_loss -0.9226 
2023-10-27 09:04:31.955608: val_loss -0.7835 
2023-10-27 09:04:31.955899: Pseudo dice [0.8671, 0.8927, 0.9657, 0.5685, 0.839] 
2023-10-27 09:04:31.956184: Epoch time: 4.34 s 
2023-10-27 09:04:33.130013:  
2023-10-27 09:04:33.130368: Epoch 973 
2023-10-27 09:04:33.130623: Current learning rate: 0.00039 
2023-10-27 09:04:37.475335: train_loss -0.9215 
2023-10-27 09:04:37.476311: val_loss -0.8043 
2023-10-27 09:04:37.476731: Pseudo dice [0.8688, 0.8982, 0.9646, 0.6147, 0.8838] 
2023-10-27 09:04:37.477045: Epoch time: 4.35 s 
2023-10-27 09:04:38.623384:  
2023-10-27 09:04:38.624169: Epoch 974 
2023-10-27 09:04:38.624457: Current learning rate: 0.00037 
2023-10-27 09:04:42.861421: train_loss -0.9219 
2023-10-27 09:04:42.861926: val_loss -0.7894 
2023-10-27 09:04:42.862249: Pseudo dice [0.8685, 0.897, 0.9667, 0.6161, 0.8635] 
2023-10-27 09:04:42.862560: Epoch time: 4.24 s 
2023-10-27 09:04:44.016372:  
2023-10-27 09:04:44.016726: Epoch 975 
2023-10-27 09:04:44.017002: Current learning rate: 0.00036 
2023-10-27 09:04:48.314117: train_loss -0.9202 
2023-10-27 09:04:48.314629: val_loss -0.7909 
2023-10-27 09:04:48.314921: Pseudo dice [0.8595, 0.8953, 0.9657, 0.5973, 0.8671] 
2023-10-27 09:04:48.315223: Epoch time: 4.3 s 
2023-10-27 09:04:49.468837:  
2023-10-27 09:04:49.469192: Epoch 976 
2023-10-27 09:04:49.469465: Current learning rate: 0.00035 
2023-10-27 09:04:53.827700: train_loss -0.9206 
2023-10-27 09:04:53.828238: val_loss -0.7974 
2023-10-27 09:04:53.828544: Pseudo dice [0.8649, 0.9, 0.9642, 0.6341, 0.884] 
2023-10-27 09:04:53.828785: Epoch time: 4.36 s 
2023-10-27 09:04:54.992987:  
2023-10-27 09:04:54.993378: Epoch 977 
2023-10-27 09:04:54.994037: Current learning rate: 0.00034 
2023-10-27 09:04:59.391594: train_loss -0.9265 
2023-10-27 09:04:59.395753: val_loss -0.8035 
2023-10-27 09:04:59.396287: Pseudo dice [0.869, 0.8957, 0.9671, 0.6248, 0.8752] 
2023-10-27 09:04:59.396599: Epoch time: 4.4 s 
2023-10-27 09:05:00.765041:  
2023-10-27 09:05:00.766147: Epoch 978 
2023-10-27 09:05:00.766819: Current learning rate: 0.00032 
2023-10-27 09:05:05.410731: train_loss -0.9237 
2023-10-27 09:05:05.414457: val_loss -0.7934 
2023-10-27 09:05:05.414945: Pseudo dice [0.8671, 0.8914, 0.9662, 0.5587, 0.8642] 
2023-10-27 09:05:05.415276: Epoch time: 4.65 s 
2023-10-27 09:05:06.586499:  
2023-10-27 09:05:06.587025: Epoch 979 
2023-10-27 09:05:06.587358: Current learning rate: 0.00031 
2023-10-27 09:05:11.058910: train_loss -0.9153 
2023-10-27 09:05:11.059392: val_loss -0.7795 
2023-10-27 09:05:11.059738: Pseudo dice [0.8666, 0.898, 0.9646, 0.5661, 0.8411] 
2023-10-27 09:05:11.060058: Epoch time: 4.47 s 
2023-10-27 09:05:12.240104:  
2023-10-27 09:05:12.240815: Epoch 980 
2023-10-27 09:05:12.241560: Current learning rate: 0.0003 
2023-10-27 09:05:17.089859: train_loss -0.9211 
2023-10-27 09:05:17.090550: val_loss -0.7858 
2023-10-27 09:05:17.091049: Pseudo dice [0.8696, 0.903, 0.9631, 0.5936, 0.862] 
2023-10-27 09:05:17.091388: Epoch time: 4.85 s 
2023-10-27 09:05:18.244939:  
2023-10-27 09:05:18.245419: Epoch 981 
2023-10-27 09:05:18.245714: Current learning rate: 0.00028 
2023-10-27 09:05:22.846493: train_loss -0.9194 
2023-10-27 09:05:22.847066: val_loss -0.7996 
2023-10-27 09:05:22.847375: Pseudo dice [0.8721, 0.8972, 0.9656, 0.5944, 0.8689] 
2023-10-27 09:05:22.847651: Epoch time: 4.6 s 
2023-10-27 09:05:24.011315:  
2023-10-27 09:05:24.011783: Epoch 982 
2023-10-27 09:05:24.012400: Current learning rate: 0.00027 
2023-10-27 09:05:28.680001: train_loss -0.9234 
2023-10-27 09:05:28.680578: val_loss -0.772 
2023-10-27 09:05:28.680900: Pseudo dice [0.8673, 0.8964, 0.9647, 0.5958, 0.8237] 
2023-10-27 09:05:28.681179: Epoch time: 4.67 s 
2023-10-27 09:05:29.833577:  
2023-10-27 09:05:29.834092: Epoch 983 
2023-10-27 09:05:29.834381: Current learning rate: 0.00026 
2023-10-27 09:05:34.517819: train_loss -0.9172 
2023-10-27 09:05:34.518412: val_loss -0.791 
2023-10-27 09:05:34.518761: Pseudo dice [0.8711, 0.9005, 0.9634, 0.4605, 0.8983] 
2023-10-27 09:05:34.519056: Epoch time: 4.68 s 
2023-10-27 09:05:35.668498:  
2023-10-27 09:05:35.668919: Epoch 984 
2023-10-27 09:05:35.669650: Current learning rate: 0.00024 
2023-10-27 09:05:40.209925: train_loss -0.9199 
2023-10-27 09:05:40.210510: val_loss -0.7971 
2023-10-27 09:05:40.210851: Pseudo dice [0.8698, 0.8984, 0.9652, 0.6006, 0.8758] 
2023-10-27 09:05:40.211157: Epoch time: 4.54 s 
2023-10-27 09:05:41.548709:  
2023-10-27 09:05:41.549278: Epoch 985 
2023-10-27 09:05:41.549956: Current learning rate: 0.00023 
2023-10-27 09:05:46.185270: train_loss -0.9178 
2023-10-27 09:05:46.185824: val_loss -0.7955 
2023-10-27 09:05:46.186163: Pseudo dice [0.8657, 0.891, 0.967, 0.601, 0.8663] 
2023-10-27 09:05:46.186436: Epoch time: 4.64 s 
2023-10-27 09:05:47.341862:  
2023-10-27 09:05:47.342388: Epoch 986 
2023-10-27 09:05:47.343113: Current learning rate: 0.00021 
2023-10-27 09:05:51.952163: train_loss -0.9187 
2023-10-27 09:05:51.952740: val_loss -0.8117 
2023-10-27 09:05:51.953075: Pseudo dice [0.8679, 0.899, 0.9641, 0.6108, 0.9006] 
2023-10-27 09:05:51.953349: Epoch time: 4.61 s 
2023-10-27 09:05:53.102984:  
2023-10-27 09:05:53.103451: Epoch 987 
2023-10-27 09:05:53.104131: Current learning rate: 0.0002 
2023-10-27 09:05:57.702518: train_loss -0.9221 
2023-10-27 09:05:57.703039: val_loss -0.7905 
2023-10-27 09:05:57.703358: Pseudo dice [0.8685, 0.8986, 0.9655, 0.5963, 0.8655] 
2023-10-27 09:05:57.703673: Epoch time: 4.6 s 
2023-10-27 09:05:58.857287:  
2023-10-27 09:05:58.858049: Epoch 988 
2023-10-27 09:05:58.858354: Current learning rate: 0.00019 
2023-10-27 09:06:03.461917: train_loss -0.9179 
2023-10-27 09:06:03.462453: val_loss -0.8061 
2023-10-27 09:06:03.462804: Pseudo dice [0.8698, 0.8922, 0.965, 0.608, 0.8828] 
2023-10-27 09:06:03.463094: Epoch time: 4.61 s 
2023-10-27 09:06:04.604762:  
2023-10-27 09:06:04.605182: Epoch 989 
2023-10-27 09:06:04.605611: Current learning rate: 0.00017 
2023-10-27 09:06:09.196833: train_loss -0.9198 
2023-10-27 09:06:09.197296: val_loss -0.8038 
2023-10-27 09:06:09.197594: Pseudo dice [0.8645, 0.8904, 0.966, 0.5672, 0.8979] 
2023-10-27 09:06:09.197875: Epoch time: 4.59 s 
2023-10-27 09:06:10.352523:  
2023-10-27 09:06:10.353002: Epoch 990 
2023-10-27 09:06:10.353462: Current learning rate: 0.00016 
2023-10-27 09:06:14.973716: train_loss -0.924 
2023-10-27 09:06:14.974234: val_loss -0.7968 
2023-10-27 09:06:14.974574: Pseudo dice [0.8639, 0.8963, 0.9656, 0.6171, 0.8713] 
2023-10-27 09:06:14.974922: Epoch time: 4.62 s 
2023-10-27 09:06:16.302746:  
2023-10-27 09:06:16.303236: Epoch 991 
2023-10-27 09:06:16.303542: Current learning rate: 0.00014 
2023-10-27 09:06:20.889442: train_loss -0.9203 
2023-10-27 09:06:20.889924: val_loss -0.8049 
2023-10-27 09:06:20.890274: Pseudo dice [0.8659, 0.8951, 0.9654, 0.6227, 0.8739] 
2023-10-27 09:06:20.890586: Epoch time: 4.59 s 
2023-10-27 09:06:22.050100:  
2023-10-27 09:06:22.050561: Epoch 992 
2023-10-27 09:06:22.050886: Current learning rate: 0.00013 
2023-10-27 09:06:26.771819: train_loss -0.9142 
2023-10-27 09:06:26.772268: val_loss -0.7817 
2023-10-27 09:06:26.772609: Pseudo dice [0.8671, 0.8977, 0.9637, 0.5711, 0.8651] 
2023-10-27 09:06:26.772883: Epoch time: 4.72 s 
2023-10-27 09:06:27.923764:  
2023-10-27 09:06:27.924157: Epoch 993 
2023-10-27 09:06:27.924497: Current learning rate: 0.00011 
2023-10-27 09:06:32.582827: train_loss -0.917 
2023-10-27 09:06:32.583319: val_loss -0.7968 
2023-10-27 09:06:32.583913: Pseudo dice [0.8642, 0.8976, 0.965, 0.5959, 0.8831] 
2023-10-27 09:06:32.584238: Epoch time: 4.66 s 
2023-10-27 09:06:33.731035:  
2023-10-27 09:06:33.731478: Epoch 994 
2023-10-27 09:06:33.731768: Current learning rate: 0.0001 
2023-10-27 09:06:38.289711: train_loss -0.9178 
2023-10-27 09:06:38.290203: val_loss -0.7983 
2023-10-27 09:06:38.290493: Pseudo dice [0.869, 0.903, 0.965, 0.62, 0.8654] 
2023-10-27 09:06:38.290778: Epoch time: 4.56 s 
2023-10-27 09:06:39.441111:  
2023-10-27 09:06:39.441861: Epoch 995 
2023-10-27 09:06:39.442455: Current learning rate: 8e-05 
2023-10-27 09:06:44.040553: train_loss -0.9245 
2023-10-27 09:06:44.043814: val_loss -0.8067 
2023-10-27 09:06:44.044194: Pseudo dice [0.8654, 0.8952, 0.9657, 0.6224, 0.893] 
2023-10-27 09:06:44.044573: Epoch time: 4.6 s 
2023-10-27 09:06:45.202685:  
2023-10-27 09:06:45.203099: Epoch 996 
2023-10-27 09:06:45.203677: Current learning rate: 7e-05 
2023-10-27 09:06:49.730948: train_loss -0.9226 
2023-10-27 09:06:49.731423: val_loss -0.795 
2023-10-27 09:06:49.731750: Pseudo dice [0.8679, 0.8938, 0.9666, 0.5787, 0.8772] 
2023-10-27 09:06:49.732052: Epoch time: 4.53 s 
2023-10-27 09:06:51.033299:  
2023-10-27 09:06:51.033742: Epoch 997 
2023-10-27 09:06:51.034038: Current learning rate: 5e-05 
2023-10-27 09:06:55.981388: train_loss -0.9223 
2023-10-27 09:06:55.981958: val_loss -0.7932 
2023-10-27 09:06:55.982329: Pseudo dice [0.8698, 0.8996, 0.9656, 0.5681, 0.8557] 
2023-10-27 09:06:55.982953: Epoch time: 4.95 s 
2023-10-27 09:06:57.161873:  
2023-10-27 09:06:57.162329: Epoch 998 
2023-10-27 09:06:57.162984: Current learning rate: 4e-05 
2023-10-27 09:07:02.048029: train_loss -0.9282 
2023-10-27 09:07:02.048541: val_loss -0.781 
2023-10-27 09:07:02.048899: Pseudo dice [0.8704, 0.8968, 0.9651, 0.4618, 0.8741] 
2023-10-27 09:07:02.049244: Epoch time: 4.89 s 
2023-10-27 09:07:03.233685:  
2023-10-27 09:07:03.234080: Epoch 999 
2023-10-27 09:07:03.234401: Current learning rate: 2e-05 
2023-10-27 09:07:07.964258: train_loss -0.9207 
2023-10-27 09:07:07.964811: val_loss -0.7784 
2023-10-27 09:07:07.965143: Pseudo dice [0.8674, 0.9012, 0.9647, 0.5942, 0.8493] 
2023-10-27 09:07:07.965436: Epoch time: 4.73 s 
2023-10-27 09:07:09.316707: Training done. 
2023-10-27 09:07:09.327815: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-27 09:07:09.328677: The split file contains 5 splits. 
2023-10-27 09:07:09.329085: Desired fold for training: 1 
2023-10-27 09:07:09.329445: This split has 35 training and 9 validation cases. 
2023-10-27 09:07:09.330000: predicting t2_haste_tra_2_2mm_003 
2023-10-27 09:07:09.754644: predicting t2_haste_tra_2_2mm_007 
2023-10-27 09:07:09.785877: predicting t2_haste_tra_2_2mm_011 
2023-10-27 09:07:09.815602: predicting t2_haste_tra_2_2mm_013 
2023-10-27 09:07:09.865701: predicting t2_haste_tra_2_2mm_103 
2023-10-27 09:07:09.905123: predicting t2_haste_tra_2_2mm_107 
2023-10-27 09:07:09.940602: predicting t2_haste_tra_2_2mm_111 
2023-10-27 09:07:09.976255: predicting t2_haste_tra_2_2mm_113 
2023-10-27 09:07:10.011933: predicting t2_haste_tra_2_2mm_207 
2023-10-27 09:07:19.464110: Validation complete 
2023-10-27 09:07:19.464430: Mean Validation Dice:  0.8048819383888821 
