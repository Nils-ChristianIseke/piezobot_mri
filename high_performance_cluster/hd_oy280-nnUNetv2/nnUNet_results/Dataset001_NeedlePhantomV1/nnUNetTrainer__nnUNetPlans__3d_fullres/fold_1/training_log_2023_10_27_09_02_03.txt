
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [24, 56, 40], 'median_image_size_in_voxels': [22.0, 56.0, 36.0], 'spacing': [2.419999837875366, 1.46875, 1.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'num_pool_per_axis': [2, 3, 3], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_NeedlePhantomV1', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.419999837875366, 1.46875, 1.46875], 'original_median_shape_after_transp': [22, 56, 36], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1610.0, 'mean': 465.1097412109375, 'median': 478.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 1246.0, 'std': 241.4937744140625}}} 
 
2023-10-27 09:02:03.864421: unpacking dataset... 
2023-10-27 09:02:08.413810: unpacking done... 
2023-10-27 09:02:08.493437: do_dummy_2d_data_aug: False 
2023-10-27 09:02:08.495746: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-27 09:02:08.496569: The split file contains 5 splits. 
2023-10-27 09:02:08.496810: Desired fold for training: 1 
2023-10-27 09:02:08.497031: This split has 35 training and 9 validation cases. 
2023-10-27 09:02:17.112126: Unable to plot network architecture: 
2023-10-27 09:02:17.112555: 'torch._C.Node' object is not subscriptable 
2023-10-27 09:02:17.143480:  
2023-10-27 09:02:17.143784: Epoch 950 
2023-10-27 09:02:17.144108: Current learning rate: 0.00067 
2023-10-27 09:02:24.773576: train_loss -0.9142 
2023-10-27 09:02:24.774145: val_loss -0.8016 
2023-10-27 09:02:24.774447: Pseudo dice [0.864, 0.9043, 0.963, 0.6267, 0.8959] 
2023-10-27 09:02:24.774724: Epoch time: 7.62 s 
2023-10-27 09:02:26.239630:  
2023-10-27 09:02:26.240383: Epoch 951 
2023-10-27 09:02:26.241097: Current learning rate: 0.00066 
2023-10-27 09:02:30.946557: train_loss -0.9191 
2023-10-27 09:02:30.947115: val_loss -0.7947 
2023-10-27 09:02:30.947440: Pseudo dice [0.8642, 0.8979, 0.9653, 0.5912, 0.8599] 
2023-10-27 09:02:30.947750: Epoch time: 4.71 s 
2023-10-27 09:02:32.096747:  
2023-10-27 09:02:32.097275: Epoch 952 
2023-10-27 09:02:32.097655: Current learning rate: 0.00065 
2023-10-27 09:02:36.757614: train_loss -0.9188 
2023-10-27 09:02:36.758117: val_loss -0.7923 
2023-10-27 09:02:36.758529: Pseudo dice [0.8611, 0.8955, 0.9658, 0.6179, 0.8574] 
2023-10-27 09:02:36.758841: Epoch time: 4.66 s 
2023-10-27 09:02:38.068546:  
2023-10-27 09:02:38.069316: Epoch 953 
2023-10-27 09:02:38.070002: Current learning rate: 0.00064 
2023-10-27 09:02:42.746768: train_loss -0.9205 
2023-10-27 09:02:42.747825: val_loss -0.793 
2023-10-27 09:02:42.748276: Pseudo dice [0.8673, 0.8993, 0.9641, 0.5845, 0.88] 
2023-10-27 09:02:42.748709: Epoch time: 4.68 s 
2023-10-27 09:02:43.909691:  
2023-10-27 09:02:43.910461: Epoch 954 
2023-10-27 09:02:43.910756: Current learning rate: 0.00063 
2023-10-27 09:02:48.509149: train_loss -0.9177 
2023-10-27 09:02:48.509718: val_loss -0.7567 
2023-10-27 09:02:48.510070: Pseudo dice [0.8646, 0.9031, 0.9647, 0.6424, 0.8117] 
2023-10-27 09:02:48.510334: Epoch time: 4.6 s 
2023-10-27 09:02:49.664568:  
2023-10-27 09:02:49.665079: Epoch 955 
2023-10-27 09:02:49.665854: Current learning rate: 0.00061 
2023-10-27 09:02:54.252075: train_loss -0.9202 
2023-10-27 09:02:54.252674: val_loss -0.8071 
2023-10-27 09:02:54.253113: Pseudo dice [0.8684, 0.8945, 0.9659, 0.6044, 0.8899] 
2023-10-27 09:02:54.253543: Epoch time: 4.59 s 
2023-10-27 09:02:55.410208:  
2023-10-27 09:02:55.410658: Epoch 956 
2023-10-27 09:02:55.410927: Current learning rate: 0.0006 
2023-10-27 09:02:59.989462: train_loss -0.9179 
2023-10-27 09:02:59.990036: val_loss -0.7762 
2023-10-27 09:02:59.990562: Pseudo dice [0.8721, 0.905, 0.9636, 0.5599, 0.8757] 
2023-10-27 09:02:59.991470: Epoch time: 4.58 s 
2023-10-27 09:03:01.148465:  
2023-10-27 09:03:01.148869: Epoch 957 
2023-10-27 09:03:01.149195: Current learning rate: 0.00059 
2023-10-27 09:03:05.862796: train_loss -0.9203 
2023-10-27 09:03:05.863395: val_loss -0.787 
2023-10-27 09:03:05.863754: Pseudo dice [0.8656, 0.8971, 0.9652, 0.5995, 0.8731] 
2023-10-27 09:03:05.864167: Epoch time: 4.71 s 
2023-10-27 09:03:07.033701:  
2023-10-27 09:03:07.034520: Epoch 958 
2023-10-27 09:03:07.034868: Current learning rate: 0.00058 
2023-10-27 09:03:11.750815: train_loss -0.9115 
2023-10-27 09:03:11.751295: val_loss -0.813 
2023-10-27 09:03:11.751612: Pseudo dice [0.8676, 0.8919, 0.9669, 0.6151, 0.884] 
2023-10-27 09:03:11.751879: Epoch time: 4.72 s 
2023-10-27 09:03:13.082582:  
2023-10-27 09:03:13.083103: Epoch 959 
2023-10-27 09:03:13.083394: Current learning rate: 0.00056 
2023-10-27 09:03:17.808165: train_loss -0.9204 
2023-10-27 09:03:17.808775: val_loss -0.8096 
2023-10-27 09:03:17.809095: Pseudo dice [0.863, 0.8951, 0.965, 0.6204, 0.887] 
2023-10-27 09:03:17.809380: Epoch time: 4.73 s 
2023-10-27 09:03:18.972939:  
2023-10-27 09:03:18.973449: Epoch 960 
2023-10-27 09:03:18.974220: Current learning rate: 0.00055 
2023-10-27 09:03:23.704690: train_loss -0.9183 
2023-10-27 09:03:23.705218: val_loss -0.8065 
2023-10-27 09:03:23.705663: Pseudo dice [0.8642, 0.8933, 0.9658, 0.6252, 0.8837] 
2023-10-27 09:03:23.706028: Epoch time: 4.73 s 
2023-10-27 09:03:24.871801:  
2023-10-27 09:03:24.872319: Epoch 961 
2023-10-27 09:03:24.873041: Current learning rate: 0.00054 
2023-10-27 09:03:29.497278: train_loss -0.9189 
2023-10-27 09:03:29.497755: val_loss -0.7889 
2023-10-27 09:03:29.498101: Pseudo dice [0.8676, 0.8999, 0.9646, 0.5773, 0.852] 
2023-10-27 09:03:29.498385: Epoch time: 4.63 s 
2023-10-27 09:03:30.643400:  
2023-10-27 09:03:30.643864: Epoch 962 
2023-10-27 09:03:30.644176: Current learning rate: 0.00053 
2023-10-27 09:03:35.345365: train_loss -0.922 
2023-10-27 09:03:35.345957: val_loss -0.7918 
2023-10-27 09:03:35.346293: Pseudo dice [0.8674, 0.8995, 0.9647, 0.5165, 0.8692] 
2023-10-27 09:03:35.346708: Epoch time: 4.7 s 
2023-10-27 09:03:36.498958:  
2023-10-27 09:03:36.499416: Epoch 963 
2023-10-27 09:03:36.499741: Current learning rate: 0.00051 
2023-10-27 09:03:41.063180: train_loss -0.9203 
2023-10-27 09:03:41.063770: val_loss -0.7864 
2023-10-27 09:03:41.064119: Pseudo dice [0.8674, 0.8908, 0.9667, 0.5989, 0.8593] 
2023-10-27 09:03:41.064440: Epoch time: 4.56 s 
2023-10-27 09:03:42.221603:  
2023-10-27 09:03:42.222112: Epoch 964 
2023-10-27 09:03:42.222950: Current learning rate: 0.0005 
2023-10-27 09:03:46.853944: train_loss -0.9177 
2023-10-27 09:03:46.854370: val_loss -0.777 
2023-10-27 09:03:46.854689: Pseudo dice [0.8622, 0.8925, 0.9653, 0.5109, 0.866] 
2023-10-27 09:03:46.855000: Epoch time: 4.63 s 
2023-10-27 09:03:47.997654:  
2023-10-27 09:03:47.998039: Epoch 965 
2023-10-27 09:03:47.998341: Current learning rate: 0.00049 
2023-10-27 09:03:52.611690: train_loss -0.9173 
2023-10-27 09:03:52.612159: val_loss -0.8014 
2023-10-27 09:03:52.612499: Pseudo dice [0.8698, 0.8899, 0.9671, 0.5867, 0.8803] 
2023-10-27 09:03:52.612791: Epoch time: 4.61 s 
2023-10-27 09:03:53.937395:  
2023-10-27 09:03:53.937982: Epoch 966 
2023-10-27 09:03:53.938655: Current learning rate: 0.00048 
2023-10-27 09:03:58.587718: train_loss -0.921 
2023-10-27 09:03:58.588322: val_loss -0.7807 
2023-10-27 09:03:58.588648: Pseudo dice [0.8726, 0.905, 0.964, 0.5005, 0.8447] 
2023-10-27 09:03:58.589434: Epoch time: 4.65 s 
2023-10-27 09:03:59.736414:  
2023-10-27 09:03:59.736779: Epoch 967 
2023-10-27 09:03:59.737090: Current learning rate: 0.00046 
2023-10-27 09:04:04.143831: train_loss -0.9175 
2023-10-27 09:04:04.144395: val_loss -0.7917 
2023-10-27 09:04:04.144734: Pseudo dice [0.8712, 0.8997, 0.9646, 0.6467, 0.8453] 
2023-10-27 09:04:04.145032: Epoch time: 4.41 s 
2023-10-27 09:04:05.304805:  
2023-10-27 09:04:05.305202: Epoch 968 
2023-10-27 09:04:05.305485: Current learning rate: 0.00045 
2023-10-27 09:04:09.642112: train_loss -0.9211 
2023-10-27 09:04:09.642642: val_loss -0.7856 
2023-10-27 09:04:09.642924: Pseudo dice [0.8678, 0.8978, 0.9628, 0.565, 0.8707] 
2023-10-27 09:04:09.643200: Epoch time: 4.34 s 
2023-10-27 09:04:10.792772:  
2023-10-27 09:04:10.793253: Epoch 969 
2023-10-27 09:04:10.793828: Current learning rate: 0.00044 
2023-10-27 09:04:15.167228: train_loss -0.9178 
2023-10-27 09:04:15.167727: val_loss -0.7998 
2023-10-27 09:04:15.168419: Pseudo dice [0.8655, 0.9022, 0.9642, 0.628, 0.8677] 
2023-10-27 09:04:15.168698: Epoch time: 4.38 s 
2023-10-27 09:04:16.317663:  
2023-10-27 09:04:16.318096: Epoch 970 
2023-10-27 09:04:16.318383: Current learning rate: 0.00043 
2023-10-27 09:04:20.713525: train_loss -0.9161 
2023-10-27 09:04:20.714301: val_loss -0.7721 
2023-10-27 09:04:20.714628: Pseudo dice [0.8692, 0.9013, 0.9645, 0.5873, 0.8235] 
2023-10-27 09:04:20.714967: Epoch time: 4.4 s 
2023-10-27 09:04:21.893350:  
2023-10-27 09:04:21.893825: Epoch 971 
2023-10-27 09:04:21.894106: Current learning rate: 0.00041 
2023-10-27 09:04:26.246673: train_loss -0.9189 
2023-10-27 09:04:26.247201: val_loss -0.7937 
2023-10-27 09:04:26.247509: Pseudo dice [0.8672, 0.897, 0.965, 0.5219, 0.8833] 
2023-10-27 09:04:26.247782: Epoch time: 4.35 s 
2023-10-27 09:04:27.620277:  
2023-10-27 09:04:27.620621: Epoch 972 
2023-10-27 09:04:27.620881: Current learning rate: 0.0004 
2023-10-27 09:04:31.954996: train_loss -0.9226 
2023-10-27 09:04:31.955608: val_loss -0.7835 
2023-10-27 09:04:31.955899: Pseudo dice [0.8671, 0.8927, 0.9657, 0.5685, 0.839] 
2023-10-27 09:04:31.956184: Epoch time: 4.34 s 
2023-10-27 09:04:33.130013:  
2023-10-27 09:04:33.130368: Epoch 973 
2023-10-27 09:04:33.130623: Current learning rate: 0.00039 
2023-10-27 09:04:37.475335: train_loss -0.9215 
2023-10-27 09:04:37.476311: val_loss -0.8043 
2023-10-27 09:04:37.476731: Pseudo dice [0.8688, 0.8982, 0.9646, 0.6147, 0.8838] 
2023-10-27 09:04:37.477045: Epoch time: 4.35 s 
2023-10-27 09:04:38.623384:  
2023-10-27 09:04:38.624169: Epoch 974 
2023-10-27 09:04:38.624457: Current learning rate: 0.00037 
2023-10-27 09:04:42.861421: train_loss -0.9219 
2023-10-27 09:04:42.861926: val_loss -0.7894 
2023-10-27 09:04:42.862249: Pseudo dice [0.8685, 0.897, 0.9667, 0.6161, 0.8635] 
2023-10-27 09:04:42.862560: Epoch time: 4.24 s 
2023-10-27 09:04:44.016372:  
2023-10-27 09:04:44.016726: Epoch 975 
2023-10-27 09:04:44.017002: Current learning rate: 0.00036 
2023-10-27 09:04:48.314117: train_loss -0.9202 
2023-10-27 09:04:48.314629: val_loss -0.7909 
2023-10-27 09:04:48.314921: Pseudo dice [0.8595, 0.8953, 0.9657, 0.5973, 0.8671] 
2023-10-27 09:04:48.315223: Epoch time: 4.3 s 
2023-10-27 09:04:49.468837:  
2023-10-27 09:04:49.469192: Epoch 976 
2023-10-27 09:04:49.469465: Current learning rate: 0.00035 
2023-10-27 09:04:53.827700: train_loss -0.9206 
2023-10-27 09:04:53.828238: val_loss -0.7974 
2023-10-27 09:04:53.828544: Pseudo dice [0.8649, 0.9, 0.9642, 0.6341, 0.884] 
2023-10-27 09:04:53.828785: Epoch time: 4.36 s 
2023-10-27 09:04:54.992987:  
2023-10-27 09:04:54.993378: Epoch 977 
2023-10-27 09:04:54.994037: Current learning rate: 0.00034 
2023-10-27 09:04:59.391594: train_loss -0.9265 
2023-10-27 09:04:59.395753: val_loss -0.8035 
2023-10-27 09:04:59.396287: Pseudo dice [0.869, 0.8957, 0.9671, 0.6248, 0.8752] 
2023-10-27 09:04:59.396599: Epoch time: 4.4 s 
2023-10-27 09:05:00.765041:  
2023-10-27 09:05:00.766147: Epoch 978 
2023-10-27 09:05:00.766819: Current learning rate: 0.00032 
2023-10-27 09:05:05.410731: train_loss -0.9237 
2023-10-27 09:05:05.414457: val_loss -0.7934 
2023-10-27 09:05:05.414945: Pseudo dice [0.8671, 0.8914, 0.9662, 0.5587, 0.8642] 
2023-10-27 09:05:05.415276: Epoch time: 4.65 s 
2023-10-27 09:05:06.586499:  
2023-10-27 09:05:06.587025: Epoch 979 
2023-10-27 09:05:06.587358: Current learning rate: 0.00031 
2023-10-27 09:05:11.058910: train_loss -0.9153 
2023-10-27 09:05:11.059392: val_loss -0.7795 
2023-10-27 09:05:11.059738: Pseudo dice [0.8666, 0.898, 0.9646, 0.5661, 0.8411] 
2023-10-27 09:05:11.060058: Epoch time: 4.47 s 
2023-10-27 09:05:12.240104:  
2023-10-27 09:05:12.240815: Epoch 980 
2023-10-27 09:05:12.241560: Current learning rate: 0.0003 
2023-10-27 09:05:17.089859: train_loss -0.9211 
2023-10-27 09:05:17.090550: val_loss -0.7858 
2023-10-27 09:05:17.091049: Pseudo dice [0.8696, 0.903, 0.9631, 0.5936, 0.862] 
2023-10-27 09:05:17.091388: Epoch time: 4.85 s 
2023-10-27 09:05:18.244939:  
2023-10-27 09:05:18.245419: Epoch 981 
2023-10-27 09:05:18.245714: Current learning rate: 0.00028 
2023-10-27 09:05:22.846493: train_loss -0.9194 
2023-10-27 09:05:22.847066: val_loss -0.7996 
2023-10-27 09:05:22.847375: Pseudo dice [0.8721, 0.8972, 0.9656, 0.5944, 0.8689] 
2023-10-27 09:05:22.847651: Epoch time: 4.6 s 
2023-10-27 09:05:24.011315:  
2023-10-27 09:05:24.011783: Epoch 982 
2023-10-27 09:05:24.012400: Current learning rate: 0.00027 
2023-10-27 09:05:28.680001: train_loss -0.9234 
2023-10-27 09:05:28.680578: val_loss -0.772 
2023-10-27 09:05:28.680900: Pseudo dice [0.8673, 0.8964, 0.9647, 0.5958, 0.8237] 
2023-10-27 09:05:28.681179: Epoch time: 4.67 s 
2023-10-27 09:05:29.833577:  
2023-10-27 09:05:29.834092: Epoch 983 
2023-10-27 09:05:29.834381: Current learning rate: 0.00026 
2023-10-27 09:05:34.517819: train_loss -0.9172 
2023-10-27 09:05:34.518412: val_loss -0.791 
2023-10-27 09:05:34.518761: Pseudo dice [0.8711, 0.9005, 0.9634, 0.4605, 0.8983] 
2023-10-27 09:05:34.519056: Epoch time: 4.68 s 
2023-10-27 09:05:35.668498:  
2023-10-27 09:05:35.668919: Epoch 984 
2023-10-27 09:05:35.669650: Current learning rate: 0.00024 
2023-10-27 09:05:40.209925: train_loss -0.9199 
2023-10-27 09:05:40.210510: val_loss -0.7971 
2023-10-27 09:05:40.210851: Pseudo dice [0.8698, 0.8984, 0.9652, 0.6006, 0.8758] 
2023-10-27 09:05:40.211157: Epoch time: 4.54 s 
2023-10-27 09:05:41.548709:  
2023-10-27 09:05:41.549278: Epoch 985 
2023-10-27 09:05:41.549956: Current learning rate: 0.00023 
2023-10-27 09:05:46.185270: train_loss -0.9178 
2023-10-27 09:05:46.185824: val_loss -0.7955 
2023-10-27 09:05:46.186163: Pseudo dice [0.8657, 0.891, 0.967, 0.601, 0.8663] 
2023-10-27 09:05:46.186436: Epoch time: 4.64 s 
2023-10-27 09:05:47.341862:  
2023-10-27 09:05:47.342388: Epoch 986 
2023-10-27 09:05:47.343113: Current learning rate: 0.00021 
2023-10-27 09:05:51.952163: train_loss -0.9187 
2023-10-27 09:05:51.952740: val_loss -0.8117 
2023-10-27 09:05:51.953075: Pseudo dice [0.8679, 0.899, 0.9641, 0.6108, 0.9006] 
2023-10-27 09:05:51.953349: Epoch time: 4.61 s 
2023-10-27 09:05:53.102984:  
2023-10-27 09:05:53.103451: Epoch 987 
2023-10-27 09:05:53.104131: Current learning rate: 0.0002 
2023-10-27 09:05:57.702518: train_loss -0.9221 
2023-10-27 09:05:57.703039: val_loss -0.7905 
2023-10-27 09:05:57.703358: Pseudo dice [0.8685, 0.8986, 0.9655, 0.5963, 0.8655] 
2023-10-27 09:05:57.703673: Epoch time: 4.6 s 
2023-10-27 09:05:58.857287:  
2023-10-27 09:05:58.858049: Epoch 988 
2023-10-27 09:05:58.858354: Current learning rate: 0.00019 
2023-10-27 09:06:03.461917: train_loss -0.9179 
2023-10-27 09:06:03.462453: val_loss -0.8061 
2023-10-27 09:06:03.462804: Pseudo dice [0.8698, 0.8922, 0.965, 0.608, 0.8828] 
2023-10-27 09:06:03.463094: Epoch time: 4.61 s 
2023-10-27 09:06:04.604762:  
2023-10-27 09:06:04.605182: Epoch 989 
2023-10-27 09:06:04.605611: Current learning rate: 0.00017 
2023-10-27 09:06:09.196833: train_loss -0.9198 
2023-10-27 09:06:09.197296: val_loss -0.8038 
2023-10-27 09:06:09.197594: Pseudo dice [0.8645, 0.8904, 0.966, 0.5672, 0.8979] 
2023-10-27 09:06:09.197875: Epoch time: 4.59 s 
2023-10-27 09:06:10.352523:  
2023-10-27 09:06:10.353002: Epoch 990 
2023-10-27 09:06:10.353462: Current learning rate: 0.00016 
2023-10-27 09:06:14.973716: train_loss -0.924 
2023-10-27 09:06:14.974234: val_loss -0.7968 
2023-10-27 09:06:14.974574: Pseudo dice [0.8639, 0.8963, 0.9656, 0.6171, 0.8713] 
2023-10-27 09:06:14.974922: Epoch time: 4.62 s 
2023-10-27 09:06:16.302746:  
2023-10-27 09:06:16.303236: Epoch 991 
2023-10-27 09:06:16.303542: Current learning rate: 0.00014 
2023-10-27 09:06:20.889442: train_loss -0.9203 
2023-10-27 09:06:20.889924: val_loss -0.8049 
2023-10-27 09:06:20.890274: Pseudo dice [0.8659, 0.8951, 0.9654, 0.6227, 0.8739] 
2023-10-27 09:06:20.890586: Epoch time: 4.59 s 
2023-10-27 09:06:22.050100:  
2023-10-27 09:06:22.050561: Epoch 992 
2023-10-27 09:06:22.050886: Current learning rate: 0.00013 
2023-10-27 09:06:26.771819: train_loss -0.9142 
2023-10-27 09:06:26.772268: val_loss -0.7817 
2023-10-27 09:06:26.772609: Pseudo dice [0.8671, 0.8977, 0.9637, 0.5711, 0.8651] 
2023-10-27 09:06:26.772883: Epoch time: 4.72 s 
2023-10-27 09:06:27.923764:  
2023-10-27 09:06:27.924157: Epoch 993 
2023-10-27 09:06:27.924497: Current learning rate: 0.00011 
2023-10-27 09:06:32.582827: train_loss -0.917 
2023-10-27 09:06:32.583319: val_loss -0.7968 
2023-10-27 09:06:32.583913: Pseudo dice [0.8642, 0.8976, 0.965, 0.5959, 0.8831] 
2023-10-27 09:06:32.584238: Epoch time: 4.66 s 
2023-10-27 09:06:33.731035:  
2023-10-27 09:06:33.731478: Epoch 994 
2023-10-27 09:06:33.731768: Current learning rate: 0.0001 
2023-10-27 09:06:38.289711: train_loss -0.9178 
2023-10-27 09:06:38.290203: val_loss -0.7983 
2023-10-27 09:06:38.290493: Pseudo dice [0.869, 0.903, 0.965, 0.62, 0.8654] 
2023-10-27 09:06:38.290778: Epoch time: 4.56 s 
2023-10-27 09:06:39.441111:  
2023-10-27 09:06:39.441861: Epoch 995 
2023-10-27 09:06:39.442455: Current learning rate: 8e-05 
2023-10-27 09:06:44.040553: train_loss -0.9245 
2023-10-27 09:06:44.043814: val_loss -0.8067 
2023-10-27 09:06:44.044194: Pseudo dice [0.8654, 0.8952, 0.9657, 0.6224, 0.893] 
2023-10-27 09:06:44.044573: Epoch time: 4.6 s 
2023-10-27 09:06:45.202685:  
2023-10-27 09:06:45.203099: Epoch 996 
2023-10-27 09:06:45.203677: Current learning rate: 7e-05 
2023-10-27 09:06:49.730948: train_loss -0.9226 
2023-10-27 09:06:49.731423: val_loss -0.795 
2023-10-27 09:06:49.731750: Pseudo dice [0.8679, 0.8938, 0.9666, 0.5787, 0.8772] 
2023-10-27 09:06:49.732052: Epoch time: 4.53 s 
2023-10-27 09:06:51.033299:  
2023-10-27 09:06:51.033742: Epoch 997 
2023-10-27 09:06:51.034038: Current learning rate: 5e-05 
2023-10-27 09:06:55.981388: train_loss -0.9223 
2023-10-27 09:06:55.981958: val_loss -0.7932 
2023-10-27 09:06:55.982329: Pseudo dice [0.8698, 0.8996, 0.9656, 0.5681, 0.8557] 
2023-10-27 09:06:55.982953: Epoch time: 4.95 s 
2023-10-27 09:06:57.161873:  
2023-10-27 09:06:57.162329: Epoch 998 
2023-10-27 09:06:57.162984: Current learning rate: 4e-05 
2023-10-27 09:07:02.048029: train_loss -0.9282 
2023-10-27 09:07:02.048541: val_loss -0.781 
2023-10-27 09:07:02.048899: Pseudo dice [0.8704, 0.8968, 0.9651, 0.4618, 0.8741] 
2023-10-27 09:07:02.049244: Epoch time: 4.89 s 
2023-10-27 09:07:03.233685:  
2023-10-27 09:07:03.234080: Epoch 999 
2023-10-27 09:07:03.234401: Current learning rate: 2e-05 
2023-10-27 09:07:07.964258: train_loss -0.9207 
2023-10-27 09:07:07.964811: val_loss -0.7784 
2023-10-27 09:07:07.965143: Pseudo dice [0.8674, 0.9012, 0.9647, 0.5942, 0.8493] 
2023-10-27 09:07:07.965436: Epoch time: 4.73 s 
2023-10-27 09:07:09.316707: Training done. 
2023-10-27 09:07:09.327815: Using splits from existing split file: /workspace/nnUNet_preprocessed/Dataset001_NeedlePhantomV1/splits_final.json 
2023-10-27 09:07:09.328677: The split file contains 5 splits. 
2023-10-27 09:07:09.329085: Desired fold for training: 1 
2023-10-27 09:07:09.329445: This split has 35 training and 9 validation cases. 
2023-10-27 09:07:09.330000: predicting t2_haste_tra_2_2mm_003 
2023-10-27 09:07:09.754644: predicting t2_haste_tra_2_2mm_007 
2023-10-27 09:07:09.785877: predicting t2_haste_tra_2_2mm_011 
2023-10-27 09:07:09.815602: predicting t2_haste_tra_2_2mm_013 
2023-10-27 09:07:09.865701: predicting t2_haste_tra_2_2mm_103 
2023-10-27 09:07:09.905123: predicting t2_haste_tra_2_2mm_107 
2023-10-27 09:07:09.940602: predicting t2_haste_tra_2_2mm_111 
2023-10-27 09:07:09.976255: predicting t2_haste_tra_2_2mm_113 
2023-10-27 09:07:10.011933: predicting t2_haste_tra_2_2mm_207 
2023-10-27 09:07:19.464110: Validation complete 
2023-10-27 09:07:19.464430: Mean Validation Dice:  0.8048819383888821 
